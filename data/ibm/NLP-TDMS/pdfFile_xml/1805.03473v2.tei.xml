<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning representations of multivariate time series with missing data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bianchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Livi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ã˜yvind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikalsen</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kampffmeyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jenssen</surname></persName>
						</author>
						<title level="a" type="main">Learning representations of multivariate time series with missing data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning compressed representations of multivariate time series (MTS) facilitates data analysis in the presence of noise and redundant information, and for a large number of variates and time steps. However, classical dimensionality reduction approaches are designed for vectorial data and cannot deal explicitly with missing values. In this work, we propose a novel autoencoder architecture based on recurrent neural networks to generate compressed representations of MTS. The proposed model can process inputs characterized by variable lengths and it is specifically designed to handle missing data. Our autoencoder learns fixed-length vectorial representations, whose pairwise similarities are aligned to a kernel function that operates in input space and that handles missing values. This allows to learn good representations, even in the presence of a significant amount of missing data. To show the effectiveness of the proposed approach, we evaluate the quality of the learned representations in several classification tasks, including those involving medical data, and we compare to other methods for dimensionality reduction. Successively, we design two frameworks based on the proposed architecture: one for imputing missing data and another for one-class classification. Finally, we analyze under what circumstances an autoencoder with recurrent layers can learn better compressed representations of MTS than feed-forward architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Real-valued multivariate time series (MTS) allow to characterize the evolution of complex systems and is the core component in many research fields and application domains <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. MTS analysis should account for relationships across variables and time steps, and, at the same time, deal with unequal time lengths <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49]</ref>. Thanks to their ability to capture long-term dependencies, recurrent neural networks (RNNs) achieve state of the art results in tasks involving time series with one or more variables. In the latter case, RNNs can also be coupled with convolutional neural networks to explicitly model relationships across the different variables <ref type="bibr" target="#b55">[56]</ref>.</p><p>In time series analysis it is important to handle missing values, which are commonly found in realworld data such as electronic health records (EHR) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>, and are usually filled with imputation techniques in a pre-processing phase <ref type="bibr" target="#b24">[25]</ref>. However, unless data are missing completely at random <ref type="bibr" target="#b29">[30]</ref>, imputation destroys useful information contained in the missingness patterns. Furthermore, an imputation method may introduce a strong bias that influences the outcome of the analysis, especially for large fractions of missing values <ref type="bibr" target="#b40">[41]</ref>. A data-driven approach has recently been proposed to learn when to switch between two particular types of imputation <ref type="bibr" target="#b14">[15]</ref>, but it relies on strong assumptions that are suitable only for specific applications.</p><p>A proper representation of the data is crucial in machine learning applications <ref type="bibr" target="#b39">[40]</ref>. While traditional approaches rely on manual feature engineering that requires time and domain expertise, representation learning aims at automatically producing features describing the original data <ref type="bibr" target="#b6">[7]</ref>. Dimensionality reduction has been a fundamental research topic in machine learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref> and its application in representation learning for extracting relevant information and generating compressed representations facilitates analysis and processing of data <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63]</ref>. This is particularly important in the case of MTS, which often contain noise and redundant information with a large number of variables and time steps <ref type="bibr" target="#b60">[61]</ref>. However, classical dimensionality reduction approaches are not designed to process sequential data, especially in the presence of missing values and variable input size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>In this paper, we propose a novel neural network architecture, called the Temporal Kernelized Autoencoder (TKAE), to learn compressed representations of real-valued MTS that contain missing data and that may be characterized by unequal lengths. We assume data to be missing at random (MAR), i.e., the probability of data to be missing depends on the observed variables and the missingness patterns might be useful to characterize the data. Under the MAR assumption, once one has conditioned on all the data at hand, any remaining missingness is completely random (i.e., it does not depend on some unobserved variable). The MAR assumption covers many practical cases and includes also those situations where data are missing completely at random, i.e., the probability of missing data is independent from any other variable and there are no missingness patterns.</p><p>Our contributions are summarized in the following.</p><p>Learning compressed representations of MTS with missing data. To handle missing data effectively, and, at the same time, avoid the undesired biases introduced by imputation, we propose a kernel alignment procedure <ref type="bibr" target="#b35">[36]</ref> that matches the dot product matrix of the learned representations with a kernel matrix. Specifically, we exploit the recently-proposed Time series Cluster Kernel (TCK) <ref type="bibr" target="#b44">[45]</ref>, which computes similarities between MTS with missing values without using imputation. By doing so, we generate representations that preserve unbiased pairwise similarities between MTS even in the presence of large amounts of missing data. The encoder and the decoder in the TKAE are implemented by stacking multiple RNNs, allowing to generate a fixed-size vectorial representation of input MTS with variable-lengths. To better capture time dependencies, we use a bidirectional RNN <ref type="bibr" target="#b26">[27]</ref> in the encoder. The final states of the forward and backward RNNs are combined by a dense nonlinear layer that reduces the dimensionality of the representation. The proposed architecture serves different purposes. First of all, it transports the data from a complex input domain to a low-dimensional vector space while preserving the original relationships between inputs described by the TCK. Once represented as ordinary vectors, the MTS can then be processed by standard classifiers or by unsupervised machine learning algorithms <ref type="bibr" target="#b63">[64]</ref>, and their indexing and retrieval are more efficient <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>. Furthermore, when the dimensionality of the data is reduced, the models can potentially be trained with fewer samples.</p><p>Frameworks for missing data imputation and anomaly detection. Contrarily to other nonlinear dimensionality reduction techniques, the TKAE provides a decoder that yields an explicit mapping back to the input space. We exploit this feature to implement frameworks for two different applications. Specifically, we use the TKAE and its decoder to (i) implement an imputation method that leverages the generalization capability of the decoder reconstruction, rather than relying on a-priori assumptions that may introduce stronger biases, and (ii) design an anomaly detector based on the reconstruction error of the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of how RNNs encode MTS.</head><p>We provide a detailed analysis of the effect of using RNNs for encoding MTS, which is the mechanism adopted in TKAE to handle inputs with variable lengths. Despite the popularity of AEs based on RNNs <ref type="bibr" target="#b59">[60]</ref> for applications focused on text <ref type="bibr" target="#b10">[11]</ref>, speech <ref type="bibr" target="#b57">[58]</ref>, and video data <ref type="bibr" target="#b56">[57]</ref>, significantly fewer research efforts have been devoted so far in applying these architectures to real-valued MTS and, in general, in the context of dynamical systems. To fill this gap, we investigate under which circumstances recurrent architectures generate better compressed representations of MTS than feed-forward networks, which use padding to obtain inputs of equal length. Results show that the effectiveness of the RNNs is related to specific intrinsic properties of the MTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Paper organization</head><p>The paper is organized as follows. In Sec. 2 we first provide the background for existing AE models and the TCK. Then, we introduce the proposed TKAE architecture. In Sec. 3, we evaluate the TKAE's capability to learn good representations both on controlled tests and real-world MTS classification datasets. Results confirm that our method significantly improves the quality of the representations as the percentage of missing data increases. Successively, in 3.2 we propose two different frameworks that exploit the properties of the TKAE decoder for (i) imputing missing data and (ii) building a one-class classifier. We achieve competitive results for the imputation task, while we outperform other state-of-the-art methods in one-class classification. In Sec. 4, we perform an in-depth analysis to investigate which MTS are better represented by an AE with RNNs. We report several results obtained in controlled environments, as well as on benchmark data, to support the findings in our analysis. Finally, Sec. 5 reports our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Autoencoder</head><p>The AE is a neural network traditionally conceived as a non-linear dimensionality reduction algorithm <ref type="bibr" target="#b30">[31]</ref>, which has been further exploited to learn representations in deep architectures <ref type="bibr" target="#b5">[6]</ref> and to pre-train neural network layers <ref type="bibr" target="#b23">[24]</ref>. An AE simultaneously learns two functions; the first one, called the encoder, is a mapping from an input domain, R Dx , to a hidden representation (code) in R Dz . The second function, the decoder, maps from R Dz back to R Dx . The encoding function Ï† : R Dx â†’ R Dz and the decoding function Ïˆ :</p><formula xml:id="formula_0">R Dz â†’ R Dx are defined as z = Ï†(x; Î¸ E );x = Ïˆ(z; Î¸ D ),<label>(1)</label></formula><p>where x âˆˆ R Dx , z âˆˆ R Dz , andx âˆˆ R Dx denote a sample from the input space, its hidden representation, and its reconstruction given by the decoder, respectively. The encoder Ï†(Â·) is usually implemented by stacking dense layers of neurons equipped with nonlinear activation functions. The decoder Ïˆ(Â·) is architecturally symmetric to the encoder and operates in reverse direction; when inputs are real-valued vectors, the decoder's squashing nonlinearities are often replaced by linear activations <ref type="bibr" target="#b61">[62]</ref>. Finally, Î¸ E and Î¸ D are the trainable parameters of the encoder and decoder, respectively. The parameters are the connection weights and biases of each layer m, i.e., Î¸ E = {W (m)</p><formula xml:id="formula_1">E , b (m) E } and Î¸ D = {W (m) D , b (m) D }.</formula><p>AEs are trained to minimize the discrepancy between the input x and its reconstructionx. In the case of real-valued inputs, this is usually achieved by minimizing a loss L r implemented as the empirical Mean Squared Error (MSE). It has been shown that for real-valued data, when the MSE between original and reconstructed input is minimized, the learned representations are good in the sense that the amount of mutual information with respect to the input is maximized <ref type="bibr" target="#b61">[62]</ref>.</p><p>In this paper, we focus on AEs with a "bottleneck", which learn an under-complete representation of the input, i.e., D z &lt; D x , retaining as much useful information as possible to allow an accurate reconstruction <ref type="bibr" target="#b30">[31]</ref>. The learned lossy, compressed representation of the input, can be exploited, e.g., for clustering and visualization tasks <ref type="bibr" target="#b41">[42]</ref>, or to train a classifier <ref type="bibr" target="#b47">[48]</ref>. The bottleneck already provides a strong regularization as it limits the variance of the model. However, further regularization can be introduced by tying the encoder and decoder weights (W D = W T E ) or by adding a 2 -norm penalty to the loss function</p><formula xml:id="formula_2">L = L r + Î»L 2 = MSE(x,x) + Î» W 2 2 ,<label>(2)</label></formula><p>where L 2 is the 2 -norm of all model weights W = {W D , W E } and Î» is the hyperparameter controlling the contribution of the regularization term. Recurrent neural networks (RNNs) are models excelling in capturing temporal dependencies in sequences <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> and are at the core of seq2seq models <ref type="bibr" target="#b59">[60]</ref>. The latter, learns fixed-size representations of sequences with unequal length and, at the same time, generates variable-length outputs.</p><p>Modern seq2seq architectures implement a powerful mechanism called attention, which provides an inductive bias that facilitates the modeling of long-term dependencies and grants a more accurate decoding if the lengths of the input sequences varies considerably <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b50">51]</ref>. However, models with attention provide a representation that is neither compact nor of fixed size and, henceforth, are not suitable for our purposes. If fact, rather than learning a single vector representation for the whole input sequence, a model with attention maintains all the encoder states generated over time, which are combined by a time-varying decoding vector at each decoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Time Series Cluster Kernel</head><p>The time series cluster kernel (TCK) <ref type="bibr" target="#b44">[45]</ref> is an algorithmic procedure to compute unsupervised kernel similarities among MTS containing missing data. The TCK is able to model time series with missing data, under the MAR assumption. The method is grounded on an ensemble learning approach that guarantees robustness with respect to hyperparameters. This ensures that the TCK works well in unsupervised settings (the ones in which AEs actually operate), where it is not possible to tune hyperparameters by means of supervised cross-validation. The base models in the ensemble are Gaussian mixture models (GMMs), whose components are fit to the dataset. By fitting GMMs with different numbers of mixtures, the TCK procedure generates partitions at different resolutions that capture both local and global structures in the data.</p><p>To further enhance diversity in the ensemble, each partition is evaluated on a random subset of MTS samples, attributes (variates), and time segments, using random initializations and randomly chosen hyperparameters. This also contributes to provide robustness with respect to hyperparameters, such as the number of mixture components. To avoid imputation, missing data are analytically marginalized away in the likelihoods. To obtain the GMM posteriors, the likelihoods are multiplied with smooth priors, whose contribution becomes stronger as the percentage of missingness increases. The TCK is then built by summing up, for each partition, the inner products between pairs of posterior assignments corresponding to different MTS. More details on the TCK are provided in A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Temporal Kernelized Autoencoder</head><p>The Temporal Kernelized Autoencoder (TKAE) is our proposed AE architecture, which is specifically conceived to learn compressed representations of variable-length MTS that may contain missing values. A schematic depiction of the TKAE is provided in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><formula xml:id="formula_3">x 1 x 2 . . . . . . x T Input MTS x T . . . . . . x 2 x 1 RNN 1 RNN 2 RNN 1 RNN 2 Encoder (stacked bidirectional RNN) h f T h b T z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder last state</head><p>Fixed size representation  The last states obtained in forward h f T and backward h b T directions are combined by a dense layer to produce a fixed-size representation z of the input. z is used to initialize the state in the decoder, which is a stacked RNN operating in generative mode and is trained to reproduce inputs by minimizing the reconstruction error L r . TKAE allows learning similarity-preserving representations of the inputs. In particular, the matrix ZZ T containing the dot products of the representations of the MTS in the dataset is aligned, by means of a cost term L k , to the kernel matrix K. The kernel matrix K is provided by the user as prior information to condition the representations. In our case, the kernel alignment generates representations whose relationships account for missing data in the input.</p><formula xml:id="formula_4">RNN 2 RNN 1 Decoder (stacked RNN)x T . . . . . .</formula><p>We assume that each MTS can be represented by a matrix X âˆˆ R V Ã—T , where V denotes the number of variates and T is the number of time steps that may vary in each MTS. Analogously to seq2seq <ref type="bibr" target="#b59">[60]</ref>, in TKAE the dense layers of standard AEs are replaced by RNNs, which process inputs sequentially and update their hidden state at each time step t according to the following mapping,</p><formula xml:id="formula_5">h t = Ï†(x t , h tâˆ’1 , Î¸ E ),<label>(3)</label></formula><p>where Î¸ E is the set of trainable parameters of the recurrent units. The recurrent layers are composed of either gated recurrent units (GRU) <ref type="bibr" target="#b16">[17]</ref> or long short-term memory (LSTM) <ref type="bibr" target="#b31">[32]</ref> cells. The choice of the cell is usually guided by the task at hand <ref type="bibr" target="#b17">[18]</ref>. Conventional RNNs make use of previous inputs to build their current hidden representation <ref type="bibr" target="#b26">[27]</ref>. However, in applications like MTS classification where the whole input is available at once, it is also possible to exploit the information contained in future inputs to generate the current state of the network. For this reason, the encoder is implemented as a stacked bidirectional RNN <ref type="bibr" target="#b54">[55]</ref> consisting of two RNNs working in parallel, each one with M layers of D z cells and transition function (3). One of the RNNs captures input dependencies going backward in time, whereas the other processes the same input but in reverse order, thus modeling relationships going forward in time. After the whole input is processed, the final states of the forward and backward RNN are denoted as h f T and h b T , respectively. While h f T is influenced by past observations, h b T depends on future ones. Hence, their combination can capture a wider range of temporal dependencies in the input. In the TKAE, dense nonlinear layer combines the two states h f T and h b T and produces an output vector z âˆˆ R Dz . The latter, is the fixed-size, vectorial representation of the MTS.</p><p>The decoder operates according to the following map,</p><formula xml:id="formula_6">x t = Ïˆ(h t ,x tâˆ’1 , Î¸ D ),<label>(4)</label></formula><p>where Ïˆ(Â·, Â·) is a stacked RNN with M layers parametrized by Î¸ D that operates in a generative mode, processing the previously generated output as a new input. To initialize the decoder, we let its initial state h 0 = z and first inputx 0 = 0, which corresponds to an "average input" if the MTS are standardized. The decoder iteratively produces outputs for T steps, T being the length of the input MTS. Unequal lengths are naturally handled since the whole architecture is independent of T . The TKAE is trained end-to-end by means of stochastic gradient descent with scheduled sampling <ref type="bibr" target="#b4">[5]</ref>. More specifically, during training the decoder input at time t is, with probability p s , the decoder output at time t âˆ’ 1 (inference mode) and with probability 1 âˆ’ p s the desired output at time t âˆ’ 1 (teacher forcing). Since the desired output is not available during the test phase, the decoder generates test data operating only in generative mode (p s = 1). In most of our experiments, scheduled sampling improved the training convergence speed, providing a practical motivation for our choice.</p><p>Analogously to standard AEs, RNNs cannot directly process data with missing values, which are thus filled beforehand with some imputed value (0, mean value, last observed value) <ref type="bibr" target="#b53">[54]</ref>. However, imputation injects biases in the data that may negatively affect the quality of the representations and conceal potentially useful information contained in the missingness patterns. To overcome these shortcomings, we introduce a kernel alignment procedure <ref type="bibr" target="#b35">[36]</ref> that allows us to preserve the pairwise similarities of the inputs in the learned representations. These pairwise similarities are encoded in a positive semi-definite matrix K that is defined by the designer and passed as input to the model. In our case, by choosing the TCK matrix as K, the learned representations will also account for missing data.</p><p>Kernel alignment is implemented by an additional regularization term in the loss function <ref type="formula" target="#formula_2">(2)</ref>, which becomes</p><formula xml:id="formula_7">L = L r + Î»L 2 + Î±L k .<label>(5)</label></formula><p>L k is the kernel alignment cost, which takes the form of a normalized Frobenius norm of the difference between two matrices: K and ZZ T , the dot product matrix between the hidden representations z of the input MTS. More specifically, the L k term is defined as</p><formula xml:id="formula_8">L k = ZZ T ZZ T F âˆ’ K K F F ,<label>(6)</label></formula><p>where Z âˆˆ R N Ã—Dz is a matrix of hidden representations relative to the N MTS in the dataset (or, more specifically, in the current mini-batch). Finally, Î±, Î» â‰¥ 0 are hyperparameters controlling the contribution of alignment and regularization costs in the overall loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The experimental section is organized as follows.</p><p>1. Quantitative evaluations of the representations in the presence of missing data. In Sec. 3.1, we evaluate the effectiveness of the kernel alignment for generating compressed representations in the presence of missing data by computing how accurately the representations are classified. Results show that the kernel alignment with the TCK greatly improves the classification accuracy of the MTS representations in the presence of large amounts of missing data.</p><p>2. Design and evaluation of decoder-based frameworks. In Sec. 3.2, we propose two novel frameworks based on the TKAE decoder for (i) imputing missing data and (ii) for one-class classification. Both frameworks exploit not only the TKAE hidden representation but also its decoder, as the results are computed by mapping the compressed representations back to the input space. The proposed frameworks are tested on two different case-studies and compared with other methods.</p><p>Experimental setup In the following, we compare the TKAE with methods for dimensionality reduction: PCA, a standard AE, and other RNN-based architectures. The learned compressed representations have the same dimensionality for all models that are taken into account. Let D x be the input dimensionality; in TKAE D x = V , as it processes recursively each single time step. On the other hand, the MTS must be unfolded into vectors when processed by PCA and AE. Therefore, in AE and PCA the input dimensionality is D x = V Â· T . We let D z be the size of the compressed representations, which corresponds to the number of RNN cells in each TKAE layer, the size of the innermost layer in AE, and the number of principal components in PCA, respectively. In all experiments we use an AE with 3 hidden layers, {D x , 30, D z , 30, D x }; the number of neurons in the intermediate layers <ref type="bibr" target="#b29">(30)</ref> has been set after preliminary experiments and is not a critical hyperparameter (comparable results were obtained using 20 or 40 neurons). As a measure of performance, we consider the MSE between the original test data and their reconstruction as produced by each model. In each experiment, we train the models for 5000 epochs with mini-batches containing 32 MTS using the Adam optimizer <ref type="bibr" target="#b36">[37]</ref> with an initial learning rate of 0.001. We independently standardize each variate of the MTS in all datasets. In each experiment, and for each method, we identify the optimal hyperparameters with k-fold cross-validation evaluated on the reconstruction error (or, in general, on the unsupervised loss function) and we report the average results on the test set, obtained in 10 independent runs. We consider only TKAE models with a maximum of three hidden layers of either LSTM or GRU cells, as deeper models generally improve performance slightly at the cost of greater complexity <ref type="bibr" target="#b51">[52]</ref>. When kernel alignment is not used (Î± = 0), we refer to the TKAE simply as the TAE.</p><p>Datasets We consider several real-world dataset from the UCI and UCR 1 repositories and two medical datasets; details are reported in Tab. 1. The datasets have been selected in order to cover a wide variety of cases, in terms of training/test sets size, number of variates V , number of classes and (variable) lengths T of the MTS. The first medical dataset is the EHR dataset, which contains blood samples collected over time, extracted from the Electronic Health Records of patients undergoing a gastrointestinal surgery at the University Hospital of North Norway in 2004-2012 <ref type="bibr" target="#b46">[47]</ref>. Each patient is represented by a MTS of V = 10 blood sample measurements collected for T = 20 days after surgery. We consider the problem of classifying patients with and without surgical site infections from their blood samples. The dataset consists of N = 883 MTS, of which 232 pertain to infected patients. The original MTS contain missing data, corresponding to measurements not collected for a given patient. Data are in random order and the first 80% are used as training set and the rest as test set.</p><p>The second medical dataset is Physionet, which contains time series of peak-to-peak and RR intervals extracted from ECGs in the 2017 Atrial Fibrillation challenge <ref type="bibr" target="#b19">[20]</ref>. The MTS are divided into 4 classes: normal (N), atrial fibrillation (A), other symptoms (O) and noisy records (âˆ¼). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantitative evaluations of MTS representations in the presence of missing data</head><p>Controlled experiments and sensitivity analysis To evaluate the effect of kernel alignment when the MTS contain missing data, we perform a controlled experiment where we compare the representations learned by the TAE (Î± = 0) and the TKAE (Î± = 0) on the Jp. Vow. dataset. This dataset does not originally contain missing data. However, similarly to previous studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">45]</ref>, we inject missing data in a controlled way by randomly removing a certain percentage of values. We vary such percentage from 10% to 90%, evaluating each time the reconstruction MSE and classification accuracy of the TAE and the TKAE encodings using kNN with k = 3. We apply zero imputation to replace missing data in the MTS. The TAE and the TKAE are configured with 2 LSTM cells, p s = 0.9 and Î» = 0.001. In the TKAE, Î± = 0.1. In <ref type="figure" target="#fig_2">Fig. 2</ref>, we show the kernel matrix K yielded by the TCK and the dot products in ZZ T of the representations of the test set when 80% of the data are missing. ZZ T is very similar to the TCK matrix, as they are both characterized by a block structure indicating that intra-class similarities in the 9 classes are much higher than inter-class similarities.  <ref type="figure" target="#fig_4">Fig. 3(a)</ref> shows how the classification accuracy and reconstruction error of the TAE and the TKAE vary as we increase the amount of missing data. The classification accuracy (blue lines) does not decrease in the TKAE when the data contain up to 50% missing values and is always higher than in the TAE. When 90% of the data are missing, the TKAE still achieves a classification accuracy of 0.7, while for the TAE it drops to 0.1. We note that the reconstruction MSE decreases for a higher amount of missingness. The reason is that by using imputation, a higher amount of missingness introduces more constant values and, therefore, there is less information to compress. <ref type="figure" target="#fig_4">Fig. 3(b)</ref> reports the classification accuracy and reconstruction error, by varying the kernel alignment parameter Î± when Î» = 0 is fixed and the percentage of missing values is 80%. It is possible to note that there is quite a large interval ranging from 0.25 to 0.7 where the classification accuracy is above 70%.  reports the sensitivity analysis for the parameter Î±, when Î» = 0 is fixed and the percentage of missing values is 80%. Panel (c) reports the sensitivity analysis for the parameter Î», when Î± = 0.5 is fixed and the percentage of missing values is 80%.</p><p>We also observe that the alignment term does not compromise input reconstruction, since the MSE does not significantly change as Î± is increased. Finally, <ref type="figure" target="#fig_4">Fig. 3</ref>(c) reports the classification accuracy and reconstruction error, by varying the L 2 regularization parameter Î», when Î± = 0.5 is fixed and the percentage of missing values is 80%. The small changes in the results demonstrate that the TKAE is not very sensitive to this hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification of MTS representations in the presence of missing data</head><p>In this second experiment, we analyze the MTS from the EHR, Japanese Vowels, and Arabic Digits datasets. Since the last two datasets are complete, missingness is introduced by removing 80% of the samples. The performance is assessed by classifying the representations of the test set generated by each model. We include in the comparison the representations generated by PCA, a standard AE, the TAE, and the Encoder-Decoder scheme for Anomaly Detection (EncDec-AD), which is a state-of-the-art architecture that generates vectorial representations of MTS <ref type="bibr" target="#b42">[43]</ref>. The main differences between EncDec-AD and the TAE, are the lack of the bidirectional encoder in EncDec-AD and a deep architecture in TAE obtained by stacking multiple RNN layers in both the encoder and the decoder. The networks considered for this experiment are configured with the parameters reported in Tab. 2. <ref type="table">Table 2</ref>: Optimal hyperparameters found with cross-validation. For AE: type of activation function (Ïˆ), l 2 regularization (Î»), and tied weights in the decoder (tw). For TAE and TKAE: type of cell Ã— number of layers, probability of scheduled sampling (p s ), kernel alignment (Î±), and l 2 regularization (Î»). Beside classification accuracy, in Tab. 3 we also report the F1 score that handles class imbalance. For example, in the EHR data the infected class is under-represented compared to not infected. As expected, the AE achieves consistently better results than PCA. In all the experiments, we observe that the TAE performs better than the EncDec-AD, which indicates the importance of the bidirectional encoder to learn MTS representations. Except for the Arabic Digits dataset, the TAE performs also slightly better than the AE. However, when using kernel alignment the performance is boosted, as indicated by the results of the TKAE that are always the best in each task. In particular, we observe that the TKAE representations achieve the best accuracy and a much higher F1 score in the EHR dataset. <ref type="figure" target="#fig_6">Fig. 4</ref> depicts the first two principal components of the representations from the EHR dataset learned by the TAE and by the TKAE. It is possible to recognize the effect of the kernel alignment, as the densities of the components relative to different classes become more separated.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder-based frameworks</head><p>Imputation of missing data To perform missing data imputation, we modify the loss function of the TKAE slightly and exploit the decoder to impute missing values in the MTS. In the presence of missing data, the reconstruction MSE of the loss function can be modified to account only for non-imputed values,</p><formula xml:id="formula_9">L r = âˆ’ t ((x t âˆ’x t )m t ) 2 / t m t ,<label>(7)</label></formula><p>where m t = 0 if x t is imputed and 1 otherwise. In this way, the decoder is not constrained to reproduce the values that are imputed and, instead, freely assigns values to the entries that are originally missing. Thus, we can exploit the generalization capability of the decoder to provide an alternative form of imputation, which depends on the nonlinear relationships existing in the training data. A similar principle is followed by denoising AEs (DAEs) <ref type="bibr" target="#b61">[62]</ref>, as they try to reconstruct the original input from a corrupted version of it, where some entries are randomly removed. Therefore, after training, DAEs can be exploited to impute missing values on new unseen data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. We randomly remove approximately 50% of the values from 5 of the datasets described in Tab. 1 and compare the capability of the TKAE to reconstruct missing values with respect to other imputation techniques. As baselines, we consider mean imputation, last occurrence carried forward (LOCF), and DAE imputation <ref type="bibr" target="#b3">[4]</ref>. For the TKAE and the DAE, we use the optimal configurations identified with cross-validation, reported in Tab. 4. In the TKAE we replace the L r term with <ref type="bibr" target="#b6">(7)</ref> and we set Î± = 0.1. In the DAE, we apply a stochastic corruption of the inputs, by randomly setting input values to 0 with probability 0.5.</p><p>In Tab. 5 we report the MSE and the Pearson correlation (CORR) of the MTS with imputed missing values, with respect to the original MTS. We observe that in 4 of the 5 datasets the TKAE yields the most accurate reconstruction of the true input, followed by the DAE. However, in Jp. Vow. LOCF imputation allows retrieval of missing values with the highest accuracy. This can be explained by the <ref type="table">Table 4</ref>: Optimal hyperparameters found with cross-validation. For DAE: type of activation function (Ïˆ), Î» of l 2 regularization, and tied weights in the decoder (tw). For TKAE: type of cell, number of layers, probability of scheduled sampling (p s ), and Î» of l 2 regularization.  fact that in the MTS of the Jp. Vow. dataset very similar values are repeated for several time intervals. However, we notice that also in this case the TKAE achieves the second-best result and it outperforms the DAE.</p><p>One-class classification One-class classification and anomaly detection are applied in several domains, including healthcare <ref type="bibr" target="#b11">[12]</ref>, where non-nominal samples are scarce and often unavailable during training <ref type="bibr" target="#b32">[33]</ref>. When dealing with MTS data, RNN-based approaches have been adopted to perform anomaly detection tasks <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b65">66]</ref>. The methods based on dimensionality reduction procedures, such as AEs and energy based models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b64">65]</ref> rely on the assumption that anomalous samples do not belong to the subspace containing nominal data, which is learned during training. Therefore, the representations generated by the trained model for samples of a new, unseen class will arguably fail to capture important characteristics. Consequently, for those samples an AE would yield large reconstruction errors, which we consider as the classification scores for the new class.  <ref type="table">Table 6</ref>: AUC obtained by different one-class classification methods in detecting the MTS of atrial fibrillation class, which is not present in the training set.</p><p>For this task, we consider the real-world data from the Physionet dataset. By following a commonly adopted procedure <ref type="bibr" target="#b49">[50]</ref>, we simulate missing data by randomly removing approximately 50% of the entries in each MTS and then we exclude samples of class A from the training set (which are then considered as non-nominal). We evaluated the performance of the TKAE, the AE, and PCA in detecting class A in a test set containing samples of all classes (N,A,O,âˆ¼). As performance measure, we considered the area under ROC curve (AUC) and compared the performance also with two baseline classifiers: one-class SVM (OCSVM) <ref type="bibr" target="#b27">[28]</ref> and Isolation Forests (IF) <ref type="bibr" target="#b21">[22]</ref>. The optimal configurations are: D z = 10; EncDec-AD, TAE and TKAE with Î» = 0; TAE and TKAE with 1 layer of GRU cells, p s = 0.9; TKAE with Î± = 0.2; AE with non-linear decoder, no tied weights, and Î» = 0; OCSVM with rbf kernel width Î³ = 0.7 and Î½ = 0.5; IF with contamination 0.5. Results in Tab. 6 show that the TKAE scores the highest AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparative analysis of recurrent and feed-forward architectures for learning compressed representations of MTS</head><p>So far, we demonstrated the capability of the TKAE to learn good representations even in the presence of missing data, thanks to the kernel alignment with the TCK. Another characterizing component of the TKAE is the use of RNNs in both encoder and decoder. Recurrent layers have been successfully applied in seq2seq models <ref type="bibr" target="#b59">[60]</ref> to encode different types of sequential data, such as text and videos. However, their application to real-valued MTS has been limited so far and it is not clear yet in which cases recurrent AEs work well. Therefore, in this section we investigate when RNNs can represent MTS better than a feed-forward architecture, which processes the whole MTS at once using padding to deal with inputs of variable lengths. We show that in most cases an AE with RNNs encodes well the MTS, which justifies our choice in the design of the TKAE. However, we also report examples of negative results where RNNs fail to process MTS exhibiting certain properties.</p><p>Since we want to focus only on the effects of using recurrent layers in generating compressed representations, in the following we do not use kernel alignment, but we consider only the TAE (i.e., TKAE with Î± = 0). Synthetic data are generated to study specific MTS properties in controlled environments.</p><p>Time series with different frequencies Here, we evaluate the capability of the TAE to compress periodic signals having different frequencies and phases. We generate a dataset of sinusoids y(t) = sin(a Â· t + b), where a, b are drawn from N (0, 1) and t âˆˆ [0, 100]. The proposed task is closely related to the multiple superimposed oscillators, studied in pattern generation and frequency modulation <ref type="bibr" target="#b58">[59]</ref>. The training and test sets contain 200 and 1000 samples, respectively. We let D z = 5 and the optimal configurations are: AE with nonlinear decoder and Î» = 0.001; TAE with 2 layers of LSTM cells, Î» = 0, and p s = 1.0. The reconstruction MSE on the test set is 0.41 for PCA, 0.212 for the AE, and 0.013 for the TAE.</p><p>Both PCA and the AE process the entire time series at once. This may appear an advantage with respect to the TAE, which stores information in memory for the whole sequence length before yielding the final representation. Nonetheless, the TAE produces a better reconstruction, while the AE (and PCA) is unsuitable for this task. Indeed, in AEs a given time step t in each MTS is always processed by the same input neuron. For periodic signals, the training procedure tries to couple neurons associated to time steps with the same phase, by assigning similar weights to their connections. However, these couplings always change if inputs have different frequencies (see <ref type="figure">Fig.5</ref>). Therefore, training in the AE Hidden Neurons Input Neurons <ref type="figure">Figure 5</ref>: Periodic inputs with different frequencies generate different activation patterns in AEs. It is not possible to learn connections weights that preserve neurons couplings for each frequency. never converges as it is impossible to learn a model that generalizes well for each frequency. On the other hand, thanks to its recurrent architecture, the TAE can naturally handle inputs of different frequencies as there is no pairing between structural parameters and time steps. <ref type="figure" target="#fig_9">Fig. 6</ref> shows the reconstruction of one sample time series. The lower quality of the reconstruction yielded by the AE and by PCA can be immediately noticed. Additionally, since they are unable to reproduce the dynamics of each sample, they rather adopt a more conservative behavior and output signals with lower amplitudes that are closer (in a mean square sense) to the "average" of all the random sinusoids in the dataset.</p><p>Time series with variable lengths While the TAE can process MTS of different length, the standard AE and PCA require inputs of fixed size. The common workaround also followed in this work, is to pad the shorter MTS with zeros <ref type="bibr" target="#b43">[44]</ref>. To systematically study the performance of the different methods when the MTS have fixed or variable length, we generate data by integrating the following system of first-order Ordinary Differential Equations (ODE):</p><formula xml:id="formula_10">dy dt = Atanh (y(t)) ,<label>(8)</label></formula><p>where y âˆˆ R V , A âˆˆ R V Ã—V is a matrix with 50% sparsity and elements uniformly drawn in [âˆ’0.5, 0.5].</p><p>To guarantee system stability <ref type="bibr" target="#b8">[9]</ref>, we set the spectral radius of A to 0.8. tanh(Â·) is applied component-wise and introduces nonlinear dependencies among the variables. A MTS x âˆˆ R T Ã—V is obtained by integrating (8) for T steps, starting from a random initial condition y(0). Since a generic deterministic dynamical system can be described by an ODE system, these synthetic MTS can represent many real data. We generate two different datasets of MTS with V = 10 variables, each one with 400 and 1000 samples for training and test set, respectively. The first, ODEfix, contains MTS with same length T = 90, while in the second, ODEvar, each MTS has a random length T âˆˆ <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">90]</ref>. We let D z = 10 and compare the reconstruction MSE of PCA, the AE, and the TAE. The optimal configurations for this task are: AE with Î» = 0.001 and linear decoder; TAE with 1 LSTM layer, Î» = 0.001, and p s = 0.9. The average results obtained for 10 independent random generations of the data (A) and initialization of AE and TAE are reported in Tab. 7. In ODEfix, both the AE and PCA yield almost perfect reconstructions, which is expected due to the simplicity of the task. However, they perform worse in ODEvar despite the presence of many padded values and a consequent lower amount (on average) of information to encode in the compressed representation. On the other hand, the TAE naturally deals with variable-length inputs, since once the input sequence terminates its state and model weights during the training are no longer updated.</p><p>Dealing with a large number of variates and time steps To test the ability to learn compressed representations when the number of variates in the MTS increases, starting from (8) we generate four datasets ODE5, ODE10, ODE15, and ODE20, obtained by setting V = {5, 10, 15, 20}. The number of time steps is fixed to T = 50 in each dataset. We let D z = 10; TAE is configured with 2 layers of LSTM and p s = 0.9; Î» is 0.001 in both the AE and the TAE. We also include in the comparison an AE with tied weights in the (nonlinear) decoder, which has fewer parameters. Reconstruction errors are reported in Tab. 8. We notice that the AE performs well on MTS characterized by low dimensionality, but performance degrades when V assumes larger values. Since the AE processes MTS unrolled into a unidimensional vector, the input size grows quickly as V increases (one additional variable increases the input size by T ). Accordingly, the number of parameters in the first dense layer scales-up quickly, possibly leading to overfitting. We also notice that the tied weights regularization, despite halving the number of trainable parameters, degrades performance in each case, possibly because it hinders too much the flexibility of the model. On the other hand, the TAE complexity changes slowly, as only one single neuron is added for an additional input dimension. As a consequence, we conclude that the TAE is the best performing model when the MTS have a large number of variates. To study the performance as the lengths of MTS increase, we generate 8 datasets with the ODE system (8) by varying T âˆˆ {50, 75, 100, 125, 150, 175, 200}, while keeping V = 15 fixed. In <ref type="figure" target="#fig_10">Fig. 7</ref>, we report the reconstruction errors and note that the TAE performance decays as T increases. RNNs excel in capturing recurring patterns in the input sequence and they can model extremely long sequences whenever they are characterized by a strong periodicity. However, in this case there are no temporal patterns in the data that can be exploited by the RNNs to model the inputs. Therefore, the RNN dynamics do not converge to any fixed point and the modeling task becomes much more difficult as the input length increases. input space for imputing missing data and for one-class classification. We showed that by thresholding the reconstruction error of the decoder, the TKAE is able to outperform competing approaches on these tasks. We concluded our work by investigating which types of MTS are better modeled by a neural network auto-encoder with recurrent layers, rather than with feed-forward ones. Our results showed that in most cases an RNN-based AE is the best architecture to generated good MTS representations. This motivated our design choice for the TKAE. Our analysis revealed that an RNN excels in encoding short MTS with many variables, that are characterized by different lengths or by a varying periodicity. However, when MTS are very long and do not contain temporal patterns that can be modeled by RNNs, better performance can be achieved by replacing recurrent layers in the TKAE with standard dense layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 TCK kernel training</head><p>Input: Training set of MTS {X (n) } N n=1 , Q initializations, C maximal number of mixture components. 1: Initialize kernel matrix K = 0NÃ—N . 2: for q âˆˆ Q do <ref type="bibr">3:</ref> Compute posteriors Î  (n) (q) â‰¡ (Ï€ (n) 1 , . . . , Ï€ (n) q 2 ) T , by applying maximum a posteriori expectation maximization <ref type="bibr" target="#b44">[45]</ref> to the DiagGMM with q2 components and by randomly selecting, i. hyperparameters â„¦(q),</p><p>ii. a time segment T (q) of length T min â‰¤ |T (q)| â‰¤ Tmax , iii. attributes V(q), with cardinality V min â‰¤ |V(q)| â‰¤ Vmax, iv. a subset of MTS, Î·(q), with N min â‰¤ |Î·(q)| â‰¤ N , v. initialization of the mixture parameters Î˜(q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Update kernel matrix, Knm = Knm + Î  (n) (q) T Î  (m) (q) Î  (n) (q) Î  (m) (q) . 5: end for Output: TCK matrix K.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Schematic representation of TKAE. Inputs are processed by a stacked bidirectional RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Test set of Jp. Vow. with 80% of missing data. (a) prior K computed with TCK in input space; (b) dot products ZZ T of the representations in TKAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Classification accuracy (in blue) and reconstruction MSE (in red) on Japanese Vowels dataset. In (a), TAE and TKAE results are reported as a function of the missing values percentage. Panel (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>PCA and density of the two principal components of the representations yielded by TAE and TKAE on EHR dataset. The densities are computed with a kernel density estimator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Reconstructions obtained by PCA, AE, and TAE on a sample sinusoid, whose frequency and phase are randomly chosen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Reconstruction MSE when increasing length T of MTS in ODE15. TAE performance decreases for large T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Benchmark time series datasets. Column 2 to 5 report the number of attributes, samples in training and test set, and classes, respectively. T min is the length of the shortest MTS in the dataset and T max the longest MTS.</figDesc><table><row><cell>ECG</cell><cell>1 500</cell><cell cols="2">4500 5</cell><cell cols="3">140 140 UCR</cell></row><row><cell>Libras</cell><cell>2 180</cell><cell>180</cell><cell>15</cell><cell>45</cell><cell>45</cell><cell>[2]</cell></row><row><cell>Wafer</cell><cell>6 298</cell><cell>896</cell><cell>2</cell><cell cols="3">104 198 UCR</cell></row><row><cell cols="2">Jp. Vow. 12 270</cell><cell>370</cell><cell>9</cell><cell>7</cell><cell>29</cell><cell>UCI</cell></row><row><cell cols="2">Arab. Dig. 13 6600</cell><cell cols="2">2200 10</cell><cell>4</cell><cell>93</cell><cell>UCI</cell></row><row><cell>Auslan</cell><cell>22 1140</cell><cell cols="2">1425 95</cell><cell>45</cell><cell cols="2">136 UCI</cell></row><row><cell>EHR</cell><cell>10 892</cell><cell>223</cell><cell>2</cell><cell>20</cell><cell>20</cell><cell>[47]</cell></row><row><cell cols="2">Physionet 2 8524</cell><cell>298</cell><cell>4</cell><cell>5</cell><cell cols="2">176 [20]</cell></row></table><note>Dataset V Train Test Classes Tmin Tmax Source</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification of the blood data. F1 score is calculated considering infected as "positive" class. 6Â±0.13 67.5Â±0.34 78.1Â±0.02 78.2Â±0.03 85.1Â±0.01 85.1Â±0.01 EncDec-AD 82.9Â±0.07 60.5Â±0.18 75.9Â±0.05 76.4Â±0.05 66.3Â±0.14 65.1Â±0.11 TAE 85.3Â±0.021 68.2Â±0.22 78.6Â±0.17 79.1Â±0.15 73.1Â±0.09 72.8Â±0.11 TKAE 89.9Â±0.22 80.2Â±0.47 82.4Â±0.01 82.6Â±0.01 86.8Â±0.06 86.7Â±0.02</figDesc><table><row><cell></cell><cell cols="2">EHR</cell><cell cols="2">Jp. Vow.</cell><cell cols="2">Arab. Digits</cell></row><row><cell>Method</cell><cell>Accuracy</cell><cell>F1 score</cell><cell>Accuracy</cell><cell>F1 score</cell><cell>Accuracy</cell><cell>F1 score</cell></row><row><cell>PCA</cell><cell>83.5</cell><cell>65.1</cell><cell>76.8</cell><cell>76.7</cell><cell>84.8</cell><cell>84.8</cell></row><row><cell>AE</cell><cell>84.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>MSE and Pearson correlation (CORR) of the MTS where missing values are imputed using different methods, with respect to the original MTS (without missing values). Best and second best results are highlighted in dark and light blue, respectively.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Mean Imp. MSE CORR MSE CORR LOCF</cell><cell>MSE</cell><cell>DAE</cell><cell>CORR</cell><cell>MSE</cell><cell>TKAE CORR</cell></row><row><cell>ECG</cell><cell>0.883 0.702</cell><cell>0.393</cell><cell>0.884</cell><cell cols="4">0.157Â±0.004 0.956Â±0.001 0.151Â±0.003 0.956Â±0.001</cell></row><row><cell>Libras</cell><cell>0.505 0.666</cell><cell>0.085</cell><cell>0.949</cell><cell cols="4">0.050Â±0.001 0.970Â±0.001 0.029Â±0.002 0.978Â±0.002</cell></row><row><cell>Wafer</cell><cell>0.561 0.695</cell><cell>0.226</cell><cell>0.911</cell><cell cols="4">0.199Â±0.017 0.935Â±0.004 0.093Â±0.007 0.964Â±0.003</cell></row><row><cell cols="2">Jp. Vow. 0.502 0.699</cell><cell cols="6">0.084 0.954 0.132Â±0.001 0.926Â±0.000 0.114Â±0.003 0.938Â±0.001</cell></row><row><cell>Auslan</cell><cell>0.532 0.613</cell><cell>0.379</cell><cell>0.746</cell><cell cols="4">0.145Â±0.002 0.873Â±0.005 0.087Â±0.001 0.941Â±0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Average reconstruction MSE of MTS with fixed (ODEfix) and variable (ODEvar) length.</figDesc><table><row><cell>Dataset</cell><cell>PCA</cell><cell>AE</cell><cell>TAE</cell></row><row><cell>ODEfix</cell><cell>0.018</cell><cell>0.004</cell><cell>0.060</cell></row><row><cell>ODEvar</cell><cell>0.718</cell><cell>0.676</cell><cell>0.185</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Average reconstruction MSE on the ODE task for different values of V , obtained by TAE, AE, AE with tied weights (tw) and PCA. For AE and TAE we report the number of trainable parameters (#par). Best results are in bold.</figDesc><table><row><cell>Dataset</cell><cell>TAE MSE #par MSE #par MSE #par MSE AE AE (tw) PCA</cell></row><row><cell>ODE5</cell><cell>0.019 6130 0.04 31170 0.014 15870 0.007</cell></row><row><cell>ODE10</cell><cell>0.060 6780 0.04 61670 0.071 31370 0.018</cell></row><row><cell>ODE15</cell><cell>0.072 7430 0.106 92170 0.153 46870 0.174</cell></row><row><cell>ODE20</cell><cell>0.089 8080 0.121 122670 0.181 62370 0.211</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">archive.ics.uci.edu/ml/datasets.html, www.cs.ucr.edu/~eamonn/time_series_data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">ConclusionWe proposed the temporal kernelized autoencoder, an RNN-based model for representing MTS with missing values as fixed-size vectors. Missing values in MTS are commonly found in domains such as healthcare and derive from measurement errors, incorrect data entry or lack of observations. Through a kernel alignment performed with the time series cluster kernel, a similarity measure designed for MTS with missing data, our method learns compressed representations that preserve pairwise relationships defined in the original input space, even when data are heavily corrupted by missing values. We showed that the representations learned by the TKAE can be exploited both in supervised and unsupervised tasks. Experimental results, contrasted with other dimensionality reduction techniques on several datasets, showed that the TKAE representations are classified accurately also when the percentage of missing data is high. Through sensitivity analysis, we showed that the kernel alignment has very little impact on the reconstruction error, demonstrating that the TKAE can learn good representations even when using the alignment procedure. After training, only the TKAE encoder is used to generate the representation of the MTS, while the decoder, which is learned as part of the optimization, remain unused. To fully exploit the capabilities of the TKAE architecture, we considered two applications that take advantage of the decoder module. Specifically, we designed two frameworks based on dimensionality reduction and inverse mapping to the</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially funded by the Norwegian Research Council FRIPRO grant no. 239844 on developing the Next Generation Learning Machines. The authors would like to thank Arthur Revhaug, Rolv-Ole Lindsetmo and Knut Magne Augestad, all clinicians currently or formerly affiliated with the gastrointestinal surgery department at the University Hospital of North Norway, for preparing the blood samples dataset. LL gratefully acknowledges partial support of the Canada Research Chairs program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of the TCK algorithm</head><p>A MTS X âˆˆ R V Ã—T is represented by a sequence of V univariate time series (UTS) of length T , X = {x v âˆˆ R T | v = 1, . . . , V }, being V and T the dimension and length of X, respectively. Given a dataset of N samples, X (n) denotes the n-th MTS and a binary MTS</p><p>DiagGMM The TCK kernel matrix is built by first fitting G diagonal covariance GMM (DiagGMM) to the MTS dataset. Each DiagGMM g is parametrized by a time-dependent mean Âµ gv âˆˆ R T and a time-constant covariance matrix Î£ g = diag{Ïƒ 2 g1 , ..., Ïƒ 2 gV }, being Ïƒ 2 gv the variance of UTS v. Moreover, the data is assumed to be missing at random, i.e. the missing elements are only dependent on the observed values. Under these assumptions, missing data can be analytically integrated away <ref type="bibr" target="#b52">[53]</ref> and the pdf for each incompletely observed MTS {X, R} is given by</p><p>The conditional probabilities follows from Bayes' theorem,</p><p>The parameters of the DiagGMM are trained by means of a maximum a posteriori expectation maximization algorithm, as described in <ref type="bibr" target="#b44">[45]</ref>.</p><p>Ensemble generation To ensure diversity in the ensemble, each GMM model has a different number of components from the interval [2, C] and is trained Q times, using random initial conditions and hyperparameters. Specifically, Q = {q = (q 1 , q 2 ) | q 1 = 1, . . . Q, q 2 = 2, . . . , C} denotes the index set of the initial conditions and hyperparameters (q 1 ), and the number of components (q 2 ). Moreover, each DiagGMM is trained on a subset of the original dataset, defined by a random set of the MTS samples, a random set V of |V| â‰¤ V variables, and a randomly chosen time segment T , |T | â‰¤ T . The inner products of the posterior distributions from each mixture component are then added up to build the final TCK kernel matrix. Details are provided in Alg. 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Time series representation and similarity based on local autopatterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="476" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of the electronic health record for phenotype stratification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Beaulieu-Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="168" to="178" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Missing data imputation in the electronic health record using deeply learned autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Beaulieu-Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>World Scientific</publisher>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Foundation and Trends Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Investigating echo-state networks dynamics by means of recurrence analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="427" to="439" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiplex visibility graphs to investigate recurrent neural network dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">44037</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Recurrent Neural Networks for Short-Term Load Forecasting: An Overview and Comparative Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maiorino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>JÃ³zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1511.06349</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online anomaly detection for long-term ecg monitoring using wearable devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fragneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="482" to="492" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integration of deep feature extraction and ensemble learning for outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatfield</surname></persName>
		</author>
		<title level="m">The Analysis of Time Series: An Introduction</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Time Series Feature Learning with Applications to Health Care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="389" to="409" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van MerriÃ«nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1603.00982</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AF classification from a short single lead ecg recording: The physionet computing in cardiology challenge 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Cardiology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comparison of imputation methods for handling missing scores in biometric fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="919" to="933" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comparative evaluation of outlier detection algorithms: Experiments and analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Domingues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michiardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zouaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="406" to="421" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Impact of imputation of missing values on classification error for discrete data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhangfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kurgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3692" to="3705" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gondara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02737</idno>
		<title level="m">Multiple imputation using deep denoising autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The effective use of the one-class svm classifier for handwritten signature verification based on writer-independent parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guerbai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chibani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hadjadji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="113" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dimensionality reduction on SPD manifolds: The emergence of geometry-aware methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018-01" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="48" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinguishing &quot;missing at random&quot; and &quot;missing completely at random</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Heitjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The American Statistician</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards application of one-class classification methods to medical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Irigoien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arenas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Scientific World Journal</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Making the dynamic time warping distance warping-invariant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="35" to="52" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kernel entropy component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="847" to="860" />
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep kernelized autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>LÃ¸kse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
		<editor>P. Sharma and F. M. Bianchi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="419" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A review of unsupervised feature learning and deep learning for time-series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>LÃ¤ngkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Segmental acoustic indexing for zero resource keyword search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="5828" to="5832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature representation for statistical-learning-based object detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3542" to="3559" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Statistical Analysis with Missing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J A</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Lstm-based encoderdecoder for multi-sensor anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00148</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Smoothing of climate time series revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geophysical Research Letters</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Time series cluster kernel for learning similarities between multivariate time series with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Ã˜</forename><surname>Mikalsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soguero-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="569" to="581" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Noisy multi-label semi-supervised dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Ã˜</forename><surname>Mikalsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Soguero-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="257" to="270" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">An Unsupervised Multivariate Time Series Kernel Approach for Identifying Patients with Surgical Site Infection from Blood Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Ã˜</forename><surname>Mikalsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Soguero-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Revhaug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dual autoencoders features for imbalance classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On-line elastic similarity measures for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oregi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="506" to="517" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Missing data in educational research: A review of reporting practices and suggestions for improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Peugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Enders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Educational Research</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="556" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A dual-stage attention-based recurrent neural network for time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02971</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Optimal hyperparameters for deep LSTM-networks for sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06799</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Missing data: our view of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">147</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep r-th root of rank supervised joint binary embedding for multivariate time series retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Accurate recognition of words in scenes without character segmentation using recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="397" to="405" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generating coherent patterns of activity from chaotic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="544" to="557" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Autoregressive forests for multivariate time series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tuncel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="202" to="215" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Time series feature learning with labeled and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A brief survey on sequence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
	<note>ICML&apos;16</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08055</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
