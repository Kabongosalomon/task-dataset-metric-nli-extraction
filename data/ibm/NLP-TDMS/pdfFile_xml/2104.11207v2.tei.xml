<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Convolutional Line Parsing Haigang Gong UESTC</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xili</forename><surname>Dai</surname></persName>
							<email>daixilics@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UESTC &amp; UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Yuan</surname></persName>
							<email>xjyuan@uestc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">UESTC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
							<email>yima@eecs.berkeley.edu</email>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Convolutional Line Parsing Haigang Gong UESTC</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a one-stage Fully Convolutional Line Parsing network (F-Clip) that detects line segments from images. The proposed network is very simple and flexible with variations that gracefully trade off between speed and accuracy for different applications. F-Clip detects line segments in an end-to-end fashion by predicting them with each line's center position, length, and angle. Based on empirical observation of the distribution of line angles in real image datasets, we further customize the design of convolution kernels of our fully convolutional network to effectively exploit such statistical priors. We conduct extensive experiments and show that our method achieves a significantly better trade-off between efficiency and accuracy, resulting in a real-time line detector at up to 73 FPS on a single GPU. Such inference speed makes our method readily applicable to real-time tasks without compromising any accuracy of previous methods. Moreover, when equipped with a performance-improving backbone network, F-Clip is able to significantly outperform all stateof-the-art line detectors on accuracy at a similar or even higher frame rate. Source code https://github.com/Delay-Xili/F-Clip.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A holistic 3D representation aims to model and reconstruct a scene with high-level geometric primitives/structures such as lines, planes, and layouts <ref type="bibr" target="#b35">[36]</ref>. Unlike representations based on local features that are usually noisy and incomplete, a holistic counterpart is arguably more compact, robust, and easy to use. This belief has motivated a line of recent works on recognizing geometric structures from image observations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Among all the geometric primitives mentioned above, lines are arguably the most important and fundamental one. An accurate line detection system is essential for many downstream vision tasks such as vanishing point detection <ref type="bibr" target="#b36">[37]</ref>, camera pose estimation <ref type="bibr" target="#b4">[5]</ref>, camera calibration <ref type="bibr" target="#b33">[34]</ref>, stereo matching <ref type="bibr" target="#b30">[31]</ref>, and even full 3D reconstruction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Recently, significant progress has been made in the HG2-LB HR F-Clip L-CNN <ref type="bibr" target="#b36">[37]</ref> HAWP <ref type="bibr" target="#b28">[29]</ref> TP-LSD <ref type="bibr" target="#b9">[10]</ref> HT <ref type="bibr" target="#b13">[14]</ref>  field of line detection due to the introduction of a largescale dataset <ref type="bibr" target="#b8">[9]</ref>, effective learning methods <ref type="bibr" target="#b37">[38]</ref>, and the community's continuing effort to develop better algorithms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. Nevertheless, most of the existing methods focus primarily on accuracy, and their performance drops significantly if modified for efficiency. In this work, our goal is to develop a flexible algorithm that can achieve the best speed-accuracy trade-off ( <ref type="figure" target="#fig_1">Figure 1</ref>). We argue that the unsatisfactory speed-accuracy trade-off of existing methods mostly comes from the nature of the typical "two-stage" model design <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30]</ref>. In the first stage, thousands of line candidates are extracted. After that, based on the proposed lines, one extracts the corresponding image features and trains a small sub-network to determine whether each proposed line is correct. This approach has shown to be effective and achieves state-of-the-art accuracy so far. However, such a two-stage method sacrifices efficiency since it needs to process a large number of line candidates with an extra sub-network. Structure-wise, such a two-stage model also lacks flexibility in case we need to change the network to achieve a graceful trade-off between speed and accuracy.</p><p>As described in the seminal work <ref type="bibr" target="#b37">[38]</ref>, the "two-stage" line detection methods are motivated by "two-stage" object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>. Beyond the prevalent two-stage methods, there has been another line of work in object detection known as "one-stage" methods. One-stage object detection  <ref type="figure">Figure 2</ref>: The overall architecture of the F-Clipnetwork. Taken an image as input, a backbone convolutional network is followed by three convolution heads that predict the line segment center, length, and angle respectively as the output.</p><p>is done using a dense sliding window, implemented by a fully convolutional network. Such one-stage methods are believed to be more flexible and efficient in certain tasks, and they can achieve more than 200 FPS with decent accuracy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Hence, in this work we take on the following question:</p><p>Can we achieve a better speed-accuracy trade-off in line detection by leveraging the successful ideas of one-stage methods in object detection?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions of This Paper.</head><p>A key observation that motivates this work is that the extensive line proposal of the first stage used in most existing methods is not entirely necessary. Instead, a line segment can be viewed as an object and can be conveniently represented by its center, length, and angle. Hence, we can formulate the prediction of each of the parameter as a pixel-wise classification/regression problem. To this end, we propose a Fully Convolutional Line Parsing (F-Clip) network, which realizes the above idea via a fully convolutional network.</p><p>F-Clip has a surprisingly simple architecture as illustrated in <ref type="figure">Figure 2</ref>. It does not require any heavy network engineering or carefully designed training samplers as in <ref type="bibr" target="#b37">[38]</ref>. To detect a line, our system simply applies a convolutional neural network to extract the image features and uses two additional convolution layers to regress the center, length, and angle score map. Then, for each line center with a high score, we directly output a line segment by associating the length and angle values in the same location. Due to its simple one-stage design, the network is amenable to variations and modifications for different speed-accuracy trade-off, as we will discuss more detail in Section 3.2.</p><p>Through extensive experiments on large real-world image datasets, we will see that the proposed simple method/network achieves a surprisingly good speedaccuracy trade-off. In the latency sensitive setting, F-Clip with a simple hour-glass backbone <ref type="bibr" target="#b19">[20]</ref> can achieve 52.7 sAP 5 at 73 FPS, nearly 5 times faster than <ref type="bibr" target="#b37">[38]</ref> at a similar accuracy. Equipped with a performance-improving back-bone network <ref type="bibr" target="#b25">[26]</ref>, F-Clip can achieve 64.3 sAP 5 at 17.4 FPS, better than the state-of-the-art method <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Line Detection. The classical approach for line detection can be dated back to the 70s'. Hough transform <ref type="bibr" target="#b3">[4]</ref> detects lines by aggregating the pixel intensity in the parameter space of lines and output detected straight lines via a voting procedure. Modern methods such as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> detect lines based on local edge filtering. Recently, <ref type="bibr" target="#b28">[29]</ref> utilizes deep neural networks to improve the performance of the conventional LSD line detection algorithm <ref type="bibr" target="#b26">[27]</ref>. In our experiment, we will compare ours with this enhanced LSD algorithm <ref type="bibr" target="#b28">[29]</ref>.</p><p>Wireframe Parsing. The wireframe parsing task was first proposed in <ref type="bibr" target="#b8">[9]</ref>. The work provided a large-scale dataset with wireframe annotations, a baseline method, and a set of evaluation metrics. After that, <ref type="bibr" target="#b37">[38]</ref> proposed an end-to-end solution and significantly improved the performance. <ref type="bibr" target="#b32">[33]</ref> is a counterpart work of <ref type="bibr" target="#b37">[38]</ref> which also introduces a new annotated dataset. Meanwhile, <ref type="bibr" target="#b29">[30]</ref> was a follow-up work of <ref type="bibr" target="#b28">[29]</ref> which holds the state-of-the-art result. <ref type="bibr" target="#b13">[14]</ref> designed a hough-transform based convolutional operator for the line detection task. In order to handle the topology of junctions and lines, a graph neural network based method <ref type="bibr" target="#b17">[18]</ref> was proposed to tackle the wireframe task. Recently, LETR, <ref type="bibr" target="#b27">[28]</ref> a transformer based approach, was proposed for line segment without heuristics-driven intermediate stages for edge and junction proposal generation. Furthermore, <ref type="bibr" target="#b38">[39]</ref> proposed a pipeline for reconstructing 3D wireframes from 2D images. Strictly speaking, line detection is not wireframe parsing as it does not detect junctions of multiple line segments. Nevertheless, we will use the same metric proposed for the wireframe to evaluate the quality of our line segments with the endpoints being viewed as the junctions. In particular, we will compare with the state-of-the-art method in this category <ref type="bibr" target="#b29">[30]</ref> in our experiments.</p><p>Object Detection. Recently, the performance of line detection and wireframe parsing has all been boosted by an improvement in methods for object detection. Specifically, <ref type="bibr" target="#b37">[38]</ref> has inspired <ref type="bibr" target="#b23">[24]</ref> and many other two-stage detection methods such as <ref type="bibr" target="#b5">[6]</ref>. After that, <ref type="bibr" target="#b29">[30]</ref> combined <ref type="bibr" target="#b28">[29]</ref> with <ref type="bibr" target="#b37">[38]</ref> has pushed the performance further. <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b37">[38]</ref> are two state-of-the-art methods from 2019 and 2020, respectively. Both of them drew inspiration from the object detection community and adopted a two stage strategy. Notice that the object detection community has evolved from two-stage detector <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref> to one-stage detector <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref> or anchor free detector <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b11">12]</ref>. Motivated by these work, in this paper, we propose a F-Clip network that detects line segments from images in one stage (see <ref type="figure">Figure.</ref> 2), which aims to achieve a better trade-off between speed and accuracy. During the preparation of this paper, we become aware of a very recent work <ref type="bibr" target="#b9">[10]</ref> that casts the line detection problem as a similar learning problem of predicting three parameters for each line segment. However, they have adopted a different parameterization and a rather different network design than ours. We will discuss the differences in the next section as well as compare their algorithm with ours in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overall structure of the proposed network is illustrated in <ref type="figure">Figure 2</ref>. Given an input image, we first use a convolutional neural network to extract a shared feature map. Then the map is forwarded to separate sub-networks to predict three line representation maps: the line center map, the line length map, and the line angle map. These three maps are supervised by the pixel-wise loss between prediction and ground-truth. The network is optimized end-to-end with stochastic gradient descent. Below we give a detailed description and justification for each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Line Representation</head><p>We represent a line segment by its center, length, and angle. Denote p c ∈ R 2 as the center of line in the image coordinate, p l and p r ∈ R 2 as the left and right endpoints, respectively. We have the following relationship:</p><formula xml:id="formula_0">p l = p c + 1 2 (l cos α, l sin α) ∈ R 2 ,<label>(1)</label></formula><formula xml:id="formula_1">p r = p c − 1 2 (l cos α, l sin α) ∈ R 2 ,<label>(2)</label></formula><p>where l is the length and α is the angle between line and the horizontal direction. Any three of the above five quantities can uniquely determine a line segment. This leads to many mathematically equivalent representations of a line segment. For instance, the recent work <ref type="bibr" target="#b9">[10]</ref> uses the center p c and the x and y offsets of an end point to parameterize a segment. Among different choices, in this work, we choose to use the center p c , length l, and angle α for the following reasons: Firstly, the angle is the easiest one to predict since it can be identified accurately even from a local patch. It also has strong statistical priors (see <ref type="figure" target="#fig_4">Figure 3</ref>) that one can exploit to design more effective filters as we will elaborate more in the next subsection. In contrast to angle prediction, the network needs to perceive the whole line before making accurate predictions for length, center, and the endpoint (offsets). Secondly, since the relationship between line angle and the line's ending points is not one-to-one (one end point may be shared by multiple line segments), we choose to use line center and line length rather than the endpoints to simplify the inference. The resulting line representation gives several advantages:  in the ShanghaiTech dataset <ref type="bibr" target="#b8">[9]</ref> and the YorkUrban dataset <ref type="bibr" target="#b2">[3]</ref>, respectively. The bottom two are the convolutional blocks used in our method, conventional and customized, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">It naturally converts the line parsing problem to a pixel-</head><p>wise classification/regression problem. Such transformation enables us to build a fully-convolutional structure for this task, which is both accurate and efficient (Section 3.2).</p><p>2. Due to the pixel-wise formulation, it is not necessary to sample different kinds of lines as in <ref type="bibr" target="#b37">[38]</ref>, which significantly reduces the number of hyperparameters to tune (Section 3.3).</p><p>3. The inference algorithm is straightforward. Given a predicted center location, we can directly use the corresponding predicted length and angle to get a line segment (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fully Convolutional Line Parsing Networks</head><p>Designs of Backbone Networks. There are many advantages for our one-stage network. First, a one-stage network usually can improve the efficiency. Second, the simpler architecture allows easy customization. Line detection is often used in applications with very different requirements in speed and accuracy: e.g. the real-time localization or mapping versus offline 3D modeling from images. Some applications process one image at a time whereas others may process images in batches. Hence, in this work, we provide customized versions of our F-Clip along two lines of improvement. Along the first line, in order to speed-up the network, we simplify the backbone without sacrificing much performance. Along the second line, we show how to push the overall accuracy without compromising much speed (at least in the batch processing mode).</p><p>First of all, following the work of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38]</ref>, we adopt an hourglass network with two stack modules as our default version F-Clip (HG2). We simplify the hourglass with one stack module to get a fast version F-Clip (HG1), then reduce the number of hierarchical structure in the hourglass block, from 4 to 3 and 2, to obtain two even faster versions F-Clip (HG1-D3) and F-Clip (HG1-D2).</p><p>The second line of improvement is to increase the accuracy without sacrifice too much speed. We observe from the datasets that most line segments in man-made environments are close to being vertical or horizontal (see <ref type="figure" target="#fig_4">Figure 3</ref> (a) and (b)). Actually, this is typically the case for most real-world line detection tasks. Based on this strong statistical prior of the angle distribution, we customize the design of the line detection blocks (see <ref type="figure" target="#fig_4">Figure 3</ref> (d)) to exploit such priors with similar computational cost (compared to that in <ref type="figure" target="#fig_4">Figure 3</ref> (c)). This results in a more accurate version of F-Clip (HG2-LB) (where LB is short for line block). Moreover, to further improve the performance, we exploit multiple resolutions of the input through a parallel structure such as that in the high resolution (HR) network <ref type="bibr" target="#b25">[26]</ref>, which exhibits state-of-the-art performance for many vision tasks such image segmentation, detection, and recognition. This leads to a high-performance version of F-Clip (HR).</p><p>So overall, we have 6 different versions of F-Clip: 1) F-Clip (HG1-D2); 2) F-Clip (HG1-D3); 3) F-Clip (HG1); 4) F-Clip (HG2); 5) F-Clip (HG2-LB); and 6) F-Clip (HR). Their relative accuracy and speed are illustrated in <ref type="figure" target="#fig_1">Figure  1</ref>, and their quantitative and qualitative evaluations will be given in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="figure" target="#fig_6">Figure 5</ref> in the experiment section.</p><p>Line Score Maps. We design the size of network output to be the same as the feature map size (128×128) to make inference efficient. However, such a design will inevitably introduce quantization errors. We address this issue by introducing a local offset parameter associated with each line center. Specifically, for a ground-truth line center p c in the original image, the line center score map C is 1 only in the p c /s coordinates. For each center location, we also predict an offset value p c /s − p c /s , a line length value l, and a line angle value α.</p><p>Prediction Head. In contrast to <ref type="bibr" target="#b9">[10]</ref> where different network structures are used for different tasks, we intentionally minimize the design effort of each prediction head. For each branch, given the shared feature map of channel dimension c, we use two 3×3 convolutions with 256 output channels to refine the feature. Then we use a 1×1 convolution with task-specific output channel(s) to predict each quantity. The number of output channels is 2, 2, 1, 1 for center, offset, length, and angle prediction, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>Loss Function. We treat the problem of prediction C as a classification problem and use the focal loss <ref type="bibr" target="#b12">[13]</ref> to tackle the unbalanced positive/negative data samples:</p><formula xml:id="formula_2">L C . = − 1 N i,j C i,j (1 −Ĉ i,j ) β log(Ĉ i,j ) if C i,j = 1, (1 − C i,j )Ĉ β i,j log(1 −Ĉ i,j ) otherwise,<label>(3)</label></formula><p>where β is the hyper-parameter of the focal loss, N is the number of pixels in the score map, andĈ is the probability of each bin after softmax operation.</p><p>Following <ref type="bibr" target="#b37">[38]</ref>, we use 2 regression to predict the offset map O. The loss on O is averaged over the the number of line center points in the score map:</p><formula xml:id="formula_3">L O . = − 1 N i,j Ô i,j − O i,j 2 2 ,<label>(4)</label></formula><p>For line length and line angle prediction, the score map is normalized by sigmoid activation. Then we use the 1 loss between the predicted and ground-truth value (as we empirically found that 1 loss is better than 2 ):</p><formula xml:id="formula_4">L L . = − 1 N i,j |L i,j − L i,j |, L α . = − 1 N i,j |Â i,j − A i,j |.</formula><p>(5) The final loss used to train the network is:</p><formula xml:id="formula_5">L = λ C L C + λ O L O + λ l L l + λ α L α .<label>(6)</label></formula><p>Data Augmentation. To make the model more robust to various viewpoints and scale, we perform the following data augmentations. In the first step, an image is processed with one of the following operations with equal probability:</p><p>1. keep the original input image; 2. flip it horizontally or vertically or simultaneously; 3. rotate it by 90 • clockwise or counterclockwise.</p><p>After that, we use the random expansion augmentation in <ref type="bibr" target="#b16">[17]</ref>. Specifically, we choose a k × k region in the 512×512 zero input, and resize the image to fit inside the k × k region. We randomly sample k from <ref type="bibr">[256,</ref><ref type="bibr">512]</ref>. This augmentation is to enhance detection accuracy for short lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference</head><p>During inference, we firstly apply a non-maximum suppression (NMS) on the line center score map to remove duplicate line detection results <ref type="bibr" target="#b37">[38]</ref>. Different from <ref type="bibr" target="#b37">[38]</ref>, we leverage the SoftNMS <ref type="bibr" target="#b0">[1]</ref> from object detection to enhance the performance. Specifically,</p><formula xml:id="formula_6">C' i,j = C i,j if C i,j = max (i ,j )∈N (i,j) C i ,j δ · C i,j otherwise,<label>(7)</label></formula><p>where N (i, j) represents the 8 nearby bins around the location (i, j). Such non-maximum suppression can be implemented with a max-pooling operator. After using SoftNMS, we use the top K line centers according to their classification score. We use the corresponding predicted length and angle values to form a line according to Equation 2. The previous step only performs NMS on the point-level, without considering the effect of length and angle of a line. We hence propose a new structural NMS (StructNMS) that removes duplicate lines with the whole line structure. Starting from the line with the highest line center score (assume its index is i), we calculate the 2 distance between its two endpoints and those of another line j:</p><formula xml:id="formula_7">d = min p i l − p j l 2 2 + p i r − p j r 2 2 , p i l − p j r 2 2 + p i r − p j l 2 2 .<label>(8)</label></formula><p>Then we remove all the lines with d less than a predefined threshold τ . The procedure is applied for all the remaining line candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present our experiment results to analyze the performance of F-Clip, as well as compare with many other state-of-the-art line detection or wireframe parsing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Details for Backbone Networks. In order to make our neural network adaptable to various time efficiency requirement, F-Clip uses two different frameworks for the backbone networks: the stacked hourglass network <ref type="bibr" target="#b19">[20]</ref> for efficiency, and the HRNet <ref type="bibr" target="#b25">[26]</ref> for performance. The stacked houglass network is a simple and elegant U-shape network that was previously used for wireframe parsing, such as in L-CNN <ref type="bibr" target="#b37">[38]</ref> and HAWP <ref type="bibr" target="#b29">[30]</ref>. The configuration of our stacked houglass backbones are similar to the one in <ref type="bibr" target="#b37">[38]</ref>. The main difference is that we provide five different settings to meet different efficiency needs: 1) one stack of the hourglass network with 2 hierarchical structure in hourglass block (F-Clip (HG1-D2)); 2) one stack of the hourglass network with 3 hierarchical structure in hourglass block (F-Clip (HG1-D3)); 3) one stack of the hourglass network with 4 hierarchical structures in the hourglass block (F-Clip (HG1)); 4) two stacks of the hourglass network with 4 hierarchical structures in hourglass block, while the one in L-CNN <ref type="bibr" target="#b37">[38]</ref> only provide the models containing 2 stacks of the hourglass networks with 4 hierarchical structures in hourglass block (F-Clip (HG2)); 5) two stacks of the hourglass network with 4 hierarchical structures in the hourglass block which residual blocks are all replaced with line block (see <ref type="figure" target="#fig_4">Figure 3</ref> (d), F-Clip (HG2-LB)).</p><p>To further push the performance of F-Clip, we also employ the recent HRNet <ref type="bibr" target="#b25">[26]</ref> as our backbone feature extractor (F-Clip (HR)). HRNet is originally designed for human pose estimation tasks. HRNet uses a more complex architecture design. It starts with a high-resolution subnetwork (performing convolution on high-resolution feature maps) in its initial stages, and gradually add some low-resolution subnetworks. HRNet is designed to preserve more high-resolution details. We use the HRNet-W32 variant <ref type="bibr" target="#b25">[26]</ref> and find it performs better in F-Clip in term of accuracy, but it is much slower than the 2-stack hourglass network when batch size is equal to one. Prediction Head. The prediction head transforms the feature maps from the backbone network into the final representations. We simply use two 3×3 convolution layers followed by a 1 × 1 convolution to match the output dimension. All the convolution layers are activated with ReLU non-linearity. The channel sizes of the middle 3 × 3 convolution are 128. Training. We set a different initial learning rate 4 × 10 −4 and 4 × 10 −3 for the stack hourglass network and the high resolution network, respectively. Meanwhile, the parameter β in focal loss is also different for the two backbones (β = 5 for the stack hourglass network and β = 4 for the high resolution network). We choose the weights of the four loss terms in Equation <ref type="formula" target="#formula_5">(6)</ref> to be λ C,O,l,α = {1, 0.25, 3, 1}. We train our neural network for 300 epochs, in which we decay the learning rate 10 times at the 240th epoch and the 280th epoch. All the experiments are conducted on a single NVIDIA GTX 2080Ti GPU. We use the ADAM optimizer <ref type="bibr" target="#b10">[11]</ref>. The weight decay is set to be 1 × 10 −4 . We use a batch size that maximizes the occupancy of available GPU memory.</p><p>Inference. There are two hyper-parameters in the inference stage that need to be determined (δ in Equation <ref type="formula" target="#formula_6">(7)</ref> and τ in Equation <ref type="formula" target="#formula_7">(8)</ref>). For δ, we experimented with 10 numbers in the range from 0 to 1 (with a step 0.1) and chose the best δ = 0.8. For τ , we experimented with 6 values <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32)</ref> and chose τ = 2 eventually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>Datasets. We train and test F-Clip on the ShanghaiTech wireframe dataset <ref type="bibr" target="#b8">[9]</ref>, which contains 5,000 training images and 462 testing images of man-made scenes. We also include York Urban dataset <ref type="bibr" target="#b2">[3]</ref>, a small dataset containing 102 images, as the testing dataset to evaluate the generalizability of different methods. Baselines. We compare F-Clip with six baseline approaches: LSD <ref type="bibr" target="#b26">[27]</ref>, DWP <ref type="bibr" target="#b8">[9]</ref>, AFM <ref type="bibr" target="#b28">[29]</ref>, L-CNN <ref type="bibr" target="#b37">[38]</ref>, HAWP <ref type="bibr" target="#b29">[30]</ref>, and TP-LSD <ref type="bibr" target="#b9">[10]</ref>. Five approaches are supervised deep learning-based methods. To our best knowledge, they represent the state-of-the-art in their respective category of methods. We use the pre-trained models provided by the authors of each paper for evaluation, which are also trained on the for cross-entropy loss and focal loss, respectively. DataAug refers to the data augmentation we used: flip, rotation, and expansion. We apply SoftNMS on the above models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ShanghaiTech wireframe dataset.</head><p>Metric. The structural average precision (sAP) <ref type="bibr" target="#b37">[38]</ref>, proposed for evaluating accuracy in wireframe detection, uses the sum of squared error between the predicted end-points and their ground truths as evaluation metric. The predicted line segment will be counted as a true positive detection when its sum of squared error is less than a threshold, such as = 5, 10, 15. AP H was used in wireframe parsing <ref type="bibr" target="#b8">[9]</ref>. Instead of directly using the vectorized representation of line segments, we use heatmaps generated by rasterizing line segments for both parsing results and the groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we verify the effectiveness of our proposed method through extensive experiments. All of the experiments are performed on the ShanghaiTech dataset <ref type="bibr" target="#b8">[9]</ref> and structural AP are reported.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we analyze the choices of different training designs. Firstly, we show that using focal loss can improve the performance by about 1 point for all metrics, by comparing <ref type="table" target="#tab_0">Table 1</ref> row (a) and (b). This is because the line centers only occupy a small portion of the image, thus the ratio between positive and negative samples is very small. In this case, focal loss is effective on addressing this problem. Secondly, we show the effectiveness of our proposed rotation and expansion data augmentation. In <ref type="table" target="#tab_0">Table 1</ref> (c) and (d), adding rotation and expansion augmentation leads to an improvement of about 1 point and 3 points, respectively. These results show that by augmenting the data with different geometric transformations, one can obtain a more effective line detector that generalizes better.</p><p>Next, we show the influence of different inference strategies. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. In entry (a), our method achieves 61.5 sAP 5 using the original hard NMS in <ref type="bibr" target="#b37">[38]</ref>. Next, we apply the SoftNMS in Equation <ref type="bibr" target="#b6">(7)</ref> and the result improves by 2 points to 63.5. This is because at this stage, only point information is utilized. Thus there may be different lines with close center locations that are mistakenly removed. Setting a lower confidence instead of completely deleting such lines maintains the potential to recover such  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Other Methods</head><p>To ensure fair comparison with the previous state-ofthe-art method HAWP <ref type="bibr" target="#b29">[30]</ref>, we have re-implemented their method with slightly better results. We have adopted the same hyper-parameter settings as our best performing model F-Clip (HR), including the focal loss, backbone, longer training epoch, and data augmentation etc. The experiment details can be found in the Appendix.</p><p>In order to make fair comparison on speed, we compute the frames per second (FPS) for different methods with their latest released code using a single GPU (RTX 2080Ti). Most of the previous methods focus on the latency of the algorithm (FPS while batch-size=1) but not throughput (FPS while batch-size=max) which is a more important metric for offline batch processing. We show the throughput metric in the last column of <ref type="table" target="#tab_4">Table 3</ref> which illustrates the better parallel performance of single stage methods compare with two stage methods. <ref type="table" target="#tab_4">Table 3</ref> summarizes our results. Our F-Clip obtains stateof-the-art performance both in terms of efficiency and accuracy. Under the very challenging sAP 5 metric, using the same backbone network, our F-Clip (HG2) achieves comparable performance with previous state-of-the-art methods while being 1.4x faster when batch size is equal to one. By changing the backbone from HG2 to HG2-LB, we can get another 1.5 points gain without sacrifice too much speed. Moreover, ourF-Clip (HR) achieves state-of-the-art with a decent speed (17.4 FPS) and achieves 1.8 times higher throughput than HAWP <ref type="bibr" target="#b29">[30]</ref>. Specifically, our F-Clip (HG1) is 1 points higher than another one-stage method TP-LSD, while is 1.8 times faster. When we reduce the hierarchical structure in hourglass block to 2, the speed further increases to 73 FPS. Our method not only achieves comparable speed with LSD, but is more than 8 times accurate in terms of sAP 5 . Finally, F-Clip also achieves the state-of-the-art results on YorkUrban dataset which shows its generalizability.</p><p>The precision and recall curves of sAP <ref type="bibr" target="#b9">10</ref> and AP H are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. For the ShanghaiTech dataset, our method  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>We visualize the output of our F-Clip and other three methods L-CNN, HAWP, and TP-LSD in <ref type="figure" target="#fig_6">Figure 5</ref>. The junctions are marked cyan and lines are marked orange. Wireframes from L-CNN and HAWP are post processed using the method from Appendix A.1 in <ref type="bibr" target="#b37">[38]</ref>. Since TP-LSD and F-Clipdo not explicitly output junctions, we treat the endpoints of lines as junctions.</p><p>Both L-CNN and HAWP rely highly on the junction detection and line feature sampling, which might be prone to missing junctions or texture variations. In comparison, TP-LSD and and F-Clip are capable of detecting line segments in complicated even low-contrast environments (see third row of <ref type="figure" target="#fig_6">Figure 5</ref>). An obvious draw-back of TP-LSD is that it captures many redundant lines (see details in <ref type="figure" target="#fig_6">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we have introduced a one-stage fully convolutional line detection network F-Clip that directly outputs parameters of all line segments from an image. We formulate line segment detection as an end-to-end prediction of the center-point, length, and angle for each line segment. We show that by simply adjusting the backbone network, we are able to obtain a family of line detection networks that achieve state-of-the-art trade-off between accuracy and speed. We have conducted extensive experiments on large real-world datasets and demonstrated that this method outperforms the previous state-of-the-art wireframe parsing and line detection methods, by either improving the accuracy by a wide margin at the same frame rate or improving the speed by multi-fold at the same accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>We thank Yichao Zhou and Haozhi Qi of Berkeley for their help during all processes include ideas, experiments, and paper writing. This paper would not have been possible without their help. We also thank Kenji Tashiro of Sony for his helpful discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Material</head><p>To make a fair comparison with the previous state-ofthe-art method HAWP <ref type="bibr" target="#b29">[30]</ref>, we adopt the hyper-parameter settings including the 1) backbone, 2) longer training epoch, 3) data augmentation, and 4) focal loss on HAWP same as our best performance model F-Clip (HR).</p><p>Backbone. Our method employ a strong backbone network HRnet <ref type="bibr" target="#b25">[26]</ref> (short for HR in <ref type="table">Table 4</ref>). As shwon in <ref type="table">Table 4</ref>, the HRnet do not bring significantly performance improvement for HAWP. Training Epochs. Our method needs more training iterations to converge because we use a strong backbone network. As shown in <ref type="table">Table 4</ref> below, additional training epochs do not improve significantly the performance of state-of-the-art two-stage methods HAWP.</p><p>Data Augmentation. As shown in single-stage object detection methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>, applying a more complex data augmentation does not improve the performance of twostage networks. We also see the same phenomena, as shown in <ref type="table">Table 4</ref>.</p><p>Focal Loss. Focal loss <ref type="bibr" target="#b12">[13]</ref> is designed to handle the balance between positive and negative samples. We apply the focal loss on the junction detector of HAWP. As shown in <ref type="table">Table 4</ref> below, the focal loss makes a bad effect on the performance of HAWP.</p><p>Analysis. Both two-stage wireframe detection methods LCNN <ref type="bibr" target="#b37">[38]</ref> and HAWP are junction based methods. The performance of junction detection will dominate the performance of overall wireframe detection. Our F-Clip is a single stage method which skip the detection of junction and predicts the line directly. Compare with line detection, the local feature is enough for the detection of junction. Hence, hourglass backbone with a short training epoch <ref type="formula" target="#formula_2">(30 epochs</ref>  <ref type="table">Table 4</ref>: HAWP with longer training epochs, hrnet backbone, focal loss, and the same data augmentation as ours.  <ref type="bibr" target="#b37">[38]</ref>, HAWP <ref type="bibr" target="#b29">[30]</ref>, TP-LSD <ref type="bibr" target="#b9">[10]</ref>, F-Clip (HR), and the ground truth. We also draw the detected junctions from L-CNN and HAWP and the line endpoints from TP-LSD and F-Clip.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Speed (ms) versus accuracy (sAP 5 ) trade-off of state-ofthe-art algorithms on the ShanghaiTech wireframe dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>customized conv. block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The top two figures are the histograms of line angles</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Precision-Recall (PR) curves of sAP 10 and AP H for TP-LSD<ref type="bibr" target="#b9">[10]</ref>, L-CNN<ref type="bibr" target="#b37">[38]</ref>, HAWP<ref type="bibr" target="#b29">[30]</ref> and F-Clip (HR) on the ShanghaiTech and the YorkUrban benchmarks respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative evaluation of wireframe and line detection methods. From left to right, the columns correspond to the results from L-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study for our training details. CE and FL stand</figDesc><table><row><cell></cell><cell cols="2">Backbone LC</cell><cell>DataAug Flip Rotate Expand</cell><cell>sAP 5 sAP 10 sAP 15</cell></row><row><cell>(a)</cell><cell></cell><cell>CE</cell><cell></cell><cell>56.2 61.4 63.2</cell></row><row><cell>(b) (c)</cell><cell>Hourglass</cell><cell>FL FL</cell><cell></cell><cell>57.3 62.3 64.4 58.4 63.3 65.3</cell></row><row><cell>(d)</cell><cell></cell><cell>FL</cell><cell></cell><cell>61.2 65.8 67.8</cell></row><row><cell cols="3">(e) HRNet FL</cell><cell></cell><cell>63.5 67.4 69.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>SoftNMS StructNMS sAP 5 sAP 10 sAP 15</figDesc><table><row><cell>(a)</cell><cell>61.5 65.6 67.1</cell></row><row><cell>(b)</cell><cell>63.5 67.4 69.2</cell></row><row><cell>(c)</cell><cell>62.5 66.7 68.3</cell></row><row><cell>(d)</cell><cell>64.3 68.3 70.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study for inference details. The SoftNMS and StructNMS columns are two operators mentioned in methods. The model is an HRNet with focal loss and all the data augmentation.</figDesc><table><row><cell>mistakes. Next, we show that using the StructNMS can</cell></row><row><cell>further improve the performance by 1 point since such mech-</cell></row><row><cell>anism takes the whole line into consideration. Combining</cell></row><row><cell>these two new NMS mechanisms improves 3 points over the</cell></row><row><cell>original pipeline.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>sAP 10 AP H sAP 5 sAP 10 AP H batch-size=1 batch-size=10</figDesc><table><row><cell cols="3">ShanghaiTech sAP 5 Two-stage methods Method</cell><cell></cell><cell></cell><cell>YorkUrban</cell><cell></cell><cell>FPS</cell><cell>FPS</cell></row><row><cell>LSD (320) [27]</cell><cell>6.7</cell><cell>8.8</cell><cell>52.0</cell><cell>7.5</cell><cell>9.2</cell><cell>51.0</cell><cell>100</cell><cell>/</cell></row><row><cell>AFM [29]</cell><cell>18.5</cell><cell>24.4</cell><cell>69.2</cell><cell>7.3</cell><cell>9.4</cell><cell>48.2</cell><cell>13.5</cell><cell>/</cell></row><row><cell>DWP [9]</cell><cell>3.7</cell><cell>5.1</cell><cell>67.8</cell><cell>1.5</cell><cell>2.1</cell><cell>51.0</cell><cell>2.24</cell><cell>/</cell></row><row><cell>L-CNN [38]</cell><cell>58.9</cell><cell>62.9</cell><cell>80.3</cell><cell>24.3</cell><cell>26.4</cell><cell>58.5</cell><cell>15.6</cell><cell>16.4</cell></row><row><cell>HAWP [30]</cell><cell>62.5</cell><cell>66.5</cell><cell>84.5</cell><cell>26.1</cell><cell>28.5</cell><cell>60.6</cell><cell>26.9</cell><cell>81.3</cell></row><row><cell>HAWP (re-trained)</cell><cell>63.1</cell><cell>66.9</cell><cell>84.9</cell><cell>26.9</cell><cell>29.2</cell><cell>61.4</cell><cell>12.3</cell><cell>60.6</cell></row><row><cell>One-stage methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TP-LSD (Res34-Lite) [10]</cell><cell>56.4</cell><cell>59.7</cell><cell>/</cell><cell>24.8</cell><cell>26.8</cell><cell>/</cell><cell>65.9</cell><cell>327.6</cell></row><row><cell>TP-LSD (Res34) [10]</cell><cell>57.5</cell><cell>60.0</cell><cell>/</cell><cell>25.3</cell><cell>27.4</cell><cell>/</cell><cell>32.6</cell><cell>194.2</cell></row><row><cell>F-Clip (HG1-D2)</cell><cell>52.7</cell><cell>57.2</cell><cell>76.7</cell><cell>23.9</cell><cell>26.1</cell><cell>56.3</cell><cell>73.4</cell><cell>363.1</cell></row><row><cell>F-Clip (HG1-D3)</cell><cell>57.8</cell><cell>62.7</cell><cell>82.0</cell><cell>25.6</cell><cell>28.2</cell><cell>60.6</cell><cell>67.9</cell><cell>339.8</cell></row><row><cell>F-Clip (HG1)</cell><cell>58.6</cell><cell>63.6</cell><cell>83.0</cell><cell>24.9</cell><cell>27.4</cell><cell>60.3</cell><cell>64.1</cell><cell>324.5</cell></row><row><cell>F-Clip (HG2)</cell><cell>61.3</cell><cell>65.8</cell><cell>84.0</cell><cell>27.2</cell><cell>29.4</cell><cell>62.0</cell><cell>35.7</cell><cell>205.2</cell></row><row><cell>F-Clip (HG2-LB)</cell><cell>62.6</cell><cell>66.8</cell><cell>85.1</cell><cell>27.6</cell><cell>29.9</cell><cell>62.3</cell><cell>29.8</cell><cell>148.5</cell></row><row><cell>F-Clip (HR)</cell><cell>64.3</cell><cell>68.3</cell><cell>85.7</cell><cell>28.5</cell><cell>30.8</cell><cell>65.0</cell><cell>17.4</cell><cell>107.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative Results and Comparisons. The proposed F-Clip achieves state-of-the-art results consistently. Overall, the FPS of our F-Clip's is still significantly better than or on par with that of the six existing methods. Note that for fair apple-to-apple comparison, we have retrained the HAWP model using their latest released code and the same settings used in our F-Clip (HR). See text for details. achieves higher recall and performs better in the higher recall regime, similarly for the YorkUrban Dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>) is enough for converging to a good result, a strong backbone HRnet with a longer training epoch (300 epoch) does not bring significant performance improvement for HAWP. Meanwhile, focal loss on the junction detector even makes a bad effect on the performance of HAWP.</figDesc><table><row><cell cols="4">Method backbone epoch DataAug focal-loss sAP 5</cell></row><row><cell></cell><cell>HG</cell><cell>30</cell><cell>62.5</cell></row><row><cell></cell><cell>HG</cell><cell>300</cell><cell>62.8</cell></row><row><cell>HAWP</cell><cell>HR</cell><cell>300</cell><cell>63.1</cell></row><row><cell></cell><cell>HR</cell><cell>300</cell><cell>63.0</cell></row><row><cell></cell><cell>HR</cell><cell>300</cell><cell>62.4</cell></row><row><cell>Ours</cell><cell>HR</cell><cell>300</cell><cell>64.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Soft-NMS-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco J</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco J</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Use of the hough transformation to detect lines and curves in pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>CACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Line-based relative pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Elqursh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask-RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TP-LSD: Tri-points based line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep hough-transform line priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silvia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Planercnn: 3d plane detection and reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Planenet: Piece-wise planar reconstruction from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LGNN: A context-aware line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiakai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual representations for semantic target driven navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Fišer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Košecká</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayzaan</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Probabilistic approach to the hough transform. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Rafael Grompone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Line segment detection using transformers without edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01909</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Holisticallyattracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Line assisted light field triangulation and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bundle pooling for polygonal architecture segmentation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PPGNet: Learning point-pair graph for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Linebased camera calibration with lens distortion correction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics and Lasers in Engineering</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Holicity: A city-scale data platform for learning holistic 3d structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xili</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">NeurVPS: Neural vanishing point scanning via conic convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d manhattan wireframes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Layoutnet: Reconstructing the 3d room layout from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
