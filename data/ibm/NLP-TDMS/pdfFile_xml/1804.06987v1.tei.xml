<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Distantly Supervised Relation Extraction using Word and Entity Based Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharmistha</forename><surname>Jat</surname></persName>
							<email>sharmisthaj@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhesh</forename><surname>Khandelwal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Distantly Supervised Relation Extraction using Word and Entity Based Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation extraction is the problem of classifying the relationship between two entities in a given sentence. Distant Supervision (DS) is a popular technique for developing relation extractors starting with limited supervision. We note that most of the sentences in the distant supervision relation extraction setting are very long and may benefit from word attention for better sentence representation. Our contributions in this paper are threefold. Firstly, we propose two novel word attention models for distantlysupervised relation extraction: (1) a Bi-directional Gated Recurrent Unit (Bi-GRU) based word attention model (BGWA), (2) an entity-centric attention model (EA), and (3) a combination model which combines multiple complementary models using weighted voting method for improved relation extraction. Secondly, we introduce GDS, a new distant supervision dataset for relation extraction. GDS removes test data noise present in all previous distantsupervision benchmark datasets, making credible automatic evaluation possible. Thirdly, through extensive experiments on multiple real-world datasets, we demonstrate the effectiveness of the proposed methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classifying the semantic relationship between two entities in a sentence is termed as Relation Extraction (RE). RE from unstructured text is an important step in various Natural Language Understanding tasks, such as knowledge base construction, question-answering etc. Supervised methods have been successful on the relation extraction task <ref type="bibr" target="#b1">[Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b5">Zeng et al., 2014]</ref>. However, the extensive training data necessary for supervised learning is expensive to obtain and therefore restrictive in a Web-scale relation extraction task. * Equal Contribution to this work † This research was conducted during the author's research assistantship at Indian Institute of Science To overcome this challenge, <ref type="bibr" target="#b3">[Mintz et al., 2009]</ref> proposed a Distant Supervision (DS) method for relation extraction to help automatically generate new training data by taking an intersection between a text corpus and knowledge base. The distant supervision assumption states that for a pair of entities participating in a relation, any sentence mentioning that entity pair in the text corpora is a positive example for the relation fact. This assumption outputs evidence from multiple sentences for multiple relation labels between an entity-pair. Therefore the problem of relation extraction in distantly supervised datasets is posed as a Multi-instance Multi-label (MIML) problem <ref type="bibr" target="#b4">[Surdeanu et al., 2012]</ref>, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. However, the DS assumption is too strong and may introduce noise such as false negative samples due to missing facts in the knowledge base. In this paper, we propose relation extraction models and a new dataset to improve RE. We define 'instance' as a sentence containing an entity-pair, and 'instance set' as a set of sentences containing the same entity-pair.</p><p>It was observed by <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref> that 50% of the sentences in the Riedel2010 Distant Supervision dataset <ref type="bibr">[Riedel et al., 2010]</ref>, a popular DS benchmark dataset, had 40 or more words in them. We note that not all the words in these long sentences contribute towards expressing the given relation. In this work, we formulate various word attention mechanisms to help the relation extraction model focus on the right context in a given sentence.</p><p>The MIML assumption states that in an instance set corresponding to an entity pair, at least one sentence in that set should express the true relation assigned to the set. However, we observe that this is not always true in currently available benchmark datasets for RE in the distantly supervised setting. In particular, current datasets have noise in the test set, for example, a fact may be labelled false if it is missing in the knowledge base, leading to a false negative label in train and test set. Noise in test set impedes the right comparison of models and may favor overfitted models. To address this challenge, we build the Google Distant Supervision (GDS) dataset, a new dataset for distantly-supervised relation extraction. GDS is seeded from the Google relation extraction corpus <ref type="bibr">[Shaohua Sun and Orr, 2013]</ref>. This new dataset addresses an important shortcoming in distant supervision evaluation and makes an automatic evaluation in this setting more reliable.   <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref>. The two entities are underlined. (Section 2.1)</p><p>In summary, our contributions are: (a) we introduce the Google Distant Supervision (GDS) dataset, a new dataset for distantly-supervised relation extraction; (b) we propose two novel word attention based models for distant supervision, viz., BGWA, a BiGRU-based word attention model, and EA, an entity-centric attention model; and (c) we show efficacy of combining new and existing relation extraction models using a weighted ensemble model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Methods</head><p>In this section, we present our attention-based models for distantly supervised relation extraction. We first describe problem background and Piecewise Convolution Neural Network (PCNN), a previous-state-of-the-art model. We then introduce our Entity attention (EA) and Bi-GRU based word attention (BGWA) models. The last subsection describes a simple ensemble approach to combine predictions of various models for robust relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Relation Extraction: A relation is defined as a semantic property between a set of entities {e k }. In our task, we consider binary relations where k ∈ [1, 2], such as Born In(Barack Obama, Hawaii). Given a set of sentences S = {s i }; i ∈ [1 . . . N ], where each sentence s i contains both the entities, the task of relation extraction with distantly supervised dataset is to learn a function F r : F r (S, (e 1 , e 2 )) = 1 if relation r is true for pair(e 1 , e 2 ) 0 Otherwise PCNN: <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref> proposed the Piecewise Convolution Neural Network (PCNN), a successful model for distantly supervised relation extraction. The Success of the relation extraction task depends on extracting the right structural features from the sentence containing the entity-pair. Neural networks, such as Convolutional Neural Networks (CNNs), have been proposed to alleviate the need to manually design features for a given task <ref type="bibr" target="#b5">[Zeng et al., 2014]</ref>. As the output of CNNs is dependent on the number of tokens in the sentence, max pooling operation is often applied to remove this dependence. However, the use of a single max-pool misses out on some of these structural features useful for relation extraction task. PCNN model divides a sentence s i 's convolution filter output c i containing two entities into three parts c i1 , c i2 , c i3 -sentence context to the left of first entity, between the two entities, and to right of the second entity respectively-and performs max-pooling on each of the three parts, shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Thereby, leveraging the entity location information to retain the structural features of a sentence after the max-pooling operation.</p><formula xml:id="formula_0">pc ij = max(c ij ); 1 ≤ i ≤ N, 1 ≤ j ≤ 3</formula><p>The output of this operation is the concatenation of {pc i1 , pc i2 , pc i3 } yielding a fixed size output. The fixed size output is processed through a tanh non-linearity followed by a linear layer to produce relation probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bi-GRU based Word Attention Model (BGWA)</head><p>Consider the sentence expressing bornIn(Person, City) relation between the entity pair (Obama, Honolulu).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Former President Barack Obama was born in the city of</head><p>Honolulu, capital of the U.S. state of Hawaii</p><p>In the sentence, the phrase "was born in" helps in identifying the correct relation in the sentence. It is conceivable that identifying such key phrases or words will be helpful in improving relation extraction performance. Motivated by this, our first proposed model, Bidirectional Gated Recurrent Unit (Bi-GRU) based Word Attention Model (BGWA) uses an attention mechanism over words to identify such key phrases. To the best of our knowledge, there has been no prior work on using word attention in the distant supervision setting.</p><p>The BGWA model is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. It uses Bi-GRU to encode sentence context. <ref type="bibr">GRU [Cho et al., 2014</ref>] is a variant of Recurrent Neural Network (RNN) which was designed to capture long-range dependencies in words. A Bi-GRU runs in both forward and backward direction in a sentence to capture both sides of a word context. Only a few words in a sentence are relevant for determining the relation expressed. This degree of relevance of a word is calculated as an attention value in our model. An instance set S q consists of set of sentences,</p><formula xml:id="formula_1">[s i ; i ∈ [1 . . . N ]]. Each word in sentence s i is represented using a pre-trained embedding x ij ∈ R d×1 , j ∈ [1 . . . M ]. Passing s i through GRU gener- ates forward (h f ij ) and backward (h b ij ) hidden representations for each word. Concatenation [h f ij , h b ij ] ∈ R g×1 is used as representation for a word. w ij = [h f ij , h b ij ]; u ij = w ij × A × r; a ij = exp(u ij ) M l=1 exp(u il ) ;ŵ ij = a ij × w ij</formula><p>We define u ij , the degree of relevance of the j th word in i th sentence of the instance set, where A ∈ R g×g is a square matrix and r ∈ R g×1 is a relation query vector. Bilinear operator A determines the relevance of a word for a relation vector. Both A and r are learned parameters. Attention value a ij is calculated by taking softmax over {u ij } values.</p><p>Despite the widespread use of weighted sum to obtain sentence context embeddings s wa in attention-based settings, similar to PCNN model Section 2.1, we apply the piecewise max pooling onŵ ij before, between, and after the entity pair. Final piecewise max-pooled sentence embedding s wa ∈ R 1×3g of the sentence is processed through a tanh non-linearity and linear layer to yield probabilities for each relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Entity Attention (EA) Model</head><p>Let us once again consider the example sentence from Section 2.2 involving entity pair (Obama, Honolulu). In the sentence, for entity Obama, the word President helps in identifying that the entity is a person. This extra information helps in narrowing down the relation possibilities by looking only at the relations that occur between a person and a city. <ref type="bibr" target="#b4">[Shen and Huang, 2016]</ref> proposed an entity attention model for supervised relation extraction with a single sentence as input to the model. We modify and adapt their model for the distant supervision setting and propose Entity Attention (EA) which works with a bag of sentences. For a given bag of sentences, learning is done using the setting proposed by <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref>, wherein the sentence with the highest probability of expressing a relation in a bag is selected to train the model in each iteration.</p><p>The EA model has two components: 1) PCNN layer, and 2) Entity Attention Layer, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Consider an instance set S q with set of sentences,</p><formula xml:id="formula_2">[s i ; i ∈ [1 . . . N ]] and an entity-pair e qk , k ∈ [1, 2]. A sentence s i has M words [x ij ; j ∈ [1 . . . M ]],</formula><p>where each x ij ∈ R 1×d is a word embedding and {e emb q1 , e emb q2 } are the embeddings for the two entities. The PCNN layer is applied on the words in the sentence <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref>. The entity-specific attention u i,j,qk for j th word with respect to k th entity is calculated as follows:</p><formula xml:id="formula_3">u i,j,qk = [x ij , e emb qk ] × A k × r k , Here, [x ij , e emb qk ]</formula><p>is the concatenation of a word and the entity embedding. A k , r k are learned parameters. Bilinear operator A k determines the relevance of concatenated word &amp; entity embedding for a relation vector r k . Intuitively, attention should choose words which are related to the entity for a given relation. The u i,j,qk are normalized using a softmax function to generate a i,j,qk , the attention scores for a given word. Similar to the PCNN model in Section 2.1, the attention weighted word embeddings are pooled using piecewise pooling method to generate s ea ∈ R 1×3g dimensional sentence embeddings. The output from the PCNN layer and the entity attention layers are concatenated and then passed through a linear layer to obtain probabilities for each relation.</p><p>The entity attention model (EA) we propose is adapted to the distantly supervised setting by using two important variations from the original <ref type="bibr" target="#b4">[Shen and Huang, 2016</ref>] model (a) The EA processes a set of sentences. It uses PCNN <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref> assumption to select the sentence with highest probability of any relation. The selected sentence is used to estimate the relation probabilities for an entity-pair and for back-propagation of the error for the bag-of-sentences. (b) EA uses PCNN instead of CNN to preserve structural features in a sentence. We found the two variations to be crucial for the model to work in the distant supervision setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Bring it all together: Ensemble Model</head><p>We note that the models discussed in previous sections, BGWA, EA and PCNN, have complementary strengths. PCNN extracts high-level semantic features from sentences using CNN. Most effective features are then selected using a piecewise max-pooling layer. Entity-based attention (Section 2.3) helps in highlighting important relation words with respect to each of the entities present in the sentence, thus complimenting the PCNN-based features. Going beyond the entity-centric words, we observe that not all words in a sentence are equally important for relation extraction. The BGWA model (Section 2.2) addresses this aspect by selecting words relevant to a relation in a sentence.</p><p>In <ref type="figure" target="#fig_4">Figure 5</ref>, we plot the confidence scores of various models on the true labels of 10 randomly selected instance sets from Google Distant Supervision dataset (described in Section 3). From this figure, we observe that the proposed methods are  able to leverage signals from the entity and word attention models, even when the PCNN model is incorrect (light colored cell in the last column). This validates our assumption and motivates ensemble approach to efficiently combine these complementary models.</p><p>We combine the predictions of all the three models using a weighted voting ensemble. The weights of this model are learned using linear regression on development dataset. Assume P i,&lt;model&gt; is a vector containing probability scores for all relations with respect to i th example in developement data as given by a model. P i,&lt;model&gt; ∈ R 1×rl , where rl is the number of relations.</p><formula xml:id="formula_4">P i,EN SEM BLE = α * P i,P CN N + β * P i,EA + γ * P i,BGW A</formula><p>Here, α, β, γ are parameters learned using linear regression <ref type="bibr" target="#b3">[Pedregosa et al., 2011]</ref>. More complicated regression methods (e.g., ridge regression) did not improve the results greatly. We also experimented with a jointly learned neural ensemble by concatenating the features of all models after pooling layer followed by a linear layer. In our experiments, weighted voting ensemble method gave better results than the jointly learned model.  <ref type="table" target="#tab_3">Table 2</ref>. To alleviate noise in DS setting, we make sure that labelled relation is correct and for each instance set in GDS, there is at least one sentence in that set which expresses the relation assigned to that set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GDS: A New Dataset for Relation Extraction using Distant Supervision</head><p>We start with the human-judged Google Relation Extraction corpus 1 . This corpus consists of 5 binary relations. We construct the GDS dataset out of the relation extraction corpus using the following process. Let D GRE be the Google RE corpus, D GRE = {(s i , e i1 , e i2 , r i )} , where the i th sentence s i is annotated as expressing relation r i between the two entities e i1 and e i2 in the sentence. r i is one of the five relations mentioned in <ref type="table" target="#tab_3">Table 2</ref>. Now, for each (s i , e i1 , e i2 , r i ) ∈ D GRE , we perform the following:</p><p>• Perform web search to retrieve documents containing the two entities e i1 and e i2 .</p><p>• From such retrieved documents, select multiple text snippets containing the two entities. Each snippet is restricted to contain at most 500 words. Let S i = {s q }; q ∈ (1 . . . M ) be the set of such snippets.</p><p>• Let S i = {{s i } ∪ S i }. We now create a new instance set B i = {(S i , e i1 , e i2 , r i )}. Here, B i is an instance set for distant supervision which consists of the set of instances (sentences or snippets) S i , where the entities e i1 and e i2 are mentioned in each instance. The label r i is applied over the entire set B i .</p><formula xml:id="formula_5">D GDS = {B i } is the new GDS dataset.</formula><p>Here, each set B i is guaranteed to contain at least one sentence (s i ) which expresses the relation r i assigned to that set. An example of the sentence set expansion is shown in <ref type="figure" target="#fig_5">Figure 6</ref>. We note that such guarantee was not available in previous DS benchmarks.</p><p>We divided this dataset into a train (60%), development (10%) and test (30%) sets, such that there is no overlap among entity-pairs of these sets. Unlike currently available datasets, the availability of development dataset helps in performing model selection in a principled manner for relation extraction.</p><p>In <ref type="bibr">[Riedel et al., 2010]</ref> and subsequent work, a manual evaluation was done by validating the top 1000 confident predictions. This manual evaluation was necessary due to the noise in the test data. GDS although a small dataset in terms of the size as compared to Riedel2010 dataset, gets past such cumbersome manual evaluation and makes an automated evaluation in distantly-supervised relation extraction a reality.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Datasets: We validate effectiveness of the proposed models on two datasets summarized in Baselines: We compare proposed models with (a) Piecewise Convolution Neural Network (PCNN) <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref> and (b) Neural Relation Extraction with Selective Attention over Instances (NRE) <ref type="bibr" target="#b2">[Lin et al., 2016]</ref>. Both NRE and PCNN baseline outperform traditional baselines like MIML-RE and hence we use them as a representative state-of-the-art baseline to compare with proposed models.</p><p>Model Parameters: The parameters used for the various models are summarized in <ref type="table" target="#tab_7">Table 4</ref>. Word embeddings are initialized using the Word2Vec vectors from NYT dataset, similar to <ref type="bibr" target="#b2">[Lin et al., 2016]</ref>. Word Position feature embeddings (with respect to each entity) are randomly initialized and learned during training. Concatenation of the word embedding and position embedding results in a 60-dimensional (d w + (2 * d p )) embedding x ij for each word. We implemented PCNN model baseline following <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref> and used author provided results and implementation for NRE baseline. The EA and BGWA models were developed in PyTorch 2 . We use SGD algorithm with dropout <ref type="bibr" target="#b4">[Srivastava et al., 2014]</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Performance Comparison: <ref type="figure" target="#fig_6">Figure 7</ref> and <ref type="figure" target="#fig_7">Figure 8</ref> show the precision-recall curve for baseline and proposed algorithms on two datasets, Riedel2010-b (with development set) and the GDS dataset. Please note that the NRE model's PR-curve in <ref type="figure" target="#fig_6">Figure 7</ref> is taken from author published results which used combined train+dev set for training. This gives the NRE model an advantage over all other models in <ref type="figure" target="#fig_6">Figure 7</ref> as all of them are trained using only the train part. For the Riedel2010-b dataset, we plot the PR-curve with a maximum recall of 0.35, as the precision is too low beyond 0.35 recall. From <ref type="figure" target="#fig_6">Figure 7</ref> and <ref type="figure" target="#fig_7">Figure 8</ref>, we observe that the proposed models -BGWA and EA -achieve higher or competitive precision over the entire recall range compared to the state-ofthe-art NRE and PCNN models. PCNN model outperforms NRE model in both datasets. ENSEMBLE, a combination of proposed models BGWA, EA and PCNN in a weighted ensemble, helps in improving precision further. It achieves a significant precision gain of over 2-3% over various recall ranges for the Riedel2010-b dataset with 53 relations. This indicates that clues from combined model help results.</p><p>We observe that the BGWA model performs well on the Riedel2010-b dataset, but the trend is reversed in performance for GDS dataset where EA performs better. These two datasets have varied properties, (a) Riedel2010-b has 53 relations as opposed to 5 in the GDS dataset, (b) GDS has no label noise in the test as compared to the Riedel2010-b dataset. The performance difference between the BGWA and EA model shows that the errors by both the models are not correlated and are complimentary as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. This empirical validation encourages ensemble of these methods. We observe that  the ENSEMBLE model performs consistently well across all recall ranges in both the datasets, validating our assumption.</p><p>Visualizing Attention: We visualize the attention values of our models in <ref type="figure">Figure 9</ref>. It can be observed that the Entity 2 Attention for the 'location in' relation rightly focuses on words indicating place information like 'in', ',' and 'annopolis'. We note that entity attention is relation specific. In this case, Entity 1 Attention rightly focuses on the second entity, 'maryland' (location name), for selecting relation 'location in'. The word attention value is calculated using Bi-GRU hidden representation embeddings. Bi-GRU representation at a given time point t in a sequence is a summary of all the timepoints correlated to t, in the sequence. A high attention value for the hidden layers after processing the word 'annapolis' indicates that the sentence has rich context around the first entity to indicate location in relation. In conclusion, the attention models rightly choose the relevant words in context and help in improving relation extraction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Relation extraction in distantly supervised datasets is posed in a Multi-instance Multi-label (MIML) setting <ref type="bibr" target="#b4">[Surdeanu et al., 2012]</ref>. A large proportion of the subsequent work in this field has aimed to relax the strong assumptions that the original DS model made. <ref type="bibr">[Riedel et al., 2010]</ref> introduced the expressed-at-least-once assumption in a factor graph model as an aggregating mechanism over mention level predictions. Work by <ref type="bibr" target="#b2">[Hoffmann et al., 2011;</ref><ref type="bibr" target="#b4">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b3">Ritter et al., 2013]</ref> are crucial increments to <ref type="bibr">[Riedel et al., 2010]</ref>.</p><p>In past few years, Deep learning models <ref type="bibr" target="#b1">[Bengio, 2009]</ref> have reduced the dependence of algorithms on manually de- <ref type="figure">Figure 9</ref>: BGWA (word attention) and EA (entity attention) values for an example sentence between entity pair (maryland, annapolis) and relation location in. X-axis shows the sentence words and y-axis shows the attention scores. Please see Section 4.1 for discussion. signed features. <ref type="bibr" target="#b5">[Zeng et al., 2014]</ref> introduced the use of a CNN based model for relation extraction. <ref type="bibr" target="#b6">[Zeng et al., 2015]</ref> proposed a Piecewise Convolutional Neural Network (PCNN) model to preserve the structural features of a sentence using piecewise max-pooling approach, improving the precisionrecall curve significantly. However, PCNN method used only one sentence in the instance-set to predict the relation label and for backpropagation. <ref type="bibr" target="#b2">[Lin et al., 2016]</ref> improves upon PCNN results by introducing an attention mechanism to select a set of sentences from instance set for relation label prediction.</p><p>[ <ref type="bibr" target="#b6">Zheng et al., 2016]</ref> aimed to leverage inter-sentence information for relation extraction in a ranking model. The hypothesis explored is that for a particular entity-pair, each mention alone may not be expressive enough of the relation in question, but information from several mentions may be required to decisively make a prediction. Recently, work by <ref type="bibr" target="#b5">[Ye et al., 2016]</ref> exploit the connections between relation (class ties) to improve relation extraction performance.</p><p>A few papers propose the addition of background knowledge to reduce noise in training data. <ref type="bibr">[Weston et al., 2013]</ref> proposes a joint-embedding model for text and KB entities where the known part of the KB is utilized as part of the supervision signal. <ref type="bibr" target="#b2">[Han and Sun, 2016]</ref> use indirect supervision like consistency between relation labels, consistency between relations and arguments, and consistency between neighbour instances using Markov logic networks. <ref type="bibr" target="#b3">[Nagarajan et al., 2017]</ref> uses inter-instance-set couplings for relation extraction in multi-task setup to improve performance.</p><p>Attention models learn the importance of a feature in the supervised task through back-propogation. Attention mechanisms in neural networks have been successfully applied to a variety of problems, like machine translation <ref type="bibr" target="#b0">[Bahdanau et al., 2014]</ref>, image captioning <ref type="bibr" target="#b4">[Xu et al., 2015]</ref>, supervised relation extraction <ref type="bibr" target="#b4">[Shen and Huang, 2016]</ref>, distantly-supervised relation extraction <ref type="bibr" target="#b6">[Zheng et al., 2016]</ref> etc.</p><p>In our work, we focus on selecting the right words in a sentence using the word and entity-based attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Distant Supervision (DS) has emerged as a promising approach to bootstrap relation extractors with limited supervision. In this paper, we present three novel models for distantlysupervised relation extraction: (1) a Bi-GRU based word attention model (BGWA), (2) an entity-centric attention model (EA), and (3) and a weighted voting ensemble model, which combines multiple complementary models for improved relation extraction. We introduce GDS, a new distant supervision dataset for relation extraction. GDS removes test data noise present in all previous distant supervision benchmark datasets, making credible automatic evaluation possible. Combining proposed methods with attention-based sentence selection methods is left as future work. We plan to make our code and datasets publicly available to foster reproducible research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Multi-instance Multi-label (MIML) learning sample instance for Relation Extraction in Distant Supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Piecewise max-pooling in the PCNN model proposed by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Bi-GRU word attention (BGWA) model (Section 2.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Entity Attention (EA) Model (Section 2.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Confidence scores (indicated by color intensity, darker is better) of models on true labels of 10 randomly sampled instance sets from Google Distant Supervision dataset. Rows represent the instance sets and columns represent the model used for prediction. The heatmap shows complementarity of these models in selecting the right relation. Motivated by this evidence, the proposed Ensemble method combines the three models, viz., Word Attention (BGWA), Entity Attention (EA) and PCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Example of instance set creation in GDS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Precision-recall curves of proposed models against traditional state-of-the-art methods on the Riedel2010-b dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Precision-recall curves of various models on the GDS dataset. Please see Section 4.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>noise in the test data is troublesome as it may lead to incorrect evaluations. There are two kinds of noise added due to distant supervision assumption: (a) samples with incorrect labels due to missing Knowledge Base (KB) fact, and (b) samples with no instance supporting the KB fact. A few examples of such noise are listed inTable 1. Previous benchmark datasets in this area suffer from these drawbacks.</figDesc><table><row><cell>In order to overcome these challenges, we develop Google</cell></row><row><cell>Distant Supervision (GDS), a new dataset for relation extrac-</cell></row><row><cell>tion using distant supervision. Statistics of the new dataset</cell></row><row><cell>are summarized in</cell></row></table><note>Several benchmarks datasets for Relation Extraction (RE) using distant supervision (DS) exist [Riedel et al., 2010; Mintz et al., 2009]. DS is used to create both train and test sets in all of these datasets, resulting in the introduction of noise. While training noise in distant supervision is expected,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>already indicated they will wear no. 42 include ken griffey jr. of cincinnati, florida's dontrelle willis, carlos lee of houston, derrek lee of the cubs and detroit's gary sheffield . , philadelphia's brian dawkins and jacksonville's donovin darius have trained at a mixed martial arts gym.</figDesc><table><row><cell>S.No.</cell><cell>Entity 1</cell><cell>Entity 2</cell><cell>Test Set Label</cell><cell>Classified Relation</cell></row><row><cell>1.</cell><cell>Marlborough</cell><cell>New Hampshire</cell><cell>NA</cell><cell>/location/location/contains</cell></row><row><cell>2.</cell><cell>Katie Couric</cell><cell>CBS</cell><cell>NA</cell><cell>/business/person/company</cell></row><row><cell>S.No.</cell><cell>Entity 1</cell><cell>Entity 2</cell><cell>Test Set Label</cell><cell>Instance Set</cell></row><row><cell cols="4">3. others who have 4. Gary Sheffield Florida /people/person/ place lived Brian Dawkins place of birth Jacksonville /people/person/ according to glazer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Examples of Noise in dataset. Sample 1,2 are incorrectly labelled with NA relation in the test set due to missing facts in KB. While, Sample 4 &amp; 5's single sentence in the instance set does not support the KB relation.</figDesc><table><row><cell>Relation -Class</cell><cell>No. sentences</cell><cell>No. entity-pair</cell></row><row><cell>perGraduatedInstitution</cell><cell>4456</cell><cell>2624</cell></row><row><cell>perHasDegree</cell><cell>2969</cell><cell>1434</cell></row><row><cell>perPlaceOfBirth</cell><cell>3356</cell><cell>2159</cell></row><row><cell>perPlaceOfDeath</cell><cell>3469</cell><cell>1948</cell></row><row><cell>NA</cell><cell>4574</cell><cell>2667</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the new GDS dataset. Please see Section 3 for more details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Riedel2010 was created by aligning Freebase relations with the New York Times corpus[Riedel et al., 2010;<ref type="bibr" target="#b2">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b4">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b2">Lin et al., 2016]</ref> . We partitioned Riedel2010 train set into a new train (80%) and development set (20%). Development set is created to facilitate the learning of an ensemble model and for model selection. This resulting dataset is called Riedel2010-b. Details of the new GDS dataset is described in Section 3.Evaluation Metrics: Following<ref type="bibr" target="#b2">[Lin et al., 2016]</ref>, we use held-out evaluation scheme. The performance of each model is evaluated on a test set using Precision-Recall (PR) curve.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>for model learning. The experiments were run on GeForce GTX 1080 Ti using NVIDIA-CUDA. Model selection for all algorithms was done based on the AUC (Area Under the Curve) metric for the precision-recall curve for development dataset.</figDesc><table><row><cell>Dataset</cell><cell># relation</cell><cell># sentences</cell><cell># entity-pair</cell></row><row><cell cols="4">Reidel2010-b Dataset with development set</cell></row><row><cell>Train</cell><cell>53</cell><cell>455,771</cell><cell>233,064</cell></row><row><cell>Dev</cell><cell>53</cell><cell>114,317</cell><cell>58,635</cell></row><row><cell>Test</cell><cell>53</cell><cell>172,448</cell><cell>96,678</cell></row><row><cell cols="2">GDS Dataset</cell><cell></cell><cell></cell></row><row><cell>Train</cell><cell>5</cell><cell>11297</cell><cell>6498</cell></row><row><cell>Dev</cell><cell>5</cell><cell>1864</cell><cell>1082</cell></row><row><cell>Test</cell><cell>5</cell><cell>5663</cell><cell>3247</cell></row><row><cell>2 http://pytorch.org/</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Statistics of various datasets used in the paper.</figDesc><table><row><cell>Word Embedding Dimension</cell><cell>50</cell></row><row><cell>Word Postion Emb Dimension</cell><cell>5</cell></row><row><cell>Batch Size</cell><cell>50</cell></row><row><cell>SGD Learning Rate</cell><cell>0.1</cell></row><row><cell>Dropout Rate</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Parameter settings</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://research.googleblog.com/2013/04/50000-lessons-onhow-to-read-relation.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bengio ; Yoshua Bengio ; Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J Mooney ;</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
	</analytic>
	<monogr>
		<title level="m">Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches</title>
		<meeting><address><addrLine>Bart Van Merriënboer</addrLine></address></meeting>
		<imprint>
			<publisher>Kyunghyun Cho</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="724" to="731" />
		</imprint>
		<respStmt>
			<orgName>Bunescu and Mooney</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the conference on human language technology and empirical methods in natural language processing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
	<note>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tushar Nagarajan, Sharmistha, and Partha Talukdar. CANDiS: Coupled and attentiondriven neural distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mintz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09942</idno>
		<ptr target="https://research.googleblog.com/2013/04/50000-lessons-on-how-to-read-relation.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="367" to="378" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rahul Gupta Shaohua Sun, Ni Lao and Dave Orr. 50,000 Lessons on How to Read: a Relation Extraction Corpus. Online; accessed 15-NOV-2017</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Oksana Yakhnenko, and Nicolas Usunier. Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang ; Yatian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.7973</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Antoine Bordes</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Neural image caption generation with visual attention</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly extracting relations with class ties via effective deep ranking. CoRR</title>
		<idno>abs/1612.07602</idno>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
	<note>Relation classification via convolutional deep neural network</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
