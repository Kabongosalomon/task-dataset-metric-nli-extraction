<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GDPNet: Refining Latent Multi-View Graph for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
							<email>fuzhao001@e.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
							<email>axsun@</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng</forename><forename type="middle">Siong</forename><surname>Chng</surname></persName>
							<email>aseschng@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GDPNet: Refining Latent Multi-View Graph for Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation Extraction (RE) is to predict the relation type of two entities that are mentioned in a piece of text, e.g., a sentence or a dialogue. When the given text is long, it is challenging to identify indicative words for the relation prediction. Recent advances on RE task are from BERT-based sequence modeling and graph-based modeling of relationships among the tokens in the sequence. In this paper, we propose to construct a latent multi-view graph to capture various possible relationships among tokens. We then refine this graph to select important words for relation prediction. Finally, the representation of the refined graph and the BERT-based sequence representation are concatenated for relation extraction. Specifically, in our proposed GDPNet (Gaussian Dynamic Time Warping Pooling Net), we utilize Gaussian Graph Generator (GGG) to generate edges of the multi-view graph. The graph is then refined by Dynamic Time Warping Pooling (DTWPool). On DialogRE and TACRED, we show that GDPNet achieves the best performance on dialogue-level RE, and comparable performance with the state-of-the-arts on sentence-level RE. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Given two entities and a piece of text where the two entities are mentioned in, the task of relation extraction (RE) is to predict the semantic relation between the two entities. The piece of text serves as the context for the prediction, which can be a short sentence, a long sentence, or even a dialog.</p><p>We use an example from TACRED <ref type="bibr" target="#b30">(Zhang et al. 2017</ref>) to illustrate the RE task. In this example, we are interested in predicting the relation type between two entities: "Cathleen P. Black" and "chairwoman". Based on a sentence: "Carey will succeed Cathleen P. Black, who held the position for 15 years and will take on a new role as chairwoman of Hearst Magazines, the company said", we aim to predict the two entities' relation to be "per:title". Note that, the relation types in RE tasks are predefined. For instance, TARCED defines 41 types and a special "no relation" type if a predicted relation is not covered in the 41 predefined types.</p><p>Observe from the example, only a few words (e.g., "take a new role as") in the given context are related to the semantic relation between the two entities. Most of the remain-Copyright Â© 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 https://github.com/XueFuzhao/GDPNet  <ref type="bibr" target="#b27">(Yu et al. 2020)</ref>. S1: Hey Pheebs. S2: Hey! S1: Any sign of your brother? S2: No, but he is always late. S1: I thought you only met him once? S2: Yeah, I did. I think it sounds y'know big sistery, y'know, 'Frank's always late.' S1: Well relax, he'll be here.</p><p>Argument Pair Trigger Relation type R1 (Frank, S2) brother per:siblings R2 (S2, Pheebs) none per:alternate names ing words in the given sentence are less relevant to the prediction. Another example from DialogRE <ref type="bibr" target="#b27">(Yu et al. 2020)</ref> dataset is shown in <ref type="table" target="#tab_0">Table 1</ref>. Observe that Relation 1 (R1) can be easily predicted based on a trigger word (e.g., "brother"), despite the long conversation between S1 and S2. DialogRE even provides trigger word annotation, which is the smallest span of text that most clearly indicates the existence of the relation between two arguments (see <ref type="table" target="#tab_0">Table 1</ref>). This observation motives us to find and rely more on such indicative words for RE, particularly when the context is long. Graph-based neural models have been widely adopted for RE due to their outstanding performance. Typically, each node in graph represents a token or an entity in the given text. There are multiple ways to construct edges. Many studies rely on an external parser converting text sequences to dependency trees to initialize the graph. Errors made by the parser therefore propagate to the graph. Recent studies directly learn a latent graph from text <ref type="bibr">Ananiadou 2018, 2019;</ref><ref type="bibr" target="#b13">Hashimoto and Tsuruoka 2017)</ref>. The challenge is to handle long texts, as in the example shown in <ref type="table" target="#tab_0">Table 1</ref>. It is difficult to learn latent graphs from long sequences, with token level node representations. <ref type="bibr" target="#b4">Christopoulou, Miwa, and Ananiadou (2019)</ref> simplifies the latent graph by using predefined rules and extra labels, but these rules and labels are not readily available in raw data.</p><p>Similar to many other tasks, BERT-based models have demonstrated effectiveness on both sentence-level RE <ref type="bibr" target="#b25">(Wu and He 2019;</ref><ref type="bibr" target="#b16">Joshi et al. 2020</ref>) and dialogue-level RE <ref type="bibr" target="#b27">(Yu et al. 2020)</ref>. In BERT-based models, the "[CLS]" token is utilized as the task-specific representation for relation prediction. Although BERT can be regarded as a special case of a fully connected graph, it is too large and complex to be treated as a task-specific graph for relation extraction. More importantly, many words in the given context are less relevant to the relation prediction task.</p><p>In this paper, we propose a more general solution for constructing latent graphs without prior knowledge, for RE tasks. We start with a large latent graph initialized with all tokens/entities in the given context as nodes, based on their representations computed by BERT. Then we refine the graph through graph pooling operations with the aim of finding indicative words for relation extraction. In this sense, we focus on refining a task-specific graph on top of BERT, by making full use of BERT token representations. We believe that a small-scale task-specific graph is critical for RE model to capture the relationships among indicative words that contain rich semantic information for relation prediction.</p><p>When learning a latent graph, an edge between two tokens denotes their relationship abstracted from the text sequence. In RE tasks, the relationships between two tokens could be complex, including complicated syntactic relations and abstract semantic relations. Moreover, the relationships between two tokens are asymmetric in RE tasks. Following <ref type="bibr" target="#b14">He et al. (2015)</ref>, we introduce the multi-view graph to fully capture different possible asymmetric relations between two arbitrary tokens. More specifically, we propose a Gaussian Graph Generator (GGG) to initialize the edges of the latent multi-view graph. In GGG, we first encode each node representation into multiple Gaussian distributions. Then the edge weights are computed by measuring the Kullback-Leibler (KL) divergence between the Gaussian distributions of different nodes. Due to the asymmetry of KL divergence, the graph generated by GGG is naturally a directed graph.</p><p>After initialization, the latent multi-view graph is very large, if the input sequence is long. It is difficult for the RE model to focus on the indicative tokens for relation prediction. Thus, we propose a Dynamic Time Warping Pooling (DTWPool) to refine the graph, and to obtain hierarchical representations in an adaptive manner. By the regulation of SoftDTW <ref type="bibr" target="#b5">(Cuturi and Blondel 2017)</ref>, DTWPool refines the latent graph through a lower bound of pooling ratio, and reserves a flexible number of nodes in the multi-view graph. As a result, we obtain a task-specific graph with adaptive size to model the indicative tokens for relation extraction. Our contributions are summarized as follow:</p><p>â¢ We propose a Gaussian Graph Generator (GGG) to initialize edges for latent multi-view graph by measuring KL divergence between different Gaussian distributions of tokens.</p><p>â¢ We propose a graph pooling method, DTWPool, to refine the latent multi-view graph learned from text sequence, with a flexible pooling ratio. To the best of our knowledge, this is the first work on multi-view graph pooling.</p><p>â¢ We combine GGG and DTWPool to form the GDPNet, and evaluate GDPNet on two benchmark datasets for RE. Experimental results demonstrate the effectiveness of GDPNet against SoTA baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>We briefly review the related studies in four aspects, namely, RNN-based, graph-based, and BERT-based relation extraction methods, and graph pooling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-based Relation Extraction</head><p>Early works of relation extraction rely on hand-crafted features to represent pairs of entities <ref type="bibr" target="#b20">(Miwa and Sasaki 2014;</ref><ref type="bibr" target="#b8">Gormley, Yu, and Dredze 2015)</ref>. Model effectiveness highly depends on the quality of hand-crafted features. Current works focus on learning based models, such as recurrent neural network (RNN) for RE. <ref type="bibr" target="#b31">Zhou et al. (2016)</ref> propose bidirectional LSTM model to capture the long-term dependency between entity pairs. <ref type="bibr" target="#b30">Zhang et al. (2017)</ref> present PA-LSTM to encode global position information to boost the performance of RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-based Relation Extraction</head><p>Graph-based models are now widely adopted in RE due to its effectiveness and strength in relational reasoning. <ref type="bibr" target="#b29">Zhang, Qi, and Manning (2018)</ref> utilizes a graph convolutional network (GCN) to capture information over dependency structures.  propose an attention guided GCN (AGGCN) to improve graph representations via selfattention mechanism. AGGCN performs well on sentencelevel RE, but it relies on external parser which may cause error propagation in graph generation. To alleviate error propagation, <ref type="bibr" target="#b3">Christopoulou, Miwa, and Ananiadou (2018)</ref> and <ref type="bibr" target="#b21">Nan et al. (2020)</ref> propose to learn latent graph from text in an end-to-end manner, without the need of dependency trees generated by external parser. <ref type="bibr" target="#b10">Guo et al. (2020)</ref> treats the dependency structure as a latent variable and induces it from the unstructured text in an end-to-end fashion. In our model, we also generate a latent graph, but with two major differences. One is that our graph is a multi-view directed graph aiming to model all possible relationships between tokens. Second is that we focus on refining this multi-view graph to capture important words from long texts, for RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-based Relation Extraction</head><p>Recently, large-scale pre-trained language models, such as BERT, have achieved SoTA performances on many tasks. Several works show that BERT-based models outperform both RNN and graph-based models with a large margin <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Pooling</head><p>Compared with graph convolution, graph pooling is another important but less investigated direction for learning graph. Graphs usually contain different substructures, and different nodes in a graph may play different roles. Hence, simply applying pooling operations like sum or average to encode the global node representations in graph may cause information loss <ref type="formula">(</ref> StructPool proposes a graph pooling method based on conditional random fields, to improve the relation representations of different nodes. However, all these methods have not been evaluated on multi-view graphs, which can model complex relationships between nodes. In this paper, we propose DTWPool to process the latent multi-view graph learned from text sequence. Note that DTWPool is capable of utilizing adaptive pooling ratio instead of a fixed one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary Problem Formulation</head><p>Let X = {x 1 , x 2 , . . . , x T } be a sequence, where x t is the t th token in the sequence, and T is the number of tokens. For sentence-level RE, X denotes the given sentence. For dialogue-level RE, X represents the entire dialogue. That is, in our problem formulation, we do not explicitly distinguish sentence and dialogue. To predict relations, we are given two entities, subject entity X s and object entity X o . Both X s and X o are sub-sequences of X. An entity may contain one or more tokens, e.g., X s = {x s , x s+1 , . . . , x s+mâ1 } where s denotes the starting position of X s in sequence X and m is the number of tokens in X s . Given X, X s , and X o , the goal of relation extraction is to predict the relation r â R between X s and X o , where R is a set of predefined relation types in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-view Graph</head><p>In a multi-view graph, there exist multiple edges between a pair of nodes, each edge from one view. Formally, we can represent a multi-view graph as</p><formula xml:id="formula_0">G = (V, A 1 , A 2 , . . . , A N ),</formula><p>where V is the set of nodes, A is an adjacent matrix, and N is the number of views.</p><p>In our model, we aim to use multi-view graph to model the complex (e.g., syntactic and semantic) relationships between tokens in the given sequence. In this multi-view graph, each token in sequence X corresponds to one node. Note that an entity may contain multiple tokens, and there are multiple ways of handling entity tokens (i.e., tokens in X s and X o ). For easy presentation, we treat tokens in entities the same as other tokens in the sequence in following discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>The overall architecture of GDPNet is shown on the lefthand side in <ref type="figure" target="#fig_1">Figure 1</ref>. There are three key components: BERT module, graph module, and SoftMax classifier. The BERT module encodes tokens into the corresponding feature representations. Illustrated on the right-hand side of <ref type="figure">Figure</ref> 1, the graph module takes in token representations from BERT and constructs a multi-view graph with a Gaussian Graph Generator (GGG). Then the graph is refined through multiple interactions of graph convolution and DTWPool. Finally, the refined latent graph is fed into the SoftMax classifier to predict relation type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Module</head><p>We utilize BERT as the feature encoder to extract token representations due to its effectiveness in representation learning <ref type="bibr" target="#b16">(Joshi et al. 2020;</ref><ref type="bibr" target="#b27">Yu et al. 2020)</ref>. Given a sequence X with T tokens, we map X to a BERT input sequence</p><formula xml:id="formula_1">X input = {x 0 , x 1 , x 2 , . . . , x T , x T +1 }.</formula><p>Here, x 0 denotes the "[CLS]" token which represents the start of sequence X, and x T +1 is the "[SEP]" token which represents the end of the sequence. The corresponding token representations from BERT are denoted by H = {h 0 , h 1 , h 2 , . . . , h T , h T +1 }. Existing BERT-based solutions for RE only take h 0 , i.e., the representation of "[CLS]" token, as the input of SoftMax classifier to predict the relation type <ref type="bibr" target="#b16">(Joshi et al. 2020;</ref><ref type="bibr" target="#b27">Yu et al. 2020)</ref>. In GDPNet, we fully utilize the entire token representations H through the graph module. To be detailed shortly, the graph module learns a task-specific graph using tokens {h 1 , h 2 , . . . , h T , h T +1 }. The learned graph is then combined with h 0 to be the input to the SoftMax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Module</head><p>The graph module consists of Gaussian Graph Generator (GGG), multiple layers of graph convolution and DTW-Pool. The GGG is designed to generate the latent multi-view graph, while the graph convolution and DTWPool layers are applied for graph refinement.</p><p>Gaussian Graph Generator The output representations of BERT module are divided into two parts. h 0 for token "[CLS]" is considered as the task-specific token of the entire sequence, which is the first part. The remaining representations {h 1 , h 2 , ..., h T , h T +1 } form the second part. We generate a multi-view graph from the second part to model the relationships between tokens.</p><p>We denote the initial node representations of the latent graph as</p><formula xml:id="formula_2">V 0 = {v 0 1 , v 0 2 , . . . , v 0 T +1 },</formula><p>where each node corresponds to a token representation. Then, we propose a Gaussian Graph Generator (GGG) to initialize the edges of the latent multi-view graph, based on V 0 . Specifically, we first encode each node v 0 i into multiple Gaussian distributions as:</p><formula xml:id="formula_3">{Âµ 1 i , Âµ 2 i , . . . , Âµ N i } = g Î¸ (v 0 i ) {Ï 1 i , Ï 2 i , . . . , Ï N i } = Ï g Î¸ (v 0 i )<label>(1)</label></formula><p>where g Î¸ and g Î¸ are two trainable neural networks, Ï is a non-linear activation function and N denotes the number of views in the multi-view graph. We set the activation function Ï as the SoftPlus function, since the standard deviation of Gaussian distribution is bounded on (0, +â).</p><p>Consequently, we obtain a number of Gaussian distributions {N n 1 , N n 2 , . . . , N n T +1 } for the n th view of the multi-view graph. Each Gaussian distribution here N n i (Âµ n i , Ï n i 2 ) corresponds to a node representation v 0 i . The purpose of the multi-view graph is to capture all possible relations between tokens, so we encourage message propagation between token representations with large semantic differences. We adopt KL divergence between the Gaussian distributions of two tokens to model edge weight. Specifically, edge weight between i th node and j th node on the n th view is computed as:</p><formula xml:id="formula_4">e n ij = KL N n i (Âµ n i , Ï n i 2 )||N n j (Âµ n j , Ï n j 2 )<label>(2)</label></formula><p>After computing edges between nodes on each view, we obtain multiple adjacent matrices {A 1 , A 2 , ..., A N }, one for each view. Thus, the generated multi-view graph is written as G = (V 0 , A 1 , A 2 , ..., A N ). Due to the asymmetry nature of KL divergence, G is a directed multi-view graph.</p><p>Multi-view Graph Convolution Inspired by , we further employ multi-view graph convolution with dense connections  to capture structural information on the graphs. With the usage of dense connections, we can train a deeper model to capture both local and non-local information. The multi-view graph convolution is written as:</p><formula xml:id="formula_5">v ( ) ni = Ï ï£« ï£­ T j=1 A n ij W ( ) n k ( ) j + b ( ) n ï£¶ ï£¸ (3) where W ( ) n and b ( )</formula><p>n are the trainable weight and bias of the n th view, respectively. Ï denotes an activation function and k ( ) j is the concatenation of the initial node representation and the node representations produced in sub-layers 1, ..., â 1. The output of the first graph convolution layer</p><formula xml:id="formula_6">is V 1 = {v 1 1 , v 1 2 , . . . , v 1 T +1 } = {k ( ) 1 , k ( ) 2 , . . . , k ( ) T +1 }.</formula><p>We refer readers to the Densely Connected Graph Convolutional Network ) for more details.</p><p>Dynamic Time Warping Pooling After graph convolution updates node representations by message propagation, a Dynamic Time Warping Pooling (DTWPool) is introduced to refine the latent multi-view graph. In DTWPool, we first refer to SAGPool <ref type="bibr" target="#b19">(Lee, Lee, and Kang 2019)</ref> to calculate the attention scores on each view of the graph:</p><formula xml:id="formula_7">s ni = Î± ï£« ï£­ T j=1 A n ij W pool v j + b pool ï£¶ ï£¸<label>(4)</label></formula><p>where W pool and b pool are trainable weight and bias of the pooling operation, respectively. Î± denotes an activation function and s ni is the attention weight before the SoftMax activation. For n th view of the latent multi-view graph, we obtain a score set S n = {s n1 , s n2 , . . . , s n T +1 }. We keep the node selection method of SAGPool to retain a portion of nodes in the input graph even when the sizes and structures of the graphs are varied. After node selection, the retained nodes of n th view are a subset of the V 1 , e.g., V 2 n = {v 2 1 , v 2 5 , ..., v 2 T }. As our latent graph has multiple views, we can derive different subsets from V 1 from different views.</p><p>Existing graph pooling approaches, e.g., SAGPool, only allow a fixed ratio for node pooling. Due to the nature of multi-view graph, DTWPool refines the graph adaptively by getting the union set of nodes from different views:</p><formula xml:id="formula_8">V 2 = V 2 1 âª V 2 2 âª . . . âª V 2 N<label>(5)</label></formula><p>where V 2 is the union set of the subsets selected from all different views. If we set a fixed pooling ratio r â [0, 1] on each view, the pooling ratio of DTWPool, e.g., ratio of the number of nodes in V 2 to the number of nodes in V 1 , could be a flexible decimal r real â [r, 1]. We operate the graph convolution and DTWPool iteratively in the graph module, so we have a sequence of graphs {G 1 , G 2 , . . . , G D }, where D is the number of graph pooling layers. The number of informative nodes varies in different text sequences. It is important to preserve important information along the process of graph pooling. The nodes in this graph embed rich context information, so it would be beneficial to summarize the context into the pooled nodes. To this end, we propose to adopt SoftDTW to guide the graph pooling.  <ref type="bibr" target="#b6">(Devlin et al. 2019)</ref> 60.6 (1.2) 55.4 (0.9) 58.5 (2.0) 53.2 (1.6) BERTs <ref type="bibr" target="#b27">(Yu et al. 2020)</ref> 63.0 (1.5) 57.3 (1.2) 61.2 (0.9) 55.4 (0.9) GDPNet (our model) 67.1 (1.0) 61.5 (0.8) 64.9 (1.1) 60.1 (0.9) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Argument Pair Trigger</head><p>Relation type R1 (S1, Frank) high school per:alumni yearbook SoftDTW is a differentiable loss function, designed for finding the best possible alignment between two sequences with different lengths <ref type="bibr" target="#b5">(Cuturi and Blondel 2017)</ref>.</p><formula xml:id="formula_9">DTW Î³ (L 1 , L 2 ) = min Î³ { M, â(L 1 , L 2 ) , M â M} (6)</formula><p>Here, L 1 and L 2 are two sequences of different lengths, â(L 1 , L 2 ) is the cost matrix, and M is a set of binary alignment matrices. In GDPNet, we use the SoftDTW loss to minimize the distance between the original graph and the last pooled graph:</p><formula xml:id="formula_10">L = CSE(r,r) + Î»DTW Î³ (V 1 , V D )<label>(7)</label></formula><p>where L denotes the overall training objective, CSE is the Cross Entropy loss function, Î» is a hyper-parameter to balance the contribution of DTW, and V D is the nodes in the graph after the last DTWPool layer. Because SoftDTW loss is designed for aligning two sequences, we employ it to encourage the nodes after pooling to cover more local context representations. With SoftDTM loss, DTWPool is guided to refine the graph without losing much context information.</p><p>To minimize information loss, we concatenate the node representations of the intermediate graphs created during the pooling process to derive the final graph V , similar to learning graph dense connections . As our pooled graphs have different sizes, we only concatenate the node representations in {V 2 , V 4 , ..., V D } for all nodes that are included in graph V D . Thus, the number of nodes in the final graph V is the same as that in V D .</p><p>Classifier Given the final graph V , we adopt a neural network with max-pooling to compute the representation of the graph. The computed representation is then concatenated with the representation of "[CLS]" token h 0 to form the final representation.</p><formula xml:id="formula_11">h f inal = [h 0 ; f (V )]<label>(8)</label></formula><p>Here, f is a neural network with max-pooling, which maps the V â R QÃT to f (V ) â R qÃ1 , Q = D * q. D is the number of graph pooling layers in graph module, q is the dimension of token representation, and T is the number of nodes in V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Our proposed GDPNet can be applied to both sentence-level and dialogue-level RE tasks. Due to the differences in data formats, applicable baseline models, and the way in handling subject and object entities X s and X o , we conduct two sets of experiments, comparing GDPNets to SoTA models on the two tasks. We also show how GDPNet can be easily modified to achieve a fair comparison with SoTA models on each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue-level Relation Extraction</head><p>DialogRE is the first human-annotated dialogue-level RE dataset <ref type="bibr" target="#b27">(Yu et al. 2020)</ref>. It contains 1, 788 dialogues originating from the complete transcripts of a famous American television situation comedy. There are 36 relation types predefined in DialogRE. An example is given in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Models and Experimental Setup</head><p>We evaluate GDPNet against the recently proposed BERTs <ref type="bibr" target="#b27">(Yu et al. 2020)</ref>. BERTs is a speaker-aware modification of BERT, and achieves best performance on dialogue-level RE. For the completeness of experiments, we also include popular baseline models: CNN, LSTM, BiLSTM and BERT models. For fair comparison, we use the same input format and hyperparameter settings as in BERTs. Specifically, the given X, X s , and X o , are concatenated with classification token [CLS] and separator token [SEP] to form an input sequence [CLS]X[SEP]X s [SEP]X o [SEP]. All token representations except [CLS] are fed into our graph module. To incorporate speaker information, for the sentences that contain X s or X o , the text indicating speaker e.g., "Speaker 1", is replaced by a specific token, [S 1 ] or [S 2 ]. Note that the trigger words are treated as normal tokens. Adam (Kingma and Ba 2015) with learning rate of 3eâ5 is employed and the lower bound of pooling ratio is set to 0.7. We use 3 DTWPool layers. As graph pooling operation is performed in each DTWPool layer, only a few nodes are included in the final graph. We use both F 1 and F 1c scores as the evaluation metrics. F 1c is proposed by <ref type="bibr" target="#b27">Yu et al. (2020)</ref>, and it is computed by only taking in the early part of a dialogue as input, instead of the entire dialogue. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results on DialogRE. Observe that BERT-based models significantly outperform CNN and LSTM-based models. BERTs is superior to BERT because BERTs incorporates speaker-related information. Following the same input format and setting, GDPNet is built on top of BERTs. That is, BERTs acts as the BERT module for feature extraction in GDPNet (see <ref type="figure" target="#fig_1">Figure 1)</ref>. Shown in <ref type="table" target="#tab_2">Table 2</ref>, GDPNet outperforms BERTs by 3.7 and 4.7 points in F 1 and F 1c, respectively, on test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on DialogRE</head><p>GDPNet is designed to find key information from long sequences for effective RE. Thus, we expect that GDPNet is capable of tackling long sequences better. We group the dialogues in DialogRE test set into five subsets by their length, i.e., number of tokens. <ref type="figure" target="#fig_2">Figure 2</ref> reports F 1 scores of GDP-Net and BERTs on the five subsets. GDPNet consistently outperforms BERTs when dialogue length is more than 100 tokens. In particular, GDPNet surpasses BERTs by a large margin on the dialogues with over 400 tokens. This comparison shows that GDPNet is effective in modeling long sequences through refining the latent graph built on top of token representations.</p><p>As an example, <ref type="table" target="#tab_3">Table 3</ref> shows the tokens selected by GDPNet on the first dialogue in the test set of DialogRE. Our model selects informative tokens like "her" "school" to predict the "per:alumn" relation. Some less related tokens, like "you're actually 50?", are ignored by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We conduct ablative experiments on Di-alogRE to evaluate the effectiveness of the two main components in GDPNet, i.e., Gaussian Graph Generator and Dynamic Time Warping Pooling. The results are reported in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>We first replace the multi-view graph by a simple homogeneous graph, which is the same as setting the number of views to one. The performance degradation suggests that multi-view graph is beneficial as it models complex relationships among tokens. Next, we evaluate GGG by replac- ing GGG with multi-head attention. The results show that without GGG, performance drops. GGG initializes the latent graph by measuring the difference between two Gaussian distributions generated from node representations, which decouples the dependency between token representations and graph edges. To evaluate the impact of DTWPool, we first drop DTWloss by removing the second term in Equation 7, and there is a slight performance drop. Without DTWloss, DTWPool degenerates into the multi-view version of SAG-Pool <ref type="bibr" target="#b19">(Lee, Lee, and Kang 2019)</ref>. This result indicates that DTWPool outperforms SAGPool when tackling the multiview graph learned from token sequence. When all the DTWPool layers are removed, the performance of GDPNet decreases dramatically, which shows that DTWPool is crucial for the GDPNet. After removing both GGG and DTW-Pool, the performance of GDPNet is even worse. To summarize, DTWPool is crucial for learning a task-specific graph from a large latent multi-view graph. The final graph learned effectively filters out less useful information from a long sequence for effective relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-level Relation Extraction</head><p>We evaluate GDPNet for sentence-level RE on two datasets TACRED <ref type="bibr" target="#b30">(Zhang et al. 2017</ref>) and TACRED-Revisit <ref type="bibr" target="#b0">(Alt, Gabryszak, and Hennig 2020)</ref>. TACRED is a widely used large-scale sentence-level relation extraction dataset. It contains more than 106K sentences drawn from the yearly TACKBP4 challenge, and 42 different relations (41 common relation types and a special "no relation" type). The subject mentions in TACRED are person and organization, while object mentions are in 16 fine-grained types, including date, location, etc. The TACRED-Revisit dataset, released recently, corrects the wrong labels in the development and test sets of TACRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Models and Experimental Setup</head><p>To the best of our knowledge, SpanBERT <ref type="bibr" target="#b16">(Joshi et al. 2020</ref>) is the best performing sentence-level RE model without incorporating any external knowledge and parser. We consider SpanBERT as a strong baseline to benchmark our GDPNet. We also include RNN-and graph-based models. Meanwhile, we report the results of KnowBERT, which incorporates external resources for training <ref type="bibr" target="#b22">(Peters et al. 2019</ref>). : Performance of all models on TACRED and TACRED-Revisit. For the models without reported performance on TACRED-Revisit, we run the released code if available, and mark results obtained by asterisk(*). We also run the released code of SpanBERT on TACRED-Revisit, and we obtain the same results as reported in <ref type="bibr" target="#b0">(Alt, Gabryszak, and Hennig 2020</ref>  We use the same input format and hyperparameter settings as in SpanBERT. Subject entity X s and object entity X o are each replaced by a sequence of "[SUBJ-NER]" or "[OBJ-NER]" tokens. Then [CLS]X[SEP] forms the input to the models. This is different from the settings in DialogRE where X s and X o are appended to the input sequence X. Parameter optimization is again performed by Adam <ref type="bibr" target="#b17">(Kingma and Ba 2015)</ref> with learning rate of 2eâ5. Since the sequence length in TACRED is much shorter than that in DialogRE, we set the lower bound of pooling ratio to 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on TACRED</head><p>The results on TACRED and TACRED-Revisit are summarized in <ref type="table" target="#tab_5">Table 5</ref>. Similar observations hold, that BERT-based models (i.e., SpanBERT and KnowBERT), significantly outperform non-BERT models (i.e., LSTM, PA-LSTM, C-AGGCN and LST-AGCN) on both versions of TACRED.</p><p>GDPNet achieves comparable performance with Span-BERT 2 on TACRED, and better results on TACRED-Revisit. Compared to KnowBERT, which utilize external knowledge in its training, GDPNet's F 1 is lower by 1 point on TA-CRED, but is higher by almost 1 point on TACRED-Revisit.</p><p>Compared to dialogue, sentence is much shorter and BERT-based models are effective in capturing the key information. As GDPNet is designed for handling long se-quences, we do not expect it to outperform SoTA models, but GDPNet remains competitive for sentence-level RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Analysis</head><p>Our last experiment is to analyze DTWPool in GDPNet. DTWPool aims to identify indicative tokens for relation extraction through refining the latent multi-view graph. Table 6 reports the percentage of tokens selected in the final graph after the DTWPool process, on both DialogRE and TACRED datasets. We separate the repetitive tokens and non-repetitive tokens based on the original input, i.e., whether the word appears only once or multiple times in the input sequence. Repetitive tokens, in general, define the topic of the sentence or dialogue. However, the relation type between two entities is seldom described repetitively. In fact, given the same dialogue, we may predict different relation types between different pairs of entities. With this in mind, we consider repetitive tokens are less important compared to non-repetitive tokens for RE tasks in general. Shown in <ref type="table" target="#tab_7">Table 6</ref>, DTWPool selects more non-repetitive tokens than repetitive tokens on both datasets, in particular, on the DialogRE dataset. More interestingly, DialogRE provides manually annotated trigger tokens that are indicative to the relation type. DTWPool selects 32.1% of trigger tokens, given that only 15.6% of tokens are selected among all tokens. That is, trigger tokens are selected with a much higher chance than random. This analysis shows that DTWPool is capable of selecting indicative tokens for relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose GDPNet for relation extraction. GDPNet is designed to find indicative words from long sequences (e.g., dialogues) for effective relation extraction. We show that GDPNet achieves the best performance on dialogue-level RE. In particular, GDPNet achieves much better performance than BERT-based models when the dialogue is long. The key of the GDPNet is to construct a latent multi-view graph to model possible relationships among tokens in a long sequence, and then to refine the graph by DTWPool. From the results on DialogRE and TACRED, we show there is a great potential of this mechanism in dealing with long sequences. To evaluate the effectiveness of this mechanism on other tasks is part of our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc><ref type="bibr" target="#b25">Wu and He 2019;</ref><ref type="bibr" target="#b16">Joshi et al. 2020;</ref><ref type="bibr" target="#b27">Yu et al. 2020)</ref>.<ref type="bibr" target="#b16">Joshi et al. (2020)</ref> propose SpanBERT to learn better representations, and achieve SoTA performance on TACRED<ref type="bibr" target="#b30">(Zhang et al. 2017)</ref>, a sentence-level RE dataset. For dialogue-level RE task,<ref type="bibr" target="#b27">Yu et al. (2020)</ref> present a BERTs model, which takes the speaker information into consideration and achieves the best result. In our solution, the multi-view graph is built on top of token representations by BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The overall architecture of the proposed GDPNet. Entities E 1 and E 2 are single-token entities in the illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>GDPNet and BERTs on different dialogue lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example from DialogRE dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of all models on DialogRE. Ï denotes the standard deviation computed from five runs of each model.</figDesc><table><row><cell>Model</cell><cell>Dev set F 1 (Ï) F 1c (Ï)</cell><cell>Test set F 1 (Ï) F 1c (Ï)</cell></row><row><cell>CNN (Lawrence et al. 1997)</cell><cell cols="2">46.1 (0.7) 43.7 (0.5) 48.0 (1.5) 45.0 (1.4)</cell></row><row><cell cols="3">LSTM (Hochreiter and Schmidhuber 1997) 46.7 (1.1) 44.2 (0.8) 47.4 (0.6) 44.9 (0.7)</cell></row><row><cell>BiLSTM (Graves and Schmidhuber 2005)</cell><cell cols="2">48.1 (1.0) 44.3 (1.3) 48.6 (1.0) 45.0 (1.3)</cell></row><row><cell>BERT</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Text of the first dialogue in test set of DialogRE. No-no, that's not, that's not me Phoebe, that's her pal Phoebe. According to her high school yearbook, they were like B.F.F. Best Friends Forever.</figDesc><table><row><cell>The tokens in bold are selected by GDPNet.</cell></row><row><cell>S1: Hey, you guys! Look what I found! Look at this!</cell></row><row><cell>That's my Mom's writing! Look.</cell></row><row><cell>S2: Me and Frank and Phoebe, Graduation 1965.</cell></row><row><cell>S1: Y'know what that means?</cell></row><row><cell>S3: That you're actually 50?</cell></row><row><cell>S1:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>An ablation study on GDPNet model. For models without GGG, we replaced GGG by multi-head attention to initialize the edges of the multi-view graph. For the model without DTWloss, we remove the second term in Equation 7. All DTWPool layers are removed for the models without DTWPool.</figDesc><table><row><cell>Model</cell><cell>F 1(Ï)</cell><cell>F 1c(Ï)</cell></row><row><cell>GDPNet</cell><cell cols="2">64.9 (1.1) 60.1 (0.9)</cell></row><row><cell cols="3">with Homogeneous GGG 63.5 (0.7) 58.4 (0.6)</cell></row><row><cell>w/o GGG</cell><cell cols="2">62.1 (1.6) 58.1 (1.1)</cell></row><row><cell>w/o DTWloss</cell><cell cols="2">63.4 (1.4) 58.6 (1.3)</cell></row><row><cell>w/o DTWPool</cell><cell cols="2">48.9 (1.1) 22.4 (1.0)</cell></row><row><cell>w/o GGG &amp; DTWPool</cell><cell cols="2">48.2 (1.4) 21.8 (1.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Model</cell><cell>P r</cell><cell>TACRED Re</cell><cell>F 1</cell><cell cols="3">TACRED-Revisit P r Re F 1</cell></row><row><cell>LSTM (Zhang et al. 2017)</cell><cell cols="6">65.7 59.9 62.7 71.5* 69.7* 70.6*</cell></row><row><cell>PA-LSTM (Zhang et al. 2017)</cell><cell cols="6">65.7 64.5 65.1 74.5* 74.1* 74.3*</cell></row><row><cell cols="7">C-AGGCN (Guo, Zhang, and Lu 2019) 73.1 60.9 68.2 77.7* 73.4* 75.5*</cell></row><row><cell>LST-AGCN (Sun et al. 2020)</cell><cell>-</cell><cell>-</cell><cell cols="2">68.8 -</cell><cell>-</cell><cell>-</cell></row><row><cell>SpanBERT (Joshi et al. 2020)</cell><cell cols="6">70.8 70.9 70.8 75.7* 80.7* 78.0*</cell></row><row><cell>GDPNet (Our model)</cell><cell cols="4">72.0 69.0 70.5 79.4</cell><cell>81.0</cell><cell>80.2</cell></row><row><cell>KnowBERT (Peters et al. 2019)</cell><cell cols="4">71.6 71.4 71.5 -</cell><cell>-</cell><cell>79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Percentage (%) of the tokens selected in the final graph from sequence, e.g., an entire dialogue in DialogRE or the whole sentence in TACRED. Non-repetitive tokens are the tokens that appear only once in the sequence; repetitive tokens appear two or more times in the sequence. Trigger tokens are key tokens annotated in DialogRE.</figDesc><table><row><cell>Type of tokens</cell><cell cols="2">DialogRE TACRED</cell></row><row><cell>All tokens</cell><cell>15.6</cell><cell>66.3</cell></row><row><cell>Non-repetitive tokens</cell><cell>23.5</cell><cell>67.6</cell></row><row><cell>Repetitive tokens</cell><cell>10.0</cell><cell>58.1</cell></row><row><cell>Trigger tokens</cell><cell>32.1</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A very recent study) also combines Span-BERT with GCN, but it still relies on external parser for graph generation. In contrast, our GDPNet regards the graph as latent variable, which is more general and feasible.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Aixin Sun is supported by the Agency for Science, Technology and Research (A*STAR) AME Programmatic Fund (Grant No. A19E2b0098). Hao Zhang is supported by A*STAR AME Programmatic Funds (Grant No. A18A1b0045 and A18A2b0046).</p><p>We set hyper-parameters (epoch, batch size, learning rate and dropout rate) of the backbone model the same as the corresponding SoTA models for fair comparison, i.e., BERTs <ref type="bibr" target="#b27">(Yu et al. 2020)</ref> for DialogRE, and SpanBERT <ref type="bibr" target="#b16">(Joshi et al. 2020</ref>) for TACRED.</p><p>The hidden units of graph, number of views, and number of DTWPool layers are set according to AGGCN . Although AGGCN does not contain graph pooling operation, we use the same number of GCN layers and DTWPool layers in our evaluation.</p><p>We set a higher pooling ratio for the TACRED dataset due to its shorter sequence length compared to DialogRE. Consequently, the ratio of nodes retained in the final graph of TACRED is higher than that of DialogRE, which leads to lower SoftDTW loss for TACRED compared to DialogRE. For this reason, we set a larger weight of SoftDTW loss for TACRED.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Software Packages and Hardware Specification</head><p>The GDPNet is implemented by using PyTorch 1.4 with CUDA 10.1. Our implementation also uses the SoftDTW 3 toolkit. All experiments are conducted on a desktop with Intel i7-8750H CPU, DDR4 16GB memory, and a single NVIDIA GeForce RTX 1070 GPU. We also reproduced our results on Quadro RTX 8000 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-Parameter Settings</head><p>The hyper-parameter settings on the two datasets, DialogRE and TACRED, are listed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>DialogRE TACRED </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hennig</surname></persName>
		</author>
		<ptr target="https://github.com/Maghoumi/pytorch-softdtw-cuda" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1558" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient long-distance relation extraction with DG-SpanBERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoehndorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ArXiv abs/2004.03636</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Walk-based Model on Entity Graphs for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Papers; Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Soft-DTW: a Differentiable Loss Function for Time-Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D.</editor>
		<editor>and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="894" to="903" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph U-Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved Relation Extraction with Feature-Rich Compositional Embedding Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Latent Forests for Medical Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention Guided Graph Convolutional Networks for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural Machine Translation with Source-Side Latent Graph Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to Represent Knowledge Graphs with Gaussian Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face recognition: a convolutional neural-network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ah Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-Attention Graph Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
		<respStmt>
			<orgName>Long Beach</orgName>
		</respStmt>
	</monogr>
	<note>California</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling Joint Entity and Relation Extraction with Table Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning with Latent Structure Refinement for Document-Level Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge Enhanced Contextual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relation Extraction with Convolutional Network over Learnable Syntax-Transport Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mensah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8928" to="8935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enriching Pre-Trained Language Model with Entity Information for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2361" to="2364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dialogue-Based Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4927" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">StructPool: Structured graph pooling via conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Position-aware Attention and Supervised Data Improve Slot Filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germany</forename><surname>Berlin</surname></persName>
		</author>
		<title level="m">Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
