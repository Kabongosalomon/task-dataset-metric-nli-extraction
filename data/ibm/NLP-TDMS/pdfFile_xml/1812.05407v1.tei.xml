<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Abstractive Text Summarization by Incorporating Reader Comments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
							<email>pijili@tencent.com</email>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
							<email>renzhaochun@jd.com</email>
							<affiliation key="aff3">
								<orgName type="department">JD.com</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<email>l.bing@alibaba-inc.com</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">R&amp;D Center Singapore</orgName>
								<orgName type="department" key="dep2">Machine Intelligence Technology</orgName>
								<orgName type="department" key="dep3">Alibaba DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<email>ruiyan@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Abstractive Text Summarization by Incorporating Reader Comments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In neural abstractive summarization field, conventional sequence-to-sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect. To tackle this problem, we propose the task of reader-aware abstractive summary generation, which utilizes the reader comments to help the model produce better summary about the main aspect. Unlike traditional abstractive summarization task, reader-aware summarization confronts two main challenges: (1) Comments are informal and noisy;</p><p>(2) jointly modeling the news document and the reader comments is challenging. To tackle the above challenges, we design an adversarial learning model named reader-aware summary generator (RASG), which consists of four components:</p><p>(1) a sequence-to-sequence based summary generator; (2) a reader attention module capturing the reader focused aspects;</p><p>(3) a supervisor modeling the semantic gap between the generated summary and reader focused aspects; (4) a goal tracker producing the goal for each generation step. The supervisor and the goal tacker are used to guide the training of our framework in an adversarial manner. Extensive experiments are conducted on our large-scale real-world text summarization dataset, and the results show that RASG achieves the stateof-the-art performance in terms of both automatic metrics and human evaluations. The experimental results also demonstrate the effectiveness of each module in our framework. We release our large-scale dataset for further research 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Abstractive summarization can be regarded as a sequence mapping task that the source text is mapped to the target summary, and has drawn much attention since the deep neural networks are widely applied in natural language processing field. Recently, sequence-to-sequence (seq2seq) framework <ref type="bibr" target="#b26">(Sutskever, Vinyals, and Le 2014)</ref> has been proved effective for the task of abstractive summarization <ref type="bibr" target="#b4">(Chopra, Auli, and Rush 2016;</ref><ref type="bibr" target="#b24">See, Liu, and Manning 2017)</ref> and other text generation tasks <ref type="bibr" target="#b6">Gao et al. 2019)</ref>. In this paper, we use "aspect" to denote the topic described in a specific paragraph or a sentence of a news document, and use "main aspect" to denote the central <ref type="table">Table 1</ref>: Examples of the text summarization. The text in red denotes the focused aspect by the good summary, while the text in blue is described by the bad summary. The text with underline is the focused aspect by reader comments. <ref type="bibr">document</ref> On August 28, according to a person familiar with the matter, Toyota Motor Corporation will invest 500 million U.S. dollars into the Uber, a taxi service company, with a valuation of up to 72 billion U.S. dollars. The investment will focus on driverless car technology. However, its development path is not smooth. In March of this year, a Uber driverless car hit a woman and caused her death. In last year, Softbank also invested into Uber with a valuation of $48 billion.</p><p>comments Toyota's investment in Uber is a wise choice. $500 million investment is really a lot of money! good summary Toyota invests $500 million into Uber with a valuation of $72 billion bad summary An Uber driverless car hits a passerby to death topic which the author tends to convey to the readers. Although a document may describe an event in many different aspects, the summary of this document should always focus on the main aspect. As shown in <ref type="table">Table 1</ref>, the good summary describes the main aspect and the bad summary describes another trivial aspect that is not the main point of the document. To focus on the main aspect, some summarization methods <ref type="bibr" target="#b31">Zhou et al. 2017;</ref><ref type="bibr" target="#b2">Bansal and Chen 2018)</ref> first select several sentences about the main aspect and then generate the summary. However, it is very challenging to discover which is the main aspect of the news document. Nowadays, a great number of news comments are generated by readers to express their opinions about the event. Some comments may mention the main aspect of the document for several times. Take the case in <ref type="table">Table 1</ref> as an example, the focused aspect of the reader is "investment of Toyota" which is also the main aspect of this document. To be specific, we define "reader focused aspect" to denote the focused aspect by a reader through the comments. Intuitively, these reader comments may help the summary generator capture the main aspect of document, thereby improving the quality of the generated summary. Therefore, in this paper, we investigate a new problem setting of the task of abstractive text summarization. We name such paradigm of extension as reader-aware abstractive text summarization.</p><p>The effect of comments or social contexts in document summarization have been explored by several previous works <ref type="bibr" target="#b11">(Hu, Sun, and Lim 2008;</ref><ref type="bibr" target="#b30">Yang et al. 2011;</ref><ref type="bibr" target="#b13">Li et al. 2015;</ref>. Unlike these approaches that directly extract sentences from the original document <ref type="bibr" target="#b11">(Hu, Sun, and Lim 2008;</ref><ref type="bibr" target="#b30">Yang et al. 2011;</ref><ref type="bibr" target="#b13">Li et al. 2015)</ref>, we aim to generate a natural-sounding summary from scratch instead of extracting words from the document.</p><p>Generally, existing text summarization approaches confront two challenges when addressing reader-aware summarization task. The first challenge is that reader comments are very noisy and informative. Not all the information provided by the comments is useful when modeling the reader focused aspects. Therefore, it is crucial to make the model own the ability of capturing main aspect and filtering noisy information when incorporating reader comments. The second challenge is how to generate summaries by jointly modeling the main aspect of document and the reader focused aspect revealed by comments. Meanwhile, the model should not be sensitive to the diverse unimportant aspects introduced by some reader comments. Thus, simply absorbing all the reader aspect information to directly guide the model to generate summary is not feasible, as it will make the generator lose the ability of modeling the main aspect.</p><p>In this paper, we propose a summarization framework named reader-aware summary generator (RASG) that incorporates reader comments to improve the summarization performance. Specifically, a seq2seq architecture with attention mechanism is employed as the basic summary generator. We first calculate alignment between the reader comments words and document words, and this alignment information is regarded as reader attention representing the "reader focused aspect". Then, we treat the decoder attention weights as the focused aspect of the generated summary, a.k.a., "decoder focused aspect". After each decoding step, a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect. Given this distance, a goal tracker provides the goal to the decoder to induce it to reduce this distance. The training of our framework RASG is conducted in an adversarial way. To evaluate the performance of our model, we collect a large amount of document-summary pairs associated with several reader comments from social media website. Extensive experiments conducted on this dataset show that RASG significantly outperforms the state-of-the-art baselines in terms of ROUGE metrics and human evaluations.</p><p>To sum up, our contributions can be summarized as follows:</p><p>• We propose a reader-aware abstractive text summarization task. To solve this task, we propose an end-to-end learning framework to conduct the reader attention modeling and reader-aware summary generation.</p><p>• We design a supervisor as well as a goal tracker to guide the generator to focus on the main aspect of the document.</p><p>• To reduce the noisy information introduced by the reader comments, we propose a denoising module to identify which comments are helpful for summary generation auto-matically.</p><p>• We release a large scale abstractive text summarization dataset associated with reader comments. Experimental results on this dataset demonstrate the effectiveness of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Text summarization can be classified into extractive and abstractive methods. Extractive methods <ref type="bibr" target="#b12">(Jadhav and Rajan 2018;</ref><ref type="bibr" target="#b22">Narayan, Cohen, and Lapata 2018)</ref> read the article and get the representations of the sentences and article to select sentences. However, summaries generated by extractive methods always suffer from redundancy problem. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization is dedicated to abstractive summarization <ref type="bibr" target="#b2">(Bansal and Chen 2018;</ref><ref type="bibr" target="#b19">Ma et al. 2018b;</ref><ref type="bibr" target="#b32">Zhou et al. 2018</ref>). On the text summarization benchmark dataset CNN/DailyMail, the state-ofthe-art abstractive methods outperform the best extractive method in terms of ROUGE score. Most methods for abstractive text summarization are based on the sequence-tosequence model <ref type="bibr" target="#b26">(Sutskever, Vinyals, and Le 2014)</ref>, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder. To tackle the out-of-vocabulary problem, some researchers employ the copy mechanism to copy some words from the input document to summary <ref type="bibr" target="#b7">(Gu et al. 2016;</ref><ref type="bibr" target="#b24">See, Liu, and Manning 2017)</ref>. To capture the main aspect of document, <ref type="bibr" target="#b3">Chen et al. (2018)</ref> propose to select salient sentences and then rewrite these sentences to a concise summary. This approach achieves the state-of-theart of text summarization on CNN/DailyMail benchmark dataset. Unlike document summarization that needs to encode a long text, social media summarization usually reads short and noisy text and has become a popular task these days. After <ref type="bibr" target="#b9">Hu et al. (2015)</ref> propose a short text summarization dataset on social media and many researchers follow this task. <ref type="bibr" target="#b16">Lin et al. (2018)</ref> propose a seq2seq based model which uses an CNN to refine the representation of source context. <ref type="bibr" target="#b29">Wang et al. (2018)</ref> use convolutional seq2seq model to summarize text and use the policy gradient algorithm to directly optimize the ROUGE score. However, these summarization models do not utilize the reader's comments in generating summaries.</p><p>To consider the reader's comments into text summarization, the reader-aware summarization is proposed and it mainly takes the form of extractive approaches. Graph-based method has been used for comment oriented summarization task such as <ref type="bibr" target="#b10">(Hu, Sun, and Lim 2007;</ref><ref type="bibr" target="#b11">2008)</ref>, where they identify three relations (topic, quotation, and mention) by which comments can be linked to one another. Recently, <ref type="bibr" target="#b23">Nguyen et al. (2016)</ref> publish a small extractive sentence-comment dataset which can not be used to train neural models due to its small size. <ref type="bibr" target="#b13">Li et al. (2015)</ref> propose an unsupervised compressive multi-document summarization model using sparse coding method. Following previous work, there are some models ) using variational auto-encoder to model the latent semantic of original article and reader comments. Different from our abstractive summarization task, these related works are all based on extractive or compressive approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><p>Before presenting our approach for the reader-aware summarization, we first introduce our notations and key concepts.</p><p>To begin with, for a document</p><formula xml:id="formula_0">X d = {x d 1 , x d 2 , . . . , x d T d }, we assume there is a comment set X c = {c 1 , c 2 , . . . , c T c } where c i = {x c i,1 , x c i,2 , . . . , x c i,T c i } is the i-th comment, x d i</formula><p>denotes the i-th word in document X d , and x c i,j denotes the j-th word in i-th comment sentence c i . Given the document X d , the summary generator reads the comments X c , then generates a summaryŶ = {ŷ 1 ,ŷ 2 , . . . ,ŷ TŶ }. Finally, we use the difference between generated summaryŶ and ground truth summary Y as the training signal to optimize the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed RASG Model Overview</head><p>In this section, we propose our reader-aware summary generator, abbreviated as RASG. The overview of RASG is shown in <ref type="figure">Figure 1</ref> which can be split into four main parts:</p><p>• Summary generator is a seq2seq based architecture with attention and copy mechanisms.</p><p>• Reader attention module learns a semantic alignment between each word in document and comments, thus captures the reader focused aspect.</p><p>• Supervisor measures the semantic gap between decoder focused aspect and reader focused aspect. There is also a discriminator which uses convolutional neural network to extract features and then distinguishes how similar is decoder focused aspect to reader focused aspect.</p><p>• Goal tracker utilizes the semantic gap learned by supervisor and the features extracted learned by the discriminator to set a goal, which is further utilized as a more specific guidance for summary generator to produce better summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary generator</head><p>At the beginning, we use an embedding matrix e to map onehot representation of each word in the document X d and comments X c to a high-dimensional vector space. We denote e(x) as the embedding representation of word x. From these embedding representations, we employ a bi-directional recurrent neural network (Bi-RNN) to model the temporal interactions between words:</p><formula xml:id="formula_1">h d t = Bi-RNN d (e(x d t ), h d t−1 ),<label>(1)</label></formula><p>where h d t denotes the hidden state of t-th step in Bi-RNN for document X d . We denote the final hidden state h d T d of Bi-RNN d as the vector representation of the document X d . Following <ref type="bibr" target="#b24">(See, Liu, and Manning 2017;</ref><ref type="bibr" target="#b18">Ma et al. 2018a</ref>), we choose the long short-term memory (LSTM) as the Bi-RNN cell.</p><p>Then we apply a linear transform layer on the input document vector representation h d T d and use the output of this layer as the initial state of decoder LSTM, shown in Equation 2. In order to reduce the burden of compressing document information into initial state s 0 , we use the attention mechanism <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref> to summarize the input document into context vector f t−1 dynamically and we will show the detail of these in the following sections. We then concatenate the context vector f t−1 with the embedding of previous step output e(y t−1 ) and feed this into decoder LSTM, shown in Equation 3. We use the notion [·; ·] as the concatenation of two vectors.</p><formula xml:id="formula_2">s 0 = W d h d T d + b d ,<label>(2)</label></formula><formula xml:id="formula_3">s t = LSTM (s t−1 , [f t−1 ; e(y t−1 )]) .<label>(3)</label></formula><p>At t-th decoding step, we use the decoder state s t−1 to attend to each the document states h d and resulting in the attention distribution α t ∈ R T d , shown in Equation 5. Then we use the attention distribution α t to weighted sum the document states as the context vector f t−1 .</p><formula xml:id="formula_4">α t,i = W a tanh W s s t + W h h d i ,<label>(4)</label></formula><formula xml:id="formula_5">α t,i = exp α t,i / T d j=1 exp α t,j ,<label>(5)</label></formula><formula xml:id="formula_6">f t−1 = T d i=1 α t,i h d i .<label>(6)</label></formula><p>Finally, an output projection layer is applied to get the final generating distribution P v over vocabulary, as shown in Equation 7. We concatenate goal vector g t , gap content d t , and the output of decoder LSTM s t as the input of the output projection layer. The goal vector g t represents the goal of current generation step, the gap content d t denotes the semantic gap between generated summary and reader focused document and we will show the details of these variables in the following sections.</p><formula xml:id="formula_7">P v = softmax (W v [s t ; g t ; d t ] + b v ) ,<label>(7)</label></formula><p>In order to handle the out-of-vocabulary (OOV) problem, we equip the pointer network <ref type="bibr" target="#b7">(Gu et al. 2016;</ref><ref type="bibr" target="#b28">Vinyals, Fortunato, and Jaitly 2015;</ref><ref type="bibr" target="#b24">See, Liu, and Manning 2017)</ref> with our decoder, which makes our decoder capable to copy words from the source text. The design of the pointer network is the same as the model used in <ref type="bibr" target="#b24">(See, Liu, and Manning 2017)</ref>, thus we omit this procedure in our paper due to the limited space. We use the negative log-likelihood as the loss function:</p><formula xml:id="formula_8">L g = − TŶ t=1 log P v (y t ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denoising module</head><p>Due to the fact that reader comments are a kind of informal text, they may consist of many noisy information, and not all the comments are helpful for generating better summaries. Consequently, we employ a denoising module to distinguish which comments are helpful. First, we employ an Bi-RNN c to model the comment word embeddings:</p><formula xml:id="formula_9">h c i,t = Bi-RNN c (h c i,t−1 , e(x c i,t )),<label>(9)</label></formula><p>where h c i,t denotes the hidden state of t-th word in i-th comment c i . Next, we use average-pooling operation over these Context Vector</p><p>(1) Summary Generator <ref type="figure">Figure 1</ref>: Overview of RASG. We divide our model into four parts: (1) Summary generator generates a summary to describe the main aspect of document.</p><p>(2) Reader attention module models the readers attention of document.</p><p>(3) Supervisor models the gap of focused document aspect between generated summary and reader comments. (4) Goal tracker sets a goal of summary generator according to gap given by supervisor.</p><p>hidden states to produce a vector representation a i of i-th comment, shown in Equation 10. Finally, we apply a linear transform with sigmoid function to predict whether the comment is useful, and the sigmoid outputβ i ∈ (0, 1) also can be seen as a salience score of i-th comment given the document representation</p><formula xml:id="formula_10">h d T d . a i = avg({h c i,1 , h c i,2 , . . . , h c i,T c i }),<label>(10)</label></formula><formula xml:id="formula_11">β i = sigmoid(W s [a i ; h d T d ] + b i ),<label>(11)</label></formula><p>To train the denoising module, we use the cross entropy loss to supervise this procedure.</p><formula xml:id="formula_12">L d = − T c t=1β i log(β i ).<label>(12)</label></formula><p>where β i ∈ {0, 1} is the ground truth salience score of comments. β i = 1 denotes the i-th comment c i is helpful for generating summary and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reader attention modeling</head><p>To model the reader focused aspect, we first calculate the word alignment of reader comments towards the document. We use the embeddings of words in document and comments to calculate the semantic alignment score. Precisely, γ i,j,k is the alignment socre between the i-th document word x d i and the k-th word in the j-th comment x c j,k , as shown in <ref type="figure" target="#fig_2">Equation 13</ref>:</p><formula xml:id="formula_13">γ i,j,k = e(x d i ) e(x c j,k ),<label>(13)</label></formula><formula xml:id="formula_14">δ i,j = max({γ i,j,1 , . . . , γ i,j,T c j }),<label>(14)</label></formula><p>In Equation 14, we use a max-operation over the alignment γ i,j,· to signify whether the i-th word of document is focused by the j-th comment. We regard the alignment score δ i,j as the reader attention weight for the j-th reader comment to the i-th document word.</p><p>In order to reduce the interference caused by the noisy comments, we employ the comment salience scoreβ j obtained from the denoising module to weighted combine the j-th reader attention δ i,j , as shown in Equation 15. It means that noisy comments will contribute less in the procedure of reader attention modeling.</p><formula xml:id="formula_15">i = T c j=1 δ i,jβj ,<label>(15)</label></formula><formula xml:id="formula_16">i = exp i / T d j=1 exp j .<label>(16)</label></formula><p>Finally, we get the reader attention i ∈ R for i-th document word after a softmax function as shown in Equation 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervisor</head><p>To model the semantic gap between the generated summary and the reader focused aspects, we design a supervisor module. First, for the decoder, we need to know which aspect in document has been focused by our summary generator in the past decoding steps. We sum up the latest k attention distributions {α t , α t−1 , . . . , α t−k+1 } and result in ν t ∈ R T d as the focus distribution of generated summary over T d document words, shown in Equation 17. Then we use ν t to weighted sum the document hidden states h d · and result in m t :</p><formula xml:id="formula_17">ν t = 1/k k−1 i=0 α t−i ,<label>(17)</label></formula><formula xml:id="formula_18">m t = T d i=1 ν t,i h d i ,<label>(18)</label></formula><p>where m t represents the focused aspect by the latest k decoding steps, a.k.a., decoder focused aspect. Next, we use the reader attention · to weighted sum the document hidden states h d · :</p><formula xml:id="formula_19">u = T d i=1 i h d i ,<label>(19)</label></formula><p>where u represents the reader focused aspect. For encouraging the decoder focused aspect become similar to the reader focused aspect, we employ an CNN based discriminator to signify the difference between the decoder focused aspect m t and the reader focused aspect u. Then we can use this difference to guide the decoder focus on the reader focused aspect. Typically, the discriminator is a binary classifier which can be decomposed into a convolutional feature extractor F shown in Equation 20 and a sigmoid classification layer shown in Equation 21 and 22.</p><formula xml:id="formula_20">F(x) = relu(W c ⊗ x),<label>(20)</label></formula><formula xml:id="formula_21">τ m t = sigmoid(W f F(m t ) + b f ),<label>(21)</label></formula><formula xml:id="formula_22">τ u = sigmoid(W f F(u) + b f ),<label>(22)</label></formula><p>where ⊗ denotes the convolutional operation, trainable parameter W c denotes the convolutional kernel, and τ m t and τ u are both the classification probabilities.</p><p>Note that a token generated at time t will influence not only the gradient received at that time but also the gradient at subsequent time steps. Intuitively, the decoding attention ν t of latter decoding step is more similar to the attention of final summary than the earlier steps. Thus we propose to define the cumulative loss with a discount factor ϕ ∈ (0, 1] as the loss functions. Note that the training objective for discriminator can be interpreted as maximizing the log-likelihood for classification, whether the input x in Equation 20 comes from reader focused aspect or from decoder focused aspect.</p><formula xml:id="formula_23">L d c = T d t=1 ϕ T d −t (log τ u + log(1 − τ m t )),<label>(23)</label></formula><formula xml:id="formula_24">L g c = T d t=1 ϕ T d −t (log(1 − τ m t )).<label>(24)</label></formula><p>In order to model the gap between reader focused aspect and decoder focused aspect, we subtract the reader attention ∈ R T d by ν t ∈ R T d resulting in attention difference ζ t ∈ R T d , shown in Equation 25. Then we use the attention difference ζ t to sum up the document hidden states h d · :</p><formula xml:id="formula_25">ζ t = − ν t , d t = T d i=1 ζ t,i h d i .</formula><p>(25) where d t denotes the semantic of unfocused document aspects by summary generator, a.k.a., gap content. To encourage the summary generator focus on the unfocused document aspects, we feed the gap content d t to the generator, as shown in Equation 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal tracker</head><p>Since the discriminator only provides a scalar guiding signal τ m t at each decoding step, it becomes relatively less informative when the sentence length TŶ goes larger. Inspired by LeakGAN <ref type="bibr" target="#b8">(Guo et al. 2018)</ref>, the proposed RASG framework allows discriminator to provide additional information, denoted as goal vector g t . In view of there is certain relationship between the goal of current decoding step and previous steps, we need to model the temporal interactions between the goal of each step. More specifically, we introduce a goal tracker module, an LSTM that takes the extracted feature vector F(m t ) and gap content d t as its input at each step t and outputs a goal vector g t :</p><formula xml:id="formula_26">g t = LSTM(g t−1 , [F(m t ); d t ]).<label>(26)</label></formula><p>In order to achieve higher consistency of reader focused aspect, we feed the goal vector g t into the generator to guide the generation of the next word, as shown in Equation 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model training</head><p>As our model is trained in an adversarial manner, we resplit the parameters in our model into two parts: (1) generation module including the parameters of summary generator, reader attention module and goal tracker;</p><p>(2) discriminator module including the parameters of CNN classifier. As for training generation module, we sum up the loss function of denoising module L d , cross entropy between ground truth L g and the result of discriminator L g c , as shown in Equation 27. We use the L to optimize the parameters of generation module.</p><formula xml:id="formula_27">L = L g + L d + L g c ,<label>(27)</label></formula><p>Next, we train the discriminator module to maximize the probability of assigning the correct label to both generated aspect m t and reader focused aspect u. More specifically, we optimize the parameters of discriminator module according to the loss function L d c calculated in Equation 23.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research questions</head><p>We list four research questions that guide the experiments: RQ1: Does RASG outperform other baselines? RQ2: What is the effect of each module in RASG? RQ3: Does RASG capture useful information from noisy comments? RQ4: Can goal tracker give a helpful guidance to decoder?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We collect the document-summary-comments pair data from Weibo which is the largest social network website in China, and users can read a document and post a comment about the document on this website. Each sample of data contains a document, a summary and several reader comments. Most comments are about the readers' opinion of their focused aspect in the document. In order to train the denoising module, we should give a ground truth label β i for i-th comment. When there is at least one common word in summary and comment, we regard such comment is helpful for generating summary. Accordingly, we give the β i = 1 to i-th comment when it contains at least one common word and give β i = 0 when it does not. In total, our training dataset contains 863826 training samples. The average length of document is 67.08 words, average length of comment is 16.61 words and average length of summary is 16.56 words. The average comments number of a document is 9.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>For evaluation metrics, we adopt ROUGE score <ref type="bibr" target="#b17">(Lin 2004)</ref> which is widely applied for summarization evaluation <ref type="bibr" target="#b3">Chen et al. 2018)</ref>. The ROUGE metrics compare generated summary with the reference summary by computing overlapping lexical units, including ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (longest common subsequence).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison methods</head><p>In order to prove the effectiveness of each module in RASG, we conduct some ablation models introduced in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>To evaluate the performance of our proposed dataset and model, we compare it with the following baselines:</p><p>(1) S2S: Sequence-to-sequence framework <ref type="bibr" target="#b26">(Sutskever, Vinyals, and Le 2014)</ref> has been proposed for language generation task. (2) S2SR: We simply add the reader attention · on attention distribution α t,· in each decoding step. (3) CGU: <ref type="bibr" target="#b16">Lin et al. (2018)</ref> propose to use the convolutional gated unit to refine the source representation, which achieves the state-of-the-art performance on social media text summarization dataset. (4) LEAD1: LEAD1 is a commonly used baseline <ref type="bibr" target="#b21">(Nallapati, Zhai, and Zhou 2017;</ref><ref type="bibr" target="#b24">See, Liu, and Manning 2017)</ref>, which selects the first sentence of document as the summary. (5) TextRank: <ref type="bibr" target="#b20">Mihalcea et al. (2004)</ref> propose to build a graph, then add each sentence as a vertex and use link to represent semantic similarity. Sentences are sorted based on final scores and a greedy algorithm is employed to select summary sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We implement our experiments in TensorFlow <ref type="bibr" target="#b0">(Abadi et al. 2016</ref>) on an NVIDIA P40 GPU. The word embedding dimension is set to 256 and the number of hidden units is 512. We set the k = 5 in the Equation 17 and ϕ = 0.5 in Equation 23 and 24. We use Adagrad optimizer <ref type="bibr" target="#b5">(Duchi, Hazan, and Singer 2010)</ref> as our optimizing algorithm. We employ beam search with beam size 5 to generate more fluency summary sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall performance</head><p>For research question RQ1, we examine the performance of our model in terms of ROUGE. <ref type="table" target="#tab_1">Table 3</ref> lists performances of all comparisons in terms of ROUGE score. We see that RASG achieves a 11.0%, 9.1% and 6.6% increment over the state-of-the-art method CGU in terms of ROUGE-1, ROUGE-2 and ROUGE-L respectively. It is worth noticing that the baseline model S2SR achieves better performance than S2S which demonstrates the effectiveness of incorporating reader focused aspect in summary generation. However when compared with RASG, S2SR achieves lower performance in terms of all ROUGE score. Thus, simply adding the reader focused aspect into generation procedure is not a good reader-aware summarization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>Next, we turn to research question RQ2. We conduct ablation tests on the usage of denoising module, supervisor as well as the goal tracker and the ROUGE score result is shown in <ref type="table" target="#tab_2">Table 4</ref>. The discriminator provides the scalar training signal L g c for generator training and the feature vector F(m t ) for goal tracker. Consequently, there is an increment of 17.51% from RASG w/o GTD to RASG w/o GT in terms of ROUGE-L, which demonstrates the effectiveness of discriminator. As for the effectiveness of goal tracker, compared with RASG and RASG w/o GT, RASG w/o GTD offers a decrease of 45.23% and 17.88% in terms of ROUGE-1, respectively. This demonstrates that the goal tracker with the feature from discriminator plays an important role in producing better summary. However, using the goal tracker without the feature extracted by the discriminator does not help improve the performance of the summary generator, shown by the performance of RASG w/o GTD. Finally, RASG w/o DM offers a decrease of 10.22% compared with RASG in terms of ROUGE-L, which demonstrates the effectiveness of denoising module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denoising ability</head><p>Next, we turn to research question RQ3. Due to the fact that the denoising module is learned in a supervised way, there is a ground truth label associated with each comment. Thus when the predict salience scoreβ i &gt; 0.5 we classify it as a helpful comment and vice-versa. As the denoising module can be regarded as a binary classifier to classify each comment toβ i = 1 orβ i = 0, we calculate the classification recall score of comments to measure the performance of this module. The recall curve is shown in <ref type="figure">Figure 2</ref>. As the training progresses, the recall score is on a steady upward curve which proves the improved performance of denoising module. To conclude, the denoising module can give a meaningful salience score for the subsequent process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of goal tracker</head><p>In this section, we turn to research question RQ4. The main purpose of employing goal tracker is to help the summary generator utilize the reader focused aspect. Intuitively, we want to know whether the summary generator follows the goal set by the goal tracker. Therefore, we calculate the cosine distance between decoder attention ν TŶ ∈ R T d (in <ref type="figure">Equation 17</ref>) and reader attention ∈ R T d (in <ref type="figure">Equation 16</ref>).</p><p>In <ref type="figure">Figure 2</ref>, we compare the cosine distance between the ablation model RASG w/o GTD and RASG. RASG observes a decrease of cosine distance and conversely, the RASG w/o GTD observes an increment of cosine distance. The fact that RASG can narrow the cosine distance proves that goal tracker and discriminator can lead the generator to follow the reader focused aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human evaluation</head><p>We ask three highly-educated Ph.D. students to rate 100 generated summaries of different models according to consistency and fluency. These annotators are all native speakers. The rating score ranges from 1 to 3 and 3 is the best. We take the average score of all summaries as the final score of each model, as shown in <ref type="table" target="#tab_3">Table 5</ref>. It can be seen that RASG outperforms other baseline models in both sentence fluency and consistency by a large margin. We calculate the kappa statistics in terms of fluency and consistency, and the score is 0.33 and 0.29 respectively. To prove the significance of the above results, we also do the paired student t-test between our model and CGU model (row with shaded background), the p-value are 0.0017 and 0.0012 for fluency and consistency respectively. <ref type="figure" target="#fig_2">Figure 3</ref> shows a document and its corresponding summaries generated by different methods. We can observe that  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We also ask three highly-educated annotators to rate 100 generated summaries according to two aspects: consistency and fluency. The rating score ranges from 1 to 3, and 3 is the best. We finally take the average across summaries and annotators, as shown in <ref type="table" target="#tab_3">Table 5</ref>. Significant differences are with respect to CGU (row with shaded background). In Table 5, we can see that RASG significantly outperforms other baseline models in both sentence fluency and consistency with the original document and comments of readers. To prove the significance of the above results, we also do the paired student t-test between our model and baseline methods, the p-value are 0.0017 and 0.0012 for fluency and consistency respectively. <ref type="table" target="#tab_5">Table 6</ref> shows an example and its corresponding generated summaries by different methods. We observe that S2S only generates fluent summary, but are contradictory to the focus of reader and ground truth summary. However, RASG overcomes this shortcoming by using goal vector and gap content given by supervisor and goal tracker at training stage, and produce the summary not only fluent but also consistent with the focus of readers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have proposed the task of reader-aware summary generation, which aims to generate summaries for text from social media incorporate the comments of readers. To address this task, we have proposed reader-aware summary generator (RASG): A sequence-to-sequence based summary generator is used to encode the document and then generate the summary sentence with attention and copy mechanism. In order to capture the focused aspect by the readers, we use a reader attention module to model the align- ment of comments and document. We employ a supervisor to measure the semantic gap between aspect of generated summary and reader-focused document. Finally, a goal tracker uses the information of semantic gap and the feature extracted by the discriminator to produce a goal vector to guide the summary generator. In our experiments, we have demonstrated the effectiveness of RASG and have found significant improvements over state-of-the-art baselines in terms of ROUGE and human evaluations. Moreover, we have verified the effectiveness of each module in RASG for improving reader-aware summary generation. Future work involves extending our model to improved adversarial training skills like Wasserstein GAN. Also, we plan to pursue a novel text matching method to denoising module for improving the accuracy. S2S does generate fluent summary. However, the generated aspect is contradictory to the focused aspect of reader or ground truth summary. Meanwhile, RASG overcomes this shortcoming by using goal vector and gap content given by goal tracker and supervisor at training stage, and produces the summary that is not only fluent but also consistent with main aspect of document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a new framework named readeraware summary generator (RASG) which aims to generate summaries for document from social media incorporating the reader comments. In order to capture the reader focused aspect, we design a reader attention component with a denoising module to capture the alignment between comments and document. We employ a supervisor to measure the semantic gap between generated summary and reader focused aspect. A goal tracker uses the information of semantic gap and the feature extracted by the discriminator to produce a goal vector to guide the summary generator. In our experiments, we have demonstrated the effectiveness of RASG and have found significant improvements over state-of-theart baselines in terms of ROUGE and human evaluations. Moreover, we have verified the effectiveness of each module in RASG for improving the summarization performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Cosine distance between decoding attention and reader attention. (b) Recall score of denoising module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Cosine distance between current content and reader content.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Examples of the generated summary by RASG and other models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Ablation models for comparison. Goal Tracker RASG w/o GTD RASG w/o Goal Tracker Discriminator</figDesc><table><row><cell>Acronym</cell><cell>Gloss</cell></row><row><cell cols="2">RASG w/o DM RASG w/o Denoising Module RASG w/o G RASG w/o Gap content RASG w/o GT RASG w/o</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>ROUGE scores comparison between baselines.</figDesc><table><row><cell></cell><cell cols="3">ROUGE-1 ROUGE-2 ROUGE-L</cell></row><row><cell>S2S S2SR CGU RASG</cell><cell>23.86 24.70 27.32 30.33</cell><cell>9.86 10.00 11.36 12.39</cell><cell>23.83 24.50 25.49 27.16</cell></row><row><cell>LEAD1 TextRank</cell><cell>5.51 13.5</cell><cell>1.71 4.55</cell><cell>4.94 11.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>ROUGE scores of different ablation models.</figDesc><table><row><cell></cell><cell cols="3">ROUGE-1 ROUGE-2 ROUGE-L</cell></row><row><cell>RASG w/o DM RASG w/o G RASG w/o GT RASG w/o GTD RASG</cell><cell>27.29 28.03 22.75 19.30 30.33</cell><cell>11.01 11.24 9.17 6.95 12.39</cell><cell>24.64 25.28 20.80 17.70 27.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Consistency and fluency comparison by human evaluation.</figDesc><table><row><cell cols="4">Fluency mean variance mean variance Consistency</cell></row><row><cell>S2S CGU RASG 2.65 2.17 2.20</cell><cell>0.24 0.26 0.21</cell><cell>1.98 2.08 2.48</cell><cell>0.28 0.29 0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">: Consistency and fluency comparison by human evaluation.</cell></row><row><cell cols="4">Fluency mean variance mean variance Consistency</cell></row><row><cell>S2S CGU RASG 2.65 N 2.17 2.20</cell><cell>0.24 0.26 0.21</cell><cell>1.98 2.08 2.48 N</cell><cell>0.28 0.29 0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Examples of the generated summary by RASG and other models. document êüh: -.Q·û⌃-õ?V/ ˙Àeh˝ D,€ e˘≤íTQ ⇢ åÑíTQ ⇢˝Ö⌦⇥I¯s?V ⇢«(∞˚QŸ8-∞˚∞⇧¡ UOE⇢∫XY≤˘≠ I™Ω ™õS ∞íSsÑ˝∂ å;õõ⇥Ê¡AE Z(Lin Xu said that the Central Network Office will provide policy support to establish a nationwide capital to enter the cultivation of internet companies, improve the policies of domestic listing of internet companies; through the press on the news website to issue a press card, carry out education and training for practitioners, and strive to create new the national team and the main force of the media platform.See Long Weibo for details.) comments˝ D,≈{Àe(State-owned capital must be involved.) -.Q·û Åä&gt;™=û(áˆ⌦ Å⌥ûÑgL (The Central Network Office should not implement the measures on the documents and must implement them in a practical way !) ⌘…óQ·ûÅ}}ªªŸõ≥ §KlÜ(I feel that the Central Network Office should cure these black sheep.) reference Q · û o ;˚⇢˙À e h˝ D , € e˘≤ í T Q ⇢(Deputy director of the Central Network Office: Establishing state-owned capital to cultivate internet enterprises) S2S êü⇢™õS ∞íSsÑ˝∂ å˝∂ (Lin Xu : National team and national team working hard to build a new media platform) CGU AE Z ⇢ -. Q · û ⌃ -õ ? V / ˙À e h˝ D ,(Long Weibo: The Central Network Office will provide policy support to establish state-owned capital.) RASG -.Q·û⇢™õS ∞íSsÑ˝∂ (Central Network Office: Working hard to build a national team of new media platform.)</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their constructive comments. We would also like to thank Zhujun Zhang, Sicong Jiang for their helps on this project. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Iterative document representation learning towards summarization with polishing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4088" to="4097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Product-aware answer generation in e-commerce question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Long text generation via adversarial training with leaked information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1709.08624</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lcsts: A large scale chinese short text summarization dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comments-oriented blog summarization by sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comments-oriented document summarization: understanding documents with readers&apos; feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extractive summarization with swap-net: Sentences and words from alternating pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jadhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Readeraware multi-document summarization via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Salience estimation via variational auto-encoders for multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reader-aware multidocument summarization: An enhanced model and the first dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
		<meeting>the Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hierarchical end-to-end model for jointly improving text summarization and sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoencoder as assistant supervisor: Improving text representation for chinese social media text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Solscsum: A linked sentence-comment dataset for social context summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-X</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unified model for extractive and abstractive summarization using inconsistency loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Get the point of my utterance! learning towards effective responses with multi-head attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A reinforced topic-aware convolutional sequence-tosequence model for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Social context summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequential copying networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
