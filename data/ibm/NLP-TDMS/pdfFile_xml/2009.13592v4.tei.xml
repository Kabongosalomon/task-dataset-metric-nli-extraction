<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
							<email>kemal.oksuz@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baris</forename><surname>Can</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cam</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
							<email>eakbas@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Kalkan</surname></persName>
							<email>skalkan@metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Middle East Technical University Ankara</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose average Localisation-Recall-Precision (aLRP), a unified, bounded, balanced and ranking-based loss function for both classification and localisation tasks in object detection. aLRP extends the Localisation-Recall-Precision (LRP) performance metric (Oksuz et al., 2018)  inspired from how Average Precision (AP) Loss extends precision to a ranking-based loss function for classification . aLRP has the following distinct advantages: (i) aLRP is the first rankingbased loss function for both classification and localisation tasks. (ii) Thanks to using ranking for both tasks, aLRP naturally enforces high-quality localisation for high-precision classification. (iii) aLRP provides provable balance between positives and negatives. (iv) Compared to on average ∼6 hyperparameters in the loss functions of state-of-the-art detectors, aLRP Loss has only one hyperparameter, which we did not tune in practice. On the COCO dataset, aLRP Loss improves its ranking-based predecessor, AP Loss, up to around 5 AP points, achieves 48.9 AP without test time augmentation and outperforms all one-stage detectors. Code available at: https://github.com/kemaloksuz/aLRPLoss. * Equal contribution for senior authorship.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection requires jointly optimizing a classification objective (L c ) and a localisation objective (L r ) combined conventionally with a balancing hyperparameter (w r ) as follows:</p><formula xml:id="formula_0">L = L c + w r L r .<label>(1)</label></formula><p>Optimizing L in this manner has three critical drawbacks: (D1) It does not correlate the two tasks, and hence, does not guarantee high-quality localisation for high-precision examples ( <ref type="figure" target="#fig_10">Fig. 1</ref>). (D2) It requires a careful tuning of w r <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">28,</ref><ref type="bibr">35]</ref>, which is prohibitive since a single training may last on the order of days, and ends up with a sub-optimal constant w r <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">12]</ref>. (D3) It is adversely impeded by the positive-negative imbalance in L c and inlier-outlier imbalance in L r , thus it requires sampling strategies <ref type="bibr">[14,</ref><ref type="bibr">15]</ref> or specialized loss functions [10, 23], introducing more hyperparameters <ref type="table" target="#tab_0">(Table 1)</ref>.</p><p>A recent solution for D3 is to directly maximize Average Precision (AP) with a loss function called AP Loss <ref type="bibr" target="#b6">[7]</ref>. AP Loss is a ranking-based loss function to optimize the ranking of the classification outputs and provides balanced training between positives and negatives.</p><p>In this paper, we extend AP Loss to address all three drawbacks (D1-D3) with one, unified loss function called average Localisation Recall Precision (aLRP) Loss. In analogy with the link between precision and AP Loss, we formulate aLRP Loss as the average of LRP values <ref type="bibr">[20]</ref> over the positive examples on the Recall-Precision (RP) curve. aLRP has the following benefits: (i) It exploits ranking for both classification and localisation, enforcing high-precision detections to have high-quality  <ref type="figure" target="#fig_10">Figure 1</ref>: aLRP Loss enforces high-precision detections to have high-IoUs, while others do not. (a) Classification and three possible localisation outputs for 10 anchors and the rankings of the positive anchors with respect to (wrt) the scores (for C) and IoUs (for R 1 , R 2 and R 3 ). Since the regressor is only trained by positive anchors, "-" is assigned for negative anchors. (b,c) Performance and loss assignment comparison of R 1 , R 2 and R 3 when combined with C. When correlation between the rankings of classifier and regressor outputs decreases, performance degrades up to 17 AP (b). While any combination of L c and L r cannot distinguish them, aLRP Loss penalizes the outputs accordingly (c). The details of the calculations are presented in Appendix A. α log(max(e CE × e βSL1 ))+γ FL 8 Faster R-CNN [27] CE+α SL1+βCE+γ SL1 9 Center Net <ref type="bibr" target="#b7">[8]</ref> FL+FL+α L2+β H+γ (SL1+SL1) 10 Ours aLRP Loss 1 localisation ( <ref type="figure" target="#fig_10">Fig. 1</ref>). (ii) aLRP has a single hyperparameter (which we did not need to tune) as opposed to ∼6 in state-of-the-art loss functions <ref type="table" target="#tab_0">(Table 1</ref>). (iii) The network is trained by a single loss function that provides provable balance between positives and negatives.</p><p>Our contributions are: <ref type="bibr" target="#b0">(1)</ref> We develop a generalized framework to optimize non-differentiable ranking-based functions by extending the error-driven optimization of AP Loss. <ref type="bibr" target="#b1">(2)</ref> We prove that ranking-based loss functions conforming to this generalized form provide a natural balance between positive and negative samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Balancing L c and L r in Eq. <ref type="bibr" target="#b0">(1)</ref>, an open problem in object detection (OD) <ref type="bibr">[22]</ref>, bears important challenges: Disposing w r , and correlating L c and L r . Classification-aware regression loss <ref type="bibr" target="#b2">[3]</ref> links the branches by weighing L r of an anchor using its classification score. Following Kendall et al.</p><p>[12], LapNet <ref type="bibr" target="#b3">[4]</ref> tackled the challenge by making w r a learnable parameter based on homoscedastic uncertainty of the tasks. Other approaches <ref type="bibr">[11,</ref><ref type="bibr">31]</ref> combine the outputs of two branches during non-maximum suppression (NMS) at inference. Unlike these methods, aLRP Loss considers the ranking wrt scores for both branches and addresses the imbalance problem naturally.</p><p>Ranking-based objectives in OD: An inspiring solution for balancing classes is to optimize a ranking-based objective. However, such objectives are discrete wrt the scores, rendering their direct incorporation challenging. A solution is to use black-box solvers for an interpolated AP loss surface <ref type="bibr">[25]</ref>, which, however, provided only little gain in performance. AP Loss <ref type="bibr" target="#b6">[7]</ref> takes a different approach by using an error-driven update mechanism to calculate gradients (Sec. 2). An alternative, DR Loss [26], employs Hinge Loss to enforce a margin between the scores of the positives and negatives. Despite promising results, these methods are limited to classification and leave localisation as it is. In contrast, we propose a single, balanced, ranking-based loss to train both branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AP Loss and Error-Driven Optimization</head><p>AP Loss <ref type="bibr" target="#b6">[7]</ref> directly optimizes the following loss for AP with intersection-over-union (IoU) thresholded at 0.50:</p><formula xml:id="formula_1">L AP = 1 − AP 50 = 1 − 1 |P| i∈P precision(i) = 1 − 1 |P| i∈P rank + (i) rank(i) ,<label>(2)</label></formula><p>where P is the set of positives; rank + (i) and rank(i) are respectively the ranking positions of the ith sample among positives and all samples. rank(i) can be easily defined using a step function H(·) applied on the difference between the score of i (s i ) and the score of each other sample:</p><formula xml:id="formula_2">rank(i) = 1 + j∈P,j =i H(x ij ) + j∈N H(x ij ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">x ij = −(s i − s j ) is positive if s i &lt; s j ;</formula><p>N is the set of negatives; and H(x) = 1 if x ≥ 0 and H(x) = 0 otherwise. In practice, H(·) is replaced by x/2δ + 0.5 in the interval [−δ, δ] (in aLRP, we use δ = 1 as set by AP Loss <ref type="bibr" target="#b6">[7]</ref> empirically; this is the only hyperparameter of aLRP - <ref type="table" target="#tab_0">Table 1)</ref>. rank + (i) can be defined similarly over j ∈ P. With this notation, L AP can be rewritten as follows:</p><formula xml:id="formula_4">L AP = 1 |P| i∈P j∈N H(x ij ) rank(i) = 1 |P| i∈P j∈N L AP ij ,<label>(4)</label></formula><p>where L AP ij is called a primary term which is zero if i / ∈ P or j / ∈ N 2 . Note that this system is composed of two parts: (i) The differentiable part up to x ij , and (ii) the nondifferentiable part that follows x ij . Chen et al. proposed that an error-driven update of x ij (inspired from perceptron learning [29]) can be combined with derivatives of the differentiable part. Consider the update in x ij that minimizes L AP ij (and hence L AP ):</p><formula xml:id="formula_5">∆x ij = L AP * ij − L AP ij = 0 − L AP ij = −L AP ij , with the target, L AP *</formula><p>ij , being zero for perfect ranking. Chen et al. showed that the gradient of L AP ij wrt x ij can be taken as −∆x ij . With this, the gradient of L AP wrt scores can be calculated as follows:</p><formula xml:id="formula_6">∂L AP ∂s i = j,k ∂L AP ∂x jk ∂x jk ∂s i = − 1 |P| j,k ∆x jk ∂x jk ∂s i = 1 |P|   j ∆x ij − j ∆x ji   . (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Localisation-Recall-Precision (LRP) Performance Metric</head><p>LRP [20, 21] is a metric that quantifies classification and localisation performances jointly. Given a detection set thresholded at a score (s) and their matchings with the ground truths, LRP aims to assign an error value within [0, 1] by considering localisation, recall and precision:</p><formula xml:id="formula_7">LRP(s) = 1 N F P + N F N + N T P   N F P + N F N + k∈T P E loc (k)   ,<label>(6)</label></formula><p>2 By setting L AP ij = 0 when i / ∈ P or j / ∈ N , we do not require the yij term used by Chen et al. <ref type="bibr" target="#b6">[7]</ref>.</p><p>where N F P , N F N and N T P are the number of false positives (FP), false negatives (FN) and true positives (TP); A detection is a TP if IoU(k) ≥ τ where τ = 0.50 is the conventional TP labeling threshold, and a TP has a localisation error of E loc (k) = (1 − IoU(k))/(1 − τ ). The detection performance is, then, min s (LRP(s)) on the precision-recall (PR) curve, called optimal LRP (oLRP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Generalisation of Error-Driven Optimization for Ranking-Based Losses</head><p>Generalizing the error-driven optimization technique of AP Loss <ref type="bibr" target="#b6">[7]</ref> to other ranking-based loss functions is not trivial. In particular, identifying the primary terms is a challenge especially when the loss has components that involve only positive examples, such as the localisation error in aLRP Loss.</p><p>Given a ranking-based loss function, L = 1 Z i∈P (i), defined as a sum over individual losses, (i), at positive examples (e.g., Eq. (2)), with Z as a problem specific normalization constant, our goal is to express L as a sum of primary terms in a more general form than Eq. (4): Definition 1. The primary term L ij concerning examples i ∈ P and j ∈ N is the loss originating from i and distributed over j via a probability mass function p(j|i). Formally,</p><formula xml:id="formula_8">L ij = (i)p(j|i), for i ∈ P, j ∈ N 0, otherwise.<label>(7)</label></formula><p>Then, as desired, we can express L = 1 Z i∈P (i) in terms of L ij :</p><formula xml:id="formula_9">Theorem 1. L = 1 Z i∈P (i) = 1 Z i∈P j∈N L ij . See Appendix C for the proof.</formula><p>Eq. (7) makes it easier to define primary terms and adds more flexibility on the error distribution: e.g., AP Loss takes p(j|i) = H(x ij )/N F P (i), which distributes error uniformly (since it is reduced to 1/N F P (i)) over j ∈ N with s j ≥ s i ; though, a skewed p(j|i) can be used to promote harder examples (i.e. larger x ij ). Here, N F P (i) = j∈N H(x ij ) is the number of false positives for i ∈ P.</p><p>Now we can identify the gradients of this generalized definition following Chen et al. (Sec. 2.1): The error-driven update in x ij that would minimize L is ∆x ij = L ij * − L ij , where L ij * denotes "the primary term when i is ranked properly". Note that L ij * , which is set to zero in AP Loss, needs to be carefully defined (see Appendix G for a bad example). With ∆x ij defined, the gradients can be derived similar to Eq. (5). The steps for obtaining the gradients of L are summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>Obtaining the gradients of a ranking-based function with error-driven update. Input: A ranking-based function L = ( (i), Z), and a probability mass function p(j|i) Output: The gradient of L with respect to model output s 1: ∀i, j find primary term: L ij = (i)p(j|i) if i ∈ P, j ∈ N ; otherwise L ij = 0 (c.f. Eq. <ref type="formula" target="#formula_8">(7)</ref>). 2: ∀i, j find target primary term: L ij * = (i) * p(j|i) ( (i) * : the error on i when i is ranked properly.) <ref type="bibr">3:</ref> ∀i, j find error-driven update:</p><formula xml:id="formula_10">∆x ij = L ij * − L ij = (i) * − (i) p(j|i). 4: return 1 Z ( j ∆x ij − j ∆x ji ) for each s i ∈ s (c.f. Eq. (5)).</formula><p>This optimization provides balanced training for ranking-based losses conforming to Theorem 1:</p><formula xml:id="formula_11">Theorem 2.</formula><p>Training is balanced between positive and negative examples at each iteration; i.e. the summed gradient magnitudes of positives and negatives are equal (see Appendix C for the proof):</p><formula xml:id="formula_12">i∈P ∂L ∂s i = i∈N ∂L ∂s i .<label>(8)</label></formula><p>Deriving AP Loss. Let us derive AP Loss as a case example for this generalized framework: AP (i) is simply 1 − precision(i) = N F P (i)/rank(i), and Z = |P|. p(j|i) is assumed to be uniform, i.e. p(j|i) = H(x ij )/N F P (i). These give us L AP ij = N F P (i) rank(i)  </p><formula xml:id="formula_13">H(xij ) N F P (i) = H<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Average Localisation-Recall-Precision (aLRP) Loss</head><p>Similar to the relation between precision and AP Loss, aLRP Loss is defined as the average of LRP values ( LRP (i)) of positive examples:</p><formula xml:id="formula_14">L aLRP := 1 |P| i∈P LRP (i).<label>(9)</label></formula><p>For LRP, we assume that anchors are dense enough to cover all ground-truths, i.e. N F N = 0. Also, since a detection is enforced to follow the label of its anchor during training, TP and FP sets are replaced by the thresholded subsets of P and N , respectively. This is applied by H(·), and rank(i) = N T P + N F P from Eq. <ref type="bibr" target="#b5">(6)</ref>. Then, following the definitions in Sec. 2.1, LRP (i) is:</p><formula xml:id="formula_15">LRP (i) = 1 rank(i)   N F P (i) + E loc (i) + k∈P,k =i E loc (k)H(x ik )   .<label>(10)</label></formula><p>Note that Eq. (10) allows using robust forms of IoU-based losses (e.g. generalized IoU (GIoU) [28]) only by replacing IoU Loss (i.e. 1 − IoU(i)) in E loc (i) and normalizing the range to [0, 1].</p><p>In order to provide more insight and facilitate gradient derivation, we split Eq. (9) into two as localisation and classification components such that L aLRP = L aLRP</p><formula xml:id="formula_16">cls + L aLRP loc , where L aLRP cls = 1 |P| i∈P N F P (i) rank(i) , and L aLRP loc = 1 |P| i∈P 1 rank(i)   E loc (i) + k∈P,k =i E loc (k)H(x ik )   .</formula><p>(11) are not differentiable wrt the classification scores, and therefore, we need the generalized framework from Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Optimization of the aLRP Loss</head><p>Using the same error distribution from AP Loss, the primary terms of aLRP Loss can be defined as L aLRP ij = LRP (i)p(j|i). As for the target primary terms, we use the following desired LRP Error:</p><formula xml:id="formula_17">LRP (i) * = 1 rank(i)   : 0 N F P (i) + E loc (i) + : 0 k∈P,k =i E loc (k)H(x ik )   = E loc (i) rank(i) ,<label>(12)</label></formula><p>yielding a target primary term, L aLRP ij * = LRP (i) * p(j|i), which includes localisation error and can be non-zero when s i &lt; s j , unlike AP Loss. Then, the resulting error-driven update for x ij is (line 3 of Algorithm 1):</p><formula xml:id="formula_18">∆x ij = LRP (i) * − LRP (i) p(j|i) = − 1 rank(i)   N F P (i) + k∈P,k =i E loc (k)H(x ik )   H(x ij ) N F P (i) .<label>(13)</label></formula><p>Finally, ∂L aLRP /∂s i can be obtained with Eq. <ref type="bibr" target="#b4">(5)</ref>. Our algorithm to compute the loss and gradients is presented in Appendix E in detail and has the same time&amp;space complexity with AP Loss.  , while E loc (p 1 ) (i.e. with largest s) contributes to each L loc (i); E loc (p 3 ) (i.e. with the smallest s) only contributes once with a very low weight due to its rank normalizing L loc (p 3 ). Hence, the localisation branch effectively focuses on detections ranked higher wrt s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Self-Balancing Extension for the Localisation Task</head><p>LRP metric yields localisation error only if a detection is classified correctly (Sec. 2.2). Hence, when the classification performance is poor (e.g. especially at the beginning of training), the aLRP Loss is dominated by the classification error (N F P (i)/rank(i) ≈ 1 and LRP (i) ∈ [0, 1] in Eq. (10)). As a result, the localisation head is hardly trained at the beginning ( <ref type="figure" target="#fig_1">Fig. 3</ref>). Moreover, <ref type="figure" target="#fig_1">Fig. 3</ref> also shows that L aLRP cls /L aLRP loc varies significantly throughout training. To alleviate this, we propose a simple and dynamic self-balancing (SB) strategy using the gradient magnitudes: note that i∈P ∂L aLRP /∂s i = i∈N ∂L aLRP /∂s i ≈ L aLRP (see Theorem 2 and Appendix F). Then, assuming that the gradients wrt scores and boxes are proportional to their contributions to the aLRP Loss, we multiply ∂L aLRP /∂B by the average L aLRP /L aLRP loc of the previous epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Dataset: We train all our models on COCO trainval35K set [16] (115K images), test on minival set (5k images) and compare with the state-of-the-art (SOTA) on test-dev set (20K images). Implementation Details: For training, we use 4 v100 GPUs. The batch size is 32 for training with 512 × 512 images (aLRPLoss500), whereas it is 16 for 800 × 800 images (aLRPLoss800). Following AP Loss, our models are trained for 100 epochs using stochastic gradient descent with a momentum factor of 0.9. We use a learning rate of 0.008 for aLRPLoss500 and 0.004 for aLRPLoss800, each decreased by factor 0.1 at epochs 60 and 80. Similar to previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, standard data augmentation methods from SSD [17] are used. At test time, we rescale shorter sides of images to <ref type="table" target="#tab_5">Table 2</ref>: Ablation analysis on COCO minival. For optimal LRP (oLRP), lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rank-Based Lc Rank-Based Lr SB ATSS AP AP50 AP75 AP90 oLRP ρ AP Loss <ref type="bibr" target="#b6">[7]</ref> 35.5 58.0 37.0 9.0 71.0 0.45   500 (aLRPLoss500) or 800 (aLRPLoss800) pixels by ensuring that the longer side does not exceed 1.66× of the shorter side. NMS is applied to 1000 top-scoring detections using 0.50 as IoU threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>In this section, in order to provide a fair comparison, we build upon the official implementation of our baseline, AP Loss <ref type="bibr" target="#b4">[5]</ref>. Keeping all design choices fixed, otherwise stated, we just replace AP &amp; Smooth L1 losses by aLRP Loss to optimize RetinaNet <ref type="bibr">[15]</ref>. We conduct ablation analysis using aLRPLoss500 on ResNet-50 backbone (more ablation experiments are presented in the Appendix G).</p><p>Effect of using ranking for localisation: <ref type="table" target="#tab_5">Table 2</ref> shows that using a ranking loss for localisation improves AP (from 35.5 to 36.9). For better insight, AP 90 is also included in <ref type="table" target="#tab_5">Table 2</ref>, which shows ∼5 points increase despite similar AP 50 values. This confirms that aLRP Loss does produce high-quality outputs for both branches, and boosts the performance for larger IoUs.  <ref type="table" target="#tab_5">Table 2</ref> shows that SB provides +1.8AP gain, similar AP 50 and +8.4 points in AP 90 against AP Loss. Comparing SB with constant weighting in <ref type="table" target="#tab_2">Table 3</ref>, our SB approach provides slightly better performance than constant weighting, which requires extensive tuning and end up with different w r constants for IoU and GIoU. Finally, <ref type="table" target="#tab_3">Table 4</ref> presents that initialization of SB (i.e. its value for the first epoch) has a negligible effect on the performance even with very large values. We use 50 for initialization.</p><p>Using GIoU: <ref type="table" target="#tab_5">Table 2</ref> suggests robust IoU-based regression (GIoU) improves performance slightly.</p><p>Using ATSS: Finally, we replace the standard IoU-based assignment by ATSS [34], which uses less anchors and decreases training time notably for aLRP Loss: One iteration drops from 0.80s to 0.53s with ATSS (34% more efficient with ATSS) -this time is 0.71s and 0.28s for AP Loss and Focal Loss respectively. With ATSS, we also observe +1.3AP improvement ( <ref type="table" target="#tab_5">Table 2</ref>). See App. G for details.</p><p>Hence, we use GIoU [28] as part of aLRP Loss, and employ ATSS [34] when training RetinaNet. Potential of Correlating Classification and Localisation. We analyze two bounds: (i) A Lower Bound where localisation provides an inverse ranking compared to classification. (ii) An Upper Bound where localisation provides exactly the same ranking as classification. <ref type="table" target="#tab_4">Table 5</ref> shows that correlating ranking can have a significant effect (up to 20 AP) on the performance especially for larger IoUs. Therefore, correlating rankings promises significant improvement (up to ∼ 10AP). Moreover, while ρ is 0.44 and 0.45 for Focal Loss (results not provided in the table) and AP Loss (  , as it exhibits perfect balance between the gradients throughout training. However, we see large fluctuations in derivatives of CE and FL (left), which biases training towards positives or negatives alternately across iterations. As expected, imbalance impacts CE more as it quickly drops (right), overfitting in favor of negatives since it is dominated by the error and gradients of these large amount of negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">More insight on aLRP Loss</head><formula xml:id="formula_19">Rate= i | si |/ i | si | =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with State of the Art (SOTA)</head><p>Different from the ablation analysis, we find it useful to decrease the learning rate of aLRPLoss500 at epochs 75 and 95. For SOTA comparison, we use the mmdetection framework <ref type="bibr" target="#b5">[6]</ref> for efficiency (we reproduced <ref type="table" target="#tab_5">Table 2</ref> using our mmdetection implementation, yielding similar results -see our repository). <ref type="table" target="#tab_6">Table 6</ref> presents the results, which are discussed below:</p><p>Ranking-based Losses. aLRP Loss yields significant gains over other ranking-based solutions: e.g., compared with AP Loss, aLRP Loss provides +5.4AP for scale 500 and +5.1AP for scale 800. Similarly, for scale 800, aLRP Loss performs 4.7AP better than DR Loss with ResNeXt-101.</p><p>Methods combining branches. Although a direct comparison is not fair since different conditions are used, we observe a significant margin (around 3-5AP in scale 800) compared to other approaches that combine localisation and classification.</p><p>Comparison on scale 500. We see that, even with ResNet-101, aLRPLoss500 outperforms all other methods with 500 test scale. With ResNext-101, aLRP Loss outperforms its closest counterpart (HSD) by 2.7AP and also in all sizes (AP S -AP L ).</p><p>Comparison on scale 800. For 800 scale, aLRP Loss achieves 45.9 and 47.8AP on ResNet-101 and ResNeXt-101 backbones respectively. Also in this scale, aLRP Loss consistently outperforms its closest counterparts (i.e. FreeAnchor and CenterNet) by 2.9AP and reaches the highest results wrt all performance measures. With DCN [37], aLRP Loss reaches 48.9AP, outperforming ATSS by 1.2AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Using aLRP Loss with Different Object Detectors</head><p>Here, we use aLRP Loss to train FoveaBox [13] as an anchor-free detector, and Faster R-CNN [27] as a two-stage detector. All models use 500 scale setting, have a ResNet-50 backbone and follow our mmdetection implementation <ref type="bibr" target="#b5">[6]</ref>. Further implementation details are presented in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on FoveaBox:</head><p>To train FoveaBox, we keep the learning rate same with RetinaNet (i.e. 0.008) and only replace the loss function by aLRP Loss. <ref type="table" target="#tab_7">Table 7</ref> shows that aLRP Loss outperforms Focal Loss and AP Loss, each combined by Smooth L1 (SL1 in <ref type="table" target="#tab_7">Table 7</ref>), by 1.4 and 3.2 AP points (and similar oLRP points) respectively. Note that aLRP Loss also simplifies tuning hyperparameters of Focal Loss, which are set in FoveaBox to different values from RetinaNet. One training iteration of Focal Loss, AP Loss and aLRP Loss take 0.34, 0.47 and 0.54 sec respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Faster R-CNN:</head><p>To train Faster R-CNN, we remove sampling, use aLRP Loss to train both stages (i.e. RPN and Fast R-CNN) and reweigh aLRP Loss of RPN by 0.20. Thus, the number   of hyperparameters is reduced from nine <ref type="table" target="#tab_0">(Table 1)</ref> to three (two δs for step function, and a weight for RPN). We validated the learning rate of aLRP Loss as 0.012, and train baseline Faster R-CNN by both L1 Loss and GIoU Loss for fair comparison. aLRP Loss outperforms these baselines by more than 2.5AP and 2oLRP points while simplifying the training pipeline <ref type="table" target="#tab_8">(Table 8</ref>). One training iteration of Cross Entropy Loss (with L1) and aLRP Loss take 0.38 and 0.85 sec respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we provided a general framework for the error-driven optimization of ranking-based functions. As a special case of this generalization, we introduced aLRP Loss, a ranking-based, balanced loss function which handles the classification and localisation errors in a unified manner. aLRP Loss has only one hyperparameter which we did not need to tune, as opposed to around 6 in SOTA loss functions. We showed that using aLRP improves its baselines significantly over different detectors by simplifying parameter tuning, and outperforms all one-stage detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>We anticipate our work to significantly impact the following domains:</p><p>1. Object detection: Our loss function is unique in many important aspects: It unifies localisation and classification in a single loss function. It uses ranking for both classification and localisation. It provides provable balance between negatives and positives, similar to AP Loss.</p><p>These unique merits will contribute to a paradigm shift in the object detection community towards more capable and sophisticated loss functions such as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Other computer vision problems with multiple objectives: Problems including multiple objectives (such as instance segmentation, panoptic segmentation -which actually has classification and regression objectives) will benefit significantly from our proposal of using ranking for both classification and localisation.</p><p>3. Problems that can benefit from ranking: Many vision problems can be easily converted into a ranking problem. They can then exploit our generalized framework to easily define a loss function and to determine the derivatives.</p><p>Our paper does not have direct social implications. However, it inherits the following implications of object detectors: Object detectors can be used for surveillance purposes for the betterness of society albeit privacy concerns. When used for detecting targets, an object detector's failure may have severe consequences depending on the application (e.g. self-driving cars). Moreover, such detectors are affected by the bias in data, although they will not try to exploit them for any purposes.</p><p>[  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The Scenario</head><p>We assume that the scenario in <ref type="figure" target="#fig_10">Figure 1</ref>(a) of the paper includes five ground truths of which four of them are detected as true positives with different Intersection-over-Union (IoU) overlaps by three different detectors (i.e. C&amp;R 1 , C&amp;R 2 , C&amp;R 3 ). Each detector has a different ranking for these true positives with respect to their IoUs. In addition, the output of each detector contains the same six detections with different scores as false positives. Note that the IoUs of these false positives are marked with "-" in <ref type="figure" target="#fig_10">Figure 1</ref>(a) since they do not match with any ground truth and therefore their IoUs are not being considered neither by the performance measure (i.e. Average Precision) nor by loss computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Performance Evaluation</head><p>There are different ways to calculate Average Precision (AP) and loss values. For example, in PASCAL [9] and COCO [16] datasets, the recall domain is divided into 11 and 101 evenly spaced points, respectively, and the precision values at these points are averaged to compute AP for a single IoU threshold.</p><p>Here, we present how Average Precision (AP) is calculated in <ref type="figure" target="#fig_10">Figure 1(b)</ref>. Similar to the widely adopted performance metric, COCO-style AP, we use AP IoU with different IoU thresholds. In order to keep things simple but provide the essence of the performance metric, we use four samples with 0.15 increments (i.e. {0.50, 0.65, 0.80, 0.95}) instead of ten samples with 0.05 increments as done by original COCO-style AP.</p><p>In order to compute a single average precision with an IoU as the conventional true positive labelling threshold, denoted by AP IoU , the approaches use different methods for sampling/combining individual precision values on a PR curve. The PR curves corresponding to each detector-AP IoU pair are presented in <ref type="figure">Figure A</ref>.6. While drawing these curves, similar to Pascal-VOC and COCO, we also adopt interpolation on the PR curve, which requires keeping the larger precision value in the case that the larger one resides in a lower recall. Then, again similar to what these common methods do for a single AP threshold, we check the precision values on different recall values after splitting the recall axis equally. Here, we use 10 recall points between 0.1 and 1.0 in 0.1 increments. Then, based on the PR curves in <ref type="figure">Figure A</ref>.6, we check the precision under these different recall values and present them in <ref type="table" target="#tab_10">Table A</ref>.9. Having generated these values in <ref type="table" target="#tab_10">Table A</ref>.9 for each AP IoU s, the computation is trivial: Just averaging over these precisions (i.e. row-wise average) yields AP IoU s. Finally, averaging over these four AP IoU s produces the final detection performance as 0.37, 0.29 and 0.20 for C&amp;R 1 , C&amp;R 2 , C&amp;R 3 respectively (see <ref type="table" target="#tab_10">Table A</ref>.9). 0.50 C&amp;R 1 1.00 1.00 0.67 0.67 0.50 0.50 0.40 0.40 0.00 0.00 0.51 C&amp;R 2 1.00 1.00 0.67 0.67 0.50 0.50 0.40 0.40 0.00 0.00 0.51 C&amp;R 3 1.00 1.00 0.67 0.67 0.50 0.50 0.40 0.40 0.00 0.00 0.51 0.65 C&amp;R 1 1.00 1.00 0.67 0.67 0.50 0.50 0.00 0.00 0.00 0.00 0.43 C&amp;R 2 1.00 1.00 0.67 0.67 0.30 0.30 0.00 0.00 0.00 0.00 0.39 C&amp;R 3 0.33 0.33 0.33 0.33 0.30 0.30 0.00 0.00 0.00 0.00 0.19 0.80 C&amp;R 1 1.00 1.00 0.67 0.67 0.00 0.00 0.00 0.00 0.00 0.00 0.33 C&amp;R 2 1.00 1.00 0.20 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.24 C&amp;R 3 0.20 0.20 0.20 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.08 0.95 C&amp;R 1 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.20 C&amp;R 2 0.10 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 C&amp;R 3 0.10 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Computing the Loss Values</head><p>In this section, computing the loss values in <ref type="figure" target="#fig_10">Figure 1</ref>(c) of the paper is presented in detail. Each section is devoted to a loss function presented in <ref type="figure" target="#fig_10">Figure 1</ref>(c). To keep things simple, without loss of generality, we make the following assumptions in this section during the calculation of the classification and localisation losses:</p><p>1. The classifier has sigmoid non-linearity at the top.</p><p>2. There is only one foreground class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Similar to how localisation losses deal with scale-and translation-variance within an image,</head><p>we assume that each ground truth box is normalized as [0, 0, 1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">For each loss, the average of its contributors is reported.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Cross-entropy Loss</head><p>Cross-entropy Loss of the ith example is defined as:</p><formula xml:id="formula_20">L CE (p i ) = −I[i ∈ P] log(p i ) − I[i ∈ N ] log(1 − p i ), (A.14)</formula><p>such that p i is the confidence score of the ith example obtained by applying the sigmoid activation to the classification logit s i , and I[Q] is the Iverson bracket which is 1 if the predicate Q is true; or else it is 0.</p><p>Seeing that all detector outputs, C&amp;R1, C&amp;R2 and C&amp;R3, involve the same classification output, we apply Eq. (A.14) for each anchor on C, and then find their average as follows: respectively. Then, AP Loss for the output C in <ref type="figure" target="#fig_10">Figure 1</ref> regardless of the localisation output that it is combined with is:</p><formula xml:id="formula_21">L CE = 1 |P| + |N | pi L CE (p i ),</formula><formula xml:id="formula_22">L AP = 1 − AP 50 = 1 − 1 |P| i∈P precision(i), = 1 − 1 4</formula><p>× (1.00 + 0.67 + 0.50 + 0.40) = 0.36.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 L1 Loss</head><p>For a single ground truth,B i = [x 1 ,ŷ 1 ,x 2 ,ŷ 2 ], and its corresponding detection, B i = [x 1 , y 1 , x 2 , y 2 ], L1 Loss is defined simply by averaging over the L1 norm of the differences of the parameters of the detection boxes from their corresponding ground truths:</p><formula xml:id="formula_23">L L1 (B i , B i ) = |x 1 − x 1 | +|ŷ 1 − y 1 | +|x 2 − x 2 | +|ŷ 2 − y 2 | , (A.19)</formula><p>Then, the average L1 Loss is:  <ref type="figure">IoU(B i , B i )</ref>. Then, for all three outputs in the scenario (also an instance is illustrated in <ref type="figure">Figure A.5)</ref>, seeing that the IoU distributions are all equal, the average IoU loss of this detection set is: </p><formula xml:id="formula_24">L L1 = 1 |P| i∈P L L1 (B i , B i ),</formula><formula xml:id="formula_25">L IoU = 1 |P| i∈P 1 − IoU(B i , B i ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.5 aLRP Loss</head><p>This section calculates aLRP Loss value in the scenario, and therefore we believe that at the same time it is also a toy example to present more insight on aLRP Loss.</p><p>First, let us recall the definition of aLRP Loss from the paper to simplify tracking this section. aLRP Loss is defined as:</p><formula xml:id="formula_26">L aLRP := 1 |P| i∈P LRP (i), (A.24)</formula><p>such that</p><formula xml:id="formula_27">LRP (i) = 1 rank(i)   N F P (i) + E loc (i) + k∈P,k =i E loc (k)H(x ik )   , (A.25)</formula><p>where E loc (k) = (1 − IoU(k))/(1 − τ ). Here, we take H(x) as a step function instead of its approximation for simplicity. <ref type="table" target="#tab_10">Table A</ref>.10 presents the computation of aLRP values including all by-products for each of the four positive anchors in C&amp;R 1 , C&amp;R 2 and C&amp;R 3 . Given the table presented in <ref type="figure" target="#fig_10">Figure 1(a)</ref> in the paper, we present how each column is derived in the following steps:</p><p>1. 1 − IoU(i) is simply the IoU Loss of the positive anchors after prediction. </p><formula xml:id="formula_28">2. E loc (i) = (1 − IoU(i))/(1 − τ ) such that τ = 0.5.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Define a cumulative sum: cumsum(E</head><formula xml:id="formula_29">loc )(i) = E loc (i) + k∈P,k =i E loc (k)H(x ik ) (see Eq.</formula><p>A.25). Note that this simply corresponds to a cumulative sum on a positive example using the examples with larger scores and itself. Accordingly, in <ref type="table" target="#tab_10">Table A</ref>.10, cumsum(E loc )(i) is calculated by summing E loc )(i) column over anchors until (and including) ith example. 6. Then using cumsum(E loc )(i), N F P (i) and rank(i), LRP error on a positive example can be computed as:</p><formula xml:id="formula_30">LRP (i) = N F P (i) + cumsum(E loc )(i) rank(i) . (A.26) 7.</formula><p>In the rightmost column, aLRP Loss of a detector, L aLRP , is determined simply averaging over these single LRP values (i.e. LRP (i) ) on positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Table 1: Hyperparameters of the Loss Functions and Models</head><p>This section presents the hyperparameters of the common loss functions in object detection and how they are combined by different models in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Hyperparameters of the Individual Loss Functions</head><p>Table A.11 presents common loss functions and their hyperparameters. Note that since any change in these hyperparameter change the value of the loss function and affects its contribution to the multi-task learning nature of object detection, and, therefore w r also needs to be retuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Hyperparameters of the Loss Functions of the Models</head><p>This section discusses the loss functions of the methods discussed in <ref type="table" target="#tab_0">Table 1</ref>  Other architectures in <ref type="table" target="#tab_0">Table 1</ref> use more than two loss functions in order to learn different aspects to improve the performance: IoU-based 0 -</p><p>• FCOS [30] includes an additional centerness branch to predict the centerness of the pixels, which is trained by an additional cross entropy loss. • FreeAnchor [35] aims simultaneously to learn the assignment of the anchors to the ground truths by modeling the loss function based on maximum likelihood estimation. In <ref type="table" target="#tab_0">Table 1</ref>, one can easily identify six hyper-parameters from the loss formulation of the Free Anchor and exploiting <ref type="table" target="#tab_10">Table A</ref>.11. Moreover, the inputs of the focal loss are subject to a saturated linear function with two hyperparameters, which makes eight in total.  <ref type="table" target="#tab_0">Table 1</ref> requires nine hyperparameters. • Finally, CenterNet <ref type="bibr" target="#b7">[8]</ref>, as a state-of-the-art bottom-up method, has a loss function with several components while learning to predict the centers and the corners. It combines six individual losses, one of which is Hinge Loss with one hyperparameter. Considering the type of each, the loss function of CenterNet <ref type="bibr" target="#b7">[8]</ref> has 10 hyper-parameters in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs of Theorem 1 and Theorem 2</head><p>This section presents the proofs for the theorems presented in our paper.</p><formula xml:id="formula_31">Theorem 1. L = 1 Z i∈P (i) = 1 Z i∈P j∈N L ij .</formula><p>Proof. The ranking function is defined as:</p><formula xml:id="formula_32">L = 1 Z i∈P (i). (A.27)</formula><p>Since ∀i j∈N p(j|i) = 1, we can rewrite the definition as follows:</p><formula xml:id="formula_33">1 Z i∈P (i)   j∈N p(j|i)   . (A.28)</formula><p>Reorganizing the terms concludes the proof as follows: Proof. The gradients of a ranking-based loss function are derived as (see Algorithm 1 and Equation 5 in the paper):</p><formula xml:id="formula_34">∂L ∂s i = 1 Z   j ∆x ij − j ∆x ji   = 1 Z j ∆x ij − 1 Z j ∆x ji , (A.31)</formula><p>such that ∆x ij is the update for x ij s and defined as ∆x ij = L * ij − L ij . Note that both L ij and L * ij can be non-zero only if i ∈ P and j ∈ N following the definition of the primary term. Hence, the same applies to ∆x ij : if i / ∈ P or j / ∈ N , then ∆x ij = 0. Then using these facts, we can state in Eq. (A.31) that if i ∈ P, then j ∆x ji = 0; and if i ∈ N , then j ∆x ij = 0. Then, we can say that, only one of the terms is active in Eq. (A.31) for positives and negatives:</p><formula xml:id="formula_35">∂L ∂s i = 1 Z j ∆x ij Active if i ∈ P − 1 Z j ∆x ji Active if i ∈ N . (A.32)</formula><p>Considering that the value of a primary term cannot be less than its target, we have ∆x ij ≤ 0, which implies ∂L ∂si ≤ 0. So, we can take the absolute value outside of summation: </p><formula xml:id="formula_36">i∈P ∂L ∂s i = i∈P ∂L ∂s i , (A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Normalized Discounted Cumulative Gain (NDCG) Loss and Its Gradients: Another Case Example for our Generalized Framework</head><p>In the following we define and derive the gradients of the NDCG Loss [18] following our generalized framework presented in Section 3 of our main paper.</p><p>The NDCG loss is defined as:</p><formula xml:id="formula_37">L NDCG = 1 − 1 G max i∈P G(i) = G max − i∈P G(i) G max = i∈P G max /|P| − G(i) G max . (A.38)</formula><p>Note that different from AP Loss and aLRP Loss, here Z turns out to be 1, which makes sense since NDCG is normalized by definition. Also, based on Eq. A.38, one can identify NDCG Error on a positive as:</p><formula xml:id="formula_38">NDCG (i) = Gmax/|P|−G(i) Gmax such that G(i) = 1 log 2 (1+rank(i)) and G max = |P| i=1 log 2 (1 + i).</formula><p>Similar to AP and aLRP Loss, using p(j|i) = H(xij ) N F P (i) , the primary term of the NDCG Loss is L NDCG ij = NDCG (i)p(j|i) (line 1 of Algorithm 1 in the paper). When the positive example i is ranked properly, G(i) = 1 log 2 (1+1) = 1, and resulting desired NDCG Error is (line 2 of Algorithm 1):</p><formula xml:id="formula_39">NDCG (i) * = G max /|P| − 1 G max , (A.39)</formula><p>yielding a target primary term L NDCG ij * = NDCG i * p(j|i). Using L NDCG ij and L NDCG ij * , the update can be calculated as follows (line 3 of Algorithm 1):</p><formula xml:id="formula_40">∆x ij = L NDCG ij * − L NDCG ij = NDCG (i) * − NDCG (i) p(j|i), (A.40) = G max /|P| − G(i) G max − G max /|P| − 1 G max H(x ij ) N F P (i) , (A.41) = 1 − G(i) G max H(x ij ) N F P (i) , (A.42)</formula><p>and one can compute the gradients using Eq. 5 in the paper (line 4 of Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Computing aLRP Loss and its Gradients</head><p>This section presents the algorithm to compute aLRP Loss in detail along with an analysis of space and time complexity. For better understanding, bold font denotes multi-dimensional data structures (which can be implemented by vectors, matrices or tensors). Algorithm A.2 describes the steps to compute aLRP Loss along with the gradients for a given mini-batch.</p><p>Description of the inputs: S is the raw output of the classification branch, namely logits. For localisation, as done by IoU-based localisation losses [32, 28], the raw localisation outputs need to be converted to the boxes, which are denoted by B. We assume that M stores −1 for ignored anchors and 0 for negative anchors. For positive anchors, M stores the index of the ground truth (i.e. {1, ..., |B|}, whereB is a list of ground boxes for the mini-batch). Hence, we can find the corresponding ground truth for a positive anchor only by using M. δ is the smoothness of the piecewise linear function defined in Eq. A.43 and set to 1 following AP Loss. We use the self-balance ratio, L aLRP L aLRP cls , by averaging over its values from the previous epoch. We initialize it as 50 (i.e. see <ref type="table" target="#tab_3">Table 4</ref> in the paper).</p><p>Part 1: Initializing Variables: Lines 2-10 aim to initialize the necessary data from the inputs. While this part is obvious, please note that line 8 determines a threshold to select the relevant negative outputs. This is simply due to Eq. A.43 and the gradients of these negative examples with scores under this threshold are zero. Therefore, for the sake of time and space efficiency, they are ignored. Part 2: Computing Unnormalized Localisation Errors: Lines 12-14 compute unnormalized localisation error on each positive example. Line 12 simply finds the localisation error of each positive example and line 13 sorts these errors with respect to their scores in descending order, and Line 14 computes the cumulative sum of the sorted errors with cumsum function. In such a way, the example with the larger scores contributes to the error computed for each positive anchor with smaller scores. Note that while computing the nominator of the L aLRP loc , we employ the step function (not the piecewise linear function), since we can safely use backpropagation.  Note that to have a consistent denominator for total aLRP, we use rank to normalize both of the components. Lines 28-30 compute the gradients. While the local error is enough to determine the unnormalized gradient of a positive example, the gradient of a negative example is accumulated through the loop. Hence, we conclude that the time complexity of Algorithm A.2 is O(|N | + |P| × max(|P|, |N |)).</p><formula xml:id="formula_41">H(x) =      0, x &lt; −δ x 2δ + 0.5, −δ ≤ x ≤ δ 1, δ &lt; x.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared to AP Loss;</head><p>• aLRP Loss includes an extra computation of aLRP localisation component (i.e. lines 12-14, 27. Each of these lines requires O(|P|)).</p><p>• aLRP Loss includes an additional summation while computing the gradients with respect to the scores of the positive examples in line 29 requiring O(|P| 2 ).</p><p>• aLRP Loss discards interpolation (i.e. using interpolated AP curve), which can take up to O(|P| × |N |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Space Complexity</head><p>Algorithm A.2 does not require any data structure larger than network outputs (i.e. B, S). Then, we can safely conclude that the space complexity is similar to all of the common loss functions that is O(|S|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Details of aLRP Loss</head><p>This section provides details for aLRP Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 A Soft Sampling Perspective for aLRP Localisation Component</head><p>In sampling methods, the contribution (w i ) of the ith bounding box to the loss function is adjusted as follows:</p><formula xml:id="formula_42">L = i∈P∪N w i L(i), (A.44)</formula><p>where L(i) is the loss of the ith example. Hard and soft sampling approaches differ on the possible values of w i . For the hard sampling approaches, w i ∈ {0, 1}, thus a BB is either selected or discarded. For soft sampling approaches, w i ∈ [0, 1], i.e. the contribution of a sample is adjusted with a weight and each BB is somehow included in training. While this perspective is quite common to train the classification branch <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">15]</ref>; the localisation branch is conventionally trained by hard sampling with some exceptions (e.g. CARL <ref type="bibr" target="#b2">[3]</ref> sets w i = s i where s i is the classification score).</p><p>Here, we show that, in fact, what aLRP localisation component does is soft sampling. To see this, first let us recall the definition of the localisation component:</p><formula xml:id="formula_43">L aLRP loc = 1 |P| i∈P 1 rank(i)   E loc (i) + k∈P,k =i E loc (k)H(x ik )   , (A.45)</formula><p>which is differentiable with respect to the box parameters as discussed in the paper. With a rankingbased formulation, note that (i) the localisation error of a positive example i (i.e. E loc (i)) contributes each LRP value computed on a positive example j where s i ≥ s j (also see <ref type="figure" target="#fig_17">Fig. 2</ref> in the paper), and (ii) each LRP value computed on a positive example i is normalized by rank(i). Then, setting L(i) = E loc (i) in Eq. A.44 and accordingly taking Eq. A.45 in E loc (i) paranthesis, the weights of the positive examples (i.e. w i = 0 for negatives for the localisation component) are:</p><formula xml:id="formula_44">w i = 1 |P|      k∈P,k =i H(x ki ) rank(k)   + 1 rank(i)    . (A.46)</formula><p>Note that L(i) is based on a differentiable IoU-based regression loss and w i is its weight, which is a scaler. As a result H(x ki ) in Eq. A.46 does not need to be smoothed and we use a unit-step function (see line 14 in Algorithm A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 The Relation between aLRP Loss Value and Total Gradient Magnitudes</head><p>Here, we identify the relation between the loss value and the total magnitudes of the gradients following the generalized framework due to the fact that it is a basis for our self-balancing strategy introduced in Section 4.2 as follows:  Since we showed in Section C that i∈P ∂L ∂si = i∈N ∂L ∂si , here we show that the loss value is approximated by the total magnitude of gradients. Recall from Eq. (A.34) that total gradients of the positives can be expressed as:</p><formula xml:id="formula_45">i∈P ∂L ∂s i = 1 |P| i∈P j∈N ∆x ij . (A.48)</formula><p>Since ∆x ij ≤ 0, we can discard the absolute value by multiplying it by −1:</p><formula xml:id="formula_46">− 1 |P| i∈P j∈N ∆x ij . (A.49)</formula><p>Replacing the definition of the ∆x ij by L * ij − L ij yields:</p><formula xml:id="formula_47">− 1 |P| i∈P j∈N (L * ij − L ij ) = − 1 |P|   i∈P j∈N L * ij − i∈P j∈N L ij   (A.50) = 1 |P| i∈P j∈N L ij − 1 |P| i∈P j∈N L * ij . (A.51)</formula><p>Using Theorem 1, the first part (i.e 1 |P| i∈P j∈N L ij ) yields the loss value, L. Hence:</p><formula xml:id="formula_48">i∈P ∂L ∂s i = L − 1 |P| i∈P j∈N L * ij . (A.52)</formula><p>Reorganizing the terms, the difference between the total gradients of positives (or negatives, since they are equal -see Theorem 2) and the loss values itself is the sum of the targets normalized by number of positives:</p><formula xml:id="formula_49">L − i∈P ∂L ∂s i = 1 |P| i∈P j∈N L * ij . (A.53)</formula><p>Compared to the primary terms, the targets are very small values (if not 0). For example, for AP Loss L AP ij * = 0, and hence, loss is equal to the sum of the gradients: L = i∈P ∂L ∂si .</p><p>As for aLRP Loss, the target of a primary term is E loc (i) rank(i) H(xij ) N F P (i) , hence if H(x ij ) = 0, then the target is also 0. Else if H(x ij ) = 1, then it implies that there are some negative examples with larger scores, and rank(i) and N F P (i) are getting larger depending on these number of negative examples, which causes the denominator to grow, and hence yielding a small target as well. Then ignoring this term, we conclude that: where w r is a weight to balance the tasks is different from weighing the gradients with respect to the localisation output, B, since weighting the loss value (i.e. L aLRP loc + w r L aLRP loc ) changes the gradients of aLRP Loss with respect to the classification output as well since L aLRP loc , now weighed by w r , is also ranking-based (has rank(i) term -see Eq. 11 in the paper). Therefore, we directly add the self balance term as a multiplier of ∂L/∂B and backpropagate accordingly. On the other hand, from a practical perspective, this can simply be implemented by weighing the loss value, L aLRP loc without modifying the gradient formulation for L aLRP cls . </p><formula xml:id="formula_50">i∈P ∂L ∂s i = i∈N ∂L ∂s i ≈ L aLRP . (A.54)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Experiments</head><p>This section presents more ablation experiments, the anchor configuration we use in our models and the effect of using a wrong target for the primary term in the error-driven update rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 More Ablation Experiments: Using Self Balance and GIoU with AP Loss</head><p>We also test the effect of GIoU and our Self-balance approach on AP Loss, and present the results in <ref type="table" target="#tab_10">Table A</ref>.12:</p><p>• Using IoU-based losses with AP Loss improves the performance up to 1.0 AP as well and reaches 36.5 AP with GIoU loss.</p><p>• Our SB approach also improves AP Loss between 0.7 -1.2 AP, resulting in 37.2AP as the best performing model without using w r . However, it may not be inferred that SB performs better than constant weighting for AP Loss without a more thorough tuning of AP Loss since SB is devised to balance the gradients of localisation and classification outputs for aLRP Loss (see Section F.2).</p><p>• Comparing with the best performing model of AP Loss with 37.2AP, (i) aLRP Loss has a 1.7AP and 1.3oLRP points better performance, (ii) the gap is 4.0AP for AP 90 , and (iii) the correlation coeffient of aLRP Loss, preserves the same gap (0.48 vs 0.44 comparing the best models for AP and aLRP Losses), since applying these improvements (IoU-based losses and SB) to AP Loss does not have an effect on unifying branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Anchor Configuration</head><p>The number of anchors has a notable affect on the efficiency of training due to the time and space complexity of optimizing ranking-based loss functions by combining error-driven update and backpropagation. For this reason, different from original RetinaNet using three aspect ratios (i.e. [0.5, 1, 2]) and three scales (i.e. [2 0/2 , 2 1/2 , 2 2/2 ]) on each location, Chen et al. <ref type="bibr" target="#b6">[7]</ref> preferred the same three aspect ratios, but reduced the scales to two as [2 0/2 , 2 1/2 ] to increase the efficiency of AP Loss. In our ablation experiments, except the one that we used ATSS [34], we also followed the same anchor configuration of Chen et al. <ref type="bibr" target="#b6">[7]</ref>.</p><p>One main contribution of ATSS is to simplify the anchor design by reducing the number of required anchors to a single scale and aspect ratio (i.e. ATSS uses 1/9 and 1/6 of the anchors of RetinaNet [15] and AP Loss <ref type="bibr" target="#b6">[7]</ref> respectively), which is a perfect fit for our optimization strategy. For this reason, we used ATSS, however, we observed that the configuration in the original ATSS with a single aspect ratio and scale does not yield the best result for aLRP Loss, which may be related to the ranking nature of aLRP Loss which favors more examples to impose a more accurate ranking, loss and gradient computation. Therefore, different from ATSS configuration, we find it useful to set anchor scales [2 0/2 , 2 1/2 ] and [2 0/2 , 2 1/2 , 2 2/2 ] for aLRPLoss500 and aLRPLoss800 respectively and use a single aspect ratio with 1 following the original design of ATSS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Using a Wrong Target for the Primary Term in the Error-driven Update Rule</head><p>As discussed in our paper (Section 4.1, Equation 13), L ij * , the target value of the primary term L ij is non-zero due to the localisation error. It is easy to overlook this fact and assume that the target is zero. <ref type="figure" target="#fig_23">Fig. A.7</ref> presents this case where L ij * is set to 0 (i.e. minimum value of aLRP). In such a case, the training continues properly, similar to that of the correct case, up to a point and then diverges. Note that this occurs when the positives start to be ranked properly but are still assigned gradients since L ij * − L ij = 0 due to the nonzero localisation error. This causes i∈P ∂L ∂si &gt; i∈N ∂L ∂si , violating Theorem 2 (compare min-rate and max-rate in <ref type="figure" target="#fig_23">Fig. A.7)</ref>. Therefore, assigning proper targets as indicated in Section 3 in the paper is crucial for balanced training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Implementation Details for FoveaBox and Faster R-CNN</head><p>In this section, we provide more implementation details on the FoveaBox and Faster R-CNN models that we trained with different loss functions. All the models in this section are tested on COCO minival.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details of FoveaBox:</head><p>We train the models for 100 epochs with a learning rate decay at epochs 75 and 95. For aLRP Loss and AP Loss, we preserve the same learning rates used for RetinaNet (i.e. 0.008 and 0.002 for aLRP Loss and AP Loss respectively). As for the Focal Loss, we set the initial learning rate to 0.02 following the linear scheduling hypothesis [24] (i.e. Kong et al. set learning rate to 0.01 and use a batch size of 16). Following AP Loss official implementation, the gradients of the regression loss (i.e. Smooth L1) are averaged over the output parameters of positive boxes for AP Loss. As for Focal Loss, we follow the mmdetection implementation which averages the total regression loss by the number of positive examples. The models are tested on COCO minival by preserving the standard design by mmdetection framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details of Faster R-CNN:</head><p>To train Faster R-CNN, we first replace the softmax classifier of Fast R-CNN by the class-wise sigmoid classifiers. Instead of heuristic sampling rules, we use all anchors to train RPN and top-1000 scoring proposals per image obtained from RPN to train Fast R-CNN (i.e. same with the default training except for discarding sampling). Note that, with aLRP Loss, the loss function consists of two independent losses instead of four in the original pipeline, hence instead of three scalar weights, aLRP Loss requires a single weight for RPN head, which we tuned as 0.20. Following the positive-negative assignment rule of RPN, different from all the experiments, which use τ = 0.50, τ = 0.70 for aLRP Loss of RPN. We set the initial learning rate to 0.04 following the linear scheduling hypothesis [24] for the baselines, and decreased by a factor of 0.10 at epochs 75 and 95. Localisation loss weight is kept as 1 for L1 Loss and to 10 for GIoU Loss <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">28]</ref>. The models are tested on COCO minival by preserving the standard design by mmdetection framework. We do not train Faster R-CNN with AP Loss due to the difficulty to tune Faster R-CNN for a different loss function.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Performance in AP = (AP 50 +AP 65 +AP 80 +AP 95 )0.0025+0.10+0.175+0.25 aLRP Loss(1): 0.10/1 + (1+0.10+0.40)/3+(3+0.1+0.4+0.7)/6+(6+0.1+0.4+0.7+1)/10= 0.1+0.5+0.7+0.82=2.12/4=0.53 aLRP Loss(1): 0.40/1 + (1+0.40+0.70)/3+(3+1.+0.4+0.7)/6+(6+0.1+0.4+0.7+1)/10= 0.4+0.7+0.85+0.82=2.77/4=0.6925 aLRP Loss(3): 1/1 + (1+1+0.70)/3+(3+1+0.7+0.4)/6+(6+0.1+0.4+0.7+1)/10= 1+0.90+0.85+0.82=0.8925 (a) 3 possible localization outputs (R 1 -R 3 ) for the same classifier output (C) (Orange: Positive anchors, Gray: Negative anchors) (c) Comparison of different loss functions (Red: Improper ordering, Green: Proper ordering)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )</head><label>3</label><figDesc>We introduce aLRP Loss (and its gradients) as a special case of this generalized formulation. Replacing AP and SmoothL1 losses by aLRP Loss for training RetinaNet improves the performance by up to 5.4AP, and our best model reaches 48.9AP without test time augmentation, outperforming all existing one-stage detectors with significant margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>xij ) rank(i) (c.f. L AP ij in Eq. (4)). Then, since L AP ij * = 0, ∆x ij = 0 − L AP ij = −L AP ij in Eq. (5). Deriving Normalized Discounted Cumulative Gain Loss [18]: See Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Figure 2 :</head><label>32</label><figDesc>aLRP Loss assigns gradients to each branch based on the outputs of both branches. Examples on the PR curve are in sorted order wrt scores (s). L refers to L aLRP . (a) A p i 's gradient wrt its score considers (i) localisation errors of examples with larger s (e.g. high E loc (p 1 ) increases the gradient of s p2 to suppress p 1 ), (ii) number of negatives with larger s. (b) Gradients wrt s of the negatives: The gradient of a p i is uniformly distributed over the negatives with larger s. Summed contributions from all positives determine the gradient of a negative. (c) Gradients of the box parameters: While p 1 (with highest s) is included in total localisation error on each positive, i.e. L loc (i) = 1 rank(i) (E loc (i) + k∈P,k =i E loc (k)H(x ik )), p 3 is included once with the largest rank(p i ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>L</head><label></label><figDesc>aLRP is differentiable wrt the estimated box parameters, B, since E loc is differentiable [28, 32] (i.e. the derivatives of L aLRP cls and rank(·) wrt B are 0). However, L aLRP cls and L aLRP loc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>aLRP Loss and its components. The localisation component is self-balanced.Interpretation of the Components:A distinctive property of aLRP Loss is that classification and localisation errors are handled in a unified manner: i.e. with aLRP, both classification and localisation branches use the entire output of the detector, instead of working in their separate domains as conventionally done. As shown inFig. 2(a,b), L aLRP cls takes into account localisation errors of detections with larger scores (s) and promotes the detections with larger IoUs to have higher s, or suppresses the detections with high-s&amp;low-IoU. Similarly, L aLRP loc inherently weighs each positive based on its classification rank (see Appendix F for the weights): the contribution of a positive increases if it has a larger s. To illustrate, inFig. 2(c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Performance Measures: COCO-style AP [16] and when possible optimal LRP [20] (Sec. 2.2) are used for comparison. For more insight into aLRP Loss, we use Pearson correlation coefficient (ρ) to measure correlation between the rankings of classification and localisation, averaged over classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Effect of Self-Balancing (SB): Section 4.2 and Fig. 3 discussed how L aLRP cls and L aLRP loc behave during training and introduced self-balancing to improve training of the localisation branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>(left) The rate of the total gradient magnitudes of negatives to positives. (right) Loss values. Analysing Balance Between Positives and Negatives. For this analysis, we compare Cross Entropy Loss (CE), Focal Loss (FL) and aLRP Loss on RetinaNet trained for 12 epochs and average results over 10 runs. Fig. 4 experimentally confirms Theorem 2 for aLRP Loss (L aLRP cls )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>APPENDIX A Details of Figure 1 :</head><label>1</label><figDesc>Comparison of Loss Functions on a Toy ExampleThis section aims to present the scenario considered inFigure 1of the main paper. Section A.1 explains the scenario, Section A.2 and Section A.3 clarify how the performance measures (AP, AP 50 , etc.) and loss values (cross-entropy, AP Loss, aLRP Loss, etc.) are calculated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure A. 5 :Figure A. 6 :</head><label>56</label><figDesc>Visualization of anchor boxes in the scenarios used inFigure 1. Green and red boxes are positive anchors and ground truths respectively. p i refers to confidence score of the anchor a i . Note that for all of the scenarios, there are additionally six false positives (seeFigure 1), which are excluded in this figure for clarity. PR curve of each detector output-AP IoU pair. Rows and columns correspond to different AP IoU and detector outputs respectively. PR curves are interpolated (see the text for more detail).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>00) + log(1 − 0.90) + log(0.80) + log(1 − 0.70) + log(1 − 0.60) + log(0.50) (A.16) + log(1 − 0.40) + log(1 − 0.30) + log(1 − 0.20) + log(0.10) , (A.17) = 0.87. (A.18) A.3.2 Average precision (AP) Loss The computation of AP Loss is very similar to the AP 50 computation described in Section A.2 except that precision is calculated on (and also averaged over) the positive examples instead of the recall values. With this intuition the precision values on the four positives are 1.00, 0.67, 0.50, 0.40</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>00 + 0.00 + 0.00 + 0.05) + (0.00 + 0.00 + 0.00 + 0.25) (A.21) +(0.00 + 0.00 + 0.00 + 0.35) + (0.00 + 0.00 + 0.00 + 0.50) , (A.22) = 0.29. (A.23) A.3.4 IoU Loss For a single example, IoU Loss is simply 1 −</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>= 1 4 ( 1 −</head><label>41</label><figDesc>0.95) + (1 − 0.80) + (1 − 0.65) + (1 − 0.50) = 0.28.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>4 .</head><label>4</label><figDesc>N F P (i) is the number of negative examples with larger scores than the ith positive anchor. (See Section 3 for the formal definition.) 5. rank(i) is the rank of an example within positives and negatives. (See Section 2 for the formal definition.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>• A different set of approaches, an example of which is Faster R-CNN [27], uses directly cross entropy loss. However, cross entropy loss requires to be accompanied by a sampling method by which a set of positive and negative examples are sampled from the set of labelled anchors to alleviate the significant class imbalance. Even for random sampler, two of the following needs to be tuned in order to ensure stable training: (i) Number of positive examples (ii) Number of negative examples (iii) The rate between positives and negatives. Moreover, for a two-stage detector, these should be tuned for both stages, which brings about additional four hyper-parameters. That's why Faster R-CNN [27] in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Theorem 2 .</head><label>2</label><figDesc>Training is balanced between positive and negative examples at each iteration; i.e. the summed gradient magnitudes of positives and negatives are equal:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Part 3 :</head><label>3</label><figDesc>Computing Gradient and Error Contribution from Each Positive: Lines 16-32 compute the gradient and error contribution from each positive example. To do so, Line 16 initializes necessary data structures. Among these data structures, while L LRP loc , L LRP cls and ∂L aLRP ∂S+ are all with size |P|, ∂L aLRP ∂S− has size |N |, whereN is the number of negative examples after ignoring the ones with scores less than τ in Line 8, and obviously |N | ≤ |N |. The loop iterates over each positive example by computing LRP values and gradients since aLRP is defined as the average LRP values over positives (see Eq. 9 in the paper). Lines 18-22 computes the relation between the corresponding positive with positives and relevant negatives, each of which requires the difference transformation followed by piecewise linear function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>(</head><label></label><figDesc>A.43) Then, using these relations, lines 23-25 compute the rank of the ith examples within positive examples, number of negative examples with larger scores (i.e. false positives) and rank of the example. Lines 26 and 27 compute aLRP classification and localisation errors on the corresponding positive example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Part 4 :</head><label>4</label><figDesc>Computing aLRP Loss and Gradients: Lines 34-40 simply derive the final aLRP value by averaging over LRP values (lines 34-36), normalize the gradients (lines 37-38) and compute gradients wrt the boxes (line 39) and applies self balancing (line 40).E.1 Time Complexity • First 16 lines of Algorithm A.2 require time between O(|P|) and O(|N |). Since for the object detection problem, the number of negative examples is quite larger than number of positive anchors (i.e. |P| &lt;&lt; |N |), we can conclude that the time complexity of first 13 lines is O(|N |). • The bottleneck of the algorithm is the loop on lines 17-32. The loop iterates over each positive example, and in each iteration while lines 21, 24 and 30 are executed for relevant negative examples, the rest of the lines is executed for positive examples. Hence the number of operations for each iteration is max(|P|, |N |) (i.e. number of relevant negatives, see lines 8-9), and overall these lines require O(|P| × max(|P|, |N |)). Note that, while in the early training epochs, |N | ≈ |N |, as the training proceeds, the classifier tends to distinguish positive examples from negative examples very well, and |N | significantly decreases implying faster mini-batch iterations. • The remaining lines between 26-33 again require time between O(|P|) and O(|N |).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Algorithm A. 2</head><label>2</label><figDesc>The algorithm to compute aLRP Loss for a mini-batch. Input: S: Logit predictions of the classifier for each anchor, B: Box predictions of the localization branch from each anchor, B: Ground truth (GT) boxes, M: Matching of the anchors with the GT boxes. δ: Smoothness of the piece-wise linear function (δ = 1 by default). w ASB : ASB weight, computed using L aLRP L aLRP cls values from previous epoch. Output: L aLRP : aLRP loss, ∂L aLRP ∂S : Gradients wrt logits, ∂L aLRP ∂B : Gradients wrt boxes. 1: // Part 1: Initializing Variables 2: idx + := The indices of M where M &gt; 0. 3: M + := The values of M where M &gt; 0. 4: B + := The values of B at indices idx + . 5: S + := The values of S at indices idx + . 6: idx sorted + := The indices of S + once it is sorted in descending order. 7: S sorted + := The values of S + when ordered according to idx sorted + . 8: τ = min(S + ) − δ. 9: idx − := The indices of M where M = 0 and s j ≥ τ (i.e. relevant negatives only). 10: S − := The values of S at indices idx − . 11: // Part 2: Computing Unnormalized Localisation Errors 12: E Loc = 1−IoU(B+,B+) 1−τ . (or E Loc = (1−GIoU(B+,B+))/2 1−τ for GIoU Loss [28].) 13: E sorted Loc := The values of E Loc when ordered according to idx sorted + . 14: E cumsum Loc = cumsum(E sorted Loc ) 15: // Part 3: Computing Gradient and Error Contribution from Each Positive 16: Initialize , L LRP loc , L LRP cls , ∂L aLRP ∂S+ and ∂L aLRP ∂S− . 17: for each s i ∈ S sorted + do 18: X + := Difference transform of s i with the logit of each positive example. 19: R + := The relation of i ∈ P with each j ∈ P using Eq. A.43 with input X + . 20: R + [i] = 0 21: X − := Difference transform of s i with the logit of each negative example. 22: R − := The relation of i ∈ P with each j ∈ N using Eq. A.43 with input X + . 23: rank + = 1 + sum(R + ) 24: FP = sum(R − ) 25: rank = rank + + FP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>F. 3</head><label>3</label><figDesc>Self-balancing the Gradients Instead of the Loss Value Instead of localisation the loss, L aLRP loc , we multiply ∂L/∂B by the average L aLRP /L aLRP loc of the previous epoch. This is because formulating aLRP Loss as L aLRP loc + w r L aLRP loc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure A. 7 :</head><label>7</label><figDesc>(left) The rate of the total gradient magnitudes of negatives to positives. (right) Loss values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>State-of-the-art loss functions have several hyperparameters (6.4 on avg.). aLRP Loss has only one for step-function approximation (Sec. 2.1). See Appendix B for descriptions of the required hyperparameters. FL: Focal Loss, CE: Cross Entropy, SL1: Smooth L1, H: Hinge Loss.</figDesc><table><row><cell>Method</cell><cell>L</cell><cell>Number of hyperparameters</cell></row><row><cell>AP Loss [7]</cell><cell>AP Loss+α SL1</cell><cell>3</cell></row><row><cell>Focal Loss [15]</cell><cell>FL+ α SL1</cell><cell>4</cell></row><row><cell>FCOS [30]</cell><cell>FL+α IoU+β CE</cell><cell>4</cell></row><row><cell>DR Loss [26]</cell><cell>DR Loss+α SL1</cell><cell>5</cell></row><row><cell>FreeAnchor [35]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>SB does not require tuning and slightly outperforms constant weighting for both IoU types. IoU 36.9 37.8 38.5 38.6 38.3 37.1 36.0 38.7 w GIoU 36.0 37.0 37.9 38.7 38.8 38.7 38.8 38.9</figDesc><table><row><cell>wr</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>SB</cell></row><row><cell>w</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>SB is not affected significantly by the initial weight in the first epoch (w r ) even for large values.</figDesc><table><row><cell>wr</cell><cell>1</cell><cell>50</cell><cell>100 500</cell></row><row><cell cols="4">AP 38.8 38.9 38.7 38.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effect of correlating rankings. AP AP50 AP75 AP90 aLRP Loss 0.48 38.7 58.1 40.6 17.4 Lower Bound −1.00 28.6 58.1 23.6 5.6 Upper Bound 1.00 48.1 58.1 51.9 33.9</figDesc><table /><note>L ρ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 )</head><label>2</label><figDesc>, respectively, aLRP Loss yields higher correlation (0.48, 0.49).</figDesc><table><row><cell>0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2</cell><cell>Legend Min Rate Max Rate 1/4.269 1083.708 1/5.731 4.790 1/1.000 1.000 Iteration Cross Entropy Focal Loss aLRP Loss 0K 25K 50K 75K 100K 125K 150K 175K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison with the SOTA detectors on COCO test-dev. S, ×1.66 implies that the image is rescaled such that its longer side cannot exceed 1.66 × S where S is the size of the shorter side. R:ResNet, X:ResNeXt, H:HourglassNet, D:DarkNet, De:DeNet. We use ResNeXt101 64x4d. ×1.66 50.2 70.3 53.9 32.0 53.1 63.0 † : multiscale training, ‡ : SSD-like augmentation, * : Soft NMS [1] and flip augmentation at test time</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Training Size</cell><cell>Test Size AP AP50 AP75 APS APM APL</cell></row><row><cell>One-Stage Methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RefineDet [33]  ‡</cell><cell>R-101</cell><cell>512 × 512</cell><cell>512 × 512 36.4 57.5 39.5 16.6 39.9 51.4</cell></row><row><cell>EFGRNet [19]  ‡</cell><cell>R-101</cell><cell>512 × 512</cell><cell>512 × 512 39.0 58.8 42.3 17.8 43.6 54.5</cell></row><row><cell>ExtremeNet [36]  *  ‡</cell><cell>H-104</cell><cell>511 × 511</cell><cell>original 40.2 55.5 43.2 20.4 43.2 53.1</cell></row><row><cell>RetinaNet [15]</cell><cell>X-101</cell><cell>800, ×1.66</cell><cell>800, ×1.66 40.8 61.1 44.1 24.1 44.2 51.2</cell></row><row><cell>HSD [2]  ‡</cell><cell>X-101</cell><cell>512 × 512</cell><cell>512 × 512 41.9 61.1 46.2 21.8 46.6 57.0</cell></row><row><cell>FCOS [30]  †</cell><cell>X-101</cell><cell cols="2">(640, 800), ×1.66 800, ×1.66 44.7 64.1 48.4 27.6 47.5 55.6</cell></row><row><cell>CenterNet [8]  *  ‡</cell><cell>H-104</cell><cell>511 × 511</cell><cell>original 44.9 62.4 48.1 25.6 47.4 57.4</cell></row><row><cell>ATSS [34]  †</cell><cell cols="3">X-101-DCN (640, 800), ×1.66 800, ×1.66 47.7 66.5 51.9 29.7 50.8 59.4</cell></row><row><cell>Ranking Losses</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AP Loss500 [7]  ‡</cell><cell>R-101</cell><cell>512 × 512</cell><cell>500, ×1.66 37.4 58.6 40.5 17.3 40.8 51.9</cell></row><row><cell>AP Loss800 [7]  ‡</cell><cell>R-101</cell><cell>800 × 800</cell><cell>800, ×1.66 40.8 63.7 43.7 25.4 43.9 50.6</cell></row><row><cell>DR Loss [26]  †</cell><cell>X-101</cell><cell cols="2">(640, 800), ×1.66 800, ×1.66 43.1 62.8 46.4 25.6 46.2 54.0</cell></row><row><cell>Combining Branches</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LapNet [4]</cell><cell>D-53</cell><cell>512 × 512</cell><cell>512 × 512 37.6 55.5 40.4 17.6 40.5 49.9</cell></row><row><cell>Fitness NMS [31]</cell><cell>De-101</cell><cell>512, ×1.66</cell><cell>768, ×1.66 39.5 58.0 42.6 18.9 43.5 54.1</cell></row><row><cell>Retina+PISA [3]</cell><cell>R-101</cell><cell>800, ×1.66</cell><cell>800, ×1.66 40.8 60.5 44.2 23.0 44.2 51.4</cell></row><row><cell>FreeAnchor [35]  †</cell><cell>X-101</cell><cell cols="2">(640, 800), ×1.66 800, ×1.66 44.9 64.3 48.5 26.8 48.3 55.9</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row><row><cell>aLRP Loss500  ‡</cell><cell>R-50</cell><cell>512 × 512</cell><cell>500, ×1.66 41.3 61.5 43.7 21.9 44.2 54.0</cell></row><row><cell>aLRP Loss500  ‡</cell><cell>R-101</cell><cell>512 × 512</cell><cell>500, ×1.66 42.8 62.9 45.5 22.4 46.2 56.8</cell></row><row><cell>aLRP Loss500  ‡</cell><cell>X-101</cell><cell>512 × 512</cell><cell>500, ×1.66 44.6 65.0 47.5 24.6 48.1 58.3</cell></row><row><cell>aLRP Loss800  ‡</cell><cell>R-101</cell><cell>800 × 800</cell><cell>800, ×1.66 45.9 66.4 49.1 28.5 48.9 56.7</cell></row><row><cell>aLRP Loss800  ‡</cell><cell>X-101</cell><cell>800 × 800</cell><cell>800, ×1.66 47.8 68.4 51.1 30.2 50.8 59.1</cell></row><row><cell>aLRP Loss800  ‡</cell><cell>X-101-DCN</cell><cell>800 × 800</cell><cell>800, ×1.66 48.9 69.3 52.5 30.8 51.5 62.1</cell></row><row><cell>Multi-Scale Test</cell><cell></cell><cell></cell><cell></cell></row><row><cell>aLRP Loss800  ‡</cell><cell>X-101-DCN</cell><cell>800 × 800</cell><cell>800,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison on FoveaBox [13]. Focal Loss+SL1 38.3 57.8 40.7 15.7 68.8 AP Loss+SL1 36.5 58.3 38.2 11.3 69.8 aLRP Loss (Ours) 39.7 58.8 41.5 18.2 67.2</figDesc><table><row><cell>L</cell><cell>AP AP50 AP75 AP90 oLRP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison on Faster R-CNN [27] L AP AP50 AP75 AP90 oLRP Cross Entropy+L1 37.8 58.1 41.0 12.2 69.3 Cross Entropy+GIoU 38.2 58.2 41.3 13.7 69.0 aLRP Loss (Ours) 40.7 60.7 43.3 18.0 66.7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>9] Everingham M, Van Gool L, Williams CKI, Winn J, Zisserman A (2010) The pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV) 88(2):303-338 [10] Girshick R (2015) Fast R-CNN. In: The IEEE International Conference on Computer Vision Savarese S (2019) Generalized intersection over union: A metric and a loss for bounding box regression. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)[29] Rosenblatt F (1958) The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review pp 65-386</figDesc><table><row><cell>(ICCV)</cell></row><row><cell>[11] Jiang B, Luo R, Mao J, Xiao T, Jiang Y (2018) Acquisition of localization confidence for</cell></row><row><cell>accurate object detection. In: The European Conference on Computer Vision (ECCV) [30] Tian Z, Shen C, Chen H, He T (2019) Fcos: Fully convolutional one-stage object detection. In:</cell></row><row><cell>The IEEE International Conference on Computer Vision (ICCV)</cell></row><row><cell>[12] Kendall A, Gal Y, Cipolla R (2018) Multi-task learning using uncertainty to weigh losses for [31] Tychsen-Smith L, Petersson L (2018) Improving object localization with fitness nms and scene geometry and semantics. In: The IEEE Conference on Computer Vision and Pattern bounded iou loss. In: The IEEE Conference on Computer Vision and Pattern Recognition Recognition (CVPR) (CVPR)</cell></row><row><cell>[13] Kong T, Sun F, Liu H, Jiang Y, Li L, Shi J (2020) Foveabox: Beyound anchor-based object [32] Yu J, Jiang Y, Wang Z, Cao Z, Huang T (2016) Unitbox: An advanced object detection network. detection. IEEE Transactions on Image Processing 29:7389-7398 In: The ACM International Conference on Multimedia</cell></row><row><cell>[14] Li B, Liu Y, Wang X (2019) Gradient harmonized single-stage detector. In: AAAI Conference</cell></row><row><cell>on Artificial Intelligence</cell></row></table><note>[15] Lin T, Goyal P, Girshick R, He K, Dollár P (2020) Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 42(2):318-327 [16] Lin TY, Maire M, Belongie S, Hays J, Perona P, Ramanan D, Dollár P, Zitnick CL (2014) Microsoft COCO: Common Objects in Context. In: The European Conference on Computer Vision (ECCV) [17] Liu W, Anguelov D, Erhan D, Szegedy C, Reed SE, Fu C, Berg AC (2016) SSD: single shot multibox detector. In: The European Conference on Computer Vision (ECCV) [18] Mohapatra P, Rolínek M, Jawahar C, Kolmogorov V, Pawan Kumar M (2018) Efficient op- timization for rank-based loss functions. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) [19] Nie J, Anwer RM, Cholakkal H, Khan FS, Pang Y, Shao L (2019) Enriched feature guided refinement network for object detection. In: The IEEE International Conference on Computer Vision (ICCV) [20] Oksuz K, Cam BC, Akbas E, Kalkan S (2018) Localization recall precision (LRP): A new performance metric for object detection. In: The European Conference on Computer Vision (ECCV) [21] Oksuz K, Cam BC, Akbas E, Kalkan S (2020) One metric to measure them all: Localisation recall precision (lrp) for evaluating visual detection tasks. arXiv 2011.10772 [22] Oksuz K, Cam BC, Kalkan S, Akbas E (2020) Imbalance problems in object detection: A review. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) pp 1-1 [23] Pang J, Chen K, Shi J, Feng H, Ouyang W, Lin D (2019) Libra R-CNN: Towards balanced learning for object detection. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) [24] Peng C, Xiao T, Li Z, Jiang Y, Zhang X, Jia K, Yu G, Sun J (2018) Megdet: A large mini-batch object detector. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) [25] Pogančić MV, Paulus A, Musil V, Martius G, Rolinek M (2020) Differentiation of blackbox combinatorial solvers. In: International Conference on Learning Representations (ICLR) [26] Qian Q, Chen L, Li H, Jin R (2020) Dr loss: Improving object detection by distributional ranking. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) [27] Ren S, He K, Girshick R, Sun J (2017) Faster R-CNN: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 39(6):1137-1149 [28] Rezatofighi H, Tsoi N, Gwak J, Sadeghian A, Reid I,[33] Zhang S, Wen L, Bian X, Lei Z, Li SZ (2018) Single-shot refinement neural network for object detection. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) [34] Zhang S, Chi C, Yao Y, Lei Z, Li SZ (2020) Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) [35] Zhang X, Wan F, Liu C, Ji R, Ye Q (2019) Freeanchor: Learning to match anchors for visual object detection. In: Advances in Neural Information Processing Systems (NeurIPS) [36] Zhou X, Zhuo J, Krahenbuhl P (2019) Bottom-up object detection by grouping extreme and center points. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) [37] Zhu X, Hu H, Lin S, Dai J (2019) Deformable convnets v2: More deformable, better results. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A .</head><label>A</label><figDesc>9: Precision of each detector output-AP IoU pair for evenly spaced recall values. This table is based on the PR curves presented in Fig. A.6. R=0.2 R=0.3 R=0.4 R=0.5 R=0.6 R=0.7 R=0.8 R=0.9 R=1.0</figDesc><table><row><cell>IoU Output</cell><cell>R=0.1</cell><cell>Precisions for Different Recalls (R)</cell><cell>AP IoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A .</head><label>A</label><figDesc>10: Per-box calculation of L aLRPOutput Anchor 1 − IoU(i) E loc (i) cumsum(E loc )(i) N F P (i) rank(i) LRP (i) L aLRP</figDesc><table><row><cell></cell><cell>a 1</cell><cell>0.05</cell><cell>0.10</cell><cell>0.10</cell><cell>0.00</cell><cell>1.00</cell><cell>0.10</cell><cell></cell></row><row><cell>C&amp;R1</cell><cell>a 3 a 6</cell><cell>0.20 0.35</cell><cell>0.40 0.70</cell><cell>0.50 1.20</cell><cell>1.00 3.00</cell><cell>3.00 6.00</cell><cell>0.50 0.70</cell><cell>0.53</cell></row><row><cell></cell><cell>a 10</cell><cell>0.50</cell><cell>1.00</cell><cell>2.20</cell><cell>6.00</cell><cell>10.00</cell><cell>0.82</cell><cell></cell></row><row><cell></cell><cell>a 1</cell><cell>0.20</cell><cell>0.40</cell><cell>0.40</cell><cell>0.00</cell><cell>1.00</cell><cell>0.40</cell><cell></cell></row><row><cell>C&amp;R2</cell><cell>a 3 a 6</cell><cell>0.35 0.50</cell><cell>0.70 1.00</cell><cell>1.10 2.10</cell><cell>1.00 3.00</cell><cell>3.00 6.00</cell><cell>0.70 0.85</cell><cell>0.69</cell></row><row><cell></cell><cell>a 10</cell><cell>0.20</cell><cell>0.40</cell><cell>2.50</cell><cell>6.00</cell><cell>10.00</cell><cell>0.82</cell><cell></cell></row><row><cell></cell><cell>a 1</cell><cell>0.50</cell><cell>1.00</cell><cell>1.00</cell><cell>0.00</cell><cell>1.00</cell><cell>1.00</cell><cell></cell></row><row><cell>C&amp;R3</cell><cell>a 3 a 6</cell><cell>0.35 0.20</cell><cell>0.70 0.40</cell><cell>1.70 2.10</cell><cell>1.00 3.00</cell><cell>3.00 6.00</cell><cell>0.90 0.85</cell><cell>0.89</cell></row><row><cell></cell><cell>a 10</cell><cell>0.05</cell><cell>0.10</cell><cell>2.20</cell><cell>6.00</cell><cell>10.00</cell><cell>0.82</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A .</head><label>A</label><figDesc>11: Common loss functions and the hyperparameters in their definitions.</figDesc><table><row><cell></cell><cell>Loss Function</cell><cell>Type</cell><cell cols="2">Number &amp; Usage of the Hyper-parameters</cell></row><row><cell></cell><cell>Cross-entropy [17, 27]</cell><cell>Score-based</cell><cell cols="2">0 Sampling methods are required</cell></row><row><cell></cell><cell>α-bal. Cross-entropy[15]</cell><cell>Score-based</cell><cell cols="2">1 The weight of the foreground anchors</cell></row><row><cell></cell><cell>Focal Loss [15]</cell><cell>Score-based</cell><cell>2</cell><cell>The weight of the foreground anchors Modulating factor for hard examples</cell></row><row><cell>c L</cell><cell>AP Loss [7]</cell><cell cols="3">Ranking-based 1 Smoothness of the step function Regularizer for foreground distribution</cell></row><row><cell></cell><cell>DR Loss [26]</cell><cell cols="2">Ranking-based 3</cell><cell>Regularizer for background distribution</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Smoothness of the loss</cell></row><row><cell></cell><cell>Smooth L 1 [10]</cell><cell>l p -based</cell><cell cols="2">1 Cut-off from L 1 loss to L 2 loss</cell></row><row><cell>L r</cell><cell>Balanced L 1 [23]</cell><cell>l p -based</cell><cell>2</cell><cell>The weight of the inlier anchors Upper bound of the loss value</cell></row><row><cell></cell><cell>IoU Loss [28]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Place ∂L aLRP ∂S+ and ∂L aLRP ∂S− into ∂L aLRP ∂S also by setting the gradients of remaining examples to 0. 38: ∂L aLRP ∂S / = |P| 39: Compute ∂L aLRP ∂B (possibly using autograd property of a deep learning library or refer to the supp. mat. of [28] for the gradients of GIoU and IoU Losses. ASB 41: return ∂L aLRP ∂S , ∂L aLRP</figDesc><table><row><cell>26: 27: 28:</cell><cell cols="4">L LRP cls [i] = FP/rank L LRP loc [i] = E cumsum Loc if FP ≥ then //For stability set to a small value (e.g. 1e − 5) [i]/rank</cell></row><row><cell>29:</cell><cell>∂L aLRP ∂S+ [i] = − FP +</cell><cell>i∈P</cell><cell>R + [i] × E cumsum Loc</cell><cell>[i] /rank</cell></row><row><cell>30:</cell><cell cols="3">∂L aLRP ∂S− + = − ∂L aLRP ∂S+ [i] × R− FP</cell><cell></cell></row><row><cell>31:</cell><cell>end if</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">32: end for</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">33: // Part 4: Computing the aLRP Loss and Gradients</cell><cell></cell></row><row><cell cols="2">34: L aLRP cls 35: L aLRP loc 36: L aLRP = L aLRP = mean(L LRP cls ) = mean(L LRP loc ) cls + L aLRP loc</cell><cell></cell><cell></cell><cell></cell></row><row><cell>37: 40:</cell><cell>∂L aLRP loc ∂B × = w</cell><cell></cell><cell></cell><cell></cell></row></table><note>∂B and L aLRP .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A .</head><label>A</label><figDesc>12: Using Self Balance and GIoU with AP Loss. For optimal LRP (oLRP), lower is better.</figDesc><table><row><cell>Lc</cell><cell>Lr</cell><cell>SB</cell><cell>AP</cell><cell cols="4">AP50 AP75 AP90 oLRP</cell><cell>ρ</cell></row><row><cell></cell><cell>Smooth L1</cell><cell></cell><cell>35.5</cell><cell>58.0</cell><cell>37.0</cell><cell>9.0</cell><cell>71.0</cell><cell>0.45</cell></row><row><cell></cell><cell>Smooth L1</cell><cell></cell><cell>36.7</cell><cell>58.2</cell><cell>39.0</cell><cell>10.8</cell><cell>70.2</cell><cell>0.44</cell></row><row><cell>AP Loss [7]</cell><cell>IoU Loss IoU Loss</cell><cell></cell><cell>36.3 37.2</cell><cell>57.9 58.1</cell><cell>37.9 39.2</cell><cell>11.8 13.1</cell><cell>70.4 69.6</cell><cell>0.44 0.44</cell></row><row><cell></cell><cell>GIoU Loss</cell><cell></cell><cell>36.5</cell><cell>58.1</cell><cell>38.1</cell><cell>11.9</cell><cell>70.2</cell><cell>0.45</cell></row><row><cell></cell><cell>GIoU Loss</cell><cell></cell><cell>37.2</cell><cell>58.3</cell><cell>39.0</cell><cell>13.4</cell><cell>69.7</cell><cell>0.44</cell></row><row><cell></cell><cell>with IoU</cell><cell></cell><cell>36.9</cell><cell>57.7</cell><cell>38.4</cell><cell>13.9</cell><cell>69.9</cell><cell>0.49</cell></row><row><cell>aLRP Loss</cell><cell>with IoU</cell><cell></cell><cell>38.7</cell><cell>58.1</cell><cell>40.6</cell><cell>17.4</cell><cell>68.5</cell><cell>0.48</cell></row><row><cell></cell><cell>with GIoU</cell><cell></cell><cell>38.9</cell><cell>58.5</cell><cell>40.5</cell><cell>17.4</cell><cell>68.4</cell><cell>0.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Legend Min Rate Max Rate</figDesc><table><row><cell></cell><cell>2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>| si | si |/ i |</cell><cell>2.0 1.4 1.6 1.8</cell><cell>L aLRP ij Correct L aLRP * = 0 ij</cell><cell>*</cell><cell>diverges</cell><cell>1/1.083 1.000 1/1.000 1.000</cell></row><row><cell>Rate= i</cell><cell>0.8 1.0 1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell cols="4">0K 25K 50K 75K 100K 125K 150K 175K Iteration</cell><cell>0K 25K 50K 75K 100K 125K 150K 175K Iteration</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical shot detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Prime Sample Attention in Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno>arXiv 1904.04821</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lapnet : Automatic balanced loss and optimal assignment for real-time dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<idno>arXiv 1911.01149</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K (last</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Accessed</surname></persName>
		</author>
		<ptr target="https://githubcom/cccorn/AP-loss" />
		<title level="m">Ap-loss</title>
		<imprint>
			<date type="published" when="2020-05-14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno>1906.07155</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ap-loss for accurate one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
