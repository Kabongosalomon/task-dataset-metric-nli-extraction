<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trajectory Space Factorization for Deep Video-Based 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
							<email>jiahao@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim</forename><forename type="middle">Hee</forename><surname>Lee</surname></persName>
							<email>gimhee.lee@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Trajectory Space Factorization for Deep Video-Based 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LIN ET AL.: TRAJECTORY SPACE FACTORIZATION FOR 3D HUMAN POSE ESTIMATION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing deep learning approaches on 3d human pose estimation for videos are either based on Recurrent or Convolutional Neural Networks (RNNs or CNNs). However, RNN-based frameworks can only tackle sequences with limited frames because sequential models are sensitive to bad frames and tend to drift over long sequences. Although existing CNN-based temporal frameworks attempt to address the sensitivity and drift problems by concurrently processing all input frames in the sequence, the existing stateof-the-art CNN-based framework is limited to 3d pose estimation of a single frame from a sequential input. In this paper, we propose a deep learning-based framework that utilizes matrix factorization for sequential 3d human poses estimation. Our approach processes all input frames concurrently to avoid the sensitivity and drift problems, and yet outputs the 3d pose estimates for every frame in the input sequence. More specifically, the 3d poses in all frames are represented as a motion matrix factorized into a trajectory bases matrix and a trajectory coefficient matrix. The trajectory bases matrix is precomputed from matrix factorization approaches such as Singular Value Decomposition (SVD) or Discrete Cosine Transform (DCT), and the problem of sequential 3d pose estimation is reduced to training a deep network to regress the trajectory coefficient matrix. We demonstrate the effectiveness of our framework on long sequences by achieving state-ofthe-art performances on multiple benchmark datasets. Our source code is available at: https://github.com/jiahaoLjh/trajectory-pose-3d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction 3d human pose estimation from a 2d image is an important task in computer vision that has a wide range of applications such as human-computer interaction, augmented/virtual reality, camera surveillance, etc. Nonetheless, 3d pose estimation remains a challenging problem due to the inherent ambiguity in the ill-posed problem of lifting 3d pose from a 2d image.</p><p>Over the years, numerous research has been done on 3d pose estimation from a single image and a sequence of input frames, respectively. Despite the intuition that 3d pose estimation should be improved by having more information from the sequence of input frames, the performance of temporal-based frameworks are unexpectedly limited compared to its single frame counterpart. One of the reasons is sequential frameworks are mostly based on RNNs <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b22">20]</ref> that are very sensitive to the quality of the estimate at each time step. Just a few frames of bad estimates would lead to dire results in the estimation for all subsequent c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1908.08289v1 [cs.CV] 22 Aug 2019 frames. Recently, CNN-based temporal frameworks <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b40">37]</ref> have shown advantages over RNNs in modeling temporal information. However, concurrently estimating all frames in a long sequence is an arduous task for data-driven approaches due to the increasing dimensionality of the output space with longer sequences, and the network for "many-to-many mapping" requires significantly more data to train. As a result, the state-of-the-art temporal framework for 3d pose estimation <ref type="bibr" target="#b32">[29]</ref> only outputs the estimate of a single frame centered on an input sequence of a few hundred frames.</p><p>A key observation is that consecutive frames of human motions are highly correlated and abrupt changes in the motions are unlikely. This suggests that we do not need to estimate the 3d pose in each frame of a sequence independently. Instead, we can effectively reduce the output dimension by decomposing the uncorrelated components. To this end, we propose a deep learning-based framework that utilizes matrix factorization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b39">36]</ref> for sequential 3d human poses estimation. More specifically, the joints of the 3d pose estimates in all frames are represented as a motion matrix factorized into a trajectory bases matrix and a trajectory coefficient matrix. The trajectory bases matrix is precomputed with SVD or cosine bases from DCT, and the problem of sequential 3d pose estimation is reduced to training a deep network to regress the trajectory coefficient matrix. We show that a relatively small number of bases (compared to the large number of input frames) can effectively approximate the human motions. We transform the extracted features from Cartesian space to trajectory space and regress the trajectory coefficients with a densely connected multilayer perceptron (MLP). The 3d poses can then be reconstructed from the linear combination of the trajectory bases with the trajectory coefficients regressed from the deep network.</p><p>Our main contributions are: (1) we present an effective framework to estimate 3d human poses for a long sequence in the trajectory space; <ref type="bibr" target="#b1">(2)</ref> we show experimentally that a small number of trajectory bases are sufficient to model human motions which greatly reduces the complexity of the network training; and (3) our method gives state-of-the-art performance on multiple benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early methods for 3d human pose estimation from 2d images use a variety of hand-crafted features such as silhouette <ref type="bibr" target="#b0">[1]</ref>, shape <ref type="bibr" target="#b27">[25]</ref>, SIFT <ref type="bibr" target="#b2">[3]</ref>, HOG <ref type="bibr" target="#b34">[31]</ref> to estimate 3d human poses. More recently, a large number of works are based on the highly popular and effective deep neural networks. Li et al. <ref type="bibr" target="#b21">[19]</ref> apply a deep convolutional neural network to jointly regress pose and detect body parts. Tekin et al. <ref type="bibr" target="#b36">[33]</ref> propose to learn a high-dimensional latent pose representation accouting for joint dependencies using auto-encoder. Pavlakos et al. <ref type="bibr" target="#b31">[28]</ref> extend the idea of regressing heat-map from 2d to 3d and use a coarse-to-fine approach to estimate the 3d heat-map for joint estimation. Sun et al. <ref type="bibr" target="#b35">[32]</ref> exploit the skeleton structure and define a compositional loss function to better learn joint interactions. Lee et al. <ref type="bibr" target="#b19">[18]</ref> propose a structure-aware multi-stage architecture to regress body parts incrementally. Training deep learning models directly from images requires huge amounts of data to be labeled with 3d ground truth. To alleviate this problem, a two-stage framework is proposed in several works. In particular, 2d poses are first estimated and subsequently used for 3d pose estimation. Yasin et al. <ref type="bibr" target="#b42">[39]</ref> and Chen et al. <ref type="bibr" target="#b6">[6]</ref> exploit the idea of 3d pose retrieval by retrieving the nearest 3d poses in a 3d library based on the 2d poses. Moreno-Noguer <ref type="bibr" target="#b26">[24]</ref> estimates the 3d pose by regressing a distance matrix from 2d space to 3d space. Martinez et al. <ref type="bibr" target="#b23">[21]</ref> propose a simple yet effective regression network based on fully-connected residual blocks. Fang et al. <ref type="bibr" target="#b12">[12]</ref> integrate kinematic knowledge into their framework and design a hierarchical grammar model. <ref type="bibr">Zhao et al. [40]</ref> propose to use graph neural networks to model structural information in human poses. Our work also follows the two-stage approach.</p><p>The inherent depth ambiguity in 3d pose estimation from monocular images limits the estimation accuracy. Extensive research has been done to exploit extra information contained in temporal sequences. Zhou et al. <ref type="bibr" target="#b45">[42]</ref> formulate an optimization problem to search for the 3d configuration with the highest probability given 2d confidence maps and solve the problem using Expectation-Maximization. Tekin et al. <ref type="bibr" target="#b37">[34]</ref> use a CNN to align bounding box of consecutive frames and then generate a spatial-temporal volume based on which they extract 3d HOG features and regress the 3d pose for the central frame. Mehta et al. <ref type="bibr" target="#b25">[23]</ref> propose a real-time system for 3d pose estimation and apply temporal filtering to yield temporally consistent 3d poses.</p><p>Recently, RNN-based frameworks are used to deal with sequential input data. Lin et al. <ref type="bibr" target="#b22">[20]</ref> use a multi-stage framework based on Long Short-term Memory (LSTM) units to estimate the 3d pose from the extracted 2d features and estimated 3d pose in the previous stage. Coskun et al. <ref type="bibr" target="#b9">[9]</ref> propose to learn a human motion model using Kalman Filter and implement it with LSTMs. Hossain et al. <ref type="bibr" target="#b14">[14]</ref> design a sequence-to-sequence network with LSTM units to first encode a sequence of motions in the form of 2d joint locations and then decode the 3d poses of the sequence. However, RNNs are sensitive to erroneous inputs and tend to drift over long sequences. To overcome the shortcomings of RNNs, a CNN-based framework is proposed by Pavllo et al. <ref type="bibr" target="#b32">[29]</ref> to aggregate temporal information using dilated convolutions. Despite being successful at regressing a single frame from a sequence of input, it cannot concurrently output the 3d pose estimations for all frames in the sequence.</p><p>Inspired by matrix factorization methods commonly used in Structure-from-Motion (SfM) <ref type="bibr" target="#b39">[36]</ref> and non-rigid SfM <ref type="bibr" target="#b5">[5]</ref>, several works <ref type="bibr" target="#b33">[30,</ref><ref type="bibr" target="#b44">41,</ref><ref type="bibr" target="#b45">42]</ref> on 3d human pose estimation factorize the sequence of 3d human poses into a linear combination of shape bases. Akhter et al. <ref type="bibr" target="#b1">[2]</ref> suggest a duality of the factorization in the trajectory space. We extend the idea of matrix factorization to learning a deep network that estimates the coefficients of the trajectory bases from a sequence of 2d poses as inputs. The 3d poses of all frames are recovered concurrently as the linear combinations of the trajectory bases with the estimated coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our approach</head><p>In this section, we introduce our framework for sequential 3d pose estimation in detail. We formulate the estimation problem as a trajectory space coefficient regression task (Sec 3.1). In particular, we use fixed trajectory bases, e.g., singular vectors on motion data or DCT bases, as the trajectory bases (Sec 3.2), and propose a novel network architecture to effectively regress the trajectory coefficients from a sequence of 2d input (Sec 3.3). An overview of our framework is shown in <ref type="figure" target="#fig_3">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Non-Rigid Structure Factorization</head><p>Given the 2d image coordinates (u, v) of J joints in F consecutive frames, the goal is to estimate the 3d Cartesian coordinates (X,Y, Z) of the J joints in all F frames. Let us denote the 3d coordinates of the J joints in the f -th frame as</p><formula xml:id="formula_0">S f = X f 1 Y f 1 Z f 1 . . . X f J Y f J Z f J .</formula><p>(1) The entire sequence of F frames are concatenated to form the motion matrix,</p><formula xml:id="formula_1">S F×3J =     X 11 Y 11 Z 11 . . . X 1J Y 1J Z 1J . . . . . . . . . . . . . . . . . . X F1 Y F1 Z F1 . . . X FJ Y FJ Z FJ     ,<label>(2)</label></formula><p>where the column space is known as the trajectory space <ref type="bibr" target="#b1">[2]</ref>. Furthermore, S can be factorized into a linear combination of K trajectory bases, i.e.,</p><formula xml:id="formula_2">S F×3J =     θ (1) 1 . . . θ (K) 1 . . . . . . θ (1) F . . . θ (K) F     F×K Θ F×K ×      a (1) x1 a (1) y1 a (1) z1 . . . a (1) xJ a (1) yJ a (1) zJ . . . . . . . . . . . . . . . . . . a (K) x1 a (K) y1 a (K) z1 . . . a (K) xJ a (K) yJ a (K) zJ      K×3J A K×3J . (3) Θ F×K is the trajectory bases matrix, where each column [ θ (k) 1 ... θ (k) F ] T ∈ R F is a trajectory basis vector.</formula><p>Here, the number of trajectory bases K is no greater than the number of frames F since rank(S F×3J ) ≤ min(F, 3J). Given a set of predefined trajectory bases Θ, the estimation of a sequence of 3d poses S turns into the problem of finding the coefficient matrix A. On the other hand, the row space of S is known as the shape space, where the factorization in Equation 3 can also be interpreted as the product of a coefficient matrix Θ in the shape space and a shape bases matrix A. The duality of the factorization in the row and column spaces is called shape-trajectory duality. Unlike approaches <ref type="bibr" target="#b33">[30,</ref><ref type="bibr" target="#b44">41,</ref><ref type="bibr" target="#b45">42]</ref> that estimate the 3d poses in shape space, we follow <ref type="bibr" target="#b1">[2]</ref> to formulate the task in the trajectory space. This is because shape bases are highly motion specific, hence, large number of bases are required to achieve low reconstruction error for any arbitrary motion type <ref type="bibr" target="#b33">[30]</ref>. In contrast, trajectory space requires significantly fewer dominant bases to model human motions due to the physics constraining the motion acceleration. Additionally, reconstructed motions from linear combination of smooth trajectory bases are temporally consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Trajectory Bases</head><p>Singular Value Decomposition (SVD). The trajectory bases matrix Θ F×K can be computed from the SVD of the motion matrix S, i.e., S F×3J = U F×F Σ F×3J V 3J×3J , where U F×F and V 3J×3J are the left and right singular vectors, and Σ F×3J is the diagonal matrix of singular values. More specifically, Θ F×K is the K columns of the left singular vectors U F×F that correspond to the K largest singular values. Multiple motion matrices are stacked into a larger matrixS F×(N×3J) , where N denotes the total number of motion matrices. The same SVD operation is applied onS F×(N×3J) to get Θ F×K . A higher number of motion matrices, i.e., large N, results in a more accurate trajectory bases matrix Θ F×K . However, in practice we are limited by the memory needed to compute SVD. Hence, we compute Θ F×K from a small portion of the available large scale datasets e.g. Human3.6M <ref type="bibr" target="#b17">[16]</ref>. Nonetheless, the result is stable enough to show the trajectory pattern within human motion. <ref type="figure" target="#fig_1">Figure 1a</ref> shows the plots of the bases computed by SVD on 10k trajectories from Human3.6M. Each plot is the F entries of a trajectory basis vector θ (k) (y-axis) against its index (x-axis) in the vector. <ref type="figure" target="#fig_1">Figure  1a</ref> show strong indication of sinusoidal form. Based on this observation, we also explore DCT that compactly model motion trajectories as pointed out in <ref type="bibr" target="#b1">[2]</ref>. For a discrete signal input {d 0 , d 1 , . . . , d F−1 } of finite length F, the DCT is defined as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete Cosine Transform (DCT). The plots of the trajectory bases vectors from</head><formula xml:id="formula_3">D k = F−1 ∑ f =0 d f cos π F f + 1 2 k , k = 0, . . . , F − 1.<label>(4)</label></formula><p>D k stands for the coefficient corresponding to the k-th cosine wave cos [ π F ( f + 1 2 )k]. The inverse DCT is used to transform the coefficients back to the original signal, i.e.,</p><formula xml:id="formula_4">d f = 1 2 D 0 + F−1 ∑ k=1 D k cos π F f + 1 2 k , f = 0, . . . , F − 1.<label>(5)</label></formula><p>Furthermore, the inverse DCT can be seen as a linear combination of the orthogonal bases 0.5, k = 0 and cos [ π F ( f + 1 2 )k], k = 1, . . . , F − 1 with the coefficients {D 0 , . . . , D F−1 }, and [d 0 , . . . , d F−1 ] T is a column in the motion matrix S. Limiting k = 0, . . . , K − 1 gives us a set of F × K orthogonal bases that is similar to Θ F×K mentioned earlier. The advantage of DCT is that the orthogonal bases are predefined without the need to learn from data. <ref type="figure" target="#fig_1">Figure 1b</ref> shows the first 3 bases of DCT, which resemble the singular vectors in <ref type="figure" target="#fig_1">Figure 1a</ref>. <ref type="figure" target="#fig_2">Figure 2</ref> (left) shows an example analysis on a subset of Human3.6M dataset (randomly sampling 100k trajectories with F = 50). We see that bases with k &gt; 10 have negligible corresponding coefficients, which is reasonable because human motions usually contain very sparse high-frequency movement. <ref type="figure" target="#fig_2">Figure 2</ref> (right) further shows that it is sufficient to reconstruct the trajectory with relatively low error even if a very small proportion of bases are utilized. Hence, we only need to regress a low number of coefficients despite large number of frames because the trajectory bases already encodes the temporal correlation within the motion. This greatly reduces the complexity of training deep networks.  <ref type="figure" target="#fig_3">Figure 3</ref> shows an overview of our network. The input of the network is the 2d image coordinates of J joints from a sequence of F consecutive frames obtained from any off-the-shelf 2d pose estimators, e.g. <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b28">26]</ref>. A MLP block with 4 repeated Linear-BatchNorm-ReLU-Dropout structures is used to extract features for each frame. The weights within the block are shared across all frames. Each feature channel across the temporal axis is stacked into a "virtual feature trajectory" and is transformed into the trajectory space via a "Transformer". The "Transformer" is simply an implementation of the DCT. An inner product is computed for the input trajectory and each of the fixed trajectory bases, and is further normalized by multiplying a scale factor of 2 F . This results in K coefficients for each feature channel, and all coefficients from all channels are concatenated and passed into another MLP block to regress the final K coefficients for all the J × 3 trajectories (X,Y, Z coordinates of J joints). The "Coefficient Regression" MLP block consists of 5 repeated Linear-BatchNorm-ReLU-Dropout structures, and differs from the "Feature Extraction" block for both the linear layer size and dropout rate. Finally, we linearly combine the K trajectory bases using the regressed coefficients to reconstruct the X,Y, Z values for all J joints across F frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Design and Implementation Details</head><p>We empirically found that by densely connecting any two linear layers in the MLP block (originally proposed for convolutional layers by <ref type="bibr" target="#b16">[15]</ref>), the network is capable of learning the length of the critical path automatically. This helps to boost the regression performance. The skip connection arrow in both "Feature Extraction" block and "Coefficient Regression" block indicates the usage of dense shortcut links. Temporal-based frameworks usually suffer from noisy 2d input. To alleviate the detrimental impact from individual noisy input frames, an extra average pooling layer is added after feature extraction block to smoothen the "feature trajectories". We found that a pooling window of length 5 is suitable to reduce noise while preserving much of the underlying signal, and we report all the experiments results under this setting. Our network is not restricted to any particular input sequence length. It can be adapted to any sequence length by simply substituting the trajectory bases in the "Transformer". The number of trajectory bases can also be adjusted by adding (or remov-ing) branches in the "Transformer". Experiments on how the number of input frames and the number of trajectory bases affect the performance are shown in Section 4.</p><p>We supervise the training of our network by minimizing L1-loss over all N sequences</p><formula xml:id="formula_5">L 3D (Ŝ) = 1 N 1 F N ∑ i=1 Ŝ i − S i 1 (6)</formula><p>because L1-loss is more robust to outliers which are common due to occlusions. HereŜ i denotes the estimated 3d poses while S i denotes the corresponding ground truth poses. We train our model using Adam <ref type="bibr" target="#b18">[17]</ref> optimizer for 100 epochs with initial learning rate of 1e −4.</p><p>We schedule the learning rate decay at epoch 60 and 85 with a shrink factor α = 0.1. We also perform data augmentation by horizontally flipping the human pose. During inference, estimated 3d poses for both original and flipped data are averaged as the final estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experimental setup. We focus our evaluation on two widely used datasets: Human3.6M <ref type="bibr" target="#b17">[16]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b24">[22]</ref>. We show both quantitative and qualitative results that demonstrate the effectiveness of our temporal estimation framework. Human3.6M is currently the largest publicly available dataset for human 3d pose estimation. The dataset consists of 3.6 million image frames captured by MoCap system in a constrained indoor studio environment. 11 actors perform 15 everyday activities such as walking, discussing, etc. We follow two standard protocols for comparison with previous works. We use 5 subjects (S1, S5, S6, S7, S8) for training and 2 subjects (S9, S11) for testing. The evaluation metric is mean per joint position error (MPJPE) in mm after aligning the hip joint to the origin. We refer to this as protocol 1. Several works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b30">27,</ref><ref type="bibr" target="#b31">28,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b35">32,</ref><ref type="bibr" target="#b45">42]</ref> also report the error after aligning further with respect to the ground truth pose via Procrustes Analysis. We refer to this as protocol 2. MPI-INF-3DHP is a recently released 3d dataset with both indoor environment (green background and studio background) and in-the-wild outdoor environment. Similar to <ref type="bibr" target="#b24">[22]</ref>, we report the 3D Percentage of Correct Keypoints (PCK) with threshold of 150mm and Area Under Curve (AUC) for a range of PCK thresholds. We follow <ref type="bibr" target="#b32">[29]</ref> in using 2d detections from the Cascaded Pyramid Network (CPN) <ref type="bibr" target="#b7">[7]</ref> as our network input. Instead of down-sampling, which is commonly done in many works that use a single-frame input, we keep the original frame rate (50fps for Human3.6M and 25fps for MPI-INF-3DHP) because having access to the complete sequence provides more detailed information. The trajectory bases are the SVD or DCT bases described in Section 3.2 for any predefined number of input frames F. More specifically, we perform SVD on the motion matrix formed with 10k randomly sampled trajectories from the training set to get the SVD bases. The DCT bases are the cosine bases defined in Equation 4 and 5. We train the network with a fixed number of K trajectory bases and F input frames. Nonetheless, we use the "sliding window approach" on input videos with L &gt; F frames. In particular, we infer the 3d poses on a sliding window of F frames for an input video with an arbitrary L &gt; F frames. We move the sliding window at a step of q &lt; F frames, where q is set to 5 for all experiments shown in this paper. Since the sliding window runs through most of the frames multiple times (except for the first and last frames), we compute the final pose for each frame as the average of all the 3d poses estimated for that frame. We note that L can range from 300 to 6000 frames for all the videos of the two datasets used in our experiments.   Results. <ref type="table" target="#tab_1">Table 1</ref> shows the results on Human3.6M under protocol 1. We report the performance of our model for input length F = 10 (with K = 2 DCT bases), F = 25 (with K = 5 DCT bases), and F = 50 (with K = 8 DCT bases) corresponding to motion of 0.2s, 0.5s and 1s respectively. Our approach with F = 50 outperforms all previous single-frame (top half of the table) and temporal-based (bottom half of the table) approaches. <ref type="table" target="#tab_2">Table 2</ref> further shows the results after rigid alignment with the ground truth. Under this protocol, we also show results that are on par with the existing state-of-the-art <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b32">29]</ref>. It is interesting to note the significant improvements of non RNN-based frameworks ( <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b32">29]</ref> and ours) over the RNN-based framework <ref type="bibr" target="#b14">[14]</ref>. Nonetheless, <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b32">29]</ref> estimate only one output frame from a sequence of input frames, while our approach generates stable 3d pose estimates for every frame of an input sequence. We report the average error of each frame in an input length of F = {10, 25, 50} frames in <ref type="figure" target="#fig_4">Figure 4a</ref>. Here, we set the video length to be the same as our network input length, i.e., L = F. The drop in performance for the frames near both ends are probably due to the small number of bases used in our network. Overall, our framework is able to generate stable estimation for majority of the frames even for longer sequences. We further evaluate our approach with ground truth 2d poses on Human3.6M as the input. As shown in <ref type="table" target="#tab_3">Table 3a</ref>, our approach significantly improves previous best result by 4.4mm (11.8%). This demonstrates that regression in the trajectory space is effective in generating highly accurate estimation even though the framework concurrently estimates the 3d poses for a long sequence of frames. We also report the comparison of using SVD and DCT bases in   it achieves similar result as DCT bases. This suggests that the model is not restricted to any specific bases. We leave the exploration on other bases as future work. To demonstrate the generalizability of our framework, we evaluate the performance of our model on MPI-INF-3DHP in <ref type="table" target="#tab_4">Table 3b</ref> with all previous works using ground truth 2d locations as input. F = 25 frames (with K = 6 DCT bases) and F = 50 frames (with K = 8 DCT bases) are reported for our model. Both settings outperform previous best results under all metrics. Qualitative visualizations are shown in <ref type="figure">Figure 5</ref>. We also conduct experiments on how the number of input frames F and bases K affect the performance of the model. Estimation errors on Human3.6M for various F and K settings are shown in <ref type="figure" target="#fig_4">Figure 4b</ref>. We show the plots of F = 5 at K ≤ 5 and F = 10 at K ≤ 10 because the number of bases cannot be more than the number of frames, i.e., K ≤ F. The plots for F = {25, 50, 100} are truncated at K = 14 since the errors stabilized approximately after K = 8. We can also see that errors converged to ∼ 47mm for F = {25, 50, 100}, where K ≥ 8. This suggests that our network design is versatile and outputs consistent results over different length of input frames. Although we empirically show the results of our network over F = {10, 25, 50} in all the previous results on Human3.6M, the results in <ref type="figure" target="#fig_4">Figure 4b</ref> demonstrate that our network is not limited to any fixed number of input frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a deep learning-based framework that utilizes matrix factorization for sequential 3d human poses estimation. In particular, we showed that the 3d poses in all frames can be represented as a motion matrix factorized into a small number of trajectory bases, and a set of trajectory coefficients. We turned the problem of sequential 3d pose estimation into training a deep network to regress the set of trajectory coefficients from all the input frames. The effectiveness of our framework is demonstrated by achieving state-of-the-art performances on multiple benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Trajectory bases for F = 50 frames. (a): Singular vectors corresponding to the 3 largest singular values as trajectory bases. (b): First 3 bases from DCT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Left: Mean of absolute coefficient values corresponding to different DCT bases. The first coefficient corresponding to the DC component of a signal is discarded in the figure. Right: Reconstruction error when truncated to different number of DCT bases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Our Network Architecture. F frames of J 2d joints are fed into a MLP for per frame feature extraction. Each feature channel along the temporal axis is transformed into trajectory space via a Transformer. Coefficients from all feature channels are then concatenated and another MLP is applied to regress the K coefficients for all 3J trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>(a): Average per frame error within a sequence of different sequence lengths. (b): Estimation error on Human3.6M for different numbers of frames and bases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on Human3.6M under Protocol 1 (no rigid alignment for post-processing). Top half of the table are single-frame works. Bottom half of the table are multi-frame works. Bold-faced numbers indicate best results.</figDesc><table><row><cell>Protocol 2</cell><cell cols="15">Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SitingD Smoke Wait WalkD Walk WalkT Avg</cell></row><row><cell>Pavlakos et al. [28]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-51.9</cell></row><row><cell>Sun et al. [32]</cell><cell>42.1</cell><cell>44.3</cell><cell>45.0</cell><cell>45.4</cell><cell>51.5</cell><cell cols="2">53.0 43.2</cell><cell>41.3</cell><cell>59.3</cell><cell>73.3</cell><cell cols="2">51.0 44.0</cell><cell>48.0</cell><cell>38.3</cell><cell>44.8 48.3</cell></row><row><cell>Martinez et al. [21]</cell><cell>39.5</cell><cell>43.2</cell><cell>46.4</cell><cell>47.0</cell><cell>51.0</cell><cell cols="2">56.0 41.4</cell><cell>40.6</cell><cell>56.5</cell><cell>69.4</cell><cell cols="2">49.2 45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1 47.7</cell></row><row><cell>Lee et al. [18] (F = 1)</cell><cell>38.0</cell><cell>39.3</cell><cell>46.3</cell><cell>44.4</cell><cell>49.0</cell><cell cols="2">55.1 40.2</cell><cell>41.1</cell><cell>53.2</cell><cell>68.9</cell><cell cols="2">51.0 39.1</cell><cell>56.4</cell><cell>33.9</cell><cell>38.5 46.2</cell></row><row><cell>Fang et al. [12]</cell><cell>38.2</cell><cell>41.7</cell><cell>43.7</cell><cell>44.9</cell><cell>48.5</cell><cell cols="2">55.3 40.2</cell><cell>38.2</cell><cell>54.5</cell><cell>64.4</cell><cell cols="2">47.2 44.3</cell><cell>47.3</cell><cell>36.7</cell><cell>41.7 45.7</cell></row><row><cell>Dabral et al. [10] (SAP-Net)</cell><cell>32.8</cell><cell>36.8</cell><cell>42.5</cell><cell>38.5</cell><cell>42.4</cell><cell cols="2">49.0 35.4</cell><cell>34.3</cell><cell>53.6</cell><cell>66.2</cell><cell cols="2">46.5 34.1</cell><cell>42.3</cell><cell>30.0</cell><cell>39.7 42.2</cell></row><row><cell>Hossain &amp; Little [14]</cell><cell>35.7</cell><cell>39.3</cell><cell>44.6</cell><cell>43.0</cell><cell>47.2</cell><cell cols="2">54.0 38.3</cell><cell>37.5</cell><cell>51.6</cell><cell>61.3</cell><cell cols="2">46.5 41.4</cell><cell>47.3</cell><cell>34.2</cell><cell>39.4 44.1</cell></row><row><cell>Pavllo et al. [29]</cell><cell>34.1</cell><cell>36.1</cell><cell>34.4</cell><cell>37.2</cell><cell>36.4</cell><cell cols="2">42.2 34.4</cell><cell>33.6</cell><cell>45.0</cell><cell>52.5</cell><cell cols="2">37.4 33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3 36.5</cell></row><row><cell>Dabral et al. [10] (TP-Net)</cell><cell>28.0</cell><cell>30.7</cell><cell>39.1</cell><cell>34.4</cell><cell>37.1</cell><cell cols="2">44.8 28.9</cell><cell>31.2</cell><cell>39.3</cell><cell>60.6</cell><cell cols="2">39.3 31.1</cell><cell>37.8</cell><cell>25.3</cell><cell>28.4 36.3</cell></row><row><cell>Ours (F = 10)</cell><cell>32.9</cell><cell>36.4</cell><cell>35.8</cell><cell>37.3</cell><cell>39.2</cell><cell cols="2">44.2 34.0</cell><cell>33.0</cell><cell>46.5</cell><cell>53.8</cell><cell cols="2">39.7 34.3</cell><cell>40.0</cell><cell>27.8</cell><cell>32.8 38.3</cell></row><row><cell>Ours (F = 25)</cell><cell>32.8</cell><cell>35.7</cell><cell>34.4</cell><cell>36.1</cell><cell>38.1</cell><cell cols="2">43.3 33.0</cell><cell>32.5</cell><cell>46.4</cell><cell>52.7</cell><cell cols="2">38.7 33.1</cell><cell>38.5</cell><cell>26.3</cell><cell>30.7 37.3</cell></row><row><cell>Ours (F = 50)</cell><cell>32.5</cell><cell>35.3</cell><cell>34.3</cell><cell>36.2</cell><cell>37.8</cell><cell cols="2">43.0 33.0</cell><cell>32.2</cell><cell>45.7</cell><cell>51.8</cell><cell cols="2">38.4 32.8</cell><cell>37.5</cell><cell>25.8</cell><cell>28.9 36.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on Human3.6M under Protocol 2 (after rigid alignment for post-processing). Top half of the table are single-frame works. Bottom half of the table are multi-frame works. Bold-faced numbers indicate best results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3a .</head><label>3a</label><figDesc>Although the SVD bases are computed on a subset of the dataset,</figDesc><table><row><cell>GT 2d</cell><cell cols="2">MPJPE GT 2d</cell><cell>MPJPE</cell><cell>MPI-INF-3DHP</cell><cell cols="2">PCK AUC MPJPE</cell></row><row><cell>Martinez et al. [21]</cell><cell>45.5</cell><cell>Pavllo et al. [29]  †</cell><cell>37.2</cell><cell>Mehta et al. [22]</cell><cell>75.7 39.3</cell><cell>-</cell></row><row><cell>Zhao et al. [40]</cell><cell>43.8</cell><cell>Ours (DCT) (F = 10)  †</cell><cell>34.4</cell><cell>Mehta et al. [23] (ResNet 50)  †</cell><cell>77.8 41.0</cell><cell>-</cell></row><row><cell>Lee et al. [18] (F = 1)</cell><cell>40.9</cell><cell>Ours (DCT) (F = 25)  †</cell><cell>33.0</cell><cell cols="2">Mehta et al. [23] (ResNet 100)  † 79.4 41.6</cell><cell>-</cell></row><row><cell>Hossain &amp; Little [14]  †</cell><cell>39.2</cell><cell>Ours (DCT) (F = 50)  †</cell><cell>32.8</cell><cell>Ours (F = 25)  †</cell><cell>83.6 51.4</cell><cell>79.8</cell></row><row><cell>Lee et al. [18] (F = 3)  †</cell><cell>38.4</cell><cell>Ours (SVD) (F = 50)  †</cell><cell>32.9</cell><cell>Ours (F = 50)  †</cell><cell>82.4 49.6</cell><cell>81.9</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>(a): Results on Human3.6M under Protocol 1 using ground truth 2d input. (b): Results on MPI-INF-3DHP using ground truth 2d input. † indicates methods using temporal information (including ours). Bold-faced numbers indicate best results.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">LIN ET AL.: TRAJECTORY SPACE FACTORIZATION FOR 3D HUMAN POSE ESTIMATION</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">LIN ET AL.: TRAJECTORY SPACE FACTORIZATION FOR 3D HUMAN POSE ESTIMATION (a) (b)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ankur Agarwal and Bill Triggs. 3d human pose from silhouettes by relevance vector regression</title>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="882" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonrigid structure from motion in trajectory space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohaib</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast algorithms for large scale conditional 3d prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recovering non-rigid 3d shape from image streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Biermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2690</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory kalman filters: Recurrent neural estimators for pose regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huseyin</forename><surname>Coskun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Achilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Dipietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>12lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
		</author>
		<title level="m">TRAJECTORY SPACE FACTORIZATION FOR 3D HUMAN POSE ESTIMATION</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5543" to="5552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation using transfer learning and improved cnn supervision. arxiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recovering 3d human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">man pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11742</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>3d hu</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Randomized trees for human pose detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05180</idno>
		<title level="m">Structured prediction of 3d human pose with deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="991" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Marquez</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3941" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shape and motion from image streams under orthography: a factorization method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
			<biblScope unit="volume">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4447" to="4455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
