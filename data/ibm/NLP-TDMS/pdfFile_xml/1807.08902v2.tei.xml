<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-produced Guidance for Weakly-supervised Object Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
							<email>xiaolin.zhang-3@student.</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<email>yunchao@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
							<email>guoliang.kang@student.</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-produced Guidance for Weakly-supervised Object Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Object Localization, Weakly Supervised Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised methods usually generate localization results based on attention maps produced by classification networks. However, the attention maps exhibit the most discriminative parts of the object which are small and sparse. We propose to generate Self-produced Guidance (SPG) masks which separate the foreground i.e., the object of interest, from the background to provide the classification networks with spatial correlation information of pixels. A stagewise approach is proposed to incorporate high confident object regions to learn the SPG masks. The high confident regions within attention maps are utilized to progressively learn the SPG masks. The masks are then used as an auxiliary pixel-level supervision to facilitate the training of classification networks. Extensive experiments on ILSVRC demonstrate that SPG is effective in producing high-quality object localizations maps. Particularly, the proposed SPG achieves the Top-1 localization error rate of 43.83% on the ILSVRC validation set, which is a new state-of-the-art error rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Weakly Supervised Learning (WSL) has been successfully applied on many tasks, such as object localization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, relation detection <ref type="bibr" target="#b7">[8]</ref> and semantic segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. WSL attracts extensive attention from researchers and practitioners because it is less dependent on massive pixel-level annotations. In this paper, we focus on Weakly Supervised Object Localization (WSOL) problem.</p><p>Existing WSOL methods locate target object regions using convolutional classification networks. Classification networks recognize various kinds of objects by identifying discriminative regions of an objects. Fully convolutional networks <ref type="bibr" target="#b13">[14]</ref> without using fully connected layers can preserve the relative positions of pixels. Therefore, the discovered discriminative regions can indicate the exact location of the target objects. <ref type="bibr">Zhou</ref>   <ref type="figure">Fig. 1</ref>. Learning process of Self-produced guidance. Given an input image, we first generate corresponding attention map with a classification network. Then the attention map is roughly split, following the rule that the region with high confidence should be the object, whereas that with low confidence should be background. The regions with medium confidence remain undefined. All these three regions constitute the seed. Selfproduced guidance is defined as the multi-stage pixel-level object mask supervised by the seed.</p><p>(e.g.AlexNet <ref type="bibr" target="#b14">[15]</ref>, VGG <ref type="bibr" target="#b15">[16]</ref> and GoogleNet <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>) and proposed a Class Activation Maps (CAM) approach to find the regions of interest using only imagelevel supervision. Following <ref type="bibr" target="#b18">[19]</ref>, CAM replaced the top fully connected layers by convolutional layers to keep the object positions and can discover the spatial distribution of discriminative regions for different classes. The key weakness of the localization maps generated by CAM is that only the most discriminative regions are highlighted, as a result we can only locate a small part of target objects. To cope with the weakness, Wei et al. <ref type="bibr" target="#b8">[9]</ref> proposed to apply additional networks for enriching object-related regions, given images of which the most discriminative regions are erased according to the attention maps from a pretrained network. Moreover, Zhang et al. <ref type="bibr" target="#b19">[20]</ref> proved the CAM method can be simplified to enable end-to-end training. Armed with this proof, an Adversarial Complementary Learning approach was proposed in <ref type="bibr" target="#b19">[20]</ref> by incorporating one additional classifier for mining complementary object regions, which can finally produce accurate object localization maps. However, all these methods ignore to explore the correlations among pixels.</p><p>We observe that images can be roughly divided into foreground and background regions. The foreground pixels usually constitute the object(s) of interests. We found that attention maps inferred from classification networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref> can effectively provide the probabilities of each pixel to be foreground or background. Although pixels of high foreground/background probabilities may not cover the entire target object/background, they still provide the important cues for getting some common patterns of target objects. Based on this, we can simply leverage those reliable foreground/background seeds as supervision to encourage the network to sense the distributions of foreground objects and background regions. Since pixels with correlations (e.g.within a same object or background) often share similar appearance, more reliable foreground/background pixels can be easily discovered by learning from the discovered seeds. With more reliable guided pixels for supervision, the entire foreground objects can be gradually distinguished from background, which will finally benefit the weakly object localization.</p><p>Inspired by the above motivation, in this paper, we propose a Self-produced Guidance (SPG) approach for learning better attention maps and getting precise positions of objects. We leverage attention maps to produce the guidance masks of foreground and background regions in a stagewise manner. The foreground/background seeds of each stage can be generated following a simple rule: 1) the regions with highly confident scores are considered as foreground; 2) the regions with very low scores are background seeds; 3) the regions with medium confidence remain undefined. The undefined regions are meant to be figured out using intermediate features. We adopt a top-down mechanism of using upper layer's output as the supervision of the lower layers to learn better object localizations. The upper layers maintain more abstract semantic information, whereas the lower layers have more specific pixel-related information. We leave the ambiguous area undefined before more regions can be defined as foreground/background using upper layer features. The more regions be defined, the stronger ability to define harder regions. After getting the guidance masks of foreground and background, we use them as auxiliary supervisions. These supervisions are expected to enable the classification network to learn pixel correlations. Consequently, attention maps can clearly indicate class-specific object regions. <ref type="figure">Figure 1</ref> illustrates the learning process of self-produced guidance. Given an input image, we firstly generate corresponding attention maps through a classification network according to the convenient method in <ref type="bibr" target="#b19">[20]</ref>. Then the attention map is roughly split into foreground/background seeds and ignored regions. The self-produced guidance are learned from these seeds with the input of intermediate features in a stagewise manner. Finally, the SPG masks of multiple layers are fused for a more precise and integrate indication of target objects.</p><p>To sum up, our main contributions are:</p><p>-We propose a stagewise approach to learn high-quality Self-produced Guidance masks which exhibit the foreground and background of a given image. -We present a weakly object localization method by incorporating self-produced supervision, which can inspire the classification network discover pixel correlations to improve the localization performance. -The proposed method achieves the new state-of-the-art with the error rate of Top-1 43.83% on ILSVRC dataset with only image-level supervision.</p><p>We discuss the proposed SPG approach in detail in Section 3. In Section 4, we empirically evaluate the proposed method on the ILSVRC2016 dataset, showing that the superiority of SPG in object localization task with only image-level supervision. We also discuss the further insights of the proposed SPG algorithms through additional experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Convolutional neural network has been widely used in object detection and localization tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. One of the earliest deep networks to detect objects in a one-stage manner is OverFeat <ref type="bibr" target="#b26">[27]</ref>, which employs a multiscale and sliding window approach to predict object boundaries. These boundaries are then applied for accumulating bounding boxes. SSD <ref type="bibr" target="#b27">[28]</ref> and YOLO <ref type="bibr" target="#b28">[29]</ref> used a similar one-stage method, and these detectors are specifically designed for speeding up the detection process. Faster-RCNN designed by Ren et al. <ref type="bibr" target="#b29">[30]</ref> has achieved great success in the object detection task. It generates region proposals and predicts highly reliable object locations in an unified network in real time. Lin et al. <ref type="bibr" target="#b30">[31]</ref> presented that the performance of Faster-RCNN can be significantly improved by constructing feature pyramids with marginal extra cost.</p><p>Although these approaches are considerably successful in detecting object of interest in images, the vast number of annotations are unaffordable for training such networks with limited budget. Weakly supervised methods alleviate this problem by using much cheaper annotations like image-level labels. Jie et al. <ref type="bibr" target="#b4">[5]</ref> proposed a self-taught learning framework by firstly selecting some high-response proposals, and then finetuning the network on the selected regions to progressively improve its detection capacity. This method highly rely on region proposals pre-processed by algorithms like Selective Search <ref type="bibr" target="#b31">[32]</ref>. The general-purpose proposal algorithms may not robust to produce accurate bounding boxes. Dong et al. <ref type="bibr" target="#b2">[3]</ref> adopted two separate networks to jointly refine the region proposals and select positive regions. High-quality attention maps are also critical for object detection and segmentation <ref type="bibr" target="#b32">[33]</ref>. Diba et al. <ref type="bibr" target="#b33">[34]</ref> proposed the attention maps can be leveraged to produce region proposals. With the assistance of these proposals, more detailed information can be easily detected.</p><p>However, these methods introduces extra computational as a result of using pre-processed region proposals and multiple networks. Zhou et al. <ref type="bibr" target="#b0">[1]</ref> discovered that the localization maps for each class can be produced by aggregating toplevel feature maps using a class-specific fully connected layer. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> introduced a different backpropagation scheme to produce contrastive response maps by passing along top-down signals downwards. However, this method supervised by solely using image labels tends to only discover a small part of the target objects. Wei et al. <ref type="bibr" target="#b8">[9]</ref> applied a similar but more efficient approach to hide discriminative regions under the guidance of a pre-trained network, and then the processed images are trained for discovering more regions of interest. These methods increases the amount of images, thus they need much more precious computational and time resources to train the networks. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> provided theoretical proof of producing class-specific attention maps during the forward pass by just selecting from the last layer feature maps, which enables the end-to-end attention learning. Also, they proposed the ACoL approach <ref type="bibr" target="#b19">[20]</ref> to efficiently mine the integral target object in an enhanced classification network.</p><p>3 Self-produced Guidance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Overview</head><p>We denote the image set as Attention map is then inferred from the classification network. Self-produced guidance maps are gradually learned with the guide of the attention map. SPG-C utilizes the self-produced guidance map as an auxiliary supervision to reinforce the quality of the attention map. GAP refers to global average pooling classes. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the architecture of the SPG approach, which mainly has four different components, including Stem, SPG-A, SPG-B and SPG-C. Different components have different structures and functionalities. We use lowercase f to denote functions and capital F to denote output feature maps. Stem is a fully convolutional network denoted as f Stem (I i , θ Stem ), where θ Stem is the parameters. The output feature maps of f Stem is denoted as F Stem . f Stem acts as a feature extractor, which takes the RGB images as input and produces high-level position-aware feature maps of multiple channels. The extracted feature maps F Stem are then fed into the following component SPG-A. We denote the SPG-</p><formula xml:id="formula_0">I = {(I i , y i )} N −1 i=0 , where y i = {0, 1, ..., C − 1} is the label of the image I i , N</formula><formula xml:id="formula_1">A component as f A (F Stem , θ A ), which is a network for image-level classification. f A (F Stem , θ A )</formula><p>is consisted of four convolutional blocks (i.e.A1, A2, A3 and A4), a global average pooling (GAP) layer <ref type="bibr" target="#b18">[19]</ref> and a softmax layer. A4 has one convolutional layer with kernel size 1 × 1 of C filters. These filters are corresponding to the attention maps of each class, so as to generate attention maps during the forward pass <ref type="bibr" target="#b19">[20]</ref>. SPG-B is leveraged to learn Self-produced guidance masks by using the seeds of foreground and background generated from attention maps. The high confident regions within attention maps are extracted to perform as supervision to learn better object regions. SPG-B leverages the intermediate feature maps from the classification network SPG-A to predict Self-produced Guidance masks. Particularly, the output features maps F A1 and F A2 of A1 and A2 are fed into the two blocks in SPG-B, respectively. Each block of SPG-B contains three convolutional layers followed by a sigmoid layer, where the first layer is to adapt the different number of channels in feature maps F A1 and F A2 . The output of SPG-B are denoted as F B1 and F B2 for the two branches, respectively. The component SPG-C uses the auxiliary SPG supervision to encourage the SPG-A to learn pixel-level correlations. SPG-C contains two convolutional layers with 3 × 3 and 1 × 1 kernels, followed by a sigmoid layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-produced Guidance Learning</head><p>Attention maps generated from classification networks can only exhibit the most discriminative parts of target objects. We propose to generate Self-produced Guidance (SPG) masks which separate the foreground, i.e.the object of interest, from the background to provide the classification networks with spatial correlation information of pixels. The generated SPG masks are then leveraged as auxiliary supervision to encourage the networks to learn correlations between pixels. Thus, pixels within the same object will have the same responses in feature maps. As the detailed information (i.e.object edge and boundary) is usually very abstract in the top-level feature maps, we employ the intermediate features to produce precise SPG masks. Indeed, some previous works use low-level feature maps to learn object regions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. These approaches require pixel-level ground-truth labels as supervision. Differently, we propose to use self-produced guidance by incorporating high confident object regions within attention maps.</p><p>In detail, for any image I i , we firstly extract its attention map O by simply from a classification network. We observe that the attention maps usually highlight the most discriminative regions of object. The initial object and background seeds can be easily obtained according to the scores in the attention maps. In particular, the regions with very low scores are considered as background, while the regions with very high scores are foreground. The rest regions are ignored during the learning process. We initialize the SPG learning process by these seeds. B2 is supervised by the seed map and it can learn the patterns of foreground and background. In this way, the pixels within the ignored regions are gradually recognized. Then, we use the same strategy to find the foreground and background seeds in the output map of B2, which are used to train the B1 branch. In such a stagewise way, the intermediate information of the neural network are employed to learn the Self-produced Guidance.</p><p>We formally define this process as follows. Given a input image of size W ×H, we denote the binarized SPG mask M ∈ {0, 1, 255} W ×H , where M x,y = 0 if the pixel at x th row and y th column belongs to background regions, M x,y = 1 if it belongs to object regions, and M x,y = 255 if it is ignored. We denote the attention map as O. The produced guidance masks can be calculated by</p><formula xml:id="formula_2">M x,y =      0 if O x,y &lt; δ l , 0 &lt; δ l &lt; 1 1 if O x,y &gt; δ h , 0 &lt; δ h &lt; 1 255 if δ l ≤ O x,y ≤ δ h , 0 &lt; δ l &lt; δ h &lt; 1<label>(1)</label></formula><p>where δ l and δ h are thresholds to identify regions in localization maps as background and foreground, respectively.</p><p>We adopt an stagewise approach to gradually learn the high-quality selfproduced supervision maps. B2 is applied to learn better self-produced maps supervised by the seed map M A . In training, only the positions labeled as 0 and 1 in the self-produced maps are served as pixel-level supervision. The pixels with values of 255 are temporarily ignored. The ignored pixels do not contribute to the loss and their gradients do not back-propagated. The network will learn the patterns from the already labeled pixels and then more regions will be recognized, because the pixels belonging to background or objects usually share much correlation. For example, the regions belong to the same object usually have the same appearance. The output of B2 is then further applied as attention maps, and better self-produced supervision masks can be calculated using the same policy in Eq. (1). After obtaining output maps of B1 and B2, these two maps are fused to generated our final self-produced supervision map. Particularly, we calculate the average of the two maps, then generate the self-produced guidance M f use according to Eq. (1).</p><p>The generated self-produced guidance is leveraged as pixel-level supervision for the classification network SPG-A. Thereby, the classification network will learn the correlation among pixels, and we will obtain better localization maps. The entire network is trained in an end-to-end manner. We adopt the crossentropy loss function for the classification learning and self-produced guidance learning. Algorithm 1 illustrates the training procedure of the proposed SPG approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training algorithm for SPG</head><p>Input: Training data I = {(Ii, yi)} N i=1 , threshold δ l and δ h 1: while training is not convergent do 2:</p><p>Update feature maps F A4 ← f A (f Stem (Ii, θ Stem ), θ A ) 3:</p><p>Extract localization map O from F A4 according to image label yi 4:</p><p>Calculate the seeds of foreground/background M A according to Eq. (1) 5:</p><p>Generate the SPG map F B2 ← f B2 (F A2 , θ B2 ) 6:</p><p>Calculate the next stage SPG maps F B1 7:</p><p>Calculate the fused maps F f use by averaging F B1 and F B2 8:</p><p>Calculate the fused SPG masks M f use ← F f use according to Eq. (1) 9:</p><p>Update the entire network θStem, θ A , θ B and θC supervised by M and yi 10: end while Output: Output the localization map O During testing, we extract the attention maps according to the class with the highest predicted scores, and then resize the maps to the same size with the original images by bilinear interpolation. For a fair comparison, we apply the same strategy utilized in <ref type="bibr" target="#b0">[1]</ref> to produce object bounding boxes based on the generated object localization maps. In particular, we firstly segment the foreground and background by a fixed threshold. Then, we seek the tight bounding boxes covering the largest connected area in the foreground pixels. The thresholds for generating bounding boxes are adjusted to the optimal values using grid search method. For more details please refer to <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation details</head><p>We evaluate the proposed SPG approach by modifying the Inception-v3 network <ref type="bibr" target="#b37">[38]</ref>. In particular, we remove the layers after the second Inception block, i.e., the third Inception block, pooling and linear layer. For a fair comparison, we build a plain version network, named SPG-plain. We add two convolutional layers of kernel size 3 × 3, stride 1, pad 1 with 1024 filters and a convolutional layer of size 1 × 1, stride 1 with 1000 units (200 for CUB-200-2011). Finally, a GAP layer and a softmax layer are added on the top. We update the plain network by adding two components (SPG-B and SPG-C). The first layers of B1 and B2 are convolutional layers of kernel size 3×3 with 288 and 768 filters, respectively. The second layers are convolutional layers of 512 filters followed by a 1 × 1 convolutional output layer. The second and third layers share parameters between B1 and B2. The strides are 1 for all convolutional layers. To keep the resolution of feature maps, we set the pad to 1 to the filters whose kernel size is 3 × 3. SPG-C is consist of two convolutional layers of kernel size 3 × 3 with 512 filters and a output convolutional layer with kernel size of 1 × 1. All branches in SPG-B and SPG-C connects to a output sigmoid layer. We use the pre-trained weights on ILSVRC <ref type="bibr" target="#b38">[39]</ref>. Following the baseline methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, input images are randomly cropped to 224 × 224 pixels after being reshaped to the size of 256 × 256. During testing, we directly resize the input images to 224×224. For classification results, we average the class scores from the softmax layer with 10 crops (4 corners plus center, same with horizontal flip).</p><p>We implement the networks using PyTorch. We finetune the networks with the initial learning rate of 0.001 (0.01 for the added layers) on ILSVRC, and it is decreased by a factor of 10 after every epoch. The batch size is 30 and the weight decay is 0.0005. The momentum of the SGD optimizer is set to 0.9. We randomly sample some images and visualize their localization maps. We adjust δ h to mine object seeds. The object seeds should include as much object pixels as possible while exclude background pixels. Similarly, δ l can be adjusted so that the background seeds should be as large as possible while exclude object regions. We choose the parameters for B1 are δ h = 0.5 and δ l = 0.05, and the parameters for B2 are δ h = 0.7 and δ l = 0.1. We train the networks on NVIDIA GeForce TITAN 1080Ti GPU with 11GB memory. Code is available at https://github.com/xiaomengyc/SPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment setup</head><p>Dataset and evaluation We evaluate the Top-1 and Top-5 localization accuracy of the proposed approach. We mainly compare our approach with other <ref type="table">Table 1</ref>. Localization error on ILSVRC validation set (* indicates methods which improve the Top-5 performance only using predictions with high scores).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>top-1 err. top-5 err. Backprop on VGGnet <ref type="bibr" target="#b40">[41]</ref> 61.12 51.46 Backprop on GoogLeNet <ref type="bibr" target="#b40">[41]</ref> 61.31 50.55 AlexNet-GAP <ref type="bibr" target="#b0">[1]</ref> 67.19 52.16 VGGnet-GAP <ref type="bibr" target="#b0">[1]</ref> 57.20 45.14 GoogLeNet-GAP <ref type="bibr" target="#b0">[1]</ref> 56. <ref type="bibr" target="#b39">40</ref> 43.00 GoogLeNet-HaS-32 <ref type="bibr" target="#b1">[2]</ref> 54.53 -VGGnet-ACoL <ref type="bibr" target="#b19">[20]</ref> 54.17 40.57 GoogLeNet-ACoL <ref type="bibr" target="#b19">[20]</ref> 53. <ref type="bibr" target="#b27">28</ref>  baseline methods on the ILSVRC 2016 dataset, as it has more than 1.2 million images of 1,000 classes for training. We report the accuracy on the validation set of 50,000 images. We also tested our algorithm on the bird dataset, CUB-200-2011 <ref type="bibr" target="#b39">[40]</ref>. CUB-200-2011 contains 11,788 images of 200 categories with 5,994 images for training and 5,794 for testing. We leverage the localization metric suggested by <ref type="bibr" target="#b38">[39]</ref>. An image has the right predicted bounding box if 1) it has the right prediction of image label; 2) and its predicted bounding box has more than 50% overlap with the ground-truth boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with the state-of-the-arts</head><p>We compare the proposed SPG approach with the state-of-the-art methods on ILSVRC validation set and CUB-200-2011 test set. Localization: <ref type="table">Table 1</ref> illustrates the localization error of various baseline algorithms on the ILSVRC val set. We observe that our baseline SPG-plain model achieves 53.71 and 41.81 of Top-1 and Top-5 localization error. Based on the SPG-plain network, the SPG strategy further reduces the localization error to Top-1 51.40 and Top-5 40.00. We illustrate the results on CUB-200-2011 in Table 2, the SPG approach achieves the localization error of Top-1 53.36%. Both results on ILSVRC and CUB outperform the state-of-the-art approach, ACoL <ref type="bibr" target="#b19">[20]</ref> which applied two classifier branches to discover complementary object regions. Following the baseline methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>, we boost the Top-5 localization error by repeatedly using the predicted bounding boxes with high classification scores. We select two bounding boxes from the top 1st and 2nd predicted classes, and one from the 3rd class. By this way, the Top-5 localization error (indicated by *) on ILSVRC is improved to 35.05%, and that on CUB-200-2011 is improved to 40.62%. To summarize, the improvement of the plain networks mainly attribute to the structure of the Inception-v3 network, which can capture larger object regions. The improvement of the SPG networks attribute to the use of the auxiliary supervision. SPG can encourage the classification network learn more pixel-level correlations, and as a result of this, the localization performance increases.</p><p>Localization performance is restricted by the classification accuracy, because the calculation of localization overlap only conducts on images which have the correct prediction of image-level labels. In order to break this limitation, we further improve the localization performance by combining our localization results with the state-of-the-art classification results, i.e., ResNet <ref type="bibr" target="#b41">[42]</ref> and DPN <ref type="bibr" target="#b42">[43]</ref>, As shown in <ref type="table" target="#tab_3">Table 3</ref>, the localization performance constantly improves with the classification results getting better. When we use the classification results from the ensemble DPN method (ensemble of DPN-92, DPN-98 and DPN-131), which has very low classification error of Top-1 15.47% and Top-5 2.70%, the localization error decreases to Top-1 43.83% and Top-5 29.36%. <ref type="figure" target="#fig_1">Figure 3</ref> shows the attention maps as well as the predicted bounding boxes with the proposed SPG on ILSVRC and CUB-200-2011. Our proposed approach can highlight nearly the entire object regions and produce precise bounding boxes. <ref type="figure">Figure 4</ref> visualizes the output of the multiple branches in generating the self-produced guidances. The attention maps generated from the classification network are leveraged to produce the seeds of foreground and background. We can observe the seeds usually cover small region of the object and background pixels. The produced seed masks (Mask-A) are then utilized as supervision for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Localization map</head><p>Mask-A SPG-B2 SPG-B1 SPG-C <ref type="figure">Fig. 4</ref>. Output maps of the proposed SPG approach. The localization maps usually only highlight small region of the object. We extract the seeds of the self-produced guidance by segmenting the confident regions of the localization maps as foreground (white) and background (black), and ignore the left regions (grey). These seeds are applied as supervision to learn better self-produced guidance maps. Finally, the learned maps are leveraged to encourage the network to improve the quality of the localization maps. Methods GT-known loc. err. AlexNet-GAP <ref type="bibr" target="#b0">[1]</ref> 45.01 AlexNet-HaS <ref type="bibr" target="#b1">[2]</ref> 41.26 AlexNet-GAP-ensemble <ref type="bibr" target="#b0">[1]</ref> 42.98 AlexNet-HaS-emsemble <ref type="bibr" target="#b1">[2]</ref> 39.67 GoogLeNet-GAP <ref type="bibr" target="#b0">[1]</ref> 41.34 GoogLeNet-HaS <ref type="bibr" target="#b1">[2]</ref> 39.43 Deconv <ref type="bibr" target="#b43">[44]</ref> 41.60 Feedback <ref type="bibr" target="#b44">[45]</ref> 38.80 MWP <ref type="bibr" target="#b34">[35]</ref> 38.70 ACoL <ref type="bibr" target="#b19">[20]</ref> 37.04 SPG-plain 37.32 SPG 35.31 the B2 branch. With such supervision information, B2 can learn more confident patterns of foreground and background pixels, and precisely predict the remaining foreground/background regions where leave undefined in Mask-A. B1 leverages the lower level feature maps and the supervision from B2 to learn more detailed regions. Finally, the self-produced guidance is obtained by fusing the two outputs of B1 and B2. This guidance is used as auxiliary supervision to encourage the classification network learn better attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitation of the localization accuracy</head><p>As calculation of the localization error rate is affected by network's classification performance. We compare the localization performance using ground-truth labels to eliminate the influence caused by classification accuracy As shown in <ref type="table" target="#tab_4">Table 4</ref>, the proposed SPG outperforms the other approaches. The Top-1 error of SPG-plain is 37.32%, which is better than other baseline approaches. With the assistance of the auxiliary supervision, the localization error with groundtruth labels reduces to 35.31%. This reveals the superiority of the attention maps generated by our method, and shows that the proposed self-produced guidance maps can successfully encourage the network learn better object regions.</p><p>Effect of the cascade learning strategy In the proposed method, we learn the self-produced guidance maps in a twostage way. The branch B2 is supervised by the guidance maps generated by the localization maps from SPG-A, while the branch B1 is supervised by selfproduced guidance from the output of B2. In order to verify the effectiveness of this two-stage method, we break this structure and use the initial seed masks as supervision for the both branches. As a result, we obtain a higher Top-1 error rate of 35.58% when providing the ground-truth classification labels. So, we can conclude that the two-stage structure utilized in SPG-B is useful to generate better self-produced guidance maps, and it is more effective for generating better attention maps. Also, we find it is helpful to share the second and third layers of B1 and B2. By removing the shared setting, the localization error rate will increase from 35.31% to 36.31%.</p><p>Effect of the auxiliary supervision We propose to use the self-produced guidance maps as a pixel-level auxiliary supervision to encourage the classification network to learn better localization maps using SPG-C. Thus, we remove SPG-C to test whether SPG-C influence the classification network. After removing SPG-C, the performance becomes worse with the Top-1 error rate of 36.06% on ILSVRC validation set when providing ground-truth labels. This reveals that the proposed self-produced guidance maps is effective to improve the quality of the localization maps by adding auxiliary supervision with SPG-C. It is notable that, the localization performance with only using SPG-B is still better than the plain version. So, the branches in SPG-B can also contribute to the improvement of localization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed the Self-produced Guidance approach for locating target object regions given only image-level labels. The proposed approach can generate high-quality self-produced guidance maps for encouraging the classification network to learn pixel-level correlations. Thereby, the networks can detect much more object regions for localization. Extensive experiments show the proposed method can detect more object regions and outperform the state-of-the-art localization methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>is the number of images and C is the number of image Overview of the proposed SPG approach. The input images are processed by Stem to extract mid-level feature maps, which are then fed into SPG-A for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the attention maps and the predicted bounding boxes of SPG on ILSVRC and CUB-200-2011. The predicted bounding boxes are in green and the ground-truth boxes are in red. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al.revisited classification networks arXiv:1807.08902v2 [cs.CV] 5 Aug 2018</figDesc><table><row><cell>Input</cell><cell>Attention</cell><cell>Seed</cell><cell>SPG</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Localization error on CUB-200-2011 test set (* indicates methods which improve the Top-5 performance only using predictions with high scores).</figDesc><table><row><cell>Methods</cell><cell>top-1 err.</cell><cell>top-5 err.</cell></row><row><cell>GoogLeNet-GAP [1]</cell><cell>59.00</cell><cell>-</cell></row><row><cell>ACoL [20]</cell><cell>54.08</cell><cell>43.49</cell></row><row><cell>SPG-plain</cell><cell>56.33</cell><cell>46.47</cell></row><row><cell>SPG</cell><cell>53.36</cell><cell>42.28</cell></row><row><cell>SPG*</cell><cell>53.36</cell><cell>40.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Localization/Classification error on ILSVRC validation set with the stateof-the-art classification results.</figDesc><table><row><cell>Methods</cell><cell>top-1 err.</cell><cell>top-5 err.</cell></row><row><cell>GoogLeNet-SPG-ResNet-50</cell><cell cols="2">48.79/26.22 38.93/8.47</cell></row><row><cell>GoogLeNet-SPG-ResNet-101</cell><cell cols="2">48.15/24.90 38.55/7.80</cell></row><row><cell>GoogLeNet-SPG-ResNet-152</cell><cell cols="2">47.92/24.39 38.53/7.59</cell></row><row><cell>GoogLeNet-SPG-DPN-92</cell><cell cols="2">45.06/17.70 37.32/3.83</cell></row><row><cell>GoogLeNet-SPG-DPN-98</cell><cell cols="2">44.92/17.42 37.34/3.67</cell></row><row><cell>GoogLeNet-SPG-DPN-131</cell><cell cols="2">44.81/17.08 37.24/3.42</cell></row><row><cell>GoogLeNet-SPG-DPN-ensemble</cell><cell cols="2">43.83/15.47 36.78/2.70</cell></row><row><cell cols="3">GoogLeNet-SPG-DPN-ensemble* 43.83/15.47 29.36/2.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Localization error on ILSVRC validation data with ground-truth labels.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04232</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A dual-network progressive approach to weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08249</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ts2c: Tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards computational baby learning: A weakly-supervised approach for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="999" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ppr-fcn: Weakly supervised visual relation detection via parallel pairwise r-fcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="7268" to="7277" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to segment with image-level annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transferable semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Network in network. ICLR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from weakly supervised data by the expectation loss svm (e-svm) algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page" from="1125" to="1133" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="2083" to="2090" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate localization for mobile device using a multi-planar city model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3733" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving into salient object subitizing and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1059" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Task-driven webpage saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Macro-micro adversarial network for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Junqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<biblScope unit="page" from="5300" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">9</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01629</idno>
		<title level="m">Dual path networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
