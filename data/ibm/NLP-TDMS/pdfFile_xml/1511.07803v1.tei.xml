<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Object Boundaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Object Boundaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art learning based boundary detection methods require extensive training data. Since labelling object boundaries is one of the most expensive types of annotations, there is a need to relax the requirement to carefully annotate images to make both the training more affordable and to extend the amount of training data. In this paper we propose a technique to generate weakly supervised annotations and show that bounding box annotations alone suffice to reach high-quality object boundaries without using any object-specific boundary annotations. With the proposed weak supervision techniques we achieve the top performance on the object boundary detection task, outperforming by a large margin the current fully supervised state-of-theart methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Boundary detection is a classic computer vision problem. It is an enabling ingredient for many vision tasks such as image/video segmentation <ref type="bibr">[1,</ref><ref type="bibr" target="#b17">10]</ref>, object proposals <ref type="bibr" target="#b22">[15]</ref>, object detection <ref type="bibr" target="#b42">[35]</ref>, and semantic labelling <ref type="bibr" target="#b9">[2]</ref>. Rather than image edges, many of these tasks require class specific objects boundaries. These are the external boundaries of object instances belonging to a specific class (or class set).</p><p>State-of-the-art boundary detection is obtained via machine learning which requires extensive training data. Yet, instance-wise boundaries are amongst the most expensive types of annotations. Compared to two clicks for a bounding box, delineating an object requires a polygon with 20~100 points, i.e. at least 10× more effort per object.</p><p>In order to make the training of new object classes affordable, and/or to increase the size of the models we train, there is a need to relax the requirement of high-quality image annotations. Hence the starting point of this paper is the following question: is it possible to obtain object-specific boundaries without having any object boundary annotations at training time?</p><p>In this paper we focus on learning object boundaries  <ref type="figure">Figure 1</ref>: Object-specific boundaries 1a differ from generic boundaries (such as the ones detected in 1d). The proposed weakly supervised approach drives boundary detection towards the objects of interest. Example results in 1e and 1f. Red/green indicate false/true positive pixels, grey is missing recall. All methods shown at 50% recall.</p><p>in a weakly supervised fashion and show that high quality object boundary detection can be obtained without using any class-specific boundary annotations. We propose several ways of generating object boundary annotations with different levels of supervision, from just using a bounding box oriented object detector to using the boundary detector trained on generic boundaries. For generating weak object boundary annotations we consider different sources, fusing unsupervised image segmentation <ref type="bibr" target="#b16">[9]</ref> and object proposal methods <ref type="bibr" target="#b37">[30,</ref><ref type="bibr" target="#b30">23]</ref> with object detectors <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b32">25]</ref>. We show that bounding box annotations alone suffice to achieve objects boundary estimates with high quality.</p><p>We present results using a decision forest <ref type="bibr" target="#b14">[7]</ref> and a convnet edge detector <ref type="bibr" target="#b40">[33]</ref>. We report top performance on Pascal object boundary detection <ref type="bibr" target="#b21">[14,</ref><ref type="bibr" target="#b15">8]</ref> with our weaksupervision approaches already surpassing previously reported fully supervised results.</p><p>Our main contributions are summarized below:</p><p>• We introduce the problem of weakly supervised objectspecific boundary detection.</p><p>• We show that good performance can be obtained on BSDS, PascalVOC12, and SBD boundary estimation using only weak-supervision (leveraging bounding box detection annotations without the need of instance-wise object boundary annotations).</p><p>• We report best known results on PascalVOC12, and SBD datasets. Our weakly supervised results alone improve over the previous fully supervised state-of-the-art.</p><p>The rest of this paper is organized as follows. Section 3 describes different types of boundary detection and the considered datasets. In Section 4 we investigate the robustness to annotation noise during training. We leverage our findings and propose several approaches for generating weak boundary annotations in Section 5. Sections 6-9 report results using the two different classifier architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Generic boundaries Boundary detection has been regained attention recently. Early methods are based on a fixed prior model of what constitutes a boundary (e.g. Canny <ref type="bibr" target="#b11">[4]</ref>). Modern methods leverage machine learning to push performance. From well crafted features and simple classifiers (gPb [1]), to powerful decision trees over fixed features (SE <ref type="bibr" target="#b14">[7]</ref>, OEF <ref type="bibr" target="#b20">[13]</ref>), and recently to end-toend learning via convnets (DeepEdge <ref type="bibr" target="#b10">[3]</ref>, N4 <ref type="bibr" target="#b18">[11]</ref>, HED <ref type="bibr" target="#b40">[33]</ref>). Convnets are usually pre-trained on large classification datasets, so as to be initialized with reasonable features. The more sophisticated the model, the more data is needed to learn it. Other than pure boundary detection, segmentation techniques (such as F&amp;H <ref type="bibr" target="#b16">[9]</ref>, gPb-owt-ucm [1], and MCG <ref type="bibr" target="#b30">[23]</ref>), can also be used to improve or to generate closed contours. A few works have addressed unsupervised detection of generic boundaries <ref type="bibr" target="#b24">[17,</ref><ref type="bibr" target="#b25">18]</ref>. PMI <ref type="bibr" target="#b24">[17]</ref> detects boundaries by modelling them as statistical anomalies amongst all local image patches, reaching competitive performance without the need for training. Recently <ref type="bibr" target="#b25">[18]</ref> proposes to train edge detectors using motion boundaries obtained from a large corpus of video data in place of human supervision. Both approaches reach similar detection performance.</p><p>Object-specific boundaries In many applications, there is interest to focus on boundaries of specific object classes. The class-specific object boundary detectors need then to be trained or tuned to the classes of interest. This problem is more recent and still relatively unexplored. <ref type="bibr" target="#b21">[14]</ref> introduced the SBD dataset to measure this task over the 20 pascal categories. <ref type="bibr" target="#b21">[14]</ref> proposes to re-weight generic boundaries using the activation regions of a detector. <ref type="bibr" target="#b36">[29]</ref> proposed to train class-specific boundary detectors, and weighted them at test time according to an image classifier.</p><p>Weakly supervised learning In this work we are interested in object-specific boundaries without using class specific boundary annotations. We only use bounding box (a) BSDS <ref type="bibr">[1]</ref> (b) VOC12 <ref type="bibr" target="#b15">[8]</ref> (c) COCO <ref type="bibr" target="#b26">[19]</ref> (d) SBD <ref type="bibr" target="#b21">[14]</ref> Figure 2: Datasets considered.</p><p>annotations, and in some experiments, generic boundaries (from BSDS [1]). Multiple works have addressed weakly supervised learning for object localization <ref type="bibr" target="#b28">[21,</ref><ref type="bibr" target="#b12">5]</ref>, object detection <ref type="bibr" target="#b31">[24,</ref><ref type="bibr" target="#b39">32]</ref>, or semantic labelling <ref type="bibr" target="#b38">[31,</ref><ref type="bibr" target="#b41">34,</ref><ref type="bibr" target="#b29">22]</ref>. To the best of our knowledge there is no previous work attempting to learn object boundaries in a weakly supervised fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Boundary detection tasks</head><p>In this work we distinguish three types of boundaries: generic boundaries ("things" and "stuff"), instance-wise boundaries (external object instance boundaries), and class specific boundaries (object instance boundaries of a certain semantic class). For detecting these three types of boundaries we consider different datasets: BSDS500 [1, 20], Pascal VOC12 <ref type="bibr" target="#b15">[8]</ref>, MS COCO <ref type="bibr" target="#b26">[19]</ref>, and SBD <ref type="bibr" target="#b21">[14]</ref>, where each represents boundary annotations of a given boundary type (see <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BSDS</head><p>We first present our results on the Berkeley Segmentation Dataset and Benchmark (BSDS) <ref type="bibr">[1,</ref><ref type="bibr" target="#b27">20]</ref>, the most established benchmark for generic boundary detection task. The dataset contains 200 training, 100 validation and 200 test images. Each image has multiple ground truth annotations. For evaluating the quality of estimated boundaries three measures are used: fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP). Following the standard approach <ref type="bibr" target="#b14">[7,</ref><ref type="bibr" target="#b11">4]</ref> prior to evaluation we apply a non-maximal suppression technique to boundary probability maps to obtain thinned edges.</p><p>VOC For evaluating instance-wise boundaries we propose to use the PASCAL VOC 2012 (VOC) segmentation dataset <ref type="bibr" target="#b15">[8]</ref>. The dataset contains 1 464 training and 1 449 validation images, annotated with contours for 20 object classes for all instances. The dataset was originally designed for semantic segmentation. Therefore only object interior pixels are marked and the boundary location is recovered from the segmentation mask. Here we consider only object boundaries without distinguishing the semantics, treating all 20 classes as one. For measuring the quality of predicted boundaries the BSDS evaluation software is used. Following <ref type="bibr" target="#b36">[29]</ref> the maxDist (maximum tolerance for edge match) is set to 0.01.</p><p>COCO To show generalization of the proposed method for instance-wise boundary detection we use the MS COCO (COCO) dataset <ref type="bibr" target="#b26">[19]</ref>. The dataset provides semantic segmentation masks for 80 object classes. For our experiments we consider only images that contain the 20 Pascal classes and objects larger than 200 pixels. The subset of COCO that contains Pascal classes consists of 65 813 training and 30 163 validation images. For computational reasons we limit evaluation to 5 000 randomly chosen images of the validation set. The BSDS evaluation software is used (maxDist = 0.01). Only object boundaries are evaluated without distinguishing the semantics.</p><p>SBD We use the Semantic Boundaries Dataset (SBD) <ref type="bibr" target="#b21">[14]</ref> for evaluating class specific object boundaries. The dataset consists of 11 318 images from the trainval set of the PAS-CAL VOC2011 challenge, divided into 8 498 training and 2 820 test images. This dataset has object instance boundaries with accurate figure/ground masks that are also labeled with one of 20 Pascal VOC classes. The boundary detection accuracy for each class is evaluated using the official evaluation software <ref type="bibr" target="#b21">[14]</ref>. During the evaluation process all internal object-specific boundaries are set to zero and the maxDist is set to 0.02. We report the mean ODS F-measure (F), and average precision (AP) across 20 classes.</p><p>Note that VOC and SBD datasets have overlap between their train and test sets. When doing experiments across datasets we make sure not to re-use any images included in the test set considered.</p><p>Baselines For our experiments we consider two different types of boundary detectors -SE <ref type="bibr" target="#b14">[7]</ref> and HED [33] -as baselines. SE is at the core of multiple related methods (SCG, MCG, OEF). SE <ref type="bibr" target="#b14">[7]</ref> builds a "structured decision forest" which is a modified decision forest, where the leaf outputs are local boundary patches (16 × 16 pixels) that are averaged at test time, and the split nodes are built taking into account the local segmentation of the ground truth input patches. It uses binary comparison over hand-crafted edge and selfsimilarity features as split decisions. By construction this method requires closed contours (i.e. segmentations) as training input. This detector is reasonably fast to train/test and yields good detection quality. HED <ref type="bibr" target="#b40">[33]</ref> is currently the top performing convnet for BSDS boundaries. It builds upon a VGG16 network pre-trained on ImageNet <ref type="bibr" target="#b35">[28]</ref>, and exploits features from all layers to build its output boundary probability map. By also exploiting the lower layers (which have higher resolution) the output is more detailed, and the fine-tuning is more effective (since all layers are guided directly towards the boundary detection task). To reach top performance, HED is trained using   <ref type="table">Table 1</ref>: Detailed BSDS results, see <ref type="figure" target="#fig_1">Figure 3</ref> and Section 4. Underline indicates ground truth baselines, and bold are our best weakly supervised results. (·) denotes the data used for training. ∆AP% indicates the ratio between the same model trained on ground truth, and the noisy input boundaries. The closer to 100%, the lower the drop due to using noisy inputs instead of ground truth. a subset of the annotated BSDS pixels, where all annotators agree <ref type="bibr" target="#b40">[33]</ref>. These are so called "consensus" annotations <ref type="bibr" target="#b23">[16]</ref>, and correspond to sparse ∼ 15% of all true positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Robustness to annotation noise</head><p>We start by exploring weakly supervised training for generic boundary detection, as considered in BSDS.</p><p>Model based approaches such as Canny <ref type="bibr" target="#b11">[4]</ref> and F&amp;H <ref type="bibr" target="#b16">[9]</ref> are able to provide low quality boundary detections. We notice that correct boundaries tend to have consistent appearance, while erroneous detections are mostly inconsistent. Robust training methods should be able to pick-up the signal in such noisy detections. <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="table">Table 1</ref> we report our results when training a structured decision forest (SE) and a convnet (a) Ground truth (HED) with noisy boundary annotations. By (·) we denote the data used for training. When training SE using either Canny (SE (Canny)) or F&amp;H (SE (F&amp;H)) we observe a notable jump in boundary detection quality. Comparing SE trained with the BSDS ground truth (fully supervised, SE (BSDS)), with the noisy labels from F&amp;H, SE (F&amp;H) closes up to 80% of the gap between SE (F&amp;H) and SE (BSDS) (∆AP% column in <ref type="table">Table 1</ref>). Using only noisy weak supervision SE (F&amp;H) is only 3 AP percent points behind from the fully supervised case (76 vs. 79).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SE In</head><formula xml:id="formula_0">(b) F&amp;H (c) F&amp;H ∩ BBs (d) GrabCut ∩ BBs (e) SeSe ∩ BBs (f) MCG ∩ BBs (g) cons. MCG ∩ BBs (h) SE(SeSe ∩ BBs) (i)</formula><p>We believe that the strong noise robustness of SE can be attributed to the way it builds its leaves. The final output of each leaf is the medoid of all segments reaching it. If the noisy boundaries are randomly spread in the image appearance space, the medoid selection will be robust.</p><p>HED The HED convnet <ref type="bibr" target="#b40">[33]</ref> reaches top quality when trained over consensus annotations. When using all annotations ("non consensus"), its performance is comparable to other convnet alternatives. When trained over F&amp;H the relative improvement is smaller than for the SE case, when combined with SE (denoted "HED(SE (F&amp;H))") it reaches 69 ∆AP% . HED (SE (F&amp;H)) provides better boundaries than SE (F&amp;H) alone, and reaches a quality comparable to the classic gPb method [1] (75 vs. 73).</p><p>On BSDS the unsupervised PMI methods provides better boundaries than our weakly supervised variants. However PMI cannot be adapted to provide object-specific boundaries. For this we need to rely on methods than can be trained, such as SE and HED.</p><p>Conclusion SE is surprisingly robust annotation noise during training. HED is also robust but to a lesser degree. By using noisy boundaries generated from unsupervised methods, we can reach a performance comparable to the bulk of current methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Weakly supervised generation of boundary annotations</head><p>Based on the observations in Section 4, we propose to train boundary detectors using data generated from weak annotations. Our weakly supervised models are trained in a regular fashion, but use generated (noisy) training data as input instead of human annotations.</p><p>We consider boundary annotations generated with three different levels of supervision: fully unsupervised, using only detection annotations, and using both detection annotations and BSDS boundary annotations (e.g. using generic boundary annotation, but zero object-specific boundaries). In this section we present the different variants of weakly supervised boundary annotations. Some of them are illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>BBs We use the bounding box annotations to train a classspecific object detector <ref type="bibr" target="#b32">[25,</ref><ref type="bibr" target="#b19">12]</ref>. We then apply this detector over the training set (and possibly a larger set of images), and retain boxes with confidence scores above 0.8. (We also experimented using directly the ground truth annotations, but saw no noticeable difference; thus we report only numbers using the "detections over the training set").</p><p>F&amp;H As a source of unsupervised boundaries we consider the classical graph based image segmentation technique proposed by <ref type="bibr" target="#b16">[9]</ref> (F&amp;H). To focus the training data on the classes of interest, we intersect these boundaries with detection bounding boxes from <ref type="bibr" target="#b32">[25]</ref> (F&amp;H ∩ BBs). Only the boundaries of segments that are contained inside a bounding box are retained.</p><p>GrabCut Boundaries from F&amp;H will trigger on any kind of boundary, including the internal boundaries of objects. A way to exclude internal object boundaries, is to extract object contours via figure-ground segmentation of the detection bounding box. We use GrabCut <ref type="bibr" target="#b33">[26]</ref> for this purpose. We also experimented with DenseCut <ref type="bibr" target="#b13">[6]</ref> and CNN+GraphCut <ref type="bibr" target="#b34">[27]</ref>, but did not obtain any gain; thus we report only GrabCut results. For the experiments reported below, for GrabCut ∩ BBs a segment is only accepted if a detection from <ref type="bibr" target="#b32">[25]</ref> has the intersection-over-union score (IoU) ≥ 0.7. If a detection bounding boxes has no matching segment, the whole region is marked as ignore (see <ref type="figure" target="#fig_2">Figure 4e</ref>) and not used during the training of boundary detectors.</p><p>Object proposals Another way to bias generation of boundary annotations towards object contours is to consider object proposals. SeSe <ref type="bibr" target="#b37">[30]</ref> is based on the F&amp;H <ref type="bibr" target="#b16">[9]</ref> segmentation (thus it is fully unsupervised), while MCG <ref type="bibr" target="#b30">[23]</ref> employs boundaries estimated via SE (BSDS) (thus uses generic boundaries annotations).</p><p>Similar to GrabCut ∩ BBs, SeSe ∩ BBs and MCG ∩ BBs are generated by matching proposals to bounding boxes (if IoU ≥ 0.9). BBs come from <ref type="bibr" target="#b19">[12]</ref> with the corresponding object proposals. When more than one proposal is matched to a detection bounding box we use the union of the proposal boundaries as positive annotations. This maximizes the recall of boundaries, and somewhat imitates the multiple human annotators in BSDS. We also experimented using only the highest overlapping proposal, but the union provides marginally better results; thus we report only the later. Since proposals matching a bounding box might have boundaries outside it, we consider them all since the bounding box itself might not cover well the underlying object. <ref type="table">Table 1</ref>, HED requires consensus boundaries to reach good performance. Thus rather than taking the union between proposal boundaries, we consider using the consensus between object proposal boundaries. The boundary is considered to be present if the agreement is higher than 70%, otherwise the boundary is ignored. We denote such generated annotations as "cons.", e.g. cons. MCG ∩ BBs (see <ref type="figure" target="#fig_2">Figure 4g</ref>). Another way to generate sparse (consensus-like) boundaries, is to threshold the boundary probability map out of an SE (·) model. SE (SeSe ∩ BBs) uses the top 15% quantile per image as weakly supervised annotations. Finally, other than consensus between proposals, we can also do consensus between methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consensus boundaries As pointed out in</head><p>cons. S&amp;G ∩ BBs is the intersection between SE (SeSe ∩ BBs), SeSe and GrabCut boundaries (fully unsupervised); while cons. all methods ∩ BBs is the intersection between MCG, SeSe and GrabCut (uses BSDS data).</p><p>Datasets Since we generate boundary annotations in a weakly supervised fashion, we are able to generate boundaries over arbitrary image sets. In our experiments we consider SBD, VOC (segmentation), and VOC + (VOC plus images from Pascal VOC12 detection task). Methods using VOC + are denoted using · + (e.g. SE (SeSe + ∩ BBs)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Structured forest VOC boundary detection</head><p>In this section we analyse the variants of weakly supervised methods for object boundary detection proposed in Section 5 as opposed to the fully supervised ones. From now on we are interested in external boundaries of objects. Therefore we employ the Pascal VOC12, treating all 20 Pascal classes as one. See details of the evaluation protocol in Section 3. We start by discussing results using SE; convnet results are presented in Section 7.</p><p>6.1. Training models with ground truth SE <ref type="figure">Figure 5</ref> and <ref type="table" target="#tab_2">Table 2</ref> show results of SE trained over the ground truth of different datasets (dashed lines). Our results of SE (VOC) are on par to the ones reported in <ref type="bibr" target="#b36">[29]</ref>. The gap between SE (VOC) and SE (BSDS) reflects the difference between generic boundaries and boundaries specific to the 20 VOC object categories (see also <ref type="figure">Figure 1</ref>).</p><p>SB To improve object-specific boundary detection, the situational boundary method SB <ref type="bibr" target="#b36">[29]</ref>, trains 20 classspecific SE models. These models are combined at test time using a convnet image classifier. The original SB results and our re-implementation SB (VOC) are shown in <ref type="figure">Figure  5</ref>. Our version obtains better results (4 percent points gain in AP) due to training the SE models with more samples per image, and using a stronger image classifier <ref type="bibr" target="#b35">[28]</ref>.</p><p>Detector + SE Rather than training and testing with 20 SE models plus an image classifier, we propose to leverage the same training data using a single SE model together with a detector <ref type="bibr" target="#b19">[12]</ref>. By computing a per-pixel maximum among all detection bounding boxes and their score, we construct an "objectness map" that we multiply with the boundary probability map from SE. False positive boundaries are thus down-scored, and boundaries in high confidence regions for the detector get boosted. The detector is trained with the same per object boundary annotations used to train the SE model, no additional data is required.</p><p>Our Det.+SE (VOC) obtains the same detection quality as SB (VOC) while using only a single SE model. These are the best reported results on this task (top of Table 2), when using the fully supervised training data.</p><p>At the cost of more expensive training and test, one could in principle also combine object detection with the situational boundaries method <ref type="bibr" target="#b36">[29]</ref>, this is out of scope of this paper and considered as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Training models using weak annotations</head><p>Given the reference performance of Det.+SE (VOC), can we reach similar boundary detection quality without using the boundary annotations from VOC? SE (·) First we consider using a SE model alone at test time. Using only the BSDS annotations leads to rather low performance (see SE (BSDS) in <ref type="figure">Figure 6</ref>). PMI shows a  trained on boundaries) fails to reach high precision. The high quality of Det.+BSDS indicates that BSDS annotations, despite being in principle "generic boundaries" in practice reflect well object boundaries, at least in the proximity of an object. This is further confirmed in Section 7. Compared to Det.+BSDS our weakly supervised annotation variants further close the gap to Det.+SE (VOC) (especially in high precision area), even when not using any BSDS data.</p><p>Conclusion Based only on bonding box annotations, our weakly supervised boundary annotations enable the Det.+ SE model to match the fully supervised case, improving over the best reported results on the task. We also observe that BSDS data allows to train models that describe well object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Convnet VOC boundary detection results</head><p>This section analyses the performance of the HED <ref type="bibr" target="#b40">[33]</ref> trained with the weakly supervised variants proposed in Section 5. We use our HED re-implementation of HED which is on par performance with the original (see <ref type="figure" target="#fig_1">Figure  3</ref>). We use the same evaluation setup as in the previous section. <ref type="figure">Figure 7</ref> and <ref type="table">Table 3</ref> show the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HED (·)</head><p>The HED(VOC) model outperforms the SE(VOC) model by a large margin. We observe in the test images that HED manages to suppress well the internal object boundaries, while SE fails to do so due to its more local nature.</p><p>Even though trained on the generic boundaries HED(BSDS) achieves high performance on the object boundary detection task. HED(BSDS) is trained on the "consensus" annotations and they are closer to object-like boundaries as the fraction of annotators agreeing on the  <ref type="table">Table 3</ref>: VOC results for HED models, see <ref type="figure">Figure 7</ref>. Underline indicates ground truth baselines, and bold are our best weakly supervised results. presence of external object boundaries is much higher than for non-object or internal object boundaries.</p><p>For training HED, in contrast to SE model, we do not need closed contours and can use the consensus between different weak annotation variants. This results in better performance. Using the consensus between boundaries of MCG proposals HED(cons. MCG ∩ BBs) improves AP by 6% compared to using the union of object proposals HED(MCG ∩ BBs) (see <ref type="table">Table 3</ref>) .</p><p>The HED models trained with weak annotations outperform the fully supervised SE(VOC) and do not reach the performance of HED(VOC). As has been shown in Section 4 the HED detector is less robust to noise than SE.</p><p>Det.+HED (·) Combining an object detector with HED(VOC) (see Det.+HED (VOC) in <ref type="figure">Figure 7)</ref> is not beneficial to the performance as the HED detector already has notion of objects and their location due to pixel-to-pixel end-to-end learning of the network.  For HED models trained with the weakly supervised variants, employing an object detector at test time brings only a slight improvement of the performance in the high precision area . The reason for this is that we already use information from the bounding box detector to generate the annotation and the convnet method is able to learn it during training.</p><p>Det.+HED (MCG ∩ BBs) outperforms Det.+ HED (BSDS) (see <ref type="table">Table 3</ref>).</p><p>Note that the HED trained with the proposed annotations, generated without using boundary ground truth, performs on par with the HED model trained on generic boundaries (Det.+HED (cons. S&amp;G∩BBs) and Det.+HED (BSDS)in <ref type="figure">Figure 7</ref>).</p><p>The qualitative results are presented in <ref type="figure">Figure 8</ref> and support the quantitative evaluation.</p><p>Conclusion Similar to other computer vision tasks deep convnet methods show superior performance. Due to the pixel-to-pixel training and global view of the image the convnet models have a notion of object and its location which allows to omit the use of the detector at test time. With our weakly supervised boundary annotations we can gain fair performance without using any instance-wise object boundary or generic boundary annotations and leave out object detection at test time by feeding object bounding box information during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">COCO boundary detection results</head><p>Additionally we show the generalization of the proposed weakly supervised variants for object boundary detection on the COCO dataset. We use the same evaluation protocol as for VOC. For weakly supervised cases the results are shown with the models trained on VOC, without re-training on COCO.</p><p>The results are summarized in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">SBD boundary detection results</head><p>In this section we analyse the performance of the proposed weakly supervised boundary variants trained with SE and HED on the SBD dataset <ref type="bibr" target="#b21">[14]</ref>. In contrast to the VOC benchmark we move from object boundaries to class specific object boundaries. We are interested in external boundaries of all annotated objects of the specific semantic class and all internal boundaries are ignored during evaluation following the benchmark <ref type="bibr" target="#b21">[14]</ref>. The results are presented in <ref type="figure" target="#fig_3">Figure 9</ref> and in <ref type="table" target="#tab_7">Table 5</ref>.</p><p>Fully supervised Applying SE model plus object detection at test time outperforms the class specific situational boundary detector (for both <ref type="bibr" target="#b36">[29]</ref> and our re-implementation) as well as the Inverse Detectors <ref type="bibr" target="#b21">[14]</ref>. The model trained with SE on ground truth performs as well as the HED detector. Both of the models are good at detecting external object boundaries; however SE, being a more local, triggers more on internal boundaries than HED. In the VOC evaluation detecting internal object boundaries is penalized, while in SBD these are ignored. This explains the small gap in the performance between SE and HED on this benchmark.</p><p>Weakly supervised The models trained with the proposed weakly-supervised boundary variants perform on par with the fully supervised detectors, while only using bounding boxes or generic boundary annotations. We show in <ref type="table" target="#tab_7">Table  5</ref> the top result with the Det. + HED(cons. S&amp;G∩BBs) model, achieving the state-of-the-art performance on the SBD benchmark. As <ref type="figure" target="#fig_3">Figure 9</ref> shows our weakly supervised approach considerably outperforms <ref type="bibr" target="#b36">[29,</ref><ref type="bibr" target="#b21">14]</ref> on all 20 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The presented experiments show that high quality object boundaries can be achieved using only detection bound-ing box annotations. With these alone, our proposed weaksupervision techniques already improve over previously reported fully supervised results for object-specific boundaries. When using generic boundary or ground truth annotations, we achieve the top performance on the object boundary detection task, outperforming previously reported results by a large margin.</p><p>To facilitate future research all the resources of this project -source code, trained models and results -will be made publicly available.</p><p>• Section B <ref type="figure" target="#fig_2">(Figure 14)</ref> shows visualization of the different variants of the proposed weakly supervised boundary annotations.</p><p>• Section C provides additional results for VOC ( <ref type="figure" target="#fig_5">Figures  10 -12</ref>). Qualitative results of boundary detection for VOC can be found in Section D <ref type="figure" target="#fig_6">(Figure 15</ref>).</p><p>• Detailed results for COCO are shown in Section E ( <ref type="figure" target="#fig_1">Figure 13</ref>). Visualization of boundary detection for COCO can be found in Section E ( <ref type="figure">Figure 16</ref>).</p><p>• Section G reports SBD results per class ( <ref type="figure" target="#fig_10">Figure 17</ref>).</p><p>Boundary detection examples for SBD are shown in Section H <ref type="figure" target="#fig_11">(Figure 18</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weakly supervised boundary annotations</head><p>In this work we propose to train boundary detectors using weakly supervised annotations. We propose and evaluate multiple strategies to generate annotations fusing different sources, such as unsupervised image segmentation <ref type="bibr" target="#b16">[9]</ref>, object proposal methods <ref type="bibr" target="#b37">[30,</ref><ref type="bibr" target="#b30">23]</ref>, and object detectors <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b32">25]</ref> (trained on bounding boxes). <ref type="figure" target="#fig_2">Figure 14</ref> illustrates the examples of the proposed weakly supervised boundary annotations, these extend the example in <ref type="figure" target="#fig_2">Figure 4</ref> of the main paper. See Section 5 of the main paper for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed results for VOC</head><p>We provide here some quantitative results mentioned in Section 5, and in <ref type="table" target="#tab_2">Table 2</ref> of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection BBs versus GT BBs</head><p>To generate weakly supervised boundary annotations we use a class-specific object detector <ref type="bibr" target="#b32">[25,</ref><ref type="bibr" target="#b19">12]</ref>. We also experiment using directly the ground truth bounding box annotations. This results in a minor drop in the performance. Using object detector allows to remove hard cases by discarding images which have zero bounding boxes with confidence scores above 0.8. from the training set, hence improvement in the performance. The results are shown in <ref type="figure">Figure 10</ref>.</p><p>GrabCut, DenseCut and CNN+GraphCut For generating weakly supervised annotations in addition to GrabCut <ref type="bibr" target="#b33">[26]</ref> we also experimented with DenseCut <ref type="bibr" target="#b13">[6]</ref> and CNN+GraphCut <ref type="bibr" target="#b34">[27]</ref>.</p><p>Employing DenseCut or CNN+GraphCut does not bring any gain opposed to GrabCut. The results are presented in <ref type="figure">Figure 11</ref>. Using VOC + Since we generate boundary annotations in a weakly supervised fashion, we are able to generate boundaries over arbitrary image sets. In our experiments we consider VOC (Pascal VOC12 segmentation task) and VOC + (VOC plus images from Pascal VOC12 detection task). <ref type="figure" target="#fig_5">Figure 12</ref> presents the results using VOC and VOC + . Methods using VOC + are denoted by · + (e.g. SE (SeSe + ∩ BBs)). Using a larger set of images for training allows to further improve the performance of SE trained with the generated boundary annotations.   <ref type="figure" target="#fig_1">Figure 13</ref> shows the generalization of the proposed weakly supervised variants for object boundary detection on the COCO dataset. For weakly supervised cases the results are shown with the models trained on VOC, without retraining on COCO. These curves complement <ref type="table">Table 3</ref> from the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. VOC boundary detection examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Detailed results for COCO</head><p>For both SE and HED the models trained on the proposed weak annotations perform as well as the fully supervised SE models. Similar to the VOC benchmark the HED model trained on ground truth shows superior performance. <ref type="figure">Figure 16</ref> shows examples of boundary detection on COCO. This figure complements <ref type="table">Table 3</ref> from the main paper. Our proposed weak-supervision techniques achieve competitive performance with fully supervised results for object-specific boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. COCO boundary detection examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Detailed results for SBD</head><p>Per-class curves <ref type="figure" target="#fig_10">Figure 17</ref> shows the per class performance of the proposed weakly supervised boundary variants trained with SE and HED on the SBD dataset <ref type="bibr" target="#b21">[14]</ref> (this figure is a breakdown of <ref type="figure" target="#fig_3">Figure 9</ref> in the main paper). In contrast to VOC and COCO we move from object boundaries to class specific object boundaries. We are interested in external boundaries of all annotated objects of the specific semantic class and all internal boundaries are ignored during evaluation following the benchmark <ref type="bibr" target="#b21">[14]</ref>. As <ref type="figure" target="#fig_10">Figure 17</ref> shows our weakly supervised approach considerably outperforms <ref type="bibr" target="#b36">[29,</ref><ref type="bibr" target="#b21">14]</ref> on all 20 classes. Compared to VOC, SE and HED results are most similar between each other because the evaluation protocol focuses on the external object boundaries (ignoring internal object boundaries), where both methods equally well. Compared to VOC, we also notice that Det.+HED (cons. S&amp;G∩BBs) performs better than Det.+HED (SBD), we attribute this to the "consensus" aspect of our generated annotations (see BSDS results, Section 4 of main paper). <ref type="figure" target="#fig_11">Figure 18</ref> shows examples of boundary detection on the SBD dataset. As the quantitative results indicate, qualitatively our weakly supervised results are on par to the fully supervised ones.      <ref type="figure" target="#fig_11">Figure 18</ref>: Qualitative results on SBD. Red/green indicates false/true positives pixels, grey is missing recall. All methods shown at 50% recall. <ref type="bibr">Det</ref>   <ref type="figure" target="#fig_11">Figure 18</ref>: Qualitative results on SBD. Red/green indicates false/true positives pixels, grey is missing recall. All methods shown at 50% recall. <ref type="bibr">Det</ref>.+SE (weak) denotes the model Det.+SE (MCG+ ∩ BBs) and Det.+HED (weak) denotes Det. +HED (cons. S&amp;G∩BBs). As the quantitative results indicate, qualitatively our weakly supervised results are on par to the fully supervised ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. SBD boundary detection examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Image (b) SE(VOC) (c) Det.+SE (VOC) (d) SE(BSDS) (e) SE (weak) (f) Det.+SE (weak)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>BSDS results. Canny and F&amp;H points indicate the boundaries used as noisy annotations. When trained over noisy annotations, both SE and HED provide a large quality improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>cons. S&amp;G∩BBs (j) cons. all methods ∩ BBs Different generated boundary annotations. Cyan/black indicates positive/ignored boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>SBD results per class. (·) denotes the data used for training. Det.+HED (weak) refers to the model Det.+ HED (cons. S&amp;G ∩ BBs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>SE(MCG \ BBs) [42] SE(MCG \ BBs GT ) VOC results: Detection BBs versus GT BBs. (·) denotes the data used for training. Legend indicates AP numbers. BBs GT denotes bounding boxes annotated by humans. VOC results: GrabCut versus DenseCut and CNN+GraphCut. (·) denotes the data used for training. Continuous/dashed line indicates models using/not using a detector at test time. Legend indicates AP numbers. Den-seCut and CNN+GraphCut perform on par with GrabCut.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>VOC results using additional VOC + images. (·) denotes the data used for training. Continuous/dashed line indicates models using/not using a detector at test time. Legend indicates AP numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 15 presents</head><label>15</label><figDesc>qualitative results of boundary detection on VOC. The presented boundary estimate examples show that high quality object boundaries can be achieved using only detection bounding box annotations. This figure extends Figure 7 of the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>COCO results. (·) denotes the data used for training. Continuous/dashed line indicates models using/not using a detector at test time. Legend indicates AP numbers. For weakly supervised cases the results are shown with the models trained on VOC, without re-training on COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>F&amp;H ∩ BBs (d) GrabCut ∩ BBs (e) SeSe ∩ BBs (f) MCG ∩ BBs (g) cons. MCG ∩ BBs (h) cons. S&amp;G ∩ BBs (i) cons. all ∩ BBs (j) SE(SeSe ∩ BBs) Different generated boundary annotations. Cyan/black indicates positive/ignored boundaries. Image Ground truth SE(BSDS) SB(VOC) Det.+SE (VOC) Det.+SE (weak) Det.+HED (weak)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Det. + SE(SBD) [30] Det. + HED(weak) [30] Det. + SE(weak) [26] Det. + HED(SBD) [21] SB(SBD) [18] SB(SBD) orig. [12] Hariharan et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :</head><label>17</label><figDesc>SBD boundary PR curves per class.(·) denotes the data used for training. Legend indicates AP numbers. Det.+ SE (weak) denotes the model Det.+SE (MCG+ ∩ BBs) and Det.+HED (weak) denotes Det. +HED (cons. S&amp;G∩BBs). For all classes our weakly supervised results are on par to the fully supervised ones. (SBD) Det.+SE (weak) Det.+HED (SBD) Det.+HED (weak)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 18 :</head><label>18</label><figDesc>Qualitative results on SBD. Red/green indicates false/true positives pixels, grey is missing recall. All methods shown at 50% recall.Det.+SE (weak) denotes the model Det.+SE (MCG+ ∩ BBs) and Det.+HED (weak) denotes Det. +HED (cons. S&amp;G∩BBs). As the quantitative results indicate, qualitatively our weakly supervised results are on par to the fully supervised ones. (SBD) Det.+SE (weak) Det.+HED (SBD) Det.+HED (weak)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Precision</cell><cell>0.5 0.6 0.7 0.8 0.9 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">[48] Det. + SE(VOC) [47] Det. + SE(BSDS) [47] Det. + PMI [47] Det. + SE(GrabCut BBs) [46] Det. + SE(SeSe + BBs) [43] SE(VOC) [43] SE(SeSe + BBs) [41] SE(GrabCut BBs) [41] PMI [40] SE(BSDS)</cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>Recall 0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell cols="13">Figure 6: VOC12 results, weakly supervised SE models.</cell></row></table><note>VOC results for SE models, see Figures 5 and 6. Underline indicates ground truth baselines, and bold are our best weakly supervised results.similar gap. The same BSDS data can be used to generate MCG object proposals over the VOC training data, and a de- tector trained on VOC bounding boxes can generate bound- ing boxes over the same images. We combined them to- gether to generate boundary annotations via MCG ∩ BBs, as described in Section 5. The weak supervision from the bounding boxes can be used to improve the performance of SE (BSDS). By extending the training set to additional pas- cal images (SE (MCG + ∩ BBs) in Table 2) we can reach the same performance as when using the ground truth VOC data. We also consider variants that do not leverage the BSDS boundary annotations, such as SeSe and GrabCut. SeSe provides essentially the same result as MCG. Det.+SE (·) Applying object detection at test time squashes the differences among all weakly supervised methods. Det.+PMI shows strong results, but (since not(·) denotes the data used for training. Continuous/dashed line indicates models using/not using a detector at test time. Legend indicates AP numbers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>COCO results, curves in supplementary material. Underline indicates ground truth baselines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>On the COCO benchmark for both SE and HED the models trained on the proposed weak annotations perform as well as the fully supervised SE models. Similar to the VOC benchmark the HED model trained on ground truth shows superior performance. Qualitative results on VOC. (·) denotes the data used for training. Red/green indicate false/true positive pixels, grey is missing recall. All methods are shown at 50% recall. Det.+SE (weak) refers to the model Det.+SE (SeSe + ∩ BBs) Det.+ HED (weak) refers to Det.+HED (cons. S&amp;G ∩ BBs). Object-specific boundaries differ from generic boundaries (such as the ones detected by SE(BSDS)). By using an object detector we can suppress non-object boundaries and focus boundary detection on the classes of interest. The proposed weakly supervised techniques allow to achieve high quality boundary estimates that are similar to the ones obtained by fully supervised methods. +HED (cons. MCG ∩ BBs) 4844   Det.+HED (cons. S&amp;G ∩ BBs) 52 47</figDesc><table><row><cell cols="2">Image</cell><cell>Ground truth</cell><cell cols="2">SE(BSDS)</cell><cell>SB(VOC)</cell><cell>Det.+SE (VOC) Det.+SE (weak) Det.+HED (weak)</cell></row><row><cell cols="3">Figure 8: Family Method</cell><cell></cell><cell>mF mAP</cell></row><row><cell>Other</cell><cell>GT</cell><cell>Hariharan et al. [14] SB(SBD) orig. [29]</cell><cell></cell><cell>28 21 39 32</cell></row><row><cell></cell><cell>GT</cell><cell>SB(SBD)</cell><cell></cell><cell>43 37</cell></row><row><cell></cell><cell></cell><cell>Det.+SE (SBD)</cell><cell></cell><cell>51 45</cell></row><row><cell></cell><cell>Other</cell><cell>Det.+SE (BSDS)</cell><cell></cell><cell>51 44</cell></row><row><cell>SE</cell><cell>GT</cell><cell>Det.+MCG (BSDS)</cell><cell></cell><cell>50 42</cell></row><row><cell></cell><cell>Weakly super-vised</cell><cell cols="2">SB(SeSe ∩ BBs) SB (MCG ∩ BBs) Det.+SE (SeSe ∩ BBs) Det.+SE (MCG ∩ BBs)</cell><cell>40 34 42 35 48 42 51 45</cell></row><row><cell></cell><cell>GT</cell><cell>HED (SBD) Det.+HED (SBD)</cell><cell></cell><cell>44 41 49 45</cell></row><row><cell></cell><cell>Other</cell><cell>HED(BSDS)</cell><cell></cell><cell>38 32</cell></row><row><cell>HED</cell><cell>GT Weakly super-vised</cell><cell cols="2">Det.+HED (BSDS) HED(cons. MCG ∩ BBs) HED (cons. S&amp;G ∩ BBs) Det.</cell><cell>49 44 41 37 44 39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>SBD results. Results are mean F(ODS)/AP across all 20 categories. (·) denotes the data used for training. See also Figure 9. Underline indicates ground truth baselines, and bold are our best weakly supervised results.</figDesc><table><row><cell>mAP</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Det. + SE(SBD) Det. + HED(SBD) Det. + HED(weak) SB(SBD) orig. Hariharan et al.</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>aeroplane</cell><cell>sheep</cell><cell>bus</cell><cell>bird</cell><cell>person</cell><cell>motorbike</cell><cell>horse</cell><cell>dog</cell><cell>cow</cell><cell>cat</cell><cell>bicycle</cell><cell>train</cell><cell>car</cell><cell>boat</cell><cell>bottle</cell><cell>tvmonitor</cell><cell>chair</cell><cell>pottedplant</cell><cell>sofa</cell><cell>diningtable</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Figure 15: Qualitative results on VOC. (·) denotes the data used for training. Red/green indicate false/true positive pixels, grey is missing recall. All methods are shown at 50% recall.Det.+SE (weak)  denotes the model Det.+SE (SeSe+ ∩ BBs) and Det.+HED (weak) denotes Det.+HED (cons. S&amp;G∩BBs). Object-specific boundaries differ from generic boundaries (such as the ones detected by SE(BSDS)). By using an object detector we can suppress non-object boundaries and focus boundary detection on the classes of interest. The proposed weakly supervised techniques allow to achieve high quality boundary estimates that are similar to the ones obtained by fully supervised methods.Det.+SE (weak) Det.+HED (COCO)Det.+HED (weak)Figure 16: Qualitative results on COCO. (·) denotes the data used for training. Red/green indicate false/true positive pixels, grey is missing recall. All methods are shown at 50% recall.Det.+SE (weak) denotes the model Det.+SE (SeSe+ ∩ BBs) and Det.+HED (weak) denotes Det.+HED (cons. S&amp;G∩BBs). Object-specific boundaries differ from generic boundaries (such as the ones detected by SE(BSDS)). By using an object detector we can suppress non-object boundaries and focus boundary detection on the classes of interest. The proposed weakly supervised techniques allow to achieve high quality boundary estimates that are similar to the ones obtained by fully supervised methods.</figDesc><table><row><cell>Precision</cell><cell>0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell>aeroplane</cell><cell>[66] [66] [64] [62] [62] [59] [42]</cell><cell>Precision</cell><cell>1 0.8 0.9 0.7 0.4 0.5 0.6</cell><cell>bicycle</cell><cell>[54] [53] [52] [50] [50] [48] [47]</cell><cell>Precision</cell><cell>1 0.8 0.9 0.7 0.4 0.5 0.6</cell><cell>bird</cell><cell>[64] [61] [61] [60] [46] [43] [16]</cell><cell>Precision</cell><cell>1 0.9 0.8 0.7 0.5 0.6 0.4</cell><cell>boat</cell><cell>[48] [48] [47] [45] [43] [41] [17]</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell cols="2">0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell></cell><cell>0</cell><cell cols="2">0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell></cell><cell>0</cell><cell cols="2">0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell></cell><cell>0</cell><cell cols="2">0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell></row><row><cell>Precision Precision</cell><cell>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.7 0.8 0.9 1 0.5 0.6 0.4</cell><cell cols="10">Image 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Ground truth Recall bottle [47] [46] [46] [45] [37] [32] [24] Precision Recall 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 chair [41] [41] [40] [38] Det.+SE (COCO) Recall SE(BSDS) Recall Recall 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 bus [61] [61] [60] [57] [53] [52] [43] 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Recall Precision 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 car [51] [51] [51] [51] [43] [40] [36] [29] [25] [19]</cell><cell>Precision</cell><cell>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</cell><cell cols="2">Recall Recall 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 cat [56] [55] [55] [55] [50] [45] [23]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>.+SE (weak) denotes the modelDet.+SE (MCG+ ∩ BBs) and Det.+HED (weak) denotes Det. +HED (cons. S&amp;G∩BBs). As the quantitative results indicate, qualitatively our weakly supervised results are on par to the fully supervised ones. Det.+SE (weak) Det.+HED (SBD) Det.+HED (weak)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>cat</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chair</cell></row><row><cell></cell><cell></cell><cell></cell><cell>cow</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dining table</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dog</cell></row><row><cell></cell><cell></cell><cell></cell><cell>horse</cell></row><row><cell></cell><cell></cell><cell></cell><cell>motorbike</cell></row><row><cell>Image</cell><cell>Ground truth</cell><cell>SB(SBD)</cell><cell>Det.+SE (SBD)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Jan Hosang for help with Fast R-CNN training; Seong Joon Oh and Bojan Pepik for valuable discussions and feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material A. Content</head><p>This supplementary material provides both additional quantitative and qualitative results:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">HED cons</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hed</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepedge</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hed(se</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sketch Tokens</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">SE(F&amp;H)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">HED(F&amp;H)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SE(Canny)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Second-order constrained parametric proposals and sequential searchbased structured prediction for semantic segmentation in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densecut: Densely connected crfs for realtime grabcut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">PAMI</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient graph-based image segmentation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">N4-fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Oriented edge forests for boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals? PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Boundary detection benchmarking: Beyond f-measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crisp boundary detection using pointwise mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised learning of edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04166</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grabcutinteractive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Situational object boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with a multi-image model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to segment under various weak supervisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">segdeepm: Exploiting segmentation and context in deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>1 [48] Det. + SE(VOC</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">\</forename><surname>Se(cnn + Graphcut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se</surname></persName>
		</author>
		<imprint>
			<pubPlace>GrabCut \ BBs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se</surname></persName>
		</author>
		<imprint>
			<pubPlace>DenseCut \ BBs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">SE(VOC)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">\</forename><surname>Se(cnn + Graphcut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">\</forename><surname>Se(densecut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">\</forename><surname>Se(grabcut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se</surname></persName>
		</author>
		<imprint>
			<publisher>VOC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se(mcg + \ Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se(mcg \ Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+ \</forename><surname>Se(sese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se</surname></persName>
		</author>
		<imprint>
			<pubPlace>SeSe \ BBs</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se(mcg + \ Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">SE(VOC)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+ \</forename><surname>Se(sese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se(mcg \ Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">\</forename><surname>Se(sese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">HED(COCO)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hed</surname></persName>
		</author>
		<imprint>
			<publisher>COCO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Det. + HED(cons. all methods \ BBs)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hed</surname></persName>
		</author>
		<imprint>
			<publisher>BSDS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Det. + HED(cons. S&amp;G \ BBs)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se</surname></persName>
		</author>
		<imprint>
			<publisher>COCO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+ \</forename><surname>Se(sese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se(mcg + \ Bbs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">+</forename><surname>Det</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se</surname></persName>
		</author>
		<imprint>
			<publisher>BSDS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">SE(COCO)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
