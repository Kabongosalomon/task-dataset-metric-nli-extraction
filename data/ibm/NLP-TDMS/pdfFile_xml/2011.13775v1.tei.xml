<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Generators with Conditionally-Independent Pixel Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anokhin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Demochkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khakhulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sterkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korzhenkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Generators with Conditionally-Independent Pixel Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Samples from our generators trained on several challenging datasets (LSUN Churches, FFHQ, Landscapes, Satellite-Buildings, Satellite-Landscapes) at resolution 256 × 256. The images are generated without spatial convolutions, upsampling, or self-attention operations. No interaction between pixels takes place during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>State-of-the-art in unconditional image generation is achieved using large-scale convolutional generators trained in an adversarial fashion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1]</ref>. While lots of nuances and ideas have contributed to the state-of-the-art recently, for many years since the introduction of DC-GAN <ref type="bibr" target="#b21">[22]</ref> such generators are based around spatial convolutional layers, also occasionally using the spatial selfattention blocks <ref type="bibr" target="#b31">[32]</ref>. Spatial convolutions are also invariably present in other popular generative architectures for images, including autoencoders <ref type="bibr" target="#b13">[14]</ref>, autoregressive generators <ref type="bibr" target="#b29">[30]</ref>, or flow models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. Thus, it may seem that spatial convolutions (or at least spatial self-attention) is an unavoidable building block for state-of-the-art image generators.</p><p>Recently, a number of works have shown that individual images or collections of images of the same scene can be encoded/synthesized using rather different deep architectures (deep multi-layer perceptrons) of a special kind <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>. Such architectures are not using spatial convolutions or spatial self-attention and yet are able to reproduce images rather well. They are, however, restricted to individual scenes. In this work, we investigate whether deep generators for unconditional image class synthesis can be built using similar architectural ideas, and, more importantly, whether the quality of such generators can be pushed to state-of-the-art.</p><p>Perhaps surprisingly, we come up with a positive answer ( <ref type="figure">Fig. 1)</ref>, at least for the medium image resolution (of 256 × 256). We have thus designed and trained deep generative architectures for diverse classes of images that achieve similar quality of generation to state-of-the-art convolutional generator StyleGANv2 <ref type="bibr" target="#b10">[11]</ref>, even surpassing this quality for some datasets. Crucially, our generators are not using any form of spatial convolutions or spatial attention in their pathway. Instead, they use coordinate encodings of individual pixels, as well as sidewise multiplicative conditioning (weight modulation) on random vectors. Aside from such conditioning, the color of each pixel in our architecture is predicted independently (hence we call our image generator architecture Conditionally-Independent Pixel Synthesis (CIPS) generators).</p><p>In addition to suggesting this class of image generators and comparing its quality with state-of-the-art convolutional generators, we also investigate the extra flexibility that is permitted by the independent processing of pixels. This includes easy extention of synthesis to nontrivial topologies (e.g. cylindrical panoramas), for which the extension of spatial convolutions is known to be nontrivial <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2]</ref>. Furthermore, the fact that pixels are synthesized independently within our generators, allows sequential synthesis for memory-constrained computing architectures. It enables our model to both improve the quality of photos and generate more pixel values in a specific areas of image (i.e. to perform foveated synthesis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Feeding pixel coordinates as an additional input to the neural network previously was successfully used in the widely known CoordConv technique <ref type="bibr" target="#b17">[18]</ref> to introduce the spatial-relational bias. Recently, the same idea was employed by the COCO-GAN <ref type="bibr" target="#b16">[17]</ref> to generate images by parts or create "looped" images like spherical panoramas. However, those models still used standard convolutions as the main synthesis operation. The synthesis process for neighboring pixels in such architectures is therefore not independent.</p><p>To the best of our knowledge, the problem of regressing a given image from pixel coordinates with a perceptron (that calculates each pixel's value independently) started from creating compositional patterns with an evolutionary approach <ref type="bibr" target="#b27">[28]</ref>. Those patterns, appealing for digital artists, were also treated as kind of differentiable image parametrization <ref type="bibr" target="#b20">[21]</ref>. However, this approach was not capable of producing photorealistic hi-res outputs (e.g., see the demo <ref type="bibr" target="#b7">[8]</ref>).</p><p>Some machine learning blogs reported experiments with GANs, where the generator was a perceptron that took a random vector and pixel coordinates as an input, and returned that pixel's value as an output <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. The described model was successfully trained on MNIST, but has not been scaled to more complex image data.</p><p>Scene-representation networks <ref type="bibr" target="#b26">[27]</ref> and later the neural radiance fields (NeRF) networks <ref type="bibr" target="#b19">[20]</ref> have demonstrated how 3D content of individual scenes can be encoded with surprising accuracy using deep perceptron networks. Following this realization, systems <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b28">[29]</ref> considered the usage of periodic activation functions and so-called Fourier features to encode the pixel (or voxel) coordinates, fed to the multi-layer perceptron. In particular, the ability to encode high-resolution individual images in this way was demonstrated. All these works however have not considered the task of learning image generators, which we address here.</p><p>The very recent (and independent) Generative Radiance Fields (GRAF) system <ref type="bibr" target="#b24">[25]</ref> showed promising results at embedding the NeRF generator into an image generator for 3D aware image synthesis. Results for such 3D aware synthesis (still limited in diversity and resolution) for certain have been demonstrated. Here, we do not consider 3Daware synthesis and instead investigate whether perceptronbased architectures can achieve high 2D image synthesis quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our generator network synthesizes images of a fixed resolution H × W and has the multi-layer perceptron-type architecture G (see <ref type="figure" target="#fig_0">Fig. 2</ref>). In more detail, the synthesis of each pixel takes a random vector z ∈ Z shared across all pixels, as well the pixel coordinates (</p><formula xml:id="formula_0">x, y) ∈ {0 . . . W − 1} × {0 . . . H − 1} as input. It then returns the RGB value c ∈ [0, 1] 3 of that pixel G : (x, y, z) → c.</formula><p>Therefore, to compute the whole output image I, the generator G is evaluated at at each pair (x, y) of the coordinate grid, while keeping the random part z fixed:</p><formula xml:id="formula_1">I = {G (x, y; z) | (x, y) ∈ mgrid (H, W )} ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">mgrid (H, W ) = {(x, y) | 0 ≤ x &lt; W, 0 ≤ y &lt; H}</formula><p>is a set of integer pixel coordinates.</p><p>Following <ref type="bibr" target="#b9">[10]</ref>, a mapping network M (also a perceptron) turns z into a style vector w ∈ W, M : z → w, and all the stochasticity in the generating process comes from this style component.</p><p>We then follow the StyleGANv2 <ref type="bibr" target="#b10">[11]</ref> approach of injecting the style w into the process of generation via weight modulation. To make the paper self-contained, we describe the procedure in brief here.</p><p>Any modulated fully-connected (ModFC) layer of our generator (see <ref type="figure" target="#fig_0">Fig. 2</ref>) can be written in the form ψ = Bφ + b, where φ ∈ R n is an input,B is a learnable weight matrix B ∈ R m×n modulated with the style w, b ∈ R m is a learnable bias, and ψ ∈ R m is an output. The modulation takes place as follows: at first, the style vector w is mapped with a small net (referred to as A in <ref type="figure" target="#fig_0">Fig. 2</ref>) to a scale vector s ∈ R n Then, the (i, j)-th entry ofB is computed aŝ</p><formula xml:id="formula_3">B ij = s j B ij + n k=1 (s k B ik ) 2 ,<label>(2)</label></formula><p>where is a small constant. After this linear mapping, a LeakyReLU function is applied to ψ. Finally, in our default configuration we add skip connections for every two layers from intermediate feature maps to RGB values and sum the contributions of RGB outputs corresponding to different layers. These skip connections naturally add values corresponding to the same pixel, and do not introduce interactions between pixels.</p><p>We note that the independence of the pixel generation process, makes our model parallelizable at inference time and, additionally, provides flexibility in the latent space z. E.g., as we show below, in some modified variants of synthesis, each pixel can be computed with a different noise vector z, though gradual variation in z is needed to achieve consistently looking images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Positional encoding</head><p>The architecture described above needs an important modification in order to achieve the state-of-the-art synthesis quality. Recently two slightly different versions of positional encoding for coordinate-based multi-layer perceptrons (MLP), producing images, were described in literature. Firstly, SIREN <ref type="bibr" target="#b25">[26]</ref> proposed a perceptron with a principled weight initialization and sine as an activation function, used throughout all the layers. Secondly, the Fourier features, introduced in <ref type="bibr" target="#b28">[29]</ref>, employed a periodic activation function in the very first layer only. In our experiments, we apply a somewhat in-between scheme: the sine function is used to obtain Fourier embedding e f o , while other layers use a standard LeakyReLU function:</p><formula xml:id="formula_4">e f o (x, y) = sin B f o (x , y ) T ,<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping Network ModFC</head><p>Fourier Features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coordinates Embeddings</head><p>ModFC However, only Fourier positional encoding usage turned out insufficient to produce plausible images. In particular, we have found out that the outputs of the synthesis tend to have multiple wave-like artifacts. Therefore, we also train a separate vector e </p><formula xml:id="formula_5">A FC + LeakyReLU Mod DeMod Normalize ModFC ...</formula><p>and serve as an input for the next perceptron layer: G (x, y; z) = G (e (x, y) ; M (z)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture details</head><p>In our experiments, both Fourier features and coordinate embeddings had the dimension of 512. The generator had 14 modulated fully-connected layers of width 512 We use leaky ReLU activation with the slope 0.2. We implement our experiments on top of the public code 1 for StyleGANv2. Our model is trained with a standard non-saturating logistic GAN loss with R 1 penalty <ref type="bibr" target="#b18">[19]</ref> applied to the discriminator D. The discriminator has a residual architecture, described in <ref type="bibr" target="#b10">[11]</ref> (we have deliberately kept the discriminator architecture intact). Networks were trained by Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with learning rate 2×10 −3 and hyperparameters: β 0 = 0, β 1 = 0.99, = 10 −8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>We now evaluate CIPS generators and their variations on a range of datasets. For the sake of efficiency, most evaluations are restricted to 256 × 256 resolution. The following datasets were considered:</p><p>• The Flickr Faces-HQ (FFHQ) <ref type="bibr" target="#b9">[10]</ref> dataset contains 70,000 high quality well-aligned, mostly near frontal human faces. This dataset is the most regular in terms of geometric alignement and the StyleGAN variants are known to perform very well in this setting.</p><p>• The LSUN Churches <ref type="bibr" target="#b30">[31]</ref> contains 126,000 outdoor photographs of churches of rather diverse architectural style. The dataset is regular, yet images all share upright orientation.</p><p>• The Landscapes dataset contains 60,000 manually collected landscape photos from the Flickr website.</p><p>• The Satellite-Buildings 2 dataset contains 280,741 images of 300 × 300 pixels (which we crop to 256 × 256 resolution and randomly rotate). This dataset has large size, and is approximately aligned in terms of scale, yet lacks consistent orientation.</p><p>• Finally, the Satellite-Landscapes 3 contains a smaller curated collection of 2,608 images of 512 × 512 resolution of satellite images depicting various impressive landscapes found on Google Earth (which we crop to 256 × 256 resolution). This is the most "textural" dataset, that lacks consistent scale or orientation.</p><p>For evaluation, we relied on commonly used metrics for image generation: Frechet Inception Distance (FID) <ref type="bibr">[</ref>  The resulting quality of our model is better in terms of precision (corresponds to plausibility of images) and worse in recall (this points to the greater number of dropped modes).</p><p>well as more recently introduced generative Precision and Recall measures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b14">15]</ref>. Our main evaluation is thus against the state-of-the-art StyleGANv2 generator <ref type="bibr" target="#b10">[11]</ref>. We took FID value for LSUN Churches directly from the original paper <ref type="bibr" target="#b10">[11]</ref> and trained StyleGANv2 on other datasets in authors' setup but without style-mixing and path regularization of generator -as noted in the original paper, these changes do not influence the FID metric. The results of this key comparison are presented in Tab. 1 and 2. Neither of the two variants of the generator dominates the other, with StyleGANv2 achieving lower (better) FID score on FFHQ and Landscapes, while CIPS generator achieving lower score on LSUN Churches and both Satellite datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>We then evaluate the importance of different parts of our model by its ablation on the FFHQ dataset (Tab. 3). We thus consider removing Fourier features, coordinate embeddings (config referred to as CIPS-NE) and replace LeakyReLU activation with sine function in all layers. We also compare the variants with residual connections (we follow Style-GANv2 <ref type="bibr" target="#b10">[11]</ref> implementation adjusting variance of residual blocks with the division by √ 2) with our main choice of cumulative projections to RGB. Additionally, the "base" configuration without skip connections and residual connections is considered. In this comparison, all models were trained for 300K iterations with batch size of 16.</p><p>As the results show, coordinate embeddings, residual blocks and cumulative projection to RGB significantly im-CIPS "base" "No embed (NE)" "No Fourier" Main "Residual" Sine Fourier Features</p><formula xml:id="formula_7">+ + - + + - Coordinate Embeddings + - + + + + Residual blocks - - - - + - Skip connections - - - + - - Sine Activation - - - - - + FID</formula><p>6.71 12.71 10.18 6.31 6.52 10.0 <ref type="table">Table 3</ref>: Effects of the modifications of CIPS generator on the FFHQ dataset in terms of Frechet Inception Distance (FID) score. Each column corresponds to a certain configuration, while rows correspond to present/missing features. The simultaneous usage of Fourier features and coordinate embeddings is necessary for a good FID score. Also, both residual connections and cumulative skip connections (default configuration) to the output outperform the plain multilayer perceptron. prove the quality of the model. The removal of coordinate embeddings most severely worsens the FID value, and affects the quality of generated images <ref type="figure" target="#fig_2">(Fig. 3)</ref>. We further investigate the importance of coordinate embeddings for the CIPS model below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Influence of positional encodings</head><p>To analyze the difference between Fourier features e f o and coordinate embeddings e co , we plotted the spectrum of these codes for the generator CIPS-base, trained on FFHQ. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, Fourier encoding generally carries lowfrequency components, whereas coordinate embeddings resemble more high-frequency details. The Principal Com-   transform the positional codes and, for example, finally produce more fine-grained details, relying on Fourier features. To demonstrate that this is not the case, we conducted the following experiment. We have zeroed out either the output of Fourier features or coordinate embeddings and showed the obtained images in <ref type="figure" target="#fig_5">Fig. 6</ref>. One can notice that the information about the facial hair's details as well as the forelock is located in the coordinate embeddings. This proves that it is the coordinate embeddings that are the key to highfrequency details of the resulting image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Spectral analysis of generated images</head><p>Recently, <ref type="bibr" target="#b3">[4]</ref> observed that the common convolutional upsampling operations can lead to the inability to learn the spectral distribution of real images, in spite of any generator architecture. In contrast, CIPS operates explicitly with the coordinate grid and has no upscaling modules, which should to improved reproduction of the spectrum. Indeed, we compare the spectrum of our models (CIPS-"base" without residual and skip connections; CIPS-NE) to Style-GANv2 and demonstrate that CIPS generators design has the advantage in the spectral domain.</p><p>The analysis of magnitude spectra for produced images is given in <ref type="figure" target="#fig_7">Fig. 7a</ref>. The spectrum of StyleGANv2 has artifacts in high-frequency regions, not present in both CIPS generators under consideration. Following prior works <ref type="bibr" target="#b3">[4]</ref>, we also use the azimuthal integration (AI) over the Fourier power spectrum <ref type="figure" target="#fig_7">(Fig. 7b</ref>). It is worth noting that AI statistics of CIPS-NE are very close to the ones of real images. However, adding the coordinate embeddings degrades a realistic spectrum while improving the quality in terms of FID (Tab. 3).</p><p>We note that the introduction of skip connections in fact makes the spectra less similar to those of natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Interpolation</head><p>We conclude the experimental part with the demonstration of the flexibility of CIPS. As well as many other generators, CIPS generators have the ability to interpolate be-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIPS-base CIPS-NE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGANv2</head><p>Real Images (a) Magnitude spectrum. Our models produce less artifacts in high frequency components (note the grid-like pattern in StyleGANv2).</p><p>Two CIPS models are difficult to distinguish between (better zoom in).  tween latent vectors with meaningful morphing <ref type="figure" target="#fig_10">(Fig. 10)</ref>. As expected, the change between the extreme images occurs smoothly and allows for the use of this property, in a   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Foveated rendering and interpolation</head><p>One of the inspiring applications of our per-pixel generator is the foveated synthesis. The foveated synthesis ability can be beneficial for computer graphics and other applications, as well as mimics human visual system. In foveated synthesis, an irregular grid of coordinates is sampled first, more dense in the area, where the gaze is assumed to be directed to, and more sparse outside of that region. After that, CIPS is evaluated on this grid (its size is less than the full resolution), and color for missing pixels of the image is filled using interpolation. The demonstration of this method is provided in <ref type="figure" target="#fig_8">Fig. 8</ref>.</p><p>Alongside the foveated rendering, we are also able to interpolate the image beyond the training resolution by simply sampling denser grids. Here we use a model, trained on images of 256×256 resolution to process a grid of 1024×1024 pixels and compare it with upsampling the results of upsampling the image synthesized at the 256×256 resolution with the Lanczos filter <ref type="bibr" target="#b15">[16]</ref>. As <ref type="figure" target="#fig_9">Fig. 9</ref> suggests, more plausible details are obtained with denser synthesis than with Lanczos filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Panorama synthesis</head><p>As CIPS is built upon a coordinate grid, it can relatively easily use non-Cartesian grids. To show this, we thus adopt a cylindrical system to produce landscape panoramas. The training setup is as follows. We uniformly sample a crop 256 × 256 from the cylindrical coordinate grid and train the generator to produce images using these coordinate crops as inputs. A similar idea was also explored in <ref type="bibr" target="#b16">[17]</ref>. We note, however, that during training we do not use any real panoramas in contrast to other coordinate-based COCO-GAN model <ref type="bibr" target="#b16">[17]</ref>. <ref type="figure">Fig. 11a and 11b</ref> provide examples of panorama samples obtained with the resulting model.</p><p>As each pixel is generated from its coordinates and style vector only, our architecture admits pixel-wise style inter-  <ref type="figure">Figure 11</ref>: Panorama blending. We linearly blend two upper images from CISP generator trained on the Landscapes dataset with a cylindrical coordinate system. The resulting image contains elements from both original panoramas: land and water integrated naturally. polation <ref type="figure">(Fig. 11c</ref>). In these examples, the style vector blends between the central part (the style of <ref type="figure">Fig. 11a</ref>) and the outer part (the style of 11b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Typical artifacts</head><p>Finally, we show the typical artifacts that keep recurring in the results of CIPS generators <ref type="figure" target="#fig_0">(Fig. 12)</ref>. We attribute the wavy texture (in hair) and repeated lines pattern (in buildings) to the periodic nature of sine activation function within the Fourier features. Also we note that sometimes CIPS produces a realistic image with a small part of the image being inconsistent with the rest and out of the domain. Our belief is that this behaviour is caused by the LeakyReLU activation function that divides the coordinate grid into parts. For each part, CIPS effectively applies its own inverse discrete Fourier transform. As CIPS generators do not use any upsampling or other pixel coordination, it is harder for the generator to safeguard against such behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a new generator model called CIPS, a high-quality architecture with conditionally independent pixel synthesis, such that the color value is computed using only random noise and coordinate position.</p><p>Our key insight is that the proposed architecture without spatial convolutions, attention or upsampling operations has the ability to compete in the model market and obtain decent quality in terms of FID and precision &amp; recall; such results have not been presented earlier for perceptron-based models. Furthermore, in the spectral domain outputs of CIPS are harder to discriminate from real images. Interestingly, CIPS-NE modification is weaker in terms of plausibility, yet <ref type="figure" target="#fig_0">Figure 12</ref>: Examples of the most common kinds of artifacts on different datasets. They are best described as wavy textures on hair, background, and glowing blobs. see text for the discussion.</p><p>has a more realistic spectrum.</p><p>Direct usage of a coordinate grid allows us to work with more complex structures, such as cylindrical panoramas, just by replacing the underlying coordinate system.</p><p>In summary, our generator demonstrates quality on par with state-of-the-art model StyleGANv2; moreover, it has applications in various diverse scenarios. We have shown that the considered model could be successfully applied to foveated rendering and super-resolution problems in their generative interpretations. Future development of our approach assumes researching these problems in their imageto-image formulations.  <ref type="table">Table 4</ref>: The number of parameters for different version of the CIPS generator. For reference, the number of parameters within the StyleGANv2 generator is also given.</p><p>In this section we provide additional information about the default version of our CIPS generator <ref type="figure" target="#fig_2">(Fig. 13</ref>). In total, its backbone contains 15 fully connected layers. The first layer projects concatenated coordinate embeddings and Fourier features into the joint space with the dimension of 512. Next, the following layer pattern is repeated seven times. The representation is put through two modulated fully-connected layers and a projection to RGB color space is computed. The projections coming from the seven iterations are summed together to create the final image. The number of parameters for the different modifications of the CIPS generator discussed in the paper are given in Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Coordinate embeddings</head><p>We also run the Principle Components Analysis (PCA) for coordinate embeddings of models trained on Land- <ref type="figure" target="#fig_3">Figure 14</ref>: Visualisation of three main principal components of coordinate embeddings for CIPS models, trained on Landscapes (left) and LSUN-Churches (center). As these datasets are not as aligned as the face dataset, there is less recognizable structure in the learned coordinate embeddings. The bottom horizontal structure in the LSUN-Churches case is likely due to frequent watermark pattern in the dataset (a sample from the model with such watermark is shown on the right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Patch  scapes and LSUN-Churches images (similar pattern for the FFHQ dataset is shown in the main paper). <ref type="figure" target="#fig_3">Fig. 14 provides</ref> the visualisation for the three main components. Note, that as these datasets are as aligned as FFHQ, there is considerably less spatial structural information in the learned embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Patch-based generation</head><p>To show one benefit of coordinate-based approach, we demonstrate the results of memory-constrained training, where the discriminator observes patches at lower resolution than the full image (inspired by the GRAF system <ref type="bibr" target="#b24">[25]</ref>). Since pixel generation is conditionally-independent, at each iteration only low-resolution patches need to be generated. Thus, only the following K × K patch is synthesized and submitted to the discriminator: σ &gt; 1 dilated patch with increased receptive field is obtained. Applying this patch sampling to real images before putting them into the discriminator may be thought of as an example of a differentiable augmentation, the usefulness of which was recently proved by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>.</p><formula xml:id="formula_8">PK,σ (u, v) = {G (u + iσ, v + jσ; z) | (i, j) ∈ mgrid (K, K)},</formula><p>Tab. 5 reports the quality (FID) for CIPS generators trained on patches of sizes 64 × 64 and 128 × 128, while the resolution of full images equals 256 × 256. <ref type="figure" target="#fig_4">Fig. 15</ref> shows the outputs of models, trained with the patch-based pipeline. In our experiments, training with smaller size of patches degrades the overall quality of resulting samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional samples</head><p>In <ref type="figure" target="#fig_5">Fig. 16</ref>, we provide additional samples from CIPS generators trained on different datasets. We also demonstrate more samples of cylindrical panoramas in <ref type="figure" target="#fig_7">Fig. 17</ref>.</p><p>Although we do not apply mixing regularization <ref type="bibr" target="#b9">[10]</ref> at train time, our model is still capable of layer-wise combination of latent variables at various depth (see <ref type="figure" target="#fig_9">Fig. 19</ref>). The examples suggest that similarly to StyleGAN, different layers of CIPS control different aspects of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Nearest neighbors</head><p>To assess the generalization ability of CIPS architecture, we also show the samples from the model trained on the FFHQ face dataset alongside the most similar faces from the train dataset. To mine the most similar faces, we extract faces using the MTCNN model <ref type="bibr" target="#b32">[33]</ref>, and then compute their embeddings using FaceNet <ref type="bibr" target="#b23">[24]</ref> (the public implementation of these models 4 was used). <ref type="figure" target="#fig_8">Fig. 18</ref> shows five nearest neighbors (w.r.t. FaceNet descriptors) for each samples. The samples generated by the model are clearly not duplicates of the training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSUN-Churches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Landscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Satellite-Landscapes</head><p>Satellite-Buildings    <ref type="figure" target="#fig_9">Figure 19</ref>: Layer-wise style mixing. The two leftmost columns contain source images A and B. In the rightmost three columns, we replace the latent code w of A with the latent code w of B at layers (left to right): 6-8, 3-5, 1-2. The visualization suggests that layers 1-2 control the pose and the shape of the head, the middle layers (3-5) control finer geometry such as the shape of eyes, eyebrows and nose, and the final layers (6-8) controls the skin color and the textures. Interestingly, this CIPS model was trained without layerwise mixing, and therefore such decomposition likely arises from the architectural prior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The Conditionally-Independent Pixel Synthesis (CIPS) generator architecture. Top: the generation pipeline, in which the coordinates (x, y) of each pixel are encoded (yellow) and processed by a fully-connected (FC) network with weights, modulated with a latent vector w, shared for all pixels. The network returns the RGB value of that pixel. Bottom: The architecture of a modulated fully-connected layer (ModFC). Note: our default configuration also includes skip connections to the output (not shown here). where x = 2x W −1 − 1 and y = 2y H−1 − 1 are pixel coordinates, uniformly mapped to the range [−1, 1] and the weight matrix B f o ∈ R 2×n is learnable, like in SIREN paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(x,y) co for each spatial position and call them coordinate embeddings. They represent H × W learnable vectors in total. For comparison of these two embedding from the spectral point of view, see Sec. 4.5. The full positional encoding e (x, y) is a concatenation of Fourier features and a coordinate embedding e (x, y) = concat e f o (x, y) , e (x,y) co</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Image corresponding to the mean style vector in the space of W for CIPS (left) and CIPS-NE (no embeddings) generators (right). Left image has more plausible details like hair which confirms the results in Tab. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The spectrum magnitude for our two kinds of positional encoding (color scale is equal for both plots). The output of coordinate embeddings clearly has more higher frequencies.(a) Fourier features (b) Coordinate embeddings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>PCA plot (3 components) for two kinds of positional encoding of CIPS-base. Coordinate embeddings contain not just more fine-grained details, but also key points of the averaged face.ponent Analysis (PCA) of the two encodings supports the same conclusion(Fig. 5)The possible explanation is simple: coordinate embeddings are trained independently for each pixel, while e f o (x, y) is a learned function of the coordinates. However, the next layers of the network could</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Influence of different types of positional encoding on the resulting image. Left: original image. Center: coordinate embeddings zeroed out (the image contains no finegrained details). Right: Fourier features zeroed out (only high-frequency details are present).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Azimuthal integration over Fourier power spectrum. The curve of StyleGANv2 has heavy distortions in most high frequency components. Surprisingly, CIPS-NE demonstrates a more realistic and smooth tail than CIPS-base, while being worse in terms of FID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Spectral analysis for models trained on FFHQ at resolution of 256 2 . All results are averaged across 5000 samples. We demonstrate that CIPS-NE is most similar to real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Images generated using foveated synthesis. In each case, the CIPS generator was sampled on a 2D Gaussian distribution concentrated in the center of an image (standard deviation = 0.4 * image size). Left to right: sampled pattern covers 5% of all pixels, 25%, 50%, 100% (full coordinate grid). Missing color values have been filled via bicubic interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Left: the generated image of resolution 256×256, upscaled with Lanczos upsampling scheme<ref type="bibr" target="#b15">[16]</ref> to 1024 × 1024. Right: the image, synthesized by CIPS, trained at resolution of 256 × 256 on the coordinate grid of resolution 1024 × 1024. Note the sharpness/plausibility of the eyelid and the more proper shape of the pupil. similar vein as in the original works (e.g.<ref type="bibr" target="#b10">[11]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Latent linear morphing between two sampled images -the left-most and right-most ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>The diagram of the CIPS generator (default ver-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>FFHQFigure 15 :</head><label>15</label><figDesc>where 0 ≤ u &lt; W − (K − 1) σ and 0 ≤ v &lt; H − (K − 1) σ are the coordinates of the corner pixel of the patch. For σ = 1 this produces dense patch, while for LSUN-Churches Samples from CIPS generators learned with memory-constrained patch-based training. Within every grid, the top row contains images from models trained with patches of size 128 × 128 and the bottom row represents outputs from training on 64 × 64 patches. While the samples obtained with such memory-constrained training are meaningful, their quality and diversity are worse compared to standard training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Samples from CIPS generators trained on various datasets. The top row of every grid shows real samples, and the remaining rows contain samples from the models. The samples from CIPS generators are plausible and diverse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>Additional samples of cylindrical panoramas, generated by the CIPS model trained on the Landscapes dataset. The training data contains standard landscape photographs from the Flickr website. No panoramas are provided to the model during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :</head><label>18</label><figDesc>Nearest neighbors for generated faces. Within each row, we show a sample from the model on the left. The remaining columns contain real images that are closest to the respective sample in terms of the FaceNet<ref type="bibr" target="#b23">[24]</ref> descriptor. The visualization suggests that the CIPS model generalizes well beyond memorization of the training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>FID on multiple datasets at resolution of 256 2 for CIPS-skips model. Note that CIPS is of comparable quality with state-of-the-art StyleGANv2, and better on Churches.The value for CIPS model on FFHQ differs from the one reported in Tab. 3 as we trained this model for more time and with larger batch size.</figDesc><table><row><cell>7] as</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Precision &amp; Recall measured on FFHQ at 256 2 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Frechet Inception Distance (FID) values for CIPS models trained on patches of varying receptive field and fixed resolution (64 × 64 and 128 × 128). The results for patch-based training are worse than the default training procedure, in which the discriminator observes the full 256 × 256 image.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/timesler/facenet-pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Watch Your Up-Convolution: CNN Based Generative Deep Neural Networks Are Failing to Reproduce Spectral Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keuper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7887" to="7896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating large images from latent vectors. blog.otoro.net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating large images from latent vectors -part two. blog.otoro.net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS, NIPS&apos;17</title>
		<meeting>NIPS, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc. 4</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convnetjs demo: Image &quot;painting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training Generative Adversarial Networks with Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8107" to="8116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, and R. Garnett</editor>
		<meeting>NeurIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An iteration method for the solution of the eigenvalue problem of linear differential and integral operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lanczos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<publisher>United States Governm. Press Office Los</publisher>
			<pubPlace>Angeles, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coco-gan: Generation by parts via conditional coordinating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the CoordConv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<meeting>NeurIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9627" to="9638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Which Training Methods for GANs do actually Converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>PMLR. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>ICML<address><addrLine>Stockholmsmässan; Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<idno>ing. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm</editor>
		<meeting>ECCV<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publish</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<title level="m">Differentiable image parameterizations. Distill</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<meeting>NIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit Neural Representations with Periodic Activation Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N P</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scene representation networks: Continuous 3d-structure-aware neural scene representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS. 2019</title>
		<meeting>NeurIPS. 2019</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Compositional pattern producing networks: A novel abstraction of development. Genetic Programming and Evolvable Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="131" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Differentiable Augmentation for Data-Efficient GAN Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
