<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Differentiable 6DoF Object Pose Estimation with Local and Global Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mercedes-Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Medhi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mercedes-Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aratrik</forename><surname>Chattopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mercedes-Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mercedes-Benz Research and Development</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Differentiable 6DoF Object Pose Estimation with Local and Global Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inferring the 6DoF pose of an object from a single RGB image is an important but challenging task, especially under heavy occlusion. While recent approaches improve upon the two stage approaches by training an end-to-end pipeline, they do not leverage local and global constraints. In this paper, we propose pairwise feature extraction to integrate local constraints, and triplet regularization to integrate global constraints for improved 6DoF object pose estimation. Coupled with better augmentation, our approach achieves state of the art results on the challenging Occlusion Linemod dataset, with a 9% improvement over the previous state of the art, and achieves competitive results on the Linemod dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating the 6DoF pose of an object is an important problem with applications in various domains like robotics <ref type="bibr" target="#b0">[1]</ref>, augmented reality <ref type="bibr" target="#b1">[2]</ref> and autonomous driving <ref type="bibr" target="#b2">[3]</ref>. With the pervasion of inexpensive RGB sensors, it is cost effective and highly beneficial to perform 6DoF pose estimation from a single RGB image without using additional depth sensors.</p><p>Some studies <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b4">[5]</ref> attempted to regress the 6DoF pose directly from the image, however, these were not as competitive as recent two stage approaches. In the first stage of two stage approaches, a correspondence estimator detects the object and estimates the 2D image projections of the 3D object points (referred to as 2D keypoints). This establishes correspondences between the 2D and 3D points. <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b6">[7]</ref>[8] used a CNN based architecture to segment out regions containing the object and regress the 2D keypoints from those regions. A recent study regressed direction vectors to the 2D keypoints from the segmented regions of the object <ref type="bibr" target="#b8">[9]</ref>. The 2D keypoints were then estimated from intersections of pairs of direction vectors. This approach was found to be more robust to occlusions of the object.</p><p>In the second stage, a RANSAC based Perspective-n-Point(PnP) algorithm serves as a pose estimator to predict the 6DoF object pose using the established 2D-3D correspondences. However, Hu et al. <ref type="bibr" target="#b9">[10]</ref> showed that RANSAC is sensitive to the ordering of the 2D-3D correspondences and computationally costly when there are many of them. Further, the non-differentiable nature of the RANSAC based pose estimator does not allow for end-to-end training of the two stage approaches with respect to the final objective, namely the object pose. Hence, Hu et al. <ref type="bibr" target="#b9">[10]</ref> proposed to replace the non-differentiable RANSAC based pose estimator with a trainable neural network to estimate the 6DoF object pose. Their end-to-end trainable model showed improved results compared to the two stage approach as validated with two state of the art correspondence estimators [8] <ref type="bibr" target="#b8">[9]</ref>. We follow up on their model with <ref type="bibr" target="#b8">[9]</ref> as the correspondence estimator as it shows superior performance and refer to it as SSPE.</p><p>While SSPE shows improved performance using end-to-end training, it does not utilize local and global geometric constraints. In this work, we propose pairwise features to utilize local information between direction vectors associated with the same 3D point, and triplet regularization to account for the global geometry between pairwise features associated with different 3D points. Coupled with increased masking augmentation, our model achieves state of the art results on the Occlusion Linemod <ref type="bibr" target="#b10">[11]</ref> dataset and competitive results on the Linemod <ref type="bibr" target="#b11">[12]</ref> dataset. In summary, our main contributions are:</p><p>• Pairwise feature extraction from direction vectors to better utilize local information • Triplet regularization to account for the global geometry of the pairwise features • State of the art results on Occlusion Linemod and competitive results on Linemod</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>We illustrate our approach in <ref type="figure" target="#fig_0">Figure 1</ref>. The correspondence estimator operates on an image and predicts a segmentation mask. It also predicts direction vectors to the 2D keypoints for each pixel in the mask. For each of the n 3D points p i , the pose estimator selects m random direction vectors u ik (1 ≤ i ≤ n, 1 ≤ k ≤ m) from the segmented region of the object. It applies a shared MLP Φ s to extract pairwise features, followed by aggregation using an aggregator Λ, and pose prediction from a second MLP Φ g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Local Constraint</head><p>A direction vector u ik is represented as a 4D input [x, y, dx, dy] where x, y is the pixel location, and dx, dy is the predicted vector from that pixel. In the first step for the SSPE pose estimator, a shared MLP is applied across all direction vectors to extract n × m local features. However, by operating on every direction vector independently the local features do not have information about the 2D keypoints. This is because a 2D keypoint is given by the intersection of a pair of direction vectors pointing to that keypoint <ref type="bibr" target="#b8">[9]</ref>. Hence, we propose to concatenate pairs of direction vectors [u ik , u il ] and provide them as input to the shared MLP Φ s . This gives us n × m</p><formula xml:id="formula_0">2 D dimensional features f ih (1 ≤ h ≤ m 2 ) termed as pairwise features. f ih = Φ s ([u ik , u il ]) 1 ≤ i ≤ n, 1 ≤ h ≤ m 2 , k = 2h − 1, l = 2h<label>(1)</label></formula><p>While Φ s can theoretically learn to approximate the intersection of direction vectors to give pairwise features with information about the 2D keypoints, we observe that adding global constraints can help learn better features for improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global Constraint</head><p>We account for the global geometry of the pairwise features by considering their association to the 3D points. We want pairwise features associated with the same 3D point to be similar to each other, and pairwise features associated with different 3D points to be dissimilar to each other. To encourage this property we introduce a triplet regularization term. This also serves as a form of proxy supervision to the shared MLP Φ s as different pairs of direction vectors associated with the same 3D point give similar pairwise features. We mine triplets online and compute the triplet regularization term as:</p><formula xml:id="formula_1">L t = 2 nm n i=1 m 2 h=1 max(S ih,jd − S ih,is + α, 0) 1 ≤ j ≤ n, i = j, 1 ≤ d, s ≤ m 2 (2)</formula><p>where α is the margin and S wx,yz is the similarity between pairwise features f wx and f yz . We use the cosine similarity function given as:</p><formula xml:id="formula_2">S wx,yz = f T wx f yz ||f wx || ||f yz || 1 ≤ w, y ≤ n, 1 ≤ x, z ≤ m 2<label>(3)</label></formula><p>Similar to SSPE, we aggregate the pairwise features and apply a second MLP to compute the pose. The pairwise features associated with each 3D point are aggregated using an aggregator Λ to give n D dimensional group features g i . We choose Λ as the mean pooling aggregator.</p><formula xml:id="formula_3">g i = Λ({f i1 , f i2 ...f i m 2 }) 1 ≤ i ≤ n<label>(4)</label></formula><p>The group features are concatenated, and the nD dimensional vector is passed through a second MLP Φ g to predict the pose as a quaternionq and translationt.</p><p>[q,t] = Φ g ([g 1 , g 2 ...g n ])</p><p>We recover the predicted rotation matrixR fromq and compute the pose loss L p as the 3D error:</p><formula xml:id="formula_5">L p = 1 n n i=1 ||(Rp i +t) − (Rp i + t)||<label>(6)</label></formula><p>where R and t are the ground truth rotation and translation.</p><p>The final loss L to optimize is a linear combination of the cross entropy segmentation loss L s and L1 vector regression loss L k from the correspondence estimator <ref type="bibr" target="#b8">[9]</ref>, and the pose loss L p and triplet regularization term L t from the pose estimator.</p><formula xml:id="formula_6">L = λ s L s + λ k L k + λ p L p + λ t L t<label>(7)</label></formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training</head><p>We use n = 9 3D key points for each object selected using the farthest point sampling algorithm. For the pose estimator, we randomly select m = 200 direction vectors for each of the 3D points. The triplet margin α is set to 0.1. The loss coefficients λ s and λ k are set to 1, λ p is set to 0.01 and λ t is set to 0.1. As per previous studies <ref type="bibr" target="#b8">[9]</ref>[10], we train separate models for each object. Training images are provided at an input resolution of 640 × 480 and augmented using scaling, translation, rotation, occlusion <ref type="bibr" target="#b15">[16]</ref>, gaussian blurring and colour jittering. We use the Adam optimizer and set the learning rate to 1e − 3 which is divided by 10 after processing 50%, 75%, and 90% of the data. All models are trained with a batch size of 32 for 300 epochs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>We benchmark our approach on the Linemod <ref type="bibr" target="#b11">[12]</ref> and Occlusion Linemod <ref type="bibr" target="#b10">[11]</ref> datasets for 8 object classes. Similar to previous approaches <ref type="bibr" target="#b8">[9]</ref>[10], we augment the Linemod train data using synthetic data. We generate 10000 images containing multiple objects using the cut and paste <ref type="bibr" target="#b16">[17]</ref> technique, and 8 × 10000 images of single objects using the rendering technique in <ref type="bibr" target="#b8">[9]</ref>.</p><p>For evaluation, we use the ADD0.1d metric <ref type="bibr" target="#b11">[12]</ref> to measure accuracy in 3D space. The ADD0.1d metric measures the average distance between the 3D model points transformed using the predicted pose and the ground truth pose. A predicted pose is assumed correct if the average distance is less than 10% of the model diameter. We report the percentage of correctly predicted poses. We use the symmetric version of the metric <ref type="bibr" target="#b4">[5]</ref> for symmetric objects, which are denoted by the * superscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We report results on the Occlusion Linemod dataset in Part I of <ref type="table" target="#tab_0">Table 1</ref>. SSPE-ours achieves state of the art results with a 9% improvement over the previous best method <ref type="bibr" target="#b12">[13]</ref>. It has the highest scores for 5 of the 8 objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our network architecture (SSPE-ours). The correspondence estimator predicts direction vectors to the 2D keypoints. Pairs of direction vectors are passed through a shared network Φ s to give pairwise features which are aggregated using an aggregator Λ, and passed through a second network Φ g to predict the pose. The color of the pairwise features indicates association to a 3D point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>t-SNE plot of the SSPE-r local features (a) and the SSPE-ours pairwise features (b) for the holepuncher object. Each colour represents the features of the 9 3D points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on Occlusion Linemod (Part I) and Linemod (Part II) using the ADD0.1d metric 1 . DPVR [13] SSPE [10] SSPE-r 2 SSPE-ours PVNet [9] DPVR [13] SSPE-r SSPE-ours We do not compare against models that perform refinement on predicted pose [14][15]. We reimplement SSPE as authors have not open sourced the training code</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Part I: Occlusion Linemod</cell><cell></cell><cell></cell><cell cols="2">Part II: Linemod</cell><cell></cell></row><row><cell cols="2">PVNet [9] Ape 15.8</cell><cell>19.2</cell><cell>19.2</cell><cell>20.8</cell><cell>18.8</cell><cell>43.6</cell><cell>69.1</cell><cell>66.7</cell><cell>52.5</cell></row><row><cell>Can</cell><cell>63.3</cell><cell>69.8</cell><cell>65.1</cell><cell>78.4</cell><cell>79.3</cell><cell>95.5</cell><cell>98.5</cell><cell>95.8</cell><cell>99.2</cell></row><row><cell>Cat</cell><cell>16.7</cell><cell>21.1</cell><cell>18.9</cell><cell>18.2</cell><cell>17.5</cell><cell>79.3</cell><cell>83.1</cell><cell>84.1</cell><cell>88.5</cell></row><row><cell>Driller</cell><cell>65.7</cell><cell>71.6</cell><cell>69.0</cell><cell>73.8</cell><cell>76.4</cell><cell>96.4</cell><cell>99.0</cell><cell>98.4</cell><cell>98.8</cell></row><row><cell>Duck</cell><cell>25.2</cell><cell>34.3</cell><cell>25.3</cell><cell>33.1</cell><cell>34.4</cell><cell>52.6</cell><cell>63.5</cell><cell>60.4</cell><cell>68.7</cell></row><row><cell>Eggbox*</cell><cell>50.2</cell><cell>47.3</cell><cell>52.0</cell><cell>46.0</cell><cell>44.6</cell><cell>99.2</cell><cell>100.0</cell><cell>99.7</cell><cell>100.0</cell></row><row><cell>Glue*</cell><cell>49.6</cell><cell>39.7</cell><cell>51.4</cell><cell>49.2</cell><cell>53.2</cell><cell>95.7</cell><cell>98.0</cell><cell>90.4</cell><cell>98.5</cell></row><row><cell>Holepuncher</cell><cell>39.7</cell><cell>45.3</cell><cell>45.6</cell><cell>53.5</cell><cell>54.7</cell><cell>81.9</cell><cell>88.2</cell><cell>85.3</cell><cell>88.1</cell></row><row><cell>Average</cell><cell>40.8</cell><cell>43.5</cell><cell>43.3</cell><cell>46.6</cell><cell>47.4</cell><cell>80.5</cell><cell>87.4</cell><cell>85.1</cell><cell>86.8</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on 4 non-symmetric and 1 symmetric object. Adding local constraints (SSPE-lc) improves performance over SSPE-r and SSPE-rp. Adding global constraints (SSPE-ours) further improves performance. Results reported on Occlusion Linemod using the ADD0.1d metric.</figDesc><table><row><cell></cell><cell cols="5">SSPE-m SSPE-r SSPE-rp SSPE-lc SSPE-ours</cell></row><row><cell>Can</cell><cell>71.9</cell><cell>78.4</cell><cell>79.5</cell><cell>77.6</cell><cell>79.3</cell></row><row><cell>Driller</cell><cell>62.4</cell><cell>73.8</cell><cell>75.5</cell><cell>76.1</cell><cell>76.4</cell></row><row><cell>Duck</cell><cell>28.8</cell><cell>33.1</cell><cell>32.3</cell><cell>32.3</cell><cell>34.4</cell></row><row><cell>Glue*</cell><cell>51.3</cell><cell>49.2</cell><cell>53.6</cell><cell>55.9</cell><cell>53.2</cell></row><row><cell>Holepuncher</cell><cell>44.7</cell><cell>53.5</cell><cell>51.1</cell><cell>52.7</cell><cell>54.7</cell></row><row><cell>Average</cell><cell>51.8</cell><cell>57.6</cell><cell>58.4</cell><cell>58.9</cell><cell>59.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We perform ablation in <ref type="table">Table 2</ref> to demonstrate the strength of our approach. Average performance of SSPE with pairwise features (SSPE-lc) is better compared to standard SSPE with the aggregator as max pooling (SSPE-r) and SSPE with the aggregator as mean pooling (SSPE-rp). Adding triplet regularization (SSPE-ours) further improves performance. To support our hypothesis we do a t-SNE visualisation of the SSPE local features and our pairwise features as shown in <ref type="figure">Figure 2</ref>. We note much better clustering for our pairwise features. This suggests our approach successfully accounts for the local and global constraints to improve end-to-end pose estimation.</p><p>We also observe that increased masking augmentation <ref type="bibr" target="#b15">[16]</ref> can help increase performance. We highlight its importance in <ref type="table">Table 2</ref> by initially setting the masking percentage to 10% − 30% (SSPEm), and then tripling it to 30% − 90% (SSPE-r). We note an average increase of 5.8 points in ADD0.1d score. Hence, we use the increased masking in all our experiments.</p><p>We additionally show results on the Linemod dataset in Part II of <ref type="table">Table 1</ref>. SSPE-ours achieves competitive results and has the highest scores for 5 of the 8 objects. It also shows improvement over SSPE-r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We show that our approach (SSPE-ours) achieves state of the art results on the challenging Occlusion Linemod dataset. We also perform ablation to demonstrate the strength of our approach. This suggests the effectiveness of local and global constraints to improve end-to-end 6DoF object pose estimation. In the future, we hope to explore geometric properties to further improve end-to-end 6DoF object pose estimation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis and observations from the first amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Causo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Wurman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="188" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose estimation for augmented reality: A handson survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Spindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2633" to="2651" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5452" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3385" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single-stage 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">6dof object pose estimation via differentiable proxy voting loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cut, paste and learn: Surprisingly easy synthesis for instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1301" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
