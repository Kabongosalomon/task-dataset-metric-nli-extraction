<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entropy Minimization vs. Diversity Maximization for Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunming</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longin</forename><forename type="middle">Jan</forename><surname>Latecki</surname></persName>
						</author>
						<title level="a" type="main">Entropy Minimization vs. Diversity Maximization for Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Domain adaptation</term>
					<term>image classification</term>
					<term>entropy minimization</term>
					<term>transfer learning</term>
					<term>VisDA challenge</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entropy minimization has been widely used in unsupervised domain adaptation (UDA). However, existing works reveal that entropy minimization only may result into collapsed trivial solutions. In this paper, we propose to avoid trivial solutions by further introducing diversity maximization. In order to achieve the possible minimum target risk for UDA, we show that diversity maximization should be elaborately balanced with entropy minimization, the degree of which can be finely controlled with the use of deep embedded validation in an unsupervised manner. The proposed minimal-entropy diversity maximization (MEDM) can be directly implemented by stochastic gradient descent without use of adversarial learning. Empirical evidence demonstrates that MEDM outperforms the state-of-theart methods on four popular domain adaptation datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The recent success of deep learning depends heavily on the large-scale fully-labeled datasets and the development of easily trainable deep neural architectures under the back-propagation algorithm, such as convolutional neural networks (CNNs) and their variants <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. In practical applications, a new target task and its dataset (target domain) may be similar to a known source task and its fully-labeled dataset (source domain). However, the difference between the source and target domains is often not negligible, which makes the previously-trained model not work well for the new task. This is known as domain shift <ref type="bibr" target="#b2">[3]</ref>. As the cost of massive labelling is often expensive, it is very attractive for the target task to exploit any existing fully-labeled source dataset and adapt the trained model to the target domain <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b12">[13]</ref>.</p><p>This domain adaptation approach is aiming to learn a discriminative classifier in the presence of domain shift <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. It can be achieved by optimizing the feature representation to minimize some measures of domain shift, typically defined as † Corresponding author. This work was supported in part by the National Natural Science Foundation of China under Grants 61372123, <ref type="bibr">61671253</ref>  the distances between the source and target domain distributions or its degraded form, such as Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> or correlation distance <ref type="bibr" target="#b17">[18]</ref>.</p><p>With the invention of generative adversarial networks <ref type="bibr" target="#b18">[19]</ref>, various adversarial methods have been proposed for the purpose of unsupervised domain adaptation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, where the domain discrepancy distance is believed to be minimized through an adversarial objective with respect to a binary domain discriminator. The domain-invariant features could be extracted whenever this binary domain discriminator cannot distinguish between the distributions of the source and target samples <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>In recent years, there is also a broad class of domain adaptation methods, which employ entropy minimization as a proxy for mitigating the the harmful effects of domain shift. The entropy minimization is performed on the target domain, which may take explicit forms <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b26">[27]</ref> or implicit forms <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Without any further regularization, it may produce trivial solutions <ref type="bibr" target="#b28">[29]</ref>.</p><p>Often, unsupervised domain adaptation (UDA) faces the challenging problem of hyperparameter selection, where the best configuration should be determined without resort to labels in the target dataset. Fortunately, deep embedded validation (DEV) <ref type="bibr" target="#b29">[30]</ref> tailored to UDA was recently proposed to solve this difficulty, which embeds adapted feature representation in the validation procedure to yield unbiased estimation of the target risk.</p><p>In this paper, we make contributions towards close-to-perfect domain adaptation with entropy minimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>We propose a minimal-entropy diversity maximization (MEDM) method for UDA. Instead of simply avoiding trivial solutions for entropy minimization, MEDM tries to find a close-to-perfect domain adaptation solution, which achieves the best possible tradeoff between entropy minimization and diversity maximization with the help of DEV <ref type="bibr" target="#b29">[30]</ref>. 2) MEDM outperforms state-of-the-art methods on four domain adaptation datasets, including VisDA-2017, Im-ageCLEF, Office-Home and Office-31. In particular, it boosts a significant accuracy margin on the largest domain adaptation dataset, VisDA-2017 classification challenge 1 . or</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Target Domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trivial Solution</head><p>Non-Trivial Solution?</p><p>: A batch of source samples : A batch of target samples </p><formula xml:id="formula_0">1 2 1ˆˆ( ) [ , , , ] t t K x f x q q q      q     1ˆ( , ) ( ) log K d k k k H q q      q     min ( , ) ( , ) ( , ) s e d E                  ( ) f  </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Entropy-Minimization-Only</head><p>Consider the problem of classifying an image x in a Kclasses problem. For UDA, we are given a source domain D s = {(x s i ; y s i )} ns i=1 of n s labeled examples and a target domain D t = {x t j } nt j=1 of n t unlabeled examples. The source domain and target domain are sampled from joint distributions P (x s ; y s ) and Q(x t ; y t ) respectively, while the identically independently distributed (IID) assumption is often violated as P = Q. Hence, the problem is to exploit a bunch of labeled images in D s for training a statistical classifier that, during inference, provides probabilities for a given test image x t ∈ D t to belong to each of the K classes. In this paper, we focus on a deep neuralnetwork based classifier y = f θ (x) (In general, the classifier network f θ depends upon a collection of parameters θ), which provides probabilities of x belonging to each class as f θ (x) = [P(y = 1|x), · · · , P(y = K|x)] .</p><p>(1)</p><p>The goal is to design the classifier y = f θ (x) such that the target risk t (f θ ) = E (x t ;y t )∼Q [f θ (x t ) = y t ] can be minimized. Since the target risk cannot be computed in the scenario of UDA, the domain adaptation theory <ref type="bibr" target="#b30">[31]</ref>  <ref type="bibr" target="#b31">[32]</ref> suggests to bound the target risk with the sum of the cross-domain discrepancy D(P ; Q) and the source risk s (f θ ) = E (x s ;y s )∼P [f θ (x s ) = y s ]. By jointly minimizing the source risk and the cross-domain discrepancy D(P ; Q), various domain adaptation methods were extensively proposed, which differ mainly in the choice of D(P ; Q).</p><p>For supervised learning on the source domain, the classifier is trained to minimize the standard supervised loss</p><formula xml:id="formula_1">L s (θ, D s ) = 1 |D s | (x,y)∈Ds (y, f θ (x))<label>(2)</label></formula><p>with (y,ŷ) = y,ŷ = − K j=1 y j logŷ j and |D s | denotes the cardinality of the set D s .</p><p>To adapt to the unlabeled target domain, a large class of domain adaptation methods also minimize the entropy loss on the target domain</p><formula xml:id="formula_2">L e (θ, D t ) = − 1 |D t | xt∈Dt f θ (x t ), log f θ (x t )<label>(3)</label></formula><p>as an efficient regularization technique. Therefore, the standard domain adaptation method with entropy-minimization-only (EMO) seeks to solve the following problem</p><formula xml:id="formula_3">min θ [L s (θ, D s ) + λL e (θ, D t )] , λ &gt; 0.<label>(4)</label></formula><p>The EMO presented in (4) was first proposed in <ref type="bibr" target="#b32">[33]</ref> for semi-supervised learning, where a decision rule is to be learned from labeled and unlabeled data, and EMO (4) enables to incorporate unlabeled data in the standard supervised learning. For the scenario of UDA considered in this paper, the difference is that unlabeled samples are sampled from the target-domain distribution Q, which may differ considerably from the sourcedomain distribution P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Insufficiency of EMO for UDA</head><p>Note that the minimization of the target risk t (f θ ) could push the network prediction f θ (x t ) towards the true solution y t = [y 1 , · · · , y K ] with y k ∈ {0, 1}, k y k = 1, namely, f θ (x t ) → [0, · · · , 1, · · · , 0], which results into the minimum value of entropy (zero). This means that entropy minimization is a necessary condition for the minimization of the target risk t (f θ ). Hence, entropy minimization may be more direct and simpler for end-to-end training of θ in order to minimize the target risk, compared to the use of more complicated crossdomain discrepancy. Unfortunately, as a necessary but not sufficient condition for minimization of the target risk t (f θ ) <ref type="bibr" target="#b28">[29]</ref>, this simple technique may result into trivial solutions, as demonstrated in the Appendix. Problem 1. As entropy minimization is necessary but not sufficient for minimization of the target risk, it is natural to ask if we can pose some further regularization to push the optimizer to find the global minima instead of the trivial local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Existing UDA Approaches with EM</head><p>Entropy minimization was first proposed in <ref type="bibr" target="#b32">[33]</ref> for semisupervised learning. In many UDA scenarios with very limited domain-shift between source and target domains, it does work, as demonstrated later in experiments. When the effect of domain-shift increases, the optimization of (4) is not enough for the purpose of UDA <ref type="bibr" target="#b28">[29]</ref>. Hence, various ancillary adaptation techniques were invoked, such as covariance alignment <ref type="bibr" target="#b28">[29]</ref>, batch normalization <ref type="bibr" target="#b24">[25]</ref> or learning by association <ref type="bibr" target="#b22">[23]</ref>.</p><p>It was argued in <ref type="bibr" target="#b28">[29]</ref> that entropy minimization could be achieved by the optimal alignment of second order statistics between source and target domains and therefore a hyperparameter validation method was proposed for balancing the reduction of the domain shift and the supervised classification on the source domain in an optimal way.</p><p>In <ref type="bibr" target="#b24">[25]</ref>, a novel domain alignment layer was introduced for reducing the domain shift by matching source and target distributions to a reference one and entropy minimization was also explicitly employed, which was believed to promote classification models with high confidence on unlabeled samples.</p><p>Long et al. <ref type="bibr" target="#b33">[34]</ref> used entropy minimization in their approach to directly measure how far samples are from a decision boundary by calculating entropy of the classifiers output.</p><p>In the appendix of <ref type="bibr" target="#b34">[35]</ref>, Satio et al. proposed an entropybased adversarial dropout regularization approach, which employed the entropy of the target samples in implementing min-max adversarial training.</p><p>In <ref type="bibr" target="#b35">[36]</ref>, entropy conditioning was employed that controls the uncertainty of classifier predictions to guarantee the transferability, which can help the proposed conditional adversarial domain adaptation (CDAN) to converge to better solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MINIMAL-ENTROPY DIVERSITY MAXIMIZATION A. Proposed Method</head><p>As the training of network is often implemented over batches of samples, the supervised loss for a given source batch S (for example, |S| = 32 for the batch size of 32) is accordingly modified as</p><formula xml:id="formula_4">L s (θ, S) = 1 |S| (x,y)∈S (y, f θ (x)).<label>(5)</label></formula><p>As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, domain adaptation requires some regularization techniques for pushing the network towards correct class prediction of unlabeled target samples. For ease of implementation, the regularization is often performed over batches of target samples. Let T ⊂ D t be any random batch of samples from target domain, which has the same batch size as that of S, namely, |T | = |S|.</p><p>With each unlabeled image x t ∈ T as input, we can perform inference over the network f θ to obtain the softmax predictions f θ (x t ). Then, we can compute the predicted category distribution in T aŝ</p><formula xml:id="formula_5">q(T ) = 1 |T | xt∈T f θ (x t ) [q 1 ,q 2 , · · · ,q K ],<label>(6)</label></formula><formula xml:id="formula_6">whereq k = 1 |T | xt∈T P(y t = k|x t ),</formula><p>and K k=1q k = 1. Note thatq(T ) is computed over T , which is dynamically changed during the batch-based training.</p><p>Without any labelling information available, the mean entropy loss over T can be computed as</p><formula xml:id="formula_7">L e (θ, T ) = − 1 |T | xt∈T f θ (x t ), log f θ (x t ) .<label>(7)</label></formula><p>The use of EMO may produce trivial solutions as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. By noting that a trivial solution shown in <ref type="figure" target="#fig_1">Figure  1</ref> often has just one category, a nontrivial domain adaptation method may resort to producing sufficient category diversity in its solution.</p><p>In this paper, we employ the entropy ofq(T ) = [q 1 ,q 2 , · · · ,q K ] (6) for measuring the category diversity in a given target batch T . Formally, this category diversity over T can be measured as</p><formula xml:id="formula_8">L d (θ, T ) H (q(T )) = − K k=1q k logq k .<label>(8)</label></formula><p>As this diversity metric does not require any priori information about the true category distribution q over D t , its computation is easy to implement in practice. Note that random shuffling should be employed in training for maximizing <ref type="bibr" target="#b7">(8)</ref>.</p><p>The objective of the proposed MEDM is to</p><formula xml:id="formula_9">min θ E S,T [L s (θ, S) + λL e (θ, T ) − βL d (θ, T )] (9)</formula><p>where E[·] denotes the expectation and λ, β ≥ 0 are weighting factors. Given λ, β, this involves the optimization of θ for the minimization of a single total loss <ref type="bibr" target="#b8">(9)</ref>, which can be directly implemented by stochastic gradient descent without use of adversarial learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Entropy-Minimization vs. Diversity-Maximization</head><p>As shown in <ref type="formula">(9)</ref>, our proposed MEDM may encourage to make prediction evenly across the batch, which, however, does not necessarily produce the evenly-distributed categories. Let q = [q 1 , · · · , q K ] be the true category distribution of the target dataset, where q k denotes the proportion of samples of the k-th class among all target samples. Theorem 1. Consider the EMO method (β = 0) in <ref type="bibr" target="#b8">(9)</ref>. If there exists a solution θ * of (9) with optimal entropy minimization, we have that</p><formula xml:id="formula_10">E T [L d (θ * , T )] = H(q * ), where q * = [q * 1 , · · · , q * K ]</formula><p>is the inferred category distribution of the target dataset when inferring over the network θ * .</p><p>Proof. In the case of β = 0, diversity maximization is not included in <ref type="bibr" target="#b8">(9)</ref>. With optimal entropy minimization, it means that</p><formula xml:id="formula_11">L e (θ * , T ) = 0.</formula><p>Since the entropy is always non-negative, we have that</p><formula xml:id="formula_12">− f θ (x t ), log f θ (x t ) = 0, ∀x t ∈ T . Hence, the network prediction f θ (x t ) for any x t ∈ T should present a peaky form, namely, f θ (x t ) → [0, · · · , 1, · · · , 0].</formula><p>Given a random batch of samples T inputting to the network θ * , we have that</p><formula xml:id="formula_13">E T {q k } = 1 |T | E T xt∈T f k θ * (x t ) = 1 |T | · (q * k |T |) = q * k .</formula><p>Therefore,</p><formula xml:id="formula_14">E T {L d (θ * , T )} = E T {H(q)} = H(q * ).</formula><p>Without the use of diversity maximization, EMO often results into trivial solutions, namely, max q * k 1 − max k q * k , where the predicted single-class samples may dominate among others. With the use of diversity maximization, it may encourage to make prediction evenly across the batch, since the maximum value of L d (θ * , T ) could be achieved whenever q * = [1/K, · · · , 1/K]. Actually, there exists a tradeoff by adjusting the parameters λ, β as justified in what follows.</p><p>Assume that the network f θ (x) could be decomposed into two subnetworks, namely, f θ (x) = C(F (x)) , where F denotes a feature extraction subnetwork and C denotes a classifier over the feature space F. For domain-adaptation, it is often assumed that the conditional distributions are unchanged by F , i.e., P (y|F (x)) = Q(y|F (x)).</p><p>Let</p><formula xml:id="formula_15">X s = {x s } ns s=1 and X t = {x t } nt t=1 . Then, whenever x t ∈ F −1 (F (X s ) ∩ F (X t ))</formula><p>, one can expect that the network can give a correct prediction with minimal entropy. While x t ∈ F −1 (F (X t ) − F (X s )), it may encourage to make prediction towards a single class with entropy minimization, since there are simply no other constraints to be enforced. Hence, we have the following conjecture. Conjecture 1. Consider the EMO method (β = 0) in <ref type="bibr" target="#b8">(9)</ref>. If there exists a solution θ * of (9) with optimal entropy minimization, we have that</p><formula xml:id="formula_16">E T [L d (θ * , T )] ≤ H(q), Algorithm 1 Fast Model Selection Process in MEDM Require: D s = D train ∪ D val , D t ; λ L 1 = {λ l } L i=1 , β B 1 = {β l } B<label>l=1</label></formula><p>1: Training Initialization: β ← 0.</p><p>2: for λ ← λ 1 , · · · , λ L do 3:</p><p>Training the network θ i over D train and D t :</p><formula xml:id="formula_17">θ i = arg min θ E [L s (θ, S) + λL e (θ, T )] 4:</formula><p>If L e (θ i , T ) → 0: λ * = λ and break 5: end for</p><formula xml:id="formula_18">6: for β ← β 1 , · · · , β B do 7:</formula><p>Training the network θ l over D train and D t : where q = [q 1 , · · · , q K ] is the ground-truth category distribution of the target dataset.</p><formula xml:id="formula_19">θ l = arg min θ E [L s (θ, S) + λ * L e (θ, T ) − βL d (θ, T )]</formula><p>Let f θ * be the perfect domain-adaptation classier, which minimizes combined source and target risk <ref type="bibr" target="#b31">[32]</ref>:</p><formula xml:id="formula_20">f θ * = arg min f θ ∈H s (f θ ) + t (f θ ).<label>(10)</label></formula><p>where H is the space of classifiers. Therefore, when inferring target samples over f θ * , one can expect reliable predictions or predictions with low entropies. Hence, Theorem 1 may still hold in this case. By restricting H to be the solution space of (9) with λ, β ≥ 0, we expect that the same conclusion holds. In fact, extensive experiments show that an explicit inclusion of entropy minimization in <ref type="bibr" target="#b8">(9)</ref> can easily drive the trained CNN model towards small predictive entropy, even coexisting with diversity maximization. Therefore, we believe that the perfect domain-adaptation classifier under the framework of (9) may output predictions with low entropies, which means that Theorem 1 may still hold, as shown in what follows.</p><formula xml:id="formula_21">Conjecture 2. Consider the perfect domain-adaptation solu- tion of f (λ * ,β * ) θ *</formula><p>under the framework of (9), namely,</p><formula xml:id="formula_22">f (λ * ,β * ) θ * = arg min θ,λ,β s (f (λ,β) θ ) + t (f (λ,β) θ ),<label>(11)</label></formula><p>we have that</p><formula xml:id="formula_23">E T [L d (θ * , T )] ≈ H(q * ),</formula><p>where q * = [q * 1 , · · · , q * K ] is the inferred category distribution of the target dataset over the network θ * .</p><p>Since the target risk for f (λ * ,β * ) θ * is expected to be small, we have that q * → q and H(q * ) → H(q). With random shuffling for batch-based training, L d (θ * , T ) → H(q * ) holds with a high probability whenever the training process for (9) under </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Synthetic → Real GTA <ref type="bibr" target="#b36">[37]</ref> 69.5 MCD <ref type="bibr" target="#b21">[22]</ref> 69.8 CDAN <ref type="bibr" target="#b35">[36]</ref> 70.0 MDD <ref type="bibr" target="#b37">[38]</ref> 74.6 MEDM(ours) 79.6</p><p>the setting of (λ * , β * ) converges. This may partially support the reasonability of the use of (9). Although we do not know the perfect domain-adaptation solution of f (λ * ,β * ) θ * , one can search over the space of (λ, β) and further resort to the validation technique <ref type="bibr" target="#b29">[30]</ref>. The essential process can be stated as follows. When β = 0 in (9), it often results in the trivial solution of one domain single class, which means that L d (θ * , T ) ≈ 0. When β increases from 0, we would expect that the category diversity (8) increases correspondingly, which can help to avoid the trivial solutions. The problem now is how to determine the best value of λ, β in (9) for finding a close-to-perfect domain adaptation solution. Fortunately, this was recently investigated in <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Selection via Deep Embedded Validation</head><p>For the proposed MEDM, the selection of hyperparameters (λ, β) is of great importance for the final performance. For UDA, the model selection should be decided without access to the labels in the target dataset. Fortunately, the recentlyproposed deep embedded validation <ref type="bibr" target="#b29">[30]</ref> has been proved very efficient for model selection, which embeds adapted feature representation into the validation procedure to obtain unbiased estimation of the target risk with bounded variance.</p><p>Consider that the feature extractor F is an end-to-end training solution of (9), it is closely connected to the parameters λ, β in (9), i.e., F F λ,β . Let λ L 1 = {λ l } L i=1 be a finite collection of λ l , where λ 1 &lt; λ 2 &lt; · · · &lt; λ L . Assume that θ * is the optimal hyper-parameter which attains the optimum of (9) with the smallest possible value of λ * among λ L 1 , which implies E S,T [L s (θ * , S) + λ * L e (θ * , T )] = min(λ * ), min(λ * ) &gt; min(λ), ∀λ ∈ λ L 1 , λ &gt; λ * . This is because that L e (θ * , T ) ≈ 0 could be more easily achieved whenever λ ≥ λ * . With the smallest possible value of λ, the degree of entropy minimization can be controlled during training with better discriminability while maintaining the transferability.</p><p>Therefore, we propose a fast model selection process for MEDM as shown in Algorithm 1, which can reduce the search space for two hyperparameters λ, β. Practically, we use L e (θ i , T ) ≤ for deciding if L e (θ i , T ) → 0 and = 0.2 is employed in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate MEDM with state-of-the-art domain adaptation methods for various transferring tasks, which include VisDA-2017, ImageCLEF-DA, Office-Home, and Office-31 datasets.</p><p>VisDA-2017 is known as the largest and highly unbalanced DA dataset and ImageCLEF-DA is a small but balanced dataset, while both Office-Home and Office-31 are slightly unbalanced DA datasets with a large number of classes (65 for Office-Home and 31 for Office-31).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setting</head><p>Throughout the experiments, we employ deep neural network architecture detailed as follows. It has a pre-trained ResNet-50/101, followed by two fully-connected layers, FC-1 of size 2048 × 1024 and FC-2 of size 1024 × K. Batch-normalization, ReLU activation and dropout are only employed at the FC-1 layer. The dropout rate is set to 0.5. The last label prediction layer of the network is omitted and features are extracted from the second to last layer. The Adam optimizer is employed with a learning rate of 0.0001. The batch size is set to 32. The learning rates of the layers trained from scratch are set to be 100 times those of fine-tuned layers. For model selection, we assume that λ, β ∈ {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9.1.0}. Finer search of λ, β may lead to the possibly-better performance.</p><p>We report the test accuracy results of MEDM, which are compared with state-of-the-art methods: Deep Adaptation Network (DAN) <ref type="bibr" target="#b15">[16]</ref>, Reverse Gradient (RevGrad) <ref type="bibr" target="#b19">[20]</ref>, Domain Adversarial Neural Network (DANN) <ref type="bibr" target="#b40">[41]</ref>, Residual Transfer Network (RTN) <ref type="bibr" target="#b33">[34]</ref>, Multi-Adversarial Domain Adaptation (MADA) <ref type="bibr" target="#b39">[40]</ref>, Generate to Adapt (GTA) <ref type="bibr" target="#b36">[37]</ref>, Maximum-Classifier-Discrepancy (MCD) <ref type="bibr" target="#b21">[22]</ref>, Conditional Domain Adversarial Network (CDAN) <ref type="bibr" target="#b35">[36]</ref>, Locality Preserving Joint Transfer (LPJT) <ref type="bibr" target="#b6">[7]</ref>, Class-specific Reconstruction Transfer Learning (CRTL) <ref type="bibr" target="#b3">[4]</ref>, Margin Disparity Discrepancy (MDD) <ref type="bibr" target="#b37">[38]</ref>, Batch Spectral Penalization (BSP) + CDAN <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VisDA-2017</head><p>The Visual Domain Adaption (VisDA) challenge <ref type="bibr" target="#b42">[43]</ref> aims to test domain adaptation methods's ability to transfer source knowledge and adapt it to novel target domains. As the largest domain-adaptation dataset, the VisDA dataset contains 280K images across 12 categories from the training, validation, and testing domains. The training domain (the source domain) is a set of synthetic 2D renderings of 3D models generated from different angles and with different lighting conditions, while the validation domain (the target domain) is a set of realistic photos. The source domain contains 152,397 synthetic images, and the target domain has 55,388 real images.</p><p>Note that the target domain is highly-unbalanced, where the number of samples for each category is [l 1 , · · · , l 12 ] = <ref type="bibr">[3646,</ref><ref type="bibr">3475,</ref><ref type="bibr">4690,</ref><ref type="bibr">10401,</ref><ref type="bibr">4691,</ref><ref type="bibr">2075,</ref><ref type="bibr">5796,</ref><ref type="bibr">4000,</ref><ref type="bibr">4549,</ref><ref type="bibr">2281,</ref><ref type="bibr">4236,</ref><ref type="bibr">5548]</ref>. Therefore, the VisDA-2017 also serves to justify the suitability of MEDM for highly-unbalanced dataset. The ground-truth category distribution can be calculated as q = 1 12 i=1 li [l 1 , · · · , l 12 ]. Then, the entropy of q can be directly computed as H(q) = 2.3927. <ref type="table" target="#tab_1">Table I compares various methods with the pretrained  ResNet-50 architecture while Table II</ref> with the pretrained ResNet-101. Our method performs the best in the final mean accuracy among various methods. It surpasses the second best over 5% in the final mean accuracy for the scenarios of both  ResNet-50 and ResNet-101. With ResNet-101, MEDM achieves the record mean-accuracy of 82.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ImageCLEF-DA</head><p>ImageCLEF-DA is a publicly-available dataset for image-CLEF 2014 domain adaptation challenge. It has 12 common categories shared by the three public datasets: Caltech-256 (C), ImageNet ILSVRC 2012 (I), and Pascal VOC 2012 (P), which are also considered as three different domains. For 12 common categories, they are aeroplane, bike, bird, boat, bottle, bus, car, dog, horse, monitor, motorbike, and people. ImageCLEF-DA is a balanced dataset with 50 images in each category and 600 images in each domain. We consider all domain combinations and build 6 domain-adaptation tasks: I → P, P → I, I → C, C → I, C → P, and P → C. <ref type="table" target="#tab_1">Table III</ref> shows the classification accuracy results for various methods on the ImageCLEF-DA dataset with the ResNet50 architecture. The result of the MEDM is obtained by only training 100 epoches, which, however, neatly outperforms the other deep adaptation methods among five adaptation tasks, I → P, P → I, C → I, C → P, P → C. The best average accuracy (88.9%) is achieved by MEDM, which improves CDAN by about 1.2%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Office-Home</head><p>Office-Home <ref type="bibr" target="#b43">[44]</ref> is a typical dataset with a large number of classes (65 classes), which containing 15,500 images from four visually very different domains: Artistic images, Clip Art, Product images, and Real-world images. We consider all domain combination among these four domains, resulting 12 domain-adaptation tasks. <ref type="table" target="#tab_1">Table IV</ref> shows the classification accuracy results on the Office-Home dataset with the ResNet50 architecture. The result of the MEDM is obtained by only training 100 epoches and the best average accuracy (69.5%) is achieved by MEDM, which improves MDD by about 1.4%. This means that MEDM performs well for the datasets of large number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Office-31</head><p>Office-31 is a standard benchmark dataset for visual domain adaptation, which has 4652 images and 31 categories collected from three domains, Amazon (A), Webcam (W) and DSLR (D). The Amazon (A) domain contains 2817 images downloaded from amazon.com. We consider all domain combination, resulting 6 domain-adaptation tasks.</p><p>For the transferring tasks over Office-31, we employ the same neural network architecture as ImageCLEF-DA. We compare the average classification accuracy of each method on 10 random experiments, and report the standard error of the classification accuracies by different experiments of the same transfer task. In all experiments, we train each model for 100 epochs and exceptions include D → A and W → A, where 200 epoches are employed.</p><p>We report the classification accuracy results on the Office-31 dataset as in <ref type="table">Table V</ref>. Office-31 has three domains of different sizes, which result into non-evenly distributed classes in each domain.</p><p>Among various domain-adaptation methods, MEDM still performs the best for the mean accuracy. MEDM performs the best for three adaptation tasks, D → W, W → D, W → A, while MDD <ref type="bibr" target="#b37">[38]</ref> performs the best for the three remaining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Study</head><p>1) Effect of λ on Transferability: Entropy minimization in MEDM can be adjusted by varying λ. As shown in Algorithm 1, MEDM encourages the use of small λ whenever the target entropy approaches zero as the training iteration goes on. When β = 0, the transferability is achieved by minimization of both the supervised loss on source domain and the entropy loss on target domain. With small λ and keeping the (target) entropy small enough at the same time, it is expected that the end-to-end training of (9) ensures better transferability.</p><p>To investigate the choice of λ on the final performance, we also show the accuracy of MEDM on the task W → A on the Office-31 dataset when λ takes its value in {0.2,0.3,0.4,0.6,0.8,1.0} and β ∈ {0.0, 0.2}. With smallest possible value of λ for L e (θ i , T ) → 0, MEDM can achieve the best performance with a suitable choice of β as shown in <ref type="table" target="#tab_1">Table VII</ref>. This means that the better transferability could be ensured with smaller possible value of λ if L e (θ i , T ) → 0 is satisfied at the end of training.</p><p>2) Effect of β on Diversity Maximization: The superiority of MEDM in the VisDA challenge shows that it is very effective for highly-unbalanced target datasets, although the category diversity is expected to achieve its maximum value when the inferred categories are uniformly-distributed. We guess that it works well due to the collaboration in meeting both requirements, namely, the minimization of entropy and the maximization of category diversity, where the parameter β (9) is used to balance two individual requirements.</p><p>To investigate the choice of β on the final performance, we also show the accuracy of MEDM with ResNet-50 by fixing λ = 1.0 and varying β ∈ {0.2, 0.3, 0.4, 0.5}. As shown in <ref type="table" target="#tab_1">Table VI</ref>, we observed that E T {L d (T )} after training 10 epoches is always less than the entropy of the ground-truth target category distribution H(p) = 2.3978, which means that the maximization of L d (T ) under the constraint of entropy minimization does not necessarily produce the uniformlydistributed categories. When β &lt; 0.3, it results into poorer performance as some categories (car/truck) simply fail to be identified. With the increase of β, the practical diversity also grows. When β increases to 0.5, it also results into significantly worse performance compared to β = 0.4. Essentially, individual entropy minimization may automatically tradeoff with diversity maximization if the values of λ, β are properly validated by the use of DEV <ref type="bibr" target="#b29">[30]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Further Inclusion of Domain Difference Loss:</head><p>We also investigate the possibility of a further inclusion of domain difference loss (e.g. MMD loss <ref type="bibr" target="#b15">[16]</ref> or adversarial loss <ref type="bibr" target="#b19">[20]</ref>) in <ref type="bibr" target="#b8">(9)</ref>. However, experiments always show worse performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Entropy minimization has been shown to be a powerful tool for domain adaptation. However, entropy minimization is insufficient for the minimization of the target risk and trivial solutions are often observed. In this paper, we propose to employ diversity maximization for avoiding the trivial solutions. We show there exists a tradeoff for entropy minimization and diversity maximization towards the close-to-perfect domain adaptation. With the recently-proposed unsupervised model selection method, we show that the proposed MEDM outperforms state-of-the-art methods on several domain adaptation datasets, boosting a large margin especially on the largest VisDA dataset for cross-domain object classification. In this appendix, we consider the transfer task of SVHN → MNIST for demonstrating the trivial solutions of EMO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX TRIVIAL SOLUTION DEMONSTRATION FOR ENTROPY-MINIMIZATION-ONLY METHOD</head><p>The MNIST handwritten digits database has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in fixed-size images. SVHN is a real-world image dataset for machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It has 73257 digits for training, 26032 digits for testing. We focus on the task SVHN → MNIST in experiments.</p><p>We employed the CNN architecture used in <ref type="bibr" target="#b19">[20]</ref>. The number of training iterations is set to 50000 and the learning rate is set to 0.001. We run 10 experiments for computing average accuracy and its deviation.</p><p>Firstly, we show that EMO simply results into a trivial solution as indicated in <ref type="figure" target="#fig_3">Figure 2</ref>, where digit-1 dominates among other categories for the model trained with entropyminimization-only (4) (λ = 1). By inferring several target batches over the trained model, we observed that digit-1 always dominates for EMO. For MEDM, the predicted category distribution, however, is very close to the true uniform distribution.</p><p>Then, we compare our method with six methods in <ref type="table" target="#tab_1">Table  VIII</ref> for unsupervised domain adaptation including state-ofthe-art methods in visual domain adaptation: Reverse Gradient (RevGrad) <ref type="bibr" target="#b19">[20]</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b27">[28]</ref>, Domain Transfer Network (DTN) <ref type="bibr" target="#b44">[45]</ref>,TRIP-PLE <ref type="bibr" target="#b25">[26]</ref>, CORrelation ALignment (CORAL) <ref type="bibr" target="#b17">[18]</ref>, Minimal-Entropy Correlation Alignment (MECA) <ref type="bibr" target="#b28">[29]</ref>. MEDM performs the best and it achieves the average accuracy of 98.7%, which improves 3.5% compared to MECA. As also shown in <ref type="table" target="#tab_1">Table  VIII</ref>, EMO simply fails to work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>MEDM tries to maximize the category diversity, which can push entropy minimization away from trivial solutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>8: end for 9 :</head><label>9</label><figDesc>Deep Embedded Validation [30]: 1) Get DEV Risks of all models R = {GetRisk(θ l )} B l=1 over D val 2) Rank the best model l * = arg min 1≤l≤B R l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Predicted category distribution<ref type="bibr" target="#b5">(6)</ref> for both MEDM (blue) and EMO (4) (red) for a batch of target samples after a training iterations of 10000: EMO (Entropy-minimization-only) results into a trivial solution, where digit-1 dominates among others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and by the Scientific Research Foundation of Nanjing University of Posts and Telecommunications under Grant NY213002. Xiaofu Wu, Quan Zhou and Zhen Yang are with the National Engineering Research Center of Communications and Networking, Nanjing University of Posts and Telecommunications, Nanjing 210003, China (E-mails: xfuwu@ieee.org, {quan.zhou,yangz}@njupt.edu.cn). Suofei Zhang is with the School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing 210003, China (E-mail: zhang-suofei@njupt.edu.cn). Chunming Zhao is with the National Mobile Commun. Research Lab., Southeast University, Nanjing 210096, China (Email: cmzhao@seu.edu.cn).</figDesc><table /><note>Longin Jan Latecki is with the Department of Computer and Information Sci- ences, Temple University, Philadelphia, Pennsylvania, USA. (Email: latecki@ temple.edu).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>ACCURACY (%) OF RESNET-50 MODEL ON VISDA-2017</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ACCURACY</head><label>II</label><figDesc>(%) OF RESNET-101 MODEL ON THE VISDA DATASET</figDesc><table><row><cell>Method</cell><cell cols="2">plane bcycl</cell><cell>bus</cell><cell>car</cell><cell cols="5">horse knife mcycl person</cell><cell>plant</cell><cell cols="4">sktbrd train truck mean</cell></row><row><cell>DAN [16]</cell><cell>87.1</cell><cell>63.0</cell><cell>76.5</cell><cell>42.0</cell><cell>90.3</cell><cell>42.9</cell><cell>85.9</cell><cell cols="2">53.1</cell><cell>49.7</cell><cell>36.3</cell><cell>85.8</cell><cell>20.7</cell><cell>61.1</cell></row><row><cell>RevGrad [20]</cell><cell>81.9</cell><cell>77.7</cell><cell>82.8</cell><cell>44.3</cell><cell>81.2</cell><cell>29.5</cell><cell>65.1</cell><cell cols="2">28.6</cell><cell>51.9</cell><cell>54.6</cell><cell>82.8</cell><cell>7.8</cell><cell>57.4</cell></row><row><cell>MCD [22]</cell><cell>87.0</cell><cell>60.9</cell><cell cols="2">83.7 64.0</cell><cell>88.9</cell><cell>79.6</cell><cell>84.7</cell><cell cols="2">76.9</cell><cell>88.6</cell><cell>40.3</cell><cell>83.0</cell><cell>25.8</cell><cell>71.9</cell></row><row><cell>CDAN [36]</cell><cell>85.2</cell><cell>66.9</cell><cell cols="2">83.0 50.8</cell><cell>84.2</cell><cell>74.9</cell><cell>88.1</cell><cell cols="2">74.5</cell><cell>83.4</cell><cell>76.0</cell><cell>81.9</cell><cell>38.0</cell><cell>73.7</cell></row><row><cell>BSP+CDAN [39]</cell><cell>92.4</cell><cell>61.0</cell><cell cols="2">81.0 57.5</cell><cell>89.0</cell><cell>80.6</cell><cell>90.1</cell><cell cols="2">77.0</cell><cell>84.2</cell><cell>77.9</cell><cell>82.1</cell><cell>38.4</cell><cell>75.9</cell></row><row><cell>MEDM(ours)</cell><cell>93.5</cell><cell>80.4</cell><cell>90.8</cell><cell>70.3</cell><cell>92.8</cell><cell>87.9</cell><cell>91.1</cell><cell cols="2">79.8</cell><cell>93.7</cell><cell>83.6</cell><cell>86.1</cell><cell>38.7</cell><cell>82.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">ACCURACY (%) ON IMAGECLEF-DA DATASET FOR UNSUPERVISED DOMAIN ADAPTATION WITH RESNET-50</cell></row><row><cell>Method</cell><cell></cell><cell>I → P</cell><cell></cell><cell>P → I</cell><cell></cell><cell>I → C</cell><cell>C → I</cell><cell></cell><cell></cell><cell>C → P</cell><cell cols="2">P → C</cell><cell>Avg</cell></row><row><cell>DAN [16]</cell><cell></cell><cell cols="12">75.0 ± 0.4 86.2 ± 0.2 93.3 ± 0.2 84.1 ± 0.4 69.8 ± 0.4 91.3 ± 0.4 83.3</cell></row><row><cell>RTN [34]</cell><cell></cell><cell cols="12">75.6 ± 0.3 86.8 ± 0.1 95.3 ± 0.1 86.9 ± 0.3 72.7 ± 0.3 92.2 ± 0.4 84.9</cell></row><row><cell cols="2">RevGrad [20]</cell><cell cols="12">75.0 ± 0.6 86.0 ± 0.3 96.2 ± 0.4 87.0 ± 0.5 74.3 ± 0.5 91.5 ± 0.6 85.0</cell></row><row><cell cols="2">MADA [40]</cell><cell cols="12">75.0 ± 0.3 87.9 ± 0.2 96.0 ± 0.3 88.8 ± 0.3 75.2 ± 0.2 92.2 ± 0.3 85.8</cell></row><row><cell cols="2">CDAN [36]</cell><cell cols="3">77.7 ± 0.3 90.7 ± 0.2</cell><cell cols="2">97.7 ± 0.3</cell><cell cols="7">91.3 ± 0.3 74.2 ± 0.2 94.3 ± 0.3 87.7</cell></row><row><cell cols="2">MEDM(Ours)</cell><cell cols="2">78.5 ± 0.5</cell><cell>93.0 ± 0.5</cell><cell cols="2">96.1 ± 0.2</cell><cell cols="2">92.8 ± 0.5</cell><cell cols="2">77.2 ± 0.7</cell><cell cols="2">95.5 ± 0.4</cell><cell>88.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="9">ACCURACY (%) ON OFFICE-HOME FOR UNSUPERVISED DOMAIN ADAPTATION WITH RESNET-50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pr Avg</cell></row><row><cell>DAN [16]</cell><cell cols="2">43.6 57.0</cell><cell cols="2">67.9</cell><cell cols="4">45.8 56.5 60.4 44.0 43.6 67.7</cell><cell>63.1</cell><cell>51.5</cell><cell>74.3 56.3</cell></row><row><cell>DANN [41]</cell><cell cols="2">45.6 59.3</cell><cell cols="2">70.1</cell><cell cols="4">47.0 58.5 60.9 46.1 43.7 68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8 57.6</cell></row><row><cell>JAN [42]</cell><cell cols="2">45.9 61.2</cell><cell cols="2">68.9</cell><cell cols="4">50.4 59.7 61.0 45.8 43.4 70.3</cell><cell>63.9</cell><cell>52.4</cell><cell>76.8 58.3</cell></row><row><cell>CDAN [36]</cell><cell cols="2">50.7 70.6</cell><cell cols="2">76.0</cell><cell cols="4">57.6 70.0 70.0 57.4 50.9 77.3</cell><cell>70.9</cell><cell>56.7</cell><cell>81.6 65.8</cell></row><row><cell>LPJT [7]</cell><cell cols="2">32.5 54.8</cell><cell cols="2">57.1</cell><cell cols="4">34.4 53.8 53.0 35.6 35.3 60.9</cell><cell>45.6</cell><cell>39.4</cell><cell>67.8 47.5</cell></row><row><cell>MDD [38]</cell><cell cols="2">54.9 73.7</cell><cell cols="2">77.8</cell><cell cols="4">60.0 71.4 71.8 61.2 53.6 78.1</cell><cell>72.5</cell><cell>60.2</cell><cell>82.3 68.1</cell></row><row><cell cols="3">MEDM(Ours) 57.1 76.1</cell><cell cols="2">80.0</cell><cell cols="4">62.0 72.7 76.0 62.3 53.4 81.2</cell><cell>69.9</cell><cell>59.8</cell><cell>83.9 69.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell></row><row><cell cols="9">ACCURACY (%) ON OFFICE-31 DATASET FOR UNSUPERVISED DOMAIN ADAPTATION WITH RESNET-50</cell></row><row><cell>Method</cell><cell></cell><cell>A → W</cell><cell></cell><cell></cell><cell>D → W</cell><cell>W → D</cell><cell>A → D</cell><cell>D → A</cell><cell>W → A</cell><cell>Avg</cell></row><row><cell>DAN [16]</cell><cell></cell><cell cols="4">83.8 ± 0.4 96.8 ± 0.2</cell><cell>99.5 ± 0.1</cell><cell cols="2">78.4 ± 0.2 66.7 ± 0.3 62.7 ± 0.2 81.3</cell></row><row><cell>RevGrad [20]</cell><cell></cell><cell cols="4">82.0 ± 0.4 96.9 ± 0.2</cell><cell>99.1 ± 0.1</cell><cell cols="2">79.7 ± 0.4 68.2 ± 0.4 67.4 ± 0.5 82.2</cell></row><row><cell>MADA [40]</cell><cell></cell><cell cols="4">90.0 ± 0.1 97.4 ± 0.1</cell><cell>99.6 ± 0.1</cell><cell cols="2">87.8 ± 0.2 70.3 ± 0.3 66.4 ± 0.3 85.2</cell></row><row><cell>CDAN [36]</cell><cell></cell><cell cols="7">94.1 ± 0.1 98.6 ± 0.1 100.0 ± 0.0 92.9 ± 0.2 71.0 ± 0.3 69.3 ± 0.3 87.7</cell></row><row><cell>CRTL [4]</cell><cell></cell><cell>77.4</cell><cell></cell><cell></cell><cell>95.7</cell><cell>97.6</cell><cell>79.5</cell><cell>81.9</cell><cell>81.8</cell><cell>85.6</cell></row><row><cell cols="2">BSP+CDAN [39]</cell><cell cols="7">93.3 ± 0.2 98.2 ± 0.2 100.0 ± 0.0 93.0 ± 0.2 73.6 ± 0.3 72.6 ± 0.3 88.5</cell></row><row><cell>MDD [38]</cell><cell></cell><cell cols="2">94.5 ± 0.3</cell><cell cols="3">98.4 ± 0.1 100.0 ± 0.0</cell><cell>93.5 ± 0.2</cell><cell>74.6 ± 0.3 72.2 ± 0.1 88.9</cell></row><row><cell>MEDM(Ours)</cell><cell></cell><cell cols="2">93.4 ± 0.6</cell><cell></cell><cell>98.8 ± 0.1</cell><cell>100.0 ± 0.0</cell><cell cols="2">93.4 ± 0.5 74.2 ± 0.2 75.4 ± 0.4 89.2</cell></row></table><note>Method Ar Cl Ar Pr Ar Rw Cl Ar Cl Pr Cl Rw Pr Ar Pr Cl Pr Rw Rw Ar Rw Cl Rw</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI THE</head><label>VI</label><figDesc>EFFECT OF β ON DIVERSITY MAXIMIZATION OVER VISDA-2017 (GROUND-TRUTH CATEGORY DIVERSITY H(q) = 2.3927)</figDesc><table><row><cell>β</cell><cell cols="2">plane bcycl</cell><cell>bus</cell><cell>car</cell><cell>horse</cell><cell>knife</cell><cell cols="7">mcycl person plant sktbrd train truck mean</cell><cell>diversity</cell></row><row><cell>0.2</cell><cell>91.6</cell><cell>77.6</cell><cell>66.4</cell><cell>1.6</cell><cell>90.7</cell><cell>83.4</cell><cell>89.4</cell><cell>78.6</cell><cell>89.7</cell><cell>74.9</cell><cell>89.6</cell><cell>0.3</cell><cell>69.5</cell><cell>2.0619</cell></row><row><cell>0.3</cell><cell>94.4</cell><cell>76.2</cell><cell cols="2">87.3 61.1</cell><cell>91.1</cell><cell>79.6</cell><cell>88.0</cell><cell>80.0</cell><cell>92.3</cell><cell>78.9</cell><cell>89.7</cell><cell>35.0</cell><cell>79.4</cell><cell>2.2446</cell></row><row><cell>0.4</cell><cell>92.7</cell><cell>83.1</cell><cell cols="2">82.2 65.8</cell><cell>89.2</cell><cell>89.9</cell><cell>79.8</cell><cell>78.6</cell><cell>91.3</cell><cell>77.7</cell><cell>90.9</cell><cell>33.8</cell><cell>79.6</cell><cell>2.2493</cell></row><row><cell>0.5</cell><cell>94.2</cell><cell>77.8</cell><cell cols="2">80.9 58.4</cell><cell>90.9</cell><cell>25.9</cell><cell>81.4</cell><cell>76.1</cell><cell>89.1</cell><cell>67.6</cell><cell>89.7</cell><cell>40.1</cell><cell>72.7</cell><cell>2.2704</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII THE</head><label>VII</label><figDesc>EFFECT OF λ ON TRANSFERABILITY FOR W → A</figDesc><table><row><cell>λ + β</cell><cell>Le</cell><cell>Acc</cell><cell>λ + β</cell><cell>Le</cell><cell>Acc</cell></row><row><cell cols="2">1.0 + 0 0.0</cell><cell>43.1</cell><cell>1.0 + 0.2</cell><cell cols="2">0.0 53.3</cell></row><row><cell cols="2">0.8 + 0 0.0</cell><cell>45.9</cell><cell>0.8 + 0.2</cell><cell cols="2">0.0 56.5</cell></row><row><cell cols="2">0.6 + 0 0.0</cell><cell>49.8</cell><cell>0.6 + 0.2</cell><cell cols="2">0.0 65.7</cell></row><row><cell cols="2">0.4 + 0 0.1</cell><cell>54.0</cell><cell>0.4 + 0.2</cell><cell cols="2">0.1 74.3</cell></row><row><cell cols="2">0.3 + 0 0.2</cell><cell>62.2</cell><cell>0.3 + 0.2</cell><cell cols="2">0.2 75.5</cell></row><row><cell cols="2">0.2 + 0 0.3</cell><cell>63.2</cell><cell>0.2 + 0.2</cell><cell cols="2">0.3 74.5</cell></row><row><cell cols="2">0.1 + 0 0.4</cell><cell>68.1</cell><cell>0.1 + 0.2</cell><cell cols="2">0.4 72.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII AVERAGE</head><label>VIII</label><figDesc>ACCURACY (%) FOR SVHN → MNIST</figDesc><table><row><cell>Method</cell><cell>Acc</cell></row><row><cell>RevGrad ( [20])</cell><cell>73.9</cell></row><row><cell>ADDA ( [34])</cell><cell>76.0</cell></row><row><cell>DTN ( [45])</cell><cell>84.4</cell></row><row><cell>TRIPPLE ( [26])</cell><cell>86.2</cell></row><row><cell>COREL ( [18])</cell><cell>90.2</cell></row><row><cell>MECA ( [46])</cell><cell>95.2</cell></row><row><cell>EMO (4)</cell><cell>44.1</cell></row><row><cell>MEDM(Ours)</cell><cell>98.7 ± 0.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Class-specific reconstruction transfer learning for visual recognition across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2424" to="2438" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved techniques for adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andreopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2622" to="2637" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised multi-target domain adaptation: An information theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3993" to="4001" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality preserving joint transfer for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="6103" to="6115" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3403" to="3417" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain invariant and class discriminative feature learning for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="4260" to="4273" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation using feature-whitening and consensus loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation: A deep max-margin gaussian process approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Minmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dilip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dumitru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep coral: correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop on Transferring and Adapting Source Knowledge in Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Minimal-entropy correlation allignment for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacopo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vittorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards accurate model selection in deep unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning bounds for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianminwang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">VisDA: A synthetic-to-real benchmark for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial alignment of class prediction uncertainties for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchiori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laarhoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPRAM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
