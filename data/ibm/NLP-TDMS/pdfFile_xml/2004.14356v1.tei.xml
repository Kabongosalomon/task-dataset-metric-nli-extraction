<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AxCell: Automatic Extraction of Results from Machine Learning Papers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Czapla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London (UCL)</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London (UCL)</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stojnic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">-waves</orgName>
								<address>
									<settlement>Wroc≈Çaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AxCell: Automatic Extraction of Results from Machine Learning Papers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub. 1 Back-translation . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper we . . . state-of-the-art machine translation . . . by 1 BLEU score . . . We open source our . . .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning studies how machines learn with respect to a task, a performance metric, and a dataset <ref type="bibr" target="#b8">(Mitchell, 2006)</ref>. The (task, dataset, metric name, metric value) tuple can therefore be seen as representing a single result of a machine learning paper. To make progress as a field we need to make comparisons between results achieved with different methodologies. In light of the explosion in the number of machine learning publications in recent years, such comparisons have become more difficult. 2 This poses serious challenges to peer review, among others. For instance, across ten language modelling papers submitted to ICLR 2018, the perplexity score of the best baseline differed by more than 50 points <ref type="bibr" target="#b11">(Ruder, 2018)</ref>.</p><p>One way to deal with the deluge of papers is to develop automatic approaches for extracting results from papers and aggregating them into leaderboards. Authors typically publish their results in a tabular format in the paper, including a selection of comparisons between their approach and past papers. Automatic extraction of result tuples from tables-and optionally metadata such as model names-enables a full comparison between published methods.</p><p>Online leaderboards for comparison have become increasingly common in the research community. But these are only available for a few tasks and do not aid the comparison of models across tasks. To fill the gap, result aggregation tools such as Papers With Code 3 and NLP-Progress 4 utilise crowdsourced community contributions to populate paper leaderboards. However, human annotation of results can be laborious and error-prone, leading to omission or misreporting of paper results. This motivates the need for a machine learning approach to create a comprehensive results resource for the field.</p><p>Existing state-of-the-art approaches for results extraction are brittle and noisy, relying on text formatting hints and tables extraction from PDF files <ref type="bibr" target="#b3">(Hou et al., 2019)</ref>. In contrast, we propose AxCell, a pipeline for automatic extraction of results from machine learning papers. AxCell breaks down the results extraction task into several subtasks including table type classification, table semantic segmentation and linking results to leaderboards. We employ an ULMFiT-based classifier architec-ture <ref type="bibr" target="#b5">(Howard and Ruder, 2018)</ref> to make full use of paper and table context to interpret tabular content, and extract results accordingly.</p><p>As a whole, this paper makes three main contributions to the literature. First, we significantly improve over the state-of-the-art for results extraction with our AxCell model. On the subset of the NLP-TDMS dataset of <ref type="bibr" target="#b3">Hou et al. (2019)</ref> where L A T E X code is available, our approach achieves a micro F 1 score of 25.8 compared to the state of the art of 7.5. Secondly, we release a structured, annotated dataset for training models for results extraction, and an evaluation dataset for evaluating the performance of models on this task. Lastly, our approach is used in an in-production setting at paperswithcode.com to semi-automatically (by aiding the human review) extract results from papers and track progress in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Results Extraction. Previous works have studied the problem of extracting results tuples (task, dataset, metric name, metric value) from papers. <ref type="bibr" target="#b12">Singh et al. (2019)</ref> perform search over publications and compose a leaderboard for a queried triplet. Similar to our approach, they use tables extracted from L A T E X sources. In contrast, they do not extract absolute metric values but rank papers and do not appear to utilise the text content of publications. Our goal in this paper is to extract complete results to create leaderboards, so unlike <ref type="bibr" target="#b12">Singh et al. (2019)</ref>, we focus on extracting raw metric values. Additionally we make use of the content of the publication as context for entity recognition and linking.</p><p>Closer to our formulation, <ref type="bibr" target="#b3">Hou et al. (2019)</ref> extract absolute metric values alongside the metric name, task and dataset. They also use text excerpts as well as direct tabular information to make inferences for table contents. They frame extraction as a natural language inference problem and apply an NLI model based on a BERT architecture <ref type="bibr" target="#b0">(Devlin et al., 2018)</ref> to extract results from PDF files. The disadvantage of this approach is that using PDFs leads to a lot of noise in structural information such as the partition of a table into cells. In our work, we explicitly utilise the structural information from the L A T E X source to extract entire tables in order to perform semantic segmentation. We demonstrate that this structural information and segmentation are crucial for boosting extraction performance. <ref type="table" target="#tab_1">Table Extraction</ref>. The more general problem of retrieving information from tables has been studied in past works <ref type="bibr" target="#b7">(Milosevic et al., 2019;</ref><ref type="bibr" target="#b1">Ghasemi-Gol and Szekely, 2018;</ref><ref type="bibr" target="#b15">Wei et al., 2006;</ref><ref type="bibr" target="#b2">Herzig et al., 2020)</ref>. Our focus in this paper is on the problem of extracting and interpreting content of tables characteristic to machine learning papers. The goal of our table semantic segmentation model is to classify cells into categories. That is, instead of performing structural segmentation when one tries to distinguish between captions, headers and rows in a stream of text (as in <ref type="bibr" target="#b10">(Pinto et al., 2003)</ref>) we focus on semantic segmentation (i.e., assigning roles to each cell) of tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>The task of paper results extraction is to take a machine learning paper as an input and extract results contained within the paper, specifically tuples of the form (task, dataset, metric name, metric value). As an example, if we were to take in the EfficientNet paper of <ref type="bibr" target="#b14">Tan and Le (2019)</ref> as an input, some example results tuples we would want to extract would be EfficientNet-B7 (Image Classification, Ima-geNet, Top 1 Accuracy, 0.844), EfficientNet-B7 (Image Classification, ImageNet, Top 5 Accuracy, 0.971) and EfficientNet (Image Classification, Stanford Cars, Accuracy, 0.947).</p><p>To tackle this problem effectively, we need to frame the problem by defining subtasks to solve that take us from paper to results. AxCell solves several subtasks: (i) table type classification, identifying whether a table in a paper has relevant results; (ii) table segmentation, segmenting and classifying table cells according to whether they hold metrics, datasets, models, etc.; and (iii) linking results to leaderboards, taking the result tuples and matching them to an existing leaderboard of results. The end-to-end system is shown in <ref type="figure">Figure 1</ref> with reference to an example. We now introduce the different components of AxCell.   <ref type="figure">Figure 1</ref>: Graphical depiction of AxCell. The extraction starts with L A T E X source code of a paper, from which we extract 1) tables and 2) text. 3) We classify the caption to filter out irrelevant tables. 4) The content of each cell is looked up in the paper's text. Retrieved mentions are used to 5) segment cells based on their meaning (see the legend in the top-right corner). The segmented table and the paper's text are used to 6) obtain contexts for each numeric cell. 7) Results tuples are scored based on contexts and numeric values are normalized to match required format. Finally, inferior results or results below a confidence threshold are 8) filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Table Type Classification</head><p>The first stage of AxCell is to categorize tables from papers into one of three categories: leaderboard tables, ablation tables and irrelevant tables. A leaderboard table contains the principal results of the paper on a selected benchmark, including comparisons with other papers. An ablation table compares different permutations of the paper's methodology. Lastly, irrelevant tables include hyperparameters, dataset statistics and other information that is not directly relevant for result extraction. For this stage we employ a classifier with a ULMFiT architecture <ref type="bibr" target="#b5">(Howard and Ruder, 2018)</ref> with LSTM layers and a SentencePiece unigram model <ref type="bibr" target="#b6">(Kudo, 2018)</ref> for tokenization. <ref type="bibr">5</ref> We train the SentencePiece model and pretrain a left-to-right ULMFiT language model on text of papers from an unlabelled dataset of arXiv articles (see Section 4). <ref type="table" target="#tab_8">Table 5</ref> in the Appendix contains details on the hyperparameters and training regime. 6 5 Our classifier uses the fast.ai implementation <ref type="bibr">(Howard and Gugger, 2020)</ref>. <ref type="bibr">6</ref> We experimented with finetuning alternative lan-</p><p>The classifier head is a standard ULMFiT classifier with a pooling layer followed by two linear layers. We treat the problem as a twolabel classification with labels: leaderboard and ablation. A table is considered irrelevant if it is neither a leaderboard nor ablation (we use a confidence threshold of 0.5). We train the model on the SegmentedTables dataset (see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Table Segmentation</head><p>The second stage of AxCell is to pass relevant tables to a table segmentation subtask. The goal is to annotate each cell of a table with a label denoting what type of data a given cell contains. To this end, we classify each table cell into one of: dataset, metric, paper model, cited model, and other (containing meta and task cells). An example segmented table is shown in <ref type="figure">Figure 1</ref>.</p><p>To help classify each table cell, we provide a context in which the cell content is mentioned. We search for cell content in the full guage models such as BERT and SciBERT but our initial experiments did not yield superior results. A full investigation of alternative models, including pretraining from scratch, is left for future research.</p><p>On TREC-6, &lt;MASK&gt; significantly improves upon training from scratch; as examples are shorter and fewer, supervised and semi-supervised &lt;MASK&gt; achieve similar results. <ref type="figure">Figure 2</ref>: An example of a text excerpt from the paper by <ref type="bibr" target="#b5">Howard and Ruder (2018)</ref> used as evidence for a cell content query with ULMFiT (covered with &lt;MASK&gt; token) as paper model. paper content using a BM25 scoring algorithm. Retrieved text fragments are then passed to a ULMFiT-based classifier with some handcrafted features for the cell. These features include information such as the position of the cell in the table, whether the cell is a header, and cell styles. A full list is available in the Appendix. For processing the retrieved text fragments, the retrieved term from the cell is replaced with a special mask &lt;MASK&gt; token to inhibit memorization of common names (see <ref type="figure">Figure 2</ref> for an example). <ref type="table">Table segmen</ref>tation can then be treated as a classification problem with 5 exclusive labels. We use the same pre-trained language model weights and SentencePiece model as for the table type classification. Results for this stage of the model are outlined in <ref type="table" target="#tab_6">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cell Context Generation</head><p>The next stage after table segmentation is to generate contexts for numeric cells. As an example, if we know a numeric cell has a dataset cell somewhere in its row, and a model cell somewhere in its column, then this table context is informative for deciding the dataset and model for this result. But there is much broader context in the paper that is useful for linking.</p><p>For example, a paper studying semantic segmentation with models evaluated on KITTI and CamVid datasets could mention semantic segmentation in the introduction, test set in a subsection referring to a results table, KITTI in the description of that table and class IoU in the column header. <ref type="figure">Figure 3</ref> shows a visual representation of this hierarchy of context.</p><p>To reflect this hierarchy we generate several types of contexts for each cell. The table context, as discussed, looks at a numeric cell and other cells in its row or column labeled as model, dataset or metric. We also define text Back-translation . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper we . . . state-of-the-art machine translation . . . by 1 BLEU score . . . We open source our . . .   <ref type="table">;</ref> an abstract context, the paper abstract; and a global paper context, containing the entire paper text. The gathered contexts are then used to link potential results to predefined leaderboards of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Linking Cells to Leaderboards</head><p>Once we have the cell contexts, the next stage of AxCell is to link them to leaderboards to form performance records. The goal is to take a metric value for a model and infer the leaderboard it is connected to. A leaderboard is defined via the triplet (task, dataset, metric name). For example: (Image Classification, ImageNet, Top 1 Accuracy) can capture papers that report performance on Image Classification for ImageNet and report Top 1 Accuracy. To simplify the problem, we assume a closeddomain with all leaderboards known in advance.</p><p>To match results to leaderboards we look for evidence in cell contexts, which we now explain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Inference From Evidences</head><p>Pieces of evidence are words or phrases that correspond to a task, dataset or metric. For example, SST-2, binary and polarity could all serve as evidence for the two-class Stanford Sentiment Treebank dataset <ref type="bibr" target="#b13">(Socher et al., 2013)</ref>. Pieces of evidence allow us to infer whether an entity has been mentioned in a given context. Using the same example, if "SST-2" appears in the table caption then this is evidence that a numeric value in the table could be linked to the Stanford Sentiment Treebank dataset.</p><p>For a given numeric cell in the table, we search the cell contexts for evidence for every entity (task, dataset, metric) and accumulate them into a set of M pieces of cell evidence E = {e 1 , . . . , e M }, with e j of the form (mention j , entity j , context j ). For example, (acc, metric, table) means "acc" metric evidence was found in cell's table context. Using this evidence set, our goal is to calculate P (y k | E), where y k is a binary variable denoting whether the cell contains results for a leaderboard k ‚àà {1, . . . , N }.</p><p>Through Bayes' Rule we know that P (y k | E) ‚àù P (E | y k ) P (y k ). We can estimate P (E | y k ) by Naive Bayes:</p><formula xml:id="formula_0">P (E | y k ) ‚âà M j P (e j | y k )</formula><p>Since e j = (mention j , entity j , context j ), to model P (e j | y k ) we need to define a joint probability model for these different elements. In our results linking model, we assume a mention can appear in a given context on purpose, to describe content of the cell, or it can be noise -falsely matched or referencing another cell. With additional simplifications we assume:</p><formula xml:id="formula_1">P (e j | y k ) = P (mention j , entity j , ctx j ) | y k ‚àù P (noise | ctx j ) ¬∑ P entity j | noise + P (¬¨noise | ctx j ) ¬∑ P mention j , entity j | y k</formula><p>where the noise probability for each context, P (noise | ctx j ), is computed using training set.</p><p>Finally, for a leaderboard y k = (y</p><formula xml:id="formula_2">(task) k , y (dataset) k , y (metric) k ) we assume that P (mention, entity | y k ) ‚àù P mention | y (entity) k</formula><p>, that is, a metric mention "acc" has the same conditional probability for leaderboard (Image Classification, ImageNet, Accuracy) as for (Natural Language Inference, SNLI, Accuracy).</p><p>We compute P mention | y (entity) k to be inversly proportional to the number of other entities of type entity with the same mention evidence (see Appendix D for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Filtering</head><p>The final step of AxCell is to filter out results with a linking score that is too low, results for cited models and inferior results (to keep only the best performing results). First, we filter out records not associated with models introduced in a paper being processed. We then remove records for which a linking score is below some given threshold. The remaining records are grouped by leaderboard and for each leaderboard only the best result is kept, based on higher is better annotation available in taxonomy; e.g. Accuracy would keep higher values, Error Rate would keep lower values. Finally, we remove all results with a linking score below the second threshold. This gives us the final list of results tuples extracted from the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>In this section we explain the datasets we used for training and evaluating AxCell for results extraction. The primary input we use for a training dataset is L A T E X source code of machine learning papers from arXiv.org. Over 90% of considered papers have source code available. This allows us to obtain a high quality dataset without common artifacts that arise from extracting data directly from PDF files <ref type="bibr" target="#b3">(Hou et al., 2019)</ref>.</p><p>For training our model we use two main datasets:</p><p>‚Ä¢ ArxivPapers: An unlabelled dataset of over 100,000 machine learning papers. Used for language model pre-training.</p><p>‚Ä¢ SegmentedTables: A table segmentation dataset where each cell is annotated according to whether it is a paper, metric, dataset, and so on. Used for table segmentation and table type classification.</p><p>We tune the linking and filtering performance of our model using a validation dataset:</p><p>‚Ä¢ LinkedResults: An annotated dataset of over 200 papers with results tuples, capturing the performance of models in the papers, and links to tables.</p><p>Lastly we evaluate the end-to-end performance of our model on our test set:</p><p>‚Ä¢ PWC Leaderboards: An annotated dataset of over 2,000 leaderboards with results tuples. Used for end-to-end performance evaluation.</p><p>We now describe in detail these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">arXiv Papers</head><p>The dataset contains 104, 723 papers published on arXiv.org between 2007-2020. 94, 616 papers are available with L A T E X sources, from which we extracted 277, 996 tables in total. Due to licensing limitations the dataset we release with this paper contains only metadata (available in the public domain) and links to articles. The dataset is unlabeled, designated for use in self-supervised pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Segmented Tables</head><p>This is a dataset for table classification and segmentation, containing 1400 annotated tables from 354 articles. The dataset provides data on dataset mentions in captions, the type of table (leaderboard, ablation, irrelevant) and ground truth cell annotations into classes: dataset, metric, paper model, cited model, meta and task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Linked Results</head><p>This is a set of 236 papers we annotated with 1148 results tuples, capturing the performance of models in the papers. Additionally we include metrics scores in a normalized form. We also record metadata such as the names of the models used in papers. Each results tuple (task, dataset, metric name, metric value) is linked to a particular table, row and cell it originates from. Note that results that appear outside of a table, for instance in the paper's text or graphs, are not present in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">PWC Leaderboards</head><p>This is a dataset of 2,295 leaderboards obtained from the Papers With Code arXiv.org labelling interface. This interface allows an annotator to take a paper and label it with results tuples. It is therefore a good ground-truth test on which to evaluate the end-to-end performance of our automated solution. Additionally, each record is linked to a cell it appears in. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We now evaluate the end-to-end performance of AxCell on the results extraction task. We evaluate on two datasets: the NLP-TDMS dataset introduced in <ref type="bibr" target="#b3">Hou et al. (2019)</ref>, in order to compare our method to the state of the art, and on our PWC Leaderboards dataset, which contains many more leaderboards and acts as a more challenging benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">NLP-TDMS Results</head><p>We compare AxCell to the TDMS-IE model from <ref type="bibr" target="#b3">Hou et al. (2019)</ref> on the NLP-TDMS dataset in <ref type="table" target="#tab_4">Table 1</ref>. The NLP-TDMS (Full) dataset contains 332 papers related to Natural Language Processing with 848 performance annotations of task, dataset, metric and score and 168 unique leaderboards. The subset NLP-TDMS (Exp) is limited to 77 leaderboards appearing in at least 5 papers. See <ref type="table" target="#tab_10">Table 7</ref> in the Appendix for dataset statistics. To compare with <ref type="bibr" target="#b3">Hou et al. (2019)</ref>, we use the Exp dataset. <ref type="bibr" target="#b3">Hou et al. (2019)</ref> extract records directly from PDF, so the methods are not fully comparable. In order to run AxCell on that dataset we limit the dataset to papers for which L A T E X source code is available. <ref type="table" target="#tab_4">Table 1</ref> shows results on that subset with TDMS-IE performance computed based on published predictions. Our solution yields significantly better results for whole records retrieval despite not being trained on their taxonomy (i.e., the zeroshot scenario in <ref type="bibr" target="#b3">Hou et al. (2019)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">PWC Leaderboards Results</head><p>Having validated the performance of our approach compared to the state of the art, we now apply it to our much larger dataset of leaderboards. Compared to the NLP-TDMS dataset, whose taxonomy consists of 77 leaderboards, our taxonomy consists of 3,445 leaderboards making prediction much more challenging.</p><p>The results of our approach for extracting each entity are detailed in <ref type="table" target="#tab_5">Table 2</ref>. We achieve reasonable performance on extracting the full TDMS (task, dataset, metric, score) tuple, which is the most challenging setting and the highest scores for extracting task and metric information. The lower scoring entities are generally the ones that depend on the quality of extraction of other entities. For example, extracting leaderboards depends on how well we extract task, dataset and metric entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Performance Studies</head><p>In this section, we perform experiments over the various steps of AxCell in order to better understand their relative importance. Our key finding is that table segmentation is the performance bottleneck of AxCell. We run our experiments on the SegmentedTables dataset introduced in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Table Type Classification</head><p>The biggest issue of table type classification is in distinguishing between leaderboard and ablation tables (see <ref type="figure" target="#fig_1">Figure 5</ref> in Appendix for the confusion matrix). These tables can be very similar structurally: ablations may even compare on the same split of data as the primary result. As the distinction is not always clear, during results retrieval we extract results from both types of tables and pick only the best result during filtering (i.e., the highest or lowest based on predicted metric).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Table Segmentation</head><p>One goal of table segmentation is to generalise to extract tables from unseen tasks. To study this, we partitioned SegmentedTables dataset into 11 folds, based on the task name extracted from paper abstracts. The fold with tables from Image Classification papers is always used as a validation set. For each of the remaining 10 folds we train 5 models with a given fold used as a test set and the other 9 folds used as training data. The final table segmentation model used in AxCell is the one with the highest micro F 1 score on the validation set. <ref type="table" target="#tab_6">Table 3</ref> shows micro precision and recall of classifying each non-numeric cell into one of 5 exclusive classes: dataset, metric, competing model, paper's model or other.</p><p>We can see that we achieve strong results on all tasks, although some tasks perform better than others. A task like semantic segmentation has less table and benchmark diversity, so benchmark tables for datasets like Cityscapes and PASCAL VOC 2012 are fairly standardised across papers. This makes extraction fairly straightforward. In contrast, the worse performing tasks are unusual in their own way. In image generation, for instance, we are less able to extract the correct dataset entity, whereas in speech recognition, our model has more problems distinguishing paper models from competing models; see <ref type="figure" target="#fig_0">Figure 4</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Linking</head><p>To evaluate linking performance in isolation of other steps we run it on tables with ground truth type and segmentation annotations. The annotations are available in the Segment-edTables dataset for 24 Speech Recognition and 32 Semantic Segmentation papers with 287 annotated leaderboard records in total. For each cell with associated leaderboard annota-tion we generate cell contexts and use linking to retrieve the top-5 predictions. We test four approaches to generate evidence of mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bag-of-Words</head><p>The full name and any word (which is not an English stop-word) occurring in the name of a metric or dataset (as found in taxonomy) is evidence of mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>We run an abbreviation detector <ref type="bibr" target="#b9">(Neumann et al., 2019)</ref> over the Arxiv-Papers dataset to extract pairs of common abbreviations and their full forms. The previous approach is extended with abbreviations of full forms occurring in name of metric or dataset. For example, with a pair (en-vi, English-Vietnamese) and dataset name IWSLT2015 English-Vietnamese, en-vi is added as mention evidence for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manually Curated</head><p>We extend the Bag-of-Words approach with list of manually curated mention evidence. Only mentions of datasets and metrics related to speech recognition and semantic segmentation are modified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combined</head><p>The previous approach extended with abbreviations.</p><p>In <ref type="table" target="#tab_7">Table 4</ref> we show Top-1 and Top-5 accuracy of the predictions over all leaderboard records from each collection of papers. Using abbreviations significantly improves the performance over bag-of-words approach. The worse performance caused by adding abbreviations to manually curated lists suggests that abbreviations could increase rate of false-positive matches of mentions. Another explanation is that manually curated lists of mentions could be biased towards leaderbords related to speech recognition and semantic segmentation due to construction of the lists.</p><p>The overall performance of the linking step allows us to use it in production environment for efficient semi-automated extraction of results. Our solution proposes to users the Top-5 predictions associated with cells they pointed, thus eliminating the tedious and errorprone step of matching the results with existing leaderboards and ensuring that metric values are correctly normalized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>We cover three possible extensions to our work for future research. First, we might want to consider methods that retrieve all results rather than just the principal results introduced in the paper. This includes extracting ablation studies to enable search over fine-grained comparison results.</p><p>Secondly, we could look more into automatic taxonomy discovery. Currently, we assume a closed-domain approach with taxonomy of leaderboards known in advance. While manually extending the taxonomy requires only adding the task, dataset and metric names, it becomes problematic to cover large fraction of the papers due to publication rate and long tail of leaderboards.</p><p>Finally, to relax the necessity of AxCell to have access to L A T E X source we consider using the ArxivPapers dataset as a corpus to train extraction working directly with PDF files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We presented an end-to-end model for extracting results from machine learning papers. Our method performs well across various tasks and leaderboards within machine learning, with a taxonomy that can be easily extended without retraining. Additionally we released a new collection of datasets for training and evaluating on the results extraction task. These datasets enable the training of more fine-grained feature extractors and detailed error analysis. We demonstrated that our approach achieves significant performance gains over the state-ofthe-art. Future work may want to build on our approach for more comprehensive extraction tasks, focussing on more types of result, as well as other information contained in papers such as architectural details and hyperparameters.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Mention Probabilities</head><p>Using the methodology from Section 3.4.1, we can calculate P (y k | E) by combining mentions P mention | y (entity) k . We simplify the notation by referring to this conditional distribution as P (m | f ) in this section. This denotes the probability that a mention evidence m for given entity f appears in a particular context of cell containing results for leaderboard with entity f .</p><p>We compute all possible mentions directly from tasks, datasets and metrics names appearing in leaderboards. For a name of dataset or metric the mentions list consists of the whole name as well as each word, without duplicates and English stop words. As tasks names often consist of common words, to limit the number of false positives the mentions list for a given task contains only that task's name.</p><p>We compute probability P (m | f ) assuming all mentions (separately for tasks, datasets and metrics) are distributed uniformly, P (f 1 | m) = P (f 2 | m) for all f 1 , f 2 for which m is a mention evidence. We then use Bayes rule to get P (m | f ), assuming that all mentions of a given type are distributed uniformly. This results in the conditional probability of a mention being inversely proportional to the number of entities having that mention evidence in common:</p><p>P (m | f ) ‚àù 1 |{g : m is mention evidence for g}| .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Confusion matrices of segmenting cells into five classes: dataset (including subdatasets), metric, model introduced in processed paper, competing model and other. Results averaged over 5 runs for each task, using 10-fold training as described in Section 6.2. o t h e r d a t a s e t p a p e r ' s m o d e l c i t e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Confusion matrix of table type classification step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table X :</head><label>X</label><figDesc>Test set evaluation. . .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3) caption classification</cell><cell cols="5">type: leaderboard table</cell><cell>dataset metric</cell><cell>paper's model cited model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>method</cell><cell cols="4">Giga Giga Giga</cell><cell>task</cell><cell>meta</cell></row><row><cell></cell><cell>1) table extraction</cell><cell></cell><cell></cell><cell cols="2">5) table segmentation</cell><cell></cell><cell cols="2">method . . . TPG-2 [8] 43.4 R-1 . . . NMT-1 47.6 NMT-2 48.2</cell><cell>R-2</cell><cell>. . . . . . . . . . . .</cell><cell>R-L</cell><cell>6) contexts generation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4) mention</cell><cell></cell><cell></cell><cell cols="3">5) table</cell><cell>cell8,2</cell><cell>cell9,2</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lookup</cell><cell></cell><cell></cell><cell cols="4">segmentation</cell><cell>model value</cell><cell>NMT-1 47.6</cell><cell>NMT-2 48.2</cell></row><row><cell></cell><cell>2) text extraction</cell><cell></cell><cell></cell><cell></cell><cell cols="7">Results on Giga Word dataset show. . . On average R-L is 2% higher. . . Compared to NMT-1 the bigger. . . The TPG-2 model introduced in [8]. . . . . .</cell><cell>table ctx. Giga, R-1 Giga, R-1 desc. ctx. test abs. ctx. translation, summarization . . . 7) linking &amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">6) contexts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>normalization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">generation</cell><cell></cell><cell cols="2">cell model</cell><cell>task</cell><cell></cell><cell>dataset</cell><cell>metric</cell><cell>value score</cell></row><row><cell>model</cell><cell>task</cell><cell>dataset</cell><cell>metric</cell><cell>value</cell><cell cols="2">8) filtering</cell><cell cols="5">8, 2 NMT-1 Summarization 8, 2 NMT-1 Summarization</cell><cell>GigaWord GigaWord</cell><cell>Rouge-1 Rouge-L</cell><cell>47.6 47.6</cell><cell>0.96 0.03</cell></row><row><cell cols="5">NMT-2 Summarization GigaWord Rouge-1 48.2</cell><cell></cell><cell></cell><cell cols="5">8, 2 NMT-1 Langauge Modeling Billion Word Perplexity 47.6</cell><cell>0.001</cell></row><row><cell></cell><cell></cell><cell>. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">9, 2 NMT-2 Summarization</cell><cell>GigaWord</cell><cell>Rouge-1</cell><cell>48.2</cell><cell>0.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. . .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. . . challenging problem . . . speech recognition . . . machine translation . . . language modeling. . . Additionally, we formally prove that . . . perplexity . . . on downstream tasks . . .</figDesc><table><row><cell></cell><cell cols="3">. . . which proves the</cell></row><row><cell></cell><cell cols="3">main theorem.</cell></row><row><cell></cell><cell cols="3">5. Experiments</cell></row><row><cell></cell><cell cols="3">. . . Table 2 presents</cell></row><row><cell></cell><cell cols="2">. . . Workshop</cell><cell>on</cell></row><row><cell></cell><cell cols="2">Statistical</cell><cell>Ma-</cell></row><row><cell></cell><cell>chine</cell><cell cols="2">Translation</cell></row><row><cell></cell><cell cols="3">datasets . . . WMT</cell></row><row><cell></cell><cell>2014</cell><cell>and</cell><cell>WMT</cell></row><row><cell></cell><cell cols="3">2017 . . . In case of</cell></row><row><cell>2. Related work . . . In [124] authors consider self-super-</cell><cell cols="3">English-German dataset . . . IWSLT 2015 . . .</cell></row><row><cell>vised textual dyslex-</cell><cell></cell><cell></cell></row><row><cell>ization task . . .</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table I :</head><label>I</label><figDesc>. . . test set. . . BLEU metric. WMT 2014 . . . . . . en-fr fr-en . . . . . . . . . . . . . . . NMT (ours) 56.3 41.8 . . . Using Context Hierarchy and Evidences for Linking. This figure highlights the context hierarchy, from the global paper to the specific table, the evidence for tasks (blue), datasets (pink) and metrics (violet) for the 56.3 value extracted from cell contexts, and lastly the result from linking. contexts: a caption context, the table caption; a mentions context, text fragments referencing the table</figDesc><table><row><cell>Linking result:</cell></row><row><cell>Task: Machine Translation</cell></row><row><cell>Dataset: WMT2014 English-French Test</cell></row><row><cell>Metric: BLEU score</cell></row><row><cell>Value: 56.3</cell></row><row><cell>Model: NMT</cell></row><row><cell>Confidence: 0.98</cell></row><row><cell>Figure 3:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>End-to-end extraction results on subset of NLP-TDMS (Exp) dataset.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Micro</cell><cell></cell><cell></cell><cell>Macro</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell cols="4">(task, dataset, metric)</cell><cell></cell></row><row><cell cols="7">TDMS-IE 53.4 66.3 59.2 57.1 66.1 58.5</cell></row><row><cell>AxCell</cell><cell cols="6">65.8 58.5 61.9 56.0 55.8 54.1</cell></row><row><cell></cell><cell cols="5">(task, dataset, metric, score)</cell></row><row><cell cols="7">TDMS-IE 6.8 8.4 7.5 8.6 9.5 8.8</cell></row><row><cell>AxCell</cell><cell cols="6">27.4 24.4 25.8 20.2 20.6 19.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Extraction results of AxCell on PWC Leaderboards dataset (restricted to our taxonomy) for entire records (TDMS), records without score (TDM) and individual entities.</figDesc><table><row><cell>Entity</cell><cell></cell><cell>Micro</cell><cell></cell><cell></cell><cell>Macro</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="7">TDMS 37.4 23.2 28.7 24.0 21.8 21.1</cell></row><row><cell>TDM</cell><cell cols="6">67.8 47.8 56.1 47.9 46.4 43.5</cell></row><row><cell>Task</cell><cell cols="6">70.6 57.3 63.3 60.7 62.6 59.7</cell></row><row><cell cols="7">Dataset 70.2 48.4 57.3 53.5 52.7 49.9</cell></row><row><cell cols="7">Metric 68.8 58.5 63.3 58.4 60.4 56.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Table segmentation results for 10-fold training with image classification papers fixed as a validation set and variable test set. Micro precision, recall and F 1 score are averaged over 5 runs.</figDesc><table><row><cell></cell><cell cols="2">validation</cell><cell></cell><cell></cell><cell>test</cell><cell></cell></row><row><cell>test set</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>image gen.</cell><cell cols="6">84.5 87.9 86.2 73.4 81.6 77.3</cell></row><row><cell>misc.</cell><cell cols="6">84.0 88.2 86.0 81.7 93.5 87.2</cell></row><row><cell cols="7">machine trans. 83.1 90.8 86.8 80.5 94.4 86.9</cell></row><row><cell>NLI</cell><cell cols="6">83.6 89.6 86.5 84.5 97.3 90.4</cell></row><row><cell cols="7">object detection 81.9 91.4 86.3 83.7 96.7 89.7</cell></row><row><cell cols="7">pose estimation 85.1 89.9 87.4 86.0 96.8 91.1</cell></row><row><cell>question ans.</cell><cell cols="6">83.6 89.5 86.4 80.4 89.6 84.8</cell></row><row><cell>semantic seg.</cell><cell cols="6">81.4 91.1 86.0 90.2 95.9 92.9</cell></row><row><cell>speech rec.</cell><cell cols="6">84.7 89.8 87.2 67.2 90.7 77.1</cell></row><row><cell>text class.</cell><cell cols="6">83.9 90.4 87.0 74.9 93.3 83.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Linking performance using ground truth annotations of table types and segmentation.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Top-1 Accuracy [%]</cell></row><row><cell>evidence</cell><cell cols="4">speech rec. TDMS T D M TDMS T D M sem. segmentation</cell></row><row><cell>BoW</cell><cell>42</cell><cell>86 45 72</cell><cell>49</cell><cell>95 71 67</cell></row><row><cell>abbrs</cell><cell>56</cell><cell>87 57 74</cell><cell>56</cell><cell>95 79 74</cell></row><row><cell>curated</cell><cell>76</cell><cell>87 77 87</cell><cell>77</cell><cell>95 89 87</cell></row><row><cell>combined</cell><cell>67</cell><cell>87 68 78</cell><cell>72</cell><cell>95 86 85</cell></row><row><cell></cell><cell></cell><cell cols="3">Top-5 Accuracy [%]</cell></row><row><cell>evidence</cell><cell cols="4">speech rec. TDMS T D M TDMS T D M sem. segmentation</cell></row><row><cell>BoW</cell><cell>72</cell><cell>88 73 84</cell><cell>82</cell><cell>99 89 93</cell></row><row><cell>abbrs</cell><cell>76</cell><cell>89 76 84</cell><cell>93</cell><cell>100 94 99</cell></row><row><cell>curated</cell><cell>85</cell><cell>90 85 91</cell><cell>97</cell><cell>99 99 99</cell></row><row><cell>combined</cell><cell>81</cell><cell>89 81 89</cell><cell>97</cell><cell>99 99 99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>ULMFiT architecture and hyperparameters used for table type classification and table segmentation.</figDesc><table><row><cell>vocabulary size</cell><cell>30K</cell></row><row><cell>tokenization</cell><cell>unigram model</cell></row><row><cell>RNN type</cell><cell>LSTM</cell></row><row><cell>recurrent layers</cell><cell>3</cell></row><row><cell>embeddings dimension</cell><cell>400</cell></row><row><cell>hidden state dimension</cell><cell>1152</cell></row><row><cell>pretraining</cell><cell>12 epochs</cell></row><row><cell>batch size</cell><cell>256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Features ForTable Segmentation</figDesc><table><row><cell>Feature</cell><cell>Description</cell><cell></cell></row><row><cell>is emphasised</cell><cell cols="2">whether text in cell is boldfaced, colored,</cell></row><row><cell></cell><cell>etc.</cell><cell></cell></row><row><cell>cell style</cell><cell>e.g. "align-left top-border"</cell><cell></cell></row><row><cell>text</cell><cell cols="2">mentions of cell's content (as in Figure 3)</cell></row><row><cell>cell content</cell><cell cols="2">cell's content without styles and references,</cell></row><row><cell></cell><cell>e.g. "ULMFiT"</cell><cell></cell></row><row><cell>row context</cell><cell>concatenated cell's row, e.g.</cell><cell>"ULMFiT</cell></row><row><cell></cell><cell cols="2">&lt;sep&gt; 94.5% &lt;sep&gt; 92.1% &lt;sep&gt;"</cell></row><row><cell cols="3">column context concatenated cell's column, e.g. "Method</cell></row><row><cell></cell><cell cols="2">&lt;sep&gt; LSTM &lt;sep&gt; GRU &lt;sep&gt; ULMFiT</cell></row><row><cell></cell><cell>&lt;sep&gt; BERT"</cell><cell></cell></row><row><cell>cell reference</cell><cell cols="2">list of reference ids used in cell, e.g. "bib4,</cell></row><row><cell></cell><cell>bib18"</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Statistics of the NLP-TDMS Full and Exp datasets.</figDesc><table><row><cell></cell><cell>Full Exp</cell></row><row><cell cols="2">unique taxonomy entries 168 77</cell></row><row><cell>unique tasks</cell><cell>35 18</cell></row><row><cell>unique datasets</cell><cell>99 44</cell></row><row><cell>unique metrics</cell><cell>72 30</cell></row><row><cell>papers</cell><cell>332 332</cell></row><row><cell>results</cell><cell>848 606</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Statistics for the PWC Leaderboards dataset with all entries (Full) and entries restricted to our taxonomy (Restricted).</figDesc><table><row><cell></cell><cell cols="2">Full Restricted</cell></row><row><cell cols="2">unique taxonomy entries 2295</cell><cell>649</cell></row><row><cell>unique tasks</cell><cell>252</cell><cell>134</cell></row><row><cell>unique datasets</cell><cell>1156</cell><cell>433</cell></row><row><cell>unique metrics</cell><cell>414</cell><cell>162</cell></row><row><cell>papers</cell><cell>733</cell><cell>516</cell></row><row><cell>results</cell><cell>5406</cell><cell>2802</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/paperswithcode/axcell 2 In 2019, over 33,000 machine learning papers were published on the arXiv.org open-access e-print archive, with a year-on-year growth of around 50% since 2015.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.paperswithcode.com/sota 4 http://nlpprogress.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Waleed Ammar, Sebastian Kohlmeier and Iz Beltagy on useful discussion and feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Training Details</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tabvec: Table vectors for classification of web tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Majid Ghasemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">A</forename><surname>Gol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szekely</surname></persName>
		</author>
		<idno>abs/1802.06290</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe≈Ç Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
		</author>
		<title level="m">Tapas: Weakly supervised table parsing via pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Jochim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Bonin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debasis</forename><surname>Ganguly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2019</title>
		<meeting>ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5203" to="5213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">2020. fastai: A layered API for deep learning. Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A framework for information extraction from tables in biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Milosevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassie</forename><surname>Gregson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IJDAR</publisher>
		</imprint>
	</monogr>
	<note>Robert Hernandez, and Goran Nenadic</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The discipline of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Department technical report CMU-ML-06-108</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Scispacy: Fast and robust models for biomedical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Table extraction using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tracking the Progress in Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated early leaderboard generation from comparative tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajdeep</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atharva</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval -41st European Conference on IR Research</title>
		<meeting><address><addrLine>Cologne, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-04-14" />
			<biblScope unit="volume">11437</biblScope>
			<biblScope unit="page" from="244" to="257" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>Seattle. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks. ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Table extraction for answer retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="589" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
