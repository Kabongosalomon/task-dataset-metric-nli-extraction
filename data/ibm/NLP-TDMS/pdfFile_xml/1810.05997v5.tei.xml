<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PREDICT THEN PROPAGATE: GRAPH NEURAL NETWORKS MEET PERSONALIZED PAGERANK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
							<email>klicpera@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
							<email>a.bojchevski@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
							<email>guennemann@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PREDICT THEN PROPAGATE: GRAPH NEURAL NETWORKS MEET PERSONALIZED PAGERANK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online. 1 Increasing the size of the neighborhood used by these algorithms, i.e. their range, is not trivial since neighborhood aggregation in this scheme is essentially a type of Laplacian smoothing and too 1 https://www.kdd.in.tum.de/ppnp 1 arXiv:1810.05997v5 [cs.LG]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are ubiquitous in the real world and its description through scientific models. They are used to study the spread of information, to optimize delivery, to recommend new books, to suggest friends, or to find a party's potential voters. Deep learning approaches have achieved great success on many important graph problems such as link prediction <ref type="bibr" target="#b15">(Grover &amp; Leskovec, 2016;</ref>, graph classification <ref type="bibr" target="#b12">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b31">Niepert et al., 2016;</ref><ref type="bibr" target="#b13">Gilmer et al., 2017)</ref> and semi-supervised node classification <ref type="bibr" target="#b43">(Yang et al., 2016;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2017)</ref>.</p><p>There are many approaches for leveraging deep learning algorithms on graphs. Node embedding methods use random walks or matrix factorization to directly train individual node embeddings, often without using node features and usually in an unsupervised manner, i.e. without leveraging node classes <ref type="bibr" target="#b33">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b40">Tang et al., 2015;</ref><ref type="bibr" target="#b30">Nandanwar &amp; Murty, 2016;</ref><ref type="bibr" target="#b15">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b35">Qiu et al., 2018)</ref>. Many other approaches use both graph structure and node features in a supervised setting. Examples for these include spectral graph convolutional neural networks <ref type="bibr" target="#b6">(Bruna et al., 2014;</ref><ref type="bibr" target="#b11">Defferrard et al., 2016)</ref>, message passing (or neighbor aggregation) algorithms <ref type="bibr" target="#b19">(Kearnes et al., 2016;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017;</ref><ref type="bibr" target="#b34">Pham et al., 2017;</ref><ref type="bibr" target="#b28">Monti et al., 2017;</ref><ref type="bibr" target="#b13">Gilmer et al., 2017)</ref>, and neighbor aggregation via recurrent neural networks <ref type="bibr" target="#b36">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b24">Li et al., 2016;</ref><ref type="bibr" target="#b10">Dai et al., 2018)</ref>. Among these categories, the class of message passing algorithms has garnered particular attention recently due to its flexibility and good performance.</p><p>Several works have been aimed at improving the basic neighborhood aggregation scheme by using attention mechanisms <ref type="bibr" target="#b19">(Kearnes et al., 2016;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017;</ref><ref type="bibr" target="#b41">Veličković et al., 2018)</ref>, random walks <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2018a;</ref><ref type="bibr" target="#b44">Ying et al., 2018;</ref>, edge features <ref type="bibr" target="#b19">(Kearnes et al., 2016;</ref><ref type="bibr" target="#b13">Gilmer et al., 2017;</ref><ref type="bibr" target="#b37">Schlichtkrull et al., 2018)</ref> and making it more scalable on large graphs <ref type="bibr" target="#b44">Ying et al., 2018)</ref>. However, all of these methods only use the information of a very limited neighborhood for each node. A larger neighborhood would be desirable to provide the model with more information, especially for nodes in the periphery or in a sparsely labelled setting. many layers lead to oversmoothing . <ref type="bibr" target="#b42">Xu et al. (2018)</ref> highlighted the same problem by establishing a relationship between the message passing algorithm termed Graph Convolutional Network (GCN) by <ref type="bibr" target="#b21">Kipf &amp; Welling (2017)</ref> and a random walk. Using this relationship we see that GCN converges to this random walk's limit distribution as the number of layers increases. The limit distribution is a property of the graph as a whole and does not take the random walk's starting (root) node into account. As such it is unsuited to describe the root node's neighborhood. Hence, GCN's performance necessarily deteriorates for a high number of layers (or aggregation/propagation steps).</p><p>To solve this issue, in this paper, we first highlight the inherent connection between the limit distribution and PageRank <ref type="bibr" target="#b32">(Page et al., 1998)</ref>. We then propose an algorithm that utilizes a propagation scheme derived from personalized PageRank instead. This algorithm adds a chance of teleporting back to the root node, which ensures that the PageRank score encodes the local neighborhood for every root node <ref type="bibr" target="#b32">(Page et al., 1998)</ref>. The teleport probability allows us to balance the needs of preserving locality (i.e. staying close to the root node to avoid oversmoothing) and leveraging the information from a large neighborhood. We show that this propagation scheme permits the use of far more (in fact, infinitely many) propagation steps without leading to oversmoothing.</p><p>Moreover, while propagation and classification are inherently intertwined in message passing, our proposed algorithm separates the neural network from the propagation scheme. This allows us to achieve a much higher range without changing the neural network, whereas in the message passing scheme every additional propagation step would require an additional layer. It also permits the independent development of the propagation algorithm and the neural network generating predictions from node features. That is, we can combine any state-of-the-art prediction method with our propagation scheme. We even found that adding our propagation scheme during inference significantly improves the accuracy of networks that were trained without using any graph information.</p><p>Our model achieves state-of-the-art results while requiring fewer parameters and less training time compared to most competing models, with a computational complexity that is linear in the number of edges. We show these results in the most thorough study (including significance testing) of message passing models using graphs with text-based features that has been done so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GRAPH CONVOLUTIONAL NETWORKS AND THEIR LIMITED RANGE</head><p>We first introduce our notation and explain the problem our model solves. Let G = (V, E) be a graph with nodes V and edges E. Let n denote the number of nodes and m the number of edges. The nodes are described by the feature matrix X ∈ R n×f , with the number of features f per node, and the class (or label) matrix Y ∈ R n×c , with the number of classes c. The graph G is described by the adjacency matrix A ∈ R n×n .Ã = A + I n denotes the adjacency matrix with added self-loops.</p><p>One simple and widely used message passing algorithm for semi-supervised classification is the Graph Convolutional Network (GCN). In the case of two message passing layers its equation is</p><formula xml:id="formula_0">Z GCN = softmax Â ReLU Â XW 0 W 1 ,<label>(1)</label></formula><p>where Z ∈ R n×c are the predicted node labels,Â =D −1/2ÃD−1/2 is the symmetrically normalized adjacency matrix with self-loops, with the diagonal degree matrixD ij = kÃ ik δ ij , and W 0 and W 1 are trainable weight matrices <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref>.</p><p>With two GCN-layers, only neighbors in the two-hop neighborhood are considered. There are essentially two reasons why a message passing algorithm like GCN cannot be trivially expanded to use a larger neighborhood. First, aggregation by averaging causes oversmoothing if too many layers are used. It, therefore, loses its focus on the local neighborhood . Second, most common aggregation schemes use learnable weight matrices in each layer. Therefore, using a larger neighborhood necessarily increases the depth and number of learnable parameters of the neural network (the second aspect can be circumvented by using weight sharing, which is typically not the case, though). However, the required neighborhood size and neural network depth are two completely orthogonal aspects. This fixed relationship is a strong limitation and leads to bad compromises.</p><p>We will start by concentrating on the first issue. <ref type="bibr" target="#b42">Xu et al. (2018)</ref> have shown that for a klayer GCN the influence score of node x on y, I(x, y) = i j ∂Zyi ∂Xxj , is proportional in expectation to a slightly modified k-step random walk distribution starting at the root node x, P rw' (x → y, k). Hence, the information of node x spreads to node y in a random walk-like manner. If we take the limit k → ∞ and the graph is irreducible and aperiodic, this random walk probability distribution P rw' (x → y, k) converges to the limit (or stationary) distribution P lim (→ y). This distribution can be obtained by solving the equation π lim =Âπ lim . Obviously, the result only depends on the graph as a whole and is independent of the random walk's starting (root) node x. This global property is therefore unsuitable for describing the root node's neighborhood.</p><formula xml:id="formula_1">h 1 h 2 h 3 h 4 h 5 h 6 α α α h i z i Personalized PageRank Neural network f θ x i Prediction x i h i = f θ (x i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PERSONALIZED PROPAGATION OF NEURAL PREDICTIONS</head><p>From message passing to personalized PageRank. We can solve the problem of lost focus by recognizing the connection between the limit distribution and PageRank <ref type="bibr" target="#b32">(Page et al., 1998)</ref>. The only differences between these two are the added self-loops and the adjacency matrix normalization inÂ. Original PageRank is calculated via π pr = A rw π pr , with A rw = AD −1 . Having made this connection we can now consider using a variant of PageRank that takes the root node into account -personalized PageRank <ref type="bibr" target="#b32">(Page et al., 1998)</ref>. We define the root node x via the teleport vector i x , which is a one-hot indicator vector. Our adaptation of personalized PageRank can be obtained for node x using the recurrent equation π ppr (i x ) = (1−α)Âπ ppr (i x )+αi x , with the teleport (or restart) probability α ∈ (0, 1]. By solving this equation, we obtain</p><formula xml:id="formula_2">π ppr (i x ) = α I n − (1 − α)Â −1 i x .<label>(2)</label></formula><p>Introducing the teleport vector i x allows us to preserve the node's local neighborhood even in the limit distribution. In this model the influence score of root node x on node y, I(x, y), is proportional to the y-th element of our personalized PageRank π ppr (i x ). This value is different for every root node. How fast it decreases as we move away from the root node can be adjusted via α. By substituting the indicator vector i x with the unit matrix I n we obtain our fully personalized PageRank matrix Π ppr = α(I n − (1 − α)Â) −1 , whose element (yx) specifies the influence score of node x on node y, I(x, y) ∝ Π ppr , i.e. the influence of x on y is equal to the influence of y on x. This inverse always exists since 1 1−α &gt; 1 and therefore cannot be an eigenvalue ofÂ (see Appendix A).</p><p>Personalized propagation of neural predictions (PPNP). To utilize the above influence scores for semi-supervised classification we generate predictions for each node based on its own features and then propagate them via our fully personalized PageRank scheme to generate the final predictions. This is the foundation of personalized propagation of neural predictions. PPNP's model equation is</p><formula xml:id="formula_3">Z PPNP = softmax α I n − (1 − α)Â −1 H , H i,: = f θ (X i,: ),<label>(3)</label></formula><p>Published as a conference paper at ICLR 2019 where X is the feature matrix and f θ a neural network with parameter set θ generating the predictions H ∈ R n×c . Note that f θ operates on each node's features independently, allowing for parallelization. Furthermore, one could substituteÂ with any propagation matrix, such as A rw .</p><p>As a consequence, PPNP separates the neural network used for generating predictions from the propagation scheme. This separation additionally solves the second issue mentioned above: the depth of the neural network is now fully independent of the propagation algorithm. As we saw when connecting GCN to PageRank, personalized PageRank can effectively use even infinitely many neighborhood aggregation layers, which is clearly not possible in the classical message passing framework. Furthermore, the separation gives us the flexibility to use any method for generating predictions, e.g. deep convolutional neural networks for graphs of images.</p><p>While generating predictions and propagating them happen consecutively during inference, it is important to note that the model is trained end-to-end. That is, the gradient flows through the propagation scheme during backpropagation (implicitly considering infinitely many neighborhood aggregation layers). Adding these propagation effects significantly improves the model's accuracy.</p><p>Efficiency analysis. Directly calculating the fully personalized PageRank matrix Π ppr , is computationally inefficient and results in a dense R n×n matrix. Using this matrix would lead to a computational complexity and memory requirement of O(n 2 ) for training and inference.</p><p>To solve this issue, reconsider the equation</p><formula xml:id="formula_4">Z = α(I n − (1 − α)Â) −1 H.</formula><p>Instead of viewing this equation as a combination of a dense fully personalized PageRank matrix with the prediction matrix, we can also view it as a variant of topic-sensitive PageRank, with each class corresponding to one topic <ref type="bibr" target="#b17">(Haveliwala, 2002)</ref>. In this view every column of H defines an (unnormalized) distribution over nodes that acts as a teleport set. Hence, we can approximate PPNP via an approximate computation of topic-sensitive PageRank.</p><p>Approximate personalized propagation of neural predictions (APPNP). More precisely, APPNP achieves linear computational complexity by approximating topic-sensitive PageRank via power iteration. While PageRank's power iteration is connected to the regular random walk, the power iteration of topic-sensitive PageRank is related to a random walk with restarts. Each power iteration (random walk/propagation) step of our topic-sensitive PageRank variant is, thus, calculated via</p><formula xml:id="formula_5">Z (0) = H = f θ (X), Z (k+1) = (1 − α)ÂZ (k) + αH, Z (K) = softmax (1 − α)ÂZ (K−1) + αH ,<label>(4)</label></formula><p>where the prediction matrix H acts as both the starting vector and the teleport set, K defines the number of power iteration steps and k ∈ [0, K −2]. Note that this method retains the graph's sparsity and never constructs an R n×n matrix. The convergence of this iterative scheme can be shown by investigating the resulting series (see Appendix B).</p><p>Note that the propagation scheme of this model does not require any additional parameters to train -as opposed to models like GCN, which typically require more parameters for each additional propagation layer. We can therefore propagate very far with very few parameters. Our experiments show that this ability is indeed very beneficial (see Section 6). A similar model expressed in the message passing framework would therefore not be able to achieve the same level of performance.</p><p>The reformulation of PPNP via fixed-point iterations illustrates a connection to the original graph neural network (GNN) model <ref type="bibr" target="#b36">(Scarselli et al., 2009</ref>). While the latter uses a learned fixed-point iteration, our approach uses a predetermined iteration (adapted personalized PageRank) and applies a learned feature transformation before propagation.</p><p>In both PPNP and APPNP, the size of the neighborhood influencing each node can be adjusted via the teleport probability α. The freedom to choose α allows us to adjust the model for different types of networks, since varying graph types require the consideration of different neighborhood sizes, as shown in Section 6 and described by <ref type="bibr" target="#b15">Grover &amp; Leskovec (2016)</ref> and Abu-El-Haija et al. (2018b). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Several works have tried to improve the training of message passing algorithms and increase the neighborhood available at each node by adding skip connections <ref type="bibr" target="#b24">(Li et al., 2016;</ref><ref type="bibr" target="#b34">Pham et al., 2017;</ref><ref type="bibr" target="#b16">Hamilton et al., 2017;</ref><ref type="bibr" target="#b44">Ying et al., 2018)</ref>. One recent approach combined skip connection with aggregation schemes <ref type="bibr" target="#b42">(Xu et al., 2018)</ref>. However, the range of these models is still limited, as apparent in the low number of message passing layers used. While it is possible to add skip connections in the neural network used by our algorithm, this would not influence the propagation scheme. Our approach to solving the range problem is therefore unrelated to these models.  facilitated training by combining message passing with co-and self-training. The improvements achieved by this combination are similar to results reported with other semi-supervised classification models <ref type="bibr" target="#b7">(Buchnik &amp; Cohen, 2018)</ref>. Note that most algorithms, including ours, can be improved using self-and co-training. However, each additional step used by these methods corresponds to a full training cycle and therefore significantly increases the training time.</p><p>Deep GNNs that avoid the oversmoothing issue have been proposed in recent works by combining residual (skip) connections with batch normalization <ref type="bibr" target="#b18">(Kawamoto et al., 2018;</ref><ref type="bibr" target="#b9">Chen et al., 2019)</ref>. However, our model solves this issue by simplifying the architecture via decoupling prediction and propagation and does not rely on ad-hoc techniques that further complicate the model and introduce additional hyperparameters. Furthermore, since PPNP increases the range without introducing additional layers and parameters it is easier and faster to train compared to a deep GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL SETUP</head><p>Recently, many experimental evaluations have suffered from superficial statistical evaluation and experimental bias from using varying training setups and overfitting. The latter is caused by experiments using a single training/validation/test split, by not distinguishing clearly between the validation and test set, and by finetuning hyperparameters to each dataset or even data split separately. Message-passing algorithms are very sensitive to both data splits and weight initialization (as clearly shown by our evaluation). Thus, a carefully designed evaluation protocol is extremely important. Our work aims to establish such a thorough evaluation protocol. First, we run each experiment 100 times on multiple random splits and initializations. Second, we split the data into a visible and a test set, which do not change. The test set was only used once to report the final performance; and in particular, has never been used to perform hyperparameter and model selection. To further prevent overfitting we use the same number of layers and hidden units, dropout rate d, L 2 regularization parameter λ, and learning rate l across datasets, since all datasets use bag-of-words as features. To prevent experimental bias we optimized the hyperparameters of all models individually using a grid search on CITESEER and CORA-ML and use the same early stopping criterion across models.</p><p>Finally, to ensure the statistical robustness of our experimental setup, we calculate confidence intervals via bootstrapping and report the p-values of a paired t-test for our main claims. To our knowledge, this is the most rigorous study on GCN-like models which has been done so far. More details about the experimental setup are provided in Appendix C.</p><p>Datasets. We use four text-classification datasets for evaluation. CITESEER <ref type="bibr" target="#b38">(Sen et al., 2008)</ref>, <ref type="bibr">CORA-ML (McCallum et al., 2000;</ref> and PUBMED <ref type="bibr" target="#b29">(Namata et al., 2012)</ref> are citation graphs, where each node represents a paper and the edges represent citations between them. In the MICROSOFT ACADEMIC graph  edges represent coauthorship. We use the largest connected component of each graph. All graphs use a bag-of-words representation of the papers' abstracts as features. While large graphs do not necessarily have a  larger diameter <ref type="bibr" target="#b22">(Leskovec et al., 2005)</ref>, note that these graphs indeed have average shortest path lengths between 5 and 10 and therefore a regular two-layer GCN cannot cover the entire graph. <ref type="table" target="#tab_0">Table 1</ref> reports the dataset statistics.</p><p>Baseline models. We compare to five state-of-the-art models: GCN (Kipf &amp; Welling, 2017), network of GCNs (N-GCN) (Abu-El-Haija et al., 2018a), graph attention networks (GAT) <ref type="bibr" target="#b41">(Veličković et al., 2018)</ref>, bootstrapped feature propagation (bt. FP) <ref type="bibr" target="#b7">(Buchnik &amp; Cohen, 2018)</ref> and jumping knowledge networks with concatenation (JK) <ref type="bibr" target="#b42">(Xu et al., 2018)</ref>. For GCN we also show the results of the (unoptimized) vanilla version (V. GCN) to demonstrate the strong impact of early stopping and hyperparameter optimization. The hyperparameters of all models are listed in Appendix D.</p><p>Model hyperparameters. To ensure a fair model comparison we used a neural network for PPNP that is structurally very similar to GCN and has the same number of parameters. We use two layers with h = 64 hidden units. We apply L 2 regularization with λ = 0.005 on the weights of the first layer and use dropout with dropout rate d = 0.5 on both layers and the adjacency matrix. For APPNP, adjacency dropout is resampled for each power iteration step. For propagation we use the teleport probability α = 0.1 and K = 10 power iteration steps for APPNP. We use α = 0.2 on the MICROSOFT ACADEMIC graph due to its structural difference (see <ref type="figure">Figure 5</ref> and its discussion). The combination of this shallow neural network with a comparatively high number of power iteration steps achieved the best results during hyperparameter optimization (see Appendix G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>Overall accuracy. The results for the accuracy (micro F1-score) are summarized in <ref type="table" target="#tab_1">Table 2</ref>. Similar trends are observed for the macro F1-score (see Appendix E). Both models significantly outperform the state-of-the-art baseline models on all datasets. Our rigorous setup might understate the improvements achieved by PPNP and APPNP -this result is statistically significant p &lt; 0.05, as tested via a paired t-test (see Appendix F). This thorough setup furthermore shows that the advantages reported by recent works practically vanish when training is harmonized, hyperparameters are properly op-  timized and multiple data splits are considered. A simple GCN with optimized hyperparameters outperforms several recently proposed models on our setup. <ref type="figure" target="#fig_2">Figure 2</ref> shows how broad the accuracy distribution of each model is. This is caused by both random initialization and different data splits (train / early stopping / test). This demonstrates how crucial a statistically rigorous evaluation is for a conclusive model comparison. Moreover, it shows the sensitivity (robustness) of each method, e.g. PPNP, APPNP and GAT typically have lower variance.</p><p>Training time per epoch. We report the average training time per epoch in <ref type="table" target="#tab_2">Table 3</ref>. We decided to only compare the training time per epoch since all hyperparameters were solely optimized for accuracy and the used early stopping criterion is very generous. Obviously, (exact) PPNP can only be applied to moderately sized graphs, while APPNP scales to large data. On average, APPNP is around 25 % slower than GCN due to its higher number of matrix multiplications. It scales similarly with graph size as GCN and is therefore significantly faster than other more sophisticated models like GAT. This is observed even though our implementation improved GAT's training time roughly by a factor of 2 compared to the reference implementation.</p><p>Training set size. Since the labeling rate is often very small for real world datasets, investigating how the models perform with a small number of training samples is very important. <ref type="figure" target="#fig_3">Figure 3</ref> shows how the number of training nodes per class n train, per class impacts the accuracy on CORA-ML (for other datasets see Appendix H). The dominance of PPNP and APPNP increases further in this sparsely labelled setting. This can be attributed to their higher range, which allows them to better propagate the information further away from the (few) training nodes. We see further evidence for this when comparing the accuracy of APPNP and GCN depending on the distance between a node and the training set (in terms of shortest path). Appendix I shows that the performance gap between APPNP and GCN tends to increase for nodes that are far away from the training nodes. That is, nodes further away from the training set benefit more from the increase in range. Number of power iteration steps. <ref type="figure">Figure 4</ref> shows how the accuracy depends on the number of power iterations for two different propagation schemes. The first mimics the standard propagation as known from GCNs (i.e. α = 0 in APPNP). As clearly shown the performance breaks down as we increase the number of power iterations K (since we approach the global PageRank solution). However, when using personalized propagation (with α = 0.1) the accuracy increases and converges to exact PPNP with infinitely many propagation steps, thus demonstrating the personalized propagation principle is indeed beneficial. As also shown in the figure, it is enough to use a moderate number of power iterations (e.g. K = 10) to effectively approximate exact PPNP. Interestingly, we've found that this number coincides with the highest shortest path distance of any node to the training set.</p><p>Teleport probability α. <ref type="figure">Figure 5</ref> shows the effect of the hyperparameter α on the accuracy on the validation set. While the optimum differs slightly for every dataset, we consistently found a teleport probability of around α ∈ [0.05, 0.2] to perform best. This probability should be adjusted for the dataset under investigation, since different graphs exhibit different neighborhood structures <ref type="bibr" target="#b15">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b1">Abu-El-Haija et al., 2018b)</ref>. Note that a higher α improves convergence speed.</p><p>Neural network without propagation. PPNP and APPNP are trained end-to-end, with the propagation scheme affecting (i) the neural network f θ during training, and (ii) the classification decision during inference. Investigating how the model performs without propagation shows if and how valuable this addition is. <ref type="figure">Figure 6</ref> shows how propagation affects both training and inference. "Never" denotes the case where no propagation is used; essentially we train and apply a standard multilayer perceptron (MLP) f θ using the features only. "Training" denotes the case where we use APPNP during training to learn f θ ; at inference time, however, only f θ is used to predict the class labels. "Inference", in contrast, denotes the case where f θ is trained without APPNP (i.e. standard MLP on features). This pretrained network with fixed weights is then used with APPNP's propagation for inference. Finally, "Inf. &amp; Training" denotes the regular APPNP, which always uses propagation.</p><p>The best results are achieved with regular APPNP, which validates our approach. However, on most datasets the accuracy decreases surprisingly little when propagating only during inference. Skipping propagation during training can significantly reduce training time for large graphs as all nodes can be handled independently. This also shows that our model can be combined with pretrained neural networks that do not incorporate any graph information and still significantly improve their accuracy. Moreover, <ref type="figure">Figure 6</ref> shows that just propagating during training can also lead to large improvements. This indicates that our model can also be applied to online/inductive learning where only the features and not the neighborhood information of an incoming (previously unobserved) node are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper we have introduced personalized propagation of neural predictions (PPNP) and its fast approximation, APPNP. We derived this model by considering the relationship between GCN and PageRank and extending it to personalized PageRank. This simple model decouples prediction and propagation and solves the limited range problem inherent in many message passing models without introducing any additional parameters. It uses the information from a large, adjustable (via the teleport probability α) neighborhood for classifying each node. The model is computationally efficient and outperforms several state-of-the-art methods for semi-supervised classification on multiple graphs in the most thorough study which has been done for GCN-like models so far.</p><p>For future work it would be interesting to combine PPNP with more complex neural networks used e.g. in computer vision or natural language processing. Furthermore, faster or incremental approximations of personalized PageRank <ref type="bibr" target="#b2">(Bahmani et al., 2010;</ref><ref type="bibr" target="#b3">2011;</ref><ref type="bibr" target="#b25">Lofgren et al., 2014)</ref> and more sophisticated propagation schemes would also benefit the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CONVERGENCE OF APPNP</head><p>APPNP uses the iterative equation</p><formula xml:id="formula_6">Z (k+1) = (1 − α)ÂZ (k) + αH.<label>(6)</label></formula><p>After the k-th propagation step, the resulting predictions are</p><formula xml:id="formula_7">Z (k) = (1 − α) kÂ k + α k−1 i=0 (1 − α) iÂ i H.<label>(7)</label></formula><p>If we take the limit k → ∞ the left term tends to 0 and the right term becomes a geometric series. The series converges since α ∈ (0, 1] andÂ is symmetrically normalized and therefore det(Â) ≤ 1, resulting in The sampling procedure is illustrated in <ref type="figure" target="#fig_5">Figure 7</ref>. The data is first split into a visible and a test set. For the visible set 1500 nodes were sampled for the citation graphs and 5000 for MICROSOFT ACADEMIC. The test set contains all remaining nodes. We use three different label sets in each experiment: A training set of 20 nodes per class, an early stopping set of 500 nodes and either a validation or test set. The validation set contains the remaining nodes of the visible set. We use 20 random seeds for determining the splits. These seeds are drawn once and fixed across runs to facilitate comparisons. We use one set of seeds for the validation splits and a different set for the test splits. Each experiment is run with 5 random initializations on each data split, leading to a total of 100 runs per experiment.</p><formula xml:id="formula_8">Z (∞) = α I n − (1 − α)Â −1 H,<label>(8)</label></formula><p>The early stopping criterion uses a patience of p = 100 and an (unreachably high) maximum of n = 10 000 epochs. The patience is reset whenever the accuracy increases or the loss decreases on the early stopping set. We choose the parameter set achieving the highest accuracy and break ties by selecting the lowest loss on this set. This criterion was inspired by GAT <ref type="bibr" target="#b41">(Veličković et al., 2018)</ref>.</p><p>We used TensorFlow (Martín <ref type="bibr" target="#b26">Abadi et al., 2015)</ref> for all experiments except bootstrapped feature propagation. All uncertainties and confidence intervals correspond to a confidence level of 95 % and were calculated by bootstrapping with 1000 samples.</p><p>We use the Adam optimizer with a learning rate of l = 0.01 and cross-entropy loss for all models <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref>. Weights are initialized as described in <ref type="bibr" target="#b14">Glorot &amp; Bengio (2010)</ref>. The feature matrix is L 1 normalized per row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D BASELINE HYPERPARAMETERS</head><p>Vanilla GCN uses the original settings of two layers with h = 16 hidden units, no dropout on the adjacency matrix, L 2 regularization parameter λ = 5 × 10 −4 and the original early stopping with a maximum of 200 steps and a patience of 10 steps based on the loss.</p><p>The optimized GCN uses two layers with h = 64 hidden units, dropout on the adjacency matrix with d = 0.5 and L 2 regularization parameter λ = 0.02.</p><p>N-GCN uses h = 16 hidden units, R = 4 heads per random walk length and random walks of up to K − 1 = 4 steps. It uses L 2 regularization on all layers with λ = 1 × 10 −5 and the attention variant for merging the predictions <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2018a)</ref>. Note that this model effectively uses RKh = 320 hidden units, which is 5 times as many units compared to GCN, GAT, and PPNP.</p><p>For GAT we use the (well optimized) original hyperparameters, except the L 2 regularization parameter λ = 0.001 and learning rate l = 0.01. As opposed to the original paper, we do not use different hyperparameters on PUBMED, as described in our experimental setup.</p><p>Bootstrapped feature propagation uses a return probability of α = 0.2, 10 propagation steps, 10 bootstrapping (self-training) steps with r = 0.1n training nodes added per step. We add the training nodes with the lowest entropy on the predictions. The number of nodes added per class is based on the class proportions estimated using the predictions. Note that this model does not include any stochasticity in its initialization. We therefore only run it once per train/early stopping/test split.</p><p>For the jumping knowledge networks we use the concatenation variant with three layers and h = 64 hidden units per layer. We apply L 2 regularization with λ = 0.001 on all layers and perform dropout with d = 0.5 on all layers but not on the adjacency matrix. F PAIRED t-TEST   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E F1 SCORE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS Academic</head><p>Figure 13: ∆ Accuracy (%) denotes the average improvement in percentage points of APPNP over GCN depending on the distance (number of hops) from the training nodes on different graphs.n denotes the number of nodes at each distance (averaged over multiple dataset splits). Note that the y-axis has a different scale per graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of (approximate) personalized propagation of neural predictions (PPNP, APPNP). Predictions are first generated from each node's own features by a neural network and then propagated using an adaptation of personalized PageRank. The model is trained end-to-end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy distributions of different models. The high standard deviation between data splits and initializations shows the importance of a rigorous evaluation, which is often omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy for different training set sizes (number of labeled nodes per class) on CORA-ML. PPNP's dominance increases further for smaller training set sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Accuracy depending on the number of propagation steps K. The accuracy breaks down for the GCN-like propagation (α = 0), while it increases and stabilizes when using APPNP (α = 0.1). Accuracy depending on teleport probability α. The optimum typically lies within α ∈ [0.05, 0.2], but changes for different types of datasets. Accuracy of APPNP with propagation used only during training/inference. Best results are achieved with full propagation, but propagating only during inference also achieves good results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>which is the equation for calculating (exact) PPNP. Illustration of the node sampling procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 12 :</head><label>812</label><figDesc>Validation accuracy of APPNP for varying numbers of neural network (NN) layers. Deep NNs do not improve the accuracy, which is probably due to the simple bag-of-words features and the small training set size. ∆ Accuracy (%) denotes the average improvement in percentage points of APPNP over GCN depending on the distance (number of hops) from the training nodes on CORA-ML.n denotes the average number of nodes at each distance. The improvement increases with distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. Shortest path length is denoted by SP.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell cols="6">Classes Features Nodes Edges Label rate Avg. SP</cell></row><row><cell>CITESEER</cell><cell>Citation</cell><cell>6</cell><cell>3703</cell><cell>2110</cell><cell>3668</cell><cell>0.036</cell><cell>9.31</cell></row><row><cell>CORA-ML</cell><cell>Citation</cell><cell>7</cell><cell>2879</cell><cell>2810</cell><cell>7981</cell><cell>0.047</cell><cell>5.27</cell></row><row><cell>PUBMED</cell><cell>Citation</cell><cell>3</cell><cell cols="3">500 19 717 44 324</cell><cell>0.003</cell><cell>6.34</cell></row><row><cell cols="2">MS ACADEMIC Co-author</cell><cell>15</cell><cell cols="3">6805 18 333 81 894</cell><cell>0.016</cell><cell>5.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average accuracy with uncertainties showing the 95 % confidence level calculated by bootstrapping. Previously reported improvements vanish on our rigorous experimental setup, while PPNP and APPNP significantly outperform the compared models on all datasets.</figDesc><table><row><cell>Model</cell><cell>CITESEER</cell><cell>CORA-ML</cell><cell>PUBMED</cell><cell>MS ACADEMIC</cell></row><row><cell>V. GCN</cell><cell>73.51 ± 0.48</cell><cell>82.30 ± 0.34</cell><cell>77.65 ± 0.40</cell><cell>91.65 ± 0.09</cell></row><row><cell>GCN</cell><cell>75.40 ± 0.30</cell><cell>83.41 ± 0.39</cell><cell>78.68 ± 0.38</cell><cell>92.10 ± 0.08</cell></row><row><cell>N-GCN</cell><cell>74.25 ± 0.40</cell><cell>82.25 ± 0.30</cell><cell>77.43 ± 0.42</cell><cell>92.86 ± 0.11</cell></row><row><cell>GAT</cell><cell>75.39 ± 0.27</cell><cell>84.37 ± 0.24</cell><cell>77.76 ± 0.44</cell><cell>91.22 ± 0.07</cell></row><row><cell>JK</cell><cell>73.03 ± 0.47</cell><cell>82.69 ± 0.35</cell><cell>77.88 ± 0.38</cell><cell>91.71 ± 0.10</cell></row><row><cell>Bt. FP</cell><cell>73.55 ± 0.57</cell><cell>80.84 ± 0.97</cell><cell>72.94 ± 1.00</cell><cell>91.61 ± 0.24</cell></row><row><cell>PPNP  *</cell><cell cols="2">75.83 ± 0.27 85.29 ± 0.25</cell><cell>-</cell><cell>-</cell></row><row><cell>APPNP</cell><cell>75.73 ± 0.30</cell><cell>85.09 ± 0.25</cell><cell cols="2">79.73 ± 0.31 93.27 ± 0.08</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* out of memory on PUBMED, MS ACADEMIC (see efficiency analysis in Section 3)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Average training time per epoch. PPNP and APPNP are only slightly slower than GCN and much faster than more sophisticated methods like GAT.</figDesc><table><row><cell>Graph</cell><cell>V. GCN</cell><cell>GCN</cell><cell cols="2">N-GCN</cell><cell>GAT</cell><cell>JK</cell><cell cols="2">Bt. FP  *  PPNP  *  *</cell><cell>APPNP</cell></row><row><cell>CITESEER</cell><cell cols="6">37.6 ms 35.3 ms 115.9 ms 187.0 ms 57.5 ms</cell><cell>-</cell><cell>49.2 ms</cell><cell>43.3 ms</cell></row><row><cell>CORA-ML</cell><cell cols="6">32.4 ms 36.5 ms 118.9 ms 217.4 ms 43.6 ms</cell><cell>-</cell><cell>55.3 ms</cell><cell>42.7 ms</cell></row><row><cell>PUBMED</cell><cell cols="6">48.6 ms 48.3 ms 342.6 ms 1029.8 ms 77.8 ms</cell><cell>-</cell><cell>-</cell><cell>64.1 ms</cell></row><row><cell cols="7">MS ACADEMIC 45.5 ms 39.2 ms 328.5 ms 772.2 ms 61.9 ms</cell><cell>-</cell><cell>-</cell><cell>59.8 ms</cell></row><row><cell cols="4">*  not applicable, since core method not trainable</cell><cell cols="5">*  *  out of memory on PUBMED, MS ACADEMIC (see efficiency analysis in Section 3)</cell></row><row><cell></cell><cell>5</cell><cell>10</cell><cell></cell><cell></cell><cell>20</cell><cell>30</cell><cell></cell><cell>40</cell><cell>60</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">n train, per class</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Average macro F1 score with uncertainties showing the 95 % confidence level calculated by bootstrapping. PPNP achieves the highest F1 score on all datasets investigated.</figDesc><table><row><cell>Model</cell><cell>CITESEER</cell><cell>CORA-ML</cell><cell>PUBMED</cell><cell>MS ACADEMIC</cell></row><row><cell>V. GCN</cell><cell>0.7002 ± 0.0043</cell><cell>0.8205 ± 0.0027</cell><cell>0.7801 ± 0.0038</cell><cell>0.9000 ± 0.0008</cell></row><row><cell>GCN</cell><cell>0.7065 ± 0.0037</cell><cell>0.8289 ± 0.0030</cell><cell>0.7883 ± 0.0032</cell><cell>0.9045 ± 0.0008</cell></row><row><cell>N-GCN</cell><cell>0.7021 ± 0.0035</cell><cell>0.8183 ± 0.0024</cell><cell>0.7773 ± 0.0040</cell><cell>0.9144 ± 0.0012</cell></row><row><cell>GAT</cell><cell>0.7062 ± 0.0029</cell><cell>0.8359 ± 0.0025</cell><cell>0.7777 ± 0.0040</cell><cell>0.8917 ± 0.0007</cell></row><row><cell>JK</cell><cell>0.6914 ± 0.0043</cell><cell>0.8202 ± 0.0026</cell><cell>0.7799 ± 0.0039</cell><cell>0.8985 ± 0.0012</cell></row><row><cell>Bt. FP</cell><cell>0.6789 ± 0.0055</cell><cell>0.8026 ± 0.0082</cell><cell>0.7448 ± 0.0079</cell><cell>0.8997 ± 0.0018</cell></row><row><cell>PPNP  *</cell><cell>0.7102 ± 0.0041</cell><cell>0.8454 ± 0.0021</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">APPNP 0.7105 ± 0.0038</cell><cell>0.8429 ± 0.0022</cell><cell cols="2">0.7966 ± 0.0031 0.9184 ± 0.0009</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* out of memory on PUBMED, MS ACADEMIC (see efficiency analysis in Section 3)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>: p-value of the paired t-test with respect to accuracy. × 10 −3 5.96 × 10 −14 --APPNP 1.77 × 10 −2 4.27 × 10 −9 2.19 × 10 −15 5.93 × 10 −13</figDesc><table><row><cell>Model</cell><cell>CITESEER</cell><cell>CORA-ML</cell><cell>PUBMED</cell><cell>MS ACADEMIC</cell></row><row><cell>PPNP</cell><cell>1.02</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>: p-value of the paired t-test with respect to F1 score. × 10 −2 4.50 × 10 −14 --APPNP 2.32 × 10 −2 1.07 × 10 −8 8.70 × 10 −14 1.99 × 10 −8 G NUMBER OF NEURAL NETWORK LAYERS</figDesc><table><row><cell>Model</cell><cell>CITESEER</cell><cell>CORA-ML</cell><cell>PUBMED</cell><cell>MS ACADEMIC</cell></row><row><cell>PPNP</cell><cell>4.49</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This research was supported by the German Research Foundation, grant GU 1409/2-1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A EXISTENCE OF Π PPR The matrix</p><p>exists iff the determinant det(I n − (1 − α)Â) = 0, which is the case iff det(Â − 1 1−α I n ) = 0, i.e. iff 1 1−α is not an eigenvalue ofÂ. This value is always larger than 1 since the teleport probability α ∈ (0, 1]. Furthermore, the symmetrically normalized matrixÂ has the same eigenvalues as the row-stochastic matrixÃ rw . This can be shown by multiplying the eigenvalue equationÂv = λv withD −1/2 from left and substituting w =D −1/2 v. This also shows that the eigenvectors ofÂ are the eigenvectors ofÃ rw scaled byD 1/2 . The largest eigenvalue of a row-stochastic matrix is 1, as can be proven using the Gershgorin circle theorem. Hence, 1 1−α cannot be an eigenvalue and Π ppr always exists.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-scale Graph Convolution for Semi-supervised Node Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. N-Gcn</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Mining and Learning with Graphs (MLG)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Watch Your Step: Learning Node Embeddings via Graph Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast Incremental and Personalized PageRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahman</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdur</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>VLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast Personalized PageRank on MapReduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahman</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NetGAN: Generating Graphs via Random Walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral Networks and Deep Locally Connected Networks on Graphs. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bootstrapped Graph Diffusions: Exposing the Power of Nonlinearity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliav</forename><surname>Buchnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edith</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Measurement and Analysis of Computing Systems (POMACS)</title>
		<meeting>the ACM on Measurement and Analysis of Computing Systems (POMACS)</meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised Community Detection with Line Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Steady-States of Iterative Algorithms over Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Topic-sensitive PageRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mean-field theory of graph neural networks in graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuro</forename><surname>Kawamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Tsubaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Obuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graphs over Time: Densification Laws, Shrinking Diameters and Possible Explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper Insights Into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FAST-PPR: scaling personalized pagerank estimation for large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lofgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seshadhri</forename><surname>Comandur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg, Dandelion Mané</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</orgName>
		</respStmt>
	</monogr>
	<note>Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Query-driven Active Surveying for Collective Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Mining and Learning with Graphs (MLG)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structural Neighborhood Based Classification of Nodes in a Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Nandanwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DeepWalk: online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Column Networks for Collective Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Semantic Web Conference (ESWC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Representation Learning Workshop</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph Attention Networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Graph Convolutional Neural Networks for Web-Scale Recommender Systems. KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
