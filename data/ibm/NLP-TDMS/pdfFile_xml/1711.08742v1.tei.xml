<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating Missing Data in Temporal Data Streams Using Multi-directional Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Zame</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
						</author>
						<title level="a" type="main">Estimating Missing Data in Temporal Data Streams Using Multi-directional Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Missing Data</term>
					<term>Temporal Data Streams</term>
					<term>Imputation</term>
					<term>Recurrent Neural Nets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Missing data is a ubiquitous problem. It is especially challenging in medical settings because many streams of measurements are collected at different -and often irregular -times. Accurate estimation of those missing measurements is critical for many reasons, including diagnosis, prognosis and treatment. Existing methods address this estimation problem by interpolating within data streams or imputing across data streams (both of which ignore important information) or ignoring the temporal aspect of the data and imposing strong assumptions about the nature of the data-generating process and/or the pattern of missing data (both of which are especially problematic for medical data). We propose a new approach, based on a novel deep learning architecture that we call a Multi-directional Recurrent Neural Network (M-RNN) that interpolates within data streams and imputes across data streams. We demonstrate the power of our approach by applying it to five real-world medical datasets. We show that it provides dramatically improved estimation of missing measurements in comparison to 11 state-of-the-art benchmarks (including Spline and Cubic Interpolations, MICE, MissForest, matrix completion and several RNN methods); typical improvements in Root Mean Square Error are between 35% -50%. Additional experiments based on the same five datasets demonstrate that the improvements provided by our method are extremely robust.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Missing data/measurements present a ubiquitous problem. The problem is especially challenging in medical settings which present time series containing many streams of measurements that are sampled at different and irregular times, and is especially important in these settings because accurate estimation of these missing measurements is often critical for accurate diagnosis, prognosis and treatment, as well as for accurate modeling and statistical analyses. This paper presents a new method for estimating missing measurements in time series data, based on a novel deep learning architecture. By comparing our method with current state-of-the-art benchmarks on a variety of real-world medical datasets, we demonstrate that our method is much more accurate in estimating missing measurements, and that this accuracy is reflected in improved prediction of outcomes.</p><p>The most familiar methods for estimating missing data follow one of three approaches, usually called interpolation, imputation and matrix completion. Interpolation methods such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> exploit the correlation among measurements at different times within each stream but ignore the correlation across streams. Imputation methods such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> exploit the correlation among measurements at the same time across different streams but ignore the correlation within streams. Because medical measurements are frequently correlated both within streams and across streams (e.g., blood pressure at a given time is correlated both with blood pressure at other times and with heart rate), each of these approaches loses potentially important information. Matrix completion methods such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> do exploit correlations within and across streams, but assume that the data is static -hence ignore the temporal component of the data -or that the data is perfectly synchronized -an assumption that is routinely violated in medical time series data. Some of these methods also make modeling assumptions about the nature of the data-generating process or of the pattern of missing data. Our approach is expressly designed to exploit both the correlation within streams and the correlation across streams and to take into account the temporal and non-synchronous character of the data; our approach makes no modeling assumptions about the X X X X X X X Time Streams X X X X X X X X X X X X X X Temporal data streams Interpolation (within streams) -Bi-RNN-Imputation (across streams) -Fully Connected layer with dropout-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-RNN</head><p>Imputed temporal data streams <ref type="figure">Figure 1</ref>: Block diagram of missing data estimation process: X = missing measurements; red lines = connections between observed values and missing values in each layer; blue lines = connections between interpolated values; dashed lines = dropout data-generating process or the pattern of missing data. (We do assume -as is standard in most of the literature -that the data is missing at random <ref type="bibr" target="#b10">[11]</ref>. Dealing with data that is not missing at random <ref type="bibr" target="#b11">[12]</ref> presents additional challenges and is left for future works.)</p><p>Our method relies on a novel neural network architecture that we call a Multi-directional Recurrent Neural Network (M-RNN). Our M-RNN contains both an interpolation block and an imputation block and it trains these blocks simultaneously, rather than separately (See <ref type="figure" target="#fig_0">Fig. 1 and 2)</ref>. Like a bi-directional RNN (Bi-RNN) <ref type="bibr" target="#b12">[13]</ref>, an M-RNN operates forward and backward within each data stream -in the intra-stream directions. An M-RNN also operates across different data streams -in the inter-stream directions. Unlike a Bi-RNN, the timing of inputs into the hidden layers of our M-RNN is lagged in the forward direction and advanced in the backward direction. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, our M-RNN architecture exploits the 3-dimensional nature of the dataset.</p><p>An important aspect of medical data is that there is often enormous uncertainty in the measured data. As is well-known, although single imputation (SI) methods may yield the most plausible/most likely estimate for each missing data point <ref type="bibr" target="#b13">[14]</ref>, they do not capture the uncertainty in the imputed data <ref type="bibr" target="#b2">[3]</ref>. Multiple imputation (MI) methods capture this uncertainty by sampling imputed values several times in order to form multiple complete imputed datasets, analyzing each imputed dataset separately and combining the results via Rubin's rule <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Capturing the uncertainty in the dataset is especially important in the medical setting, in which diagnostic, prognostic and treatment decisions must be made on the basis of the imputed values <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. In our setting, we use dropout <ref type="bibr" target="#b18">[19]</ref> to produce multiple imputations; see section 4.4.</p><p>To demonstrate the power of our method, we apply it to five different public real-world medical datasets: the MIMIC-III <ref type="bibr" target="#b19">[20]</ref> dataset, the clinical deterioration dataset used in <ref type="bibr" target="#b20">[21]</ref>, the UNOS dataset for heart transplantation, the UNOS dataset for lung transplantation (both available at https://www.unos.org/data/), and the UK Biobank dataset <ref type="bibr" target="#b21">[22]</ref>. We show that our method yields large and statistically significant improvements in estimation accuracy over previous methods, including interpolation methods such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, imputation methods such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, RNN-based imputation methods such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> and matrix completion methods such as <ref type="bibr" target="#b6">[7]</ref>. For the MIMIC-III and clinical deterioration datasets the patient measurements were made frequently (hourly basis), and our method provides Root Mean Square (RMSE) improvement of more than 50% over all 11 benchmarks. For the UNOS heart and lung transplantation datasets and the Biobank dataset, the patient measurements were made much less frequently (yearly basis), but our method still provides RMSE improvement of more than 40% in most cases, and significant improvements in the other cases. We also show that this improvement in estimation yields (smaller) improvements in the predictions of outcomes (patients' future states). A number of experiments based on these same datasets show that the extent to which our method improves on outcomes depends on the method used for prediction, on the way in which our model is optimized in training, on the amount of data available (both in terms of the number of patients for whom we have data and on the amount of data available for each patient), and on the nature and extent of missing data. These results illustrate the important point that, as mentioned earlier, there are many reasons for imputing missing data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref> -for the estimation of parameters (e.g. means or regression coefficients), for determination of confidence intervals and significance, as well as for prediction -and that no single method for imputing data can be expected to be superior on all datasets or for all reasons .</p><p>As <ref type="bibr" target="#b26">[27]</ref> has emphasized, an extremely desirable aspect of any imputation method is that it be congenial; i.e. that it should produce imputed values in a manner that preserves the original relationships between features and labels. As we demonstrate using the complete Biobank dataset, our method is also more congenial than the best competing benchmarks; see section 6.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>As we have noted, there are three standard and very widely-used methods for dealing with missing data: interpolation, imputation and matrix completion. Interpolation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> attempt to reconstruct missing data by capturing the temporal relationship within each data stream but not the relationships across streams. Imputation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> attempt to reconstruct missing data by capturing the synchronous relationships across data streams but not the temporal relationships within streams. Matrix completion methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> treat the data as static -ignoring the temporal aspect -or perfectly synchronized and assume a specific model of the data-generating process and/or the pattern of missing data.</p><p>There is also a substantial literature that uses Recurrent Neural Networks (RNNs) for prediction on the basis of time series with missing data. For example, <ref type="bibr" target="#b27">[28]</ref> first replaces all the missing values with a mean value, then uses the feedback loop from the hidden states to update the imputed values and finally uses the reconstructed data streams as inputs to a standard RNN for prediction. <ref type="bibr" target="#b28">[29]</ref> uses the Expectation-Maximization (EM) algorithm to impute the missing values and again uses the reconstructed data streams as inputs to a standard RNN for prediction. <ref type="bibr" target="#b29">[30]</ref> uses a linear model to estimate missing values from the latest measurement and the hidden state within each stream followed by a standard RNN for prediction. In the first two of these papers, missing values are imputed by using only the synchronous relationships across data streams but not the temporal relationships within streams; in the third paper, missing values are interpolated by using only the temporal relationships within each stream but not on the relationships across streams.</p><p>A more recent literature extends these methods to deal with both missing data and irregularly sampled data <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>. All of these papers use the sampling times to capture the informative missingness and time interval information to deal with irregular sampling, using the measurements, sampling information and time intervals as the inputs of an RNN. However, they differ in the replacements they use for missing values. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref> replace the missing values with 0, mean values or latest measurements -all of which are independent of either the intra-stream or inter-stream relationships or both. <ref type="bibr" target="#b24">[25]</ref> imputes the missing values using only the most recent measurements, the mean value of each stream, and the time interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>Our formulation and method are applicable to a wide variety of settings with missing data. However, for ease of exposition -and to facilitate the discussion of our application to medical datasets -it is convenient to adopt medical terminology throughout.</p><p>We consider a dataset consisting of N patients. For each patient, we have a multivariate time series data stream of length T (the length T and the other components of the dataset may depend on the patient n but for the moment we suppress the dependence on n) that consists of time stamps S, measurements X , and labels Y, sampled from an (unknown) underlying distribution F:</p><formula xml:id="formula_0">(S, X , Y) ∼ F.</formula><p>For each t the time stamp st ∈ R represents the actual time at which the measurements xt were taken. For convenience we normalize so that s1 = 0 (so that we are measuring actual times for each patient beginning from the first observation for that patient); we assume actual times are strictly increasing: st+1 &gt; st where 0 ≤ t &lt; T . Note that the measurements may not be sampled regularly, so that the interval st+1 − st between successive measurements need not be constant.</p><p>There are D streams of measurements. We view each measurement as a real number, but it will typically be the case that not every stream is actually observed/measured at st. Hence we adopt notation in which the set of possible measurements at the t-th time stamp st is R * = R ∪ { * }. We interpret x d t = * to mean that the stream d was not measured at st; otherwise x d t ∈ R is the actual measurement of stream d at st. For convenience, we scale all measurements to lie in the interval [0, 1].</p><p>It is convenient to introduce some additional notation. For each t, define the index m t d to equal 0 if x d t = * (i.e. the stream d was not measured at st) and to equal 1 if x d t ∈ [0, 1] (the stream d was measured at st). We define δ d t to be the actual amount of time that has elapsed from st since the stream d was measured previously; δ d t can be defined by setting δ d 1 = 0 and then proceeding recursively as follows:</p><formula xml:id="formula_1">δ d t = st − st−1 + δ d t−1 if t &gt; 1, m d t−1 = 0. st − st−1 if t &gt; 1, m d t−1 = 1</formula><p>Write δt for the vector of elapsed times at time stamp t and ∆ = {δ1, δ2, ..., δT }.</p><p>The label yt represents the outcome realized at time stamp t (actual time st) such as discharge, clinical deterioration, death. Y is the vector of outcomes for this patient. Again, we scale so the labels (and eventually predictions) lie in the interval [0, 1]. Frequently the outcome is binary in which case yt = 0 or yt = 1.</p><p>The information available for a particular patient n is therefore a triple consisting of a sequence of time stamps, an array of measurements at each time stamp (with the above convention about missing measurements), and an array of labels at each time stamp. It is convenient to use functional notation to identify information about a particular patient, so x d t (n) is the measurement of stream d at time stamp t for patient n, etc. The entire dataset consists of all the triples for all the patients D = {(S(n), X (n), Y(n)} N n=1 .</p><p>Our objective is to find a function f that provides the best estimate of missing values; i.e. the estimate that minimizes the estimation loss. As is usually done, we measure loss as the squared error, so if x d t is an (unobserved) actual measurement (sampled from F) andx d t = f d t (S, X ) is the estimate formed on the basis of observed data, then the mean square loss for this particular measurement is L(</p><formula xml:id="formula_2">x d t , x d t ) = (x d t − x d t ) 2 .</formula><p>Hence the formal optimization problem is to find a function f to solve:</p><formula xml:id="formula_3">min f EF T t=1 D d=1 (1 − m d t )L(x d t , x d t ) = min f EF T t=1 D d=1 (1 − m d t )(f d t (S, X , Y) − x d t ) 2 .<label>(1)</label></formula><p>Note that the function f we seek depends on the particular d and t, and on the entire array of time stamps and measurements -but not on labels (which may not be observed). Also note that the formal problem asks to find an f that minimizes the loss with respect to the true distribution. Of course we do not observe the true distribution and cannot compute the true loss, so we will minimize the empirical loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-directional Recurrent Neural Networks (M-RNN)</head><p>Suppose that stream d was not measured at time stamp t, so that x d t = * . We would like to form an estimatex d t of what the actual measurement would have been. As we have noted, familiar interpolation methods use only the measurements x d t of the fixed data stream d for other time stamps t = t (perhaps both before and after t) -but ignores the information contained in other data streams d = d; familiar imputation methods use only the measurements x d t at the fixed time t for other data streams d = d -but ignores the information contained at other times t = t. Because information is often correlated both within and across data streams, each of these familiar approaches throws away potentially useful information. Our approach forms an estimatex d t using measurements both within the given data stream and across other data streams. In principle, we could try to form the estimatex d t by using all the information in D. However, this would be impractical because it would require learning a number of parameters that is on the order of the square of the number of data streams, and also because it would create a serious danger of over-fitting. Instead, we propose an efficient hierarchical learning framework using a novel RNN architecture that effectively allows us to capture the correlations both within streams and across streams. Our approach limits the number of parameters to be learned to be of the linear order of the number data streams and avoids over-fitting. See <ref type="figure">Fig. 1</ref>.</p><p>Our basic single-imputation M-RNN consists of 2 blocks: an Interpolation block and an Imputation block; see <ref type="figure" target="#fig_0">Fig. 2</ref>. (Our construction puts the Imputation block after the Interpolation block in order to use the outputs of the Interpolation block to improve the accuracy of the Imputation block; as we discuss later, it would not be useful to put the Interpolation block after the Imputation block.) To produce multiple imputations, we adjoin an additional dropout layer to the basic single-imputation M-RNN. (We defer the details until Section 4.4.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Error/Loss</head><p>As formalized above in Equation <ref type="formula" target="#formula_3">(1)</ref>, our overall objective is to minimize the error that would be made in estimating missing measurements. Evidently, we cannot estimate the error of a measurement that was not made and hence is truly missing in the dataset. Instead we fix a measurement x d t that was made and is present in the dataset, form an estimatex d t for x d t using only the dataset with x d t removed (which we denote by D − x d t ), and then compute the error between the estimatex d t and the actual measurement x d t . As above, we use the squared error (x d t − x d t ) 2 as the loss for this particular estimate; as the total loss/error for the entire dataset D we use the mean squared error (MSE):</p><formula xml:id="formula_4">L({x d t , x d t }) = N n=1 Tn t=1 D d=1 m d t (n) × (x d t (n) − x d t (n)) 2 Tn t=1 D d=1 m d t (n)</formula><p>Note that this is the empirical error, which is actually achievable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interpolation Block</head><p>The Interpolation block constructs an interpolation function Φ that operates within a given stream. To emphasize that the estimatex d backward direction: at t, inputs of forward hidden states come from t − 1 and inputs of backward hidden states come from t + 1.</p><p>(This procedure ensures that the actual value x d t is automatically not used in the estimation ofx d t .) If we write</p><formula xml:id="formula_5">z d t = [x d t , m d t , δ d t ]</formula><p>then a more mathematical description is:</p><formula xml:id="formula_6">x d t = g(U d [ − → h d t ; ← − h d t ] + c d o ) = g( − → U d − → h d t + ← − U d ← − h d t + c d o ) − → h d t = f ( − → W d − → h d t−1 + − → V d z d t−1 + − → c d ) ← − h t = f ( ← − W d ← − h d t+1 + ← − V d z d t+1 + ← − c d )</formula><p>Here, f, g are activation functions. (In principle, any activation functions, such as ReLu, tanh, etc., could be used; here we use ReLu.)</p><p>The arrows indicate forward/backward direction. As we have emphasized, in this interpolation block, we are only using/capturing the temporal correlation within each data stream. In particular, the parameters for each data stream are learned separately, and the number of parameters that must be learned is linear in the number of streams D. (The weight matrices W, U, V are block diagonal.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Imputation Block</head><p>The Imputation blocks constructs an imputation function Ψ that operates across streams. To again emphasized that the estimatex d </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multiple Imputations</head><p>It is well-understood that to account for the uncertainty in estimating missing values, it is useful to produce multiple estimates and generate multiple imputed datasets. These multiple imputed datasets can each be analyzed using standard methods and the results can be combined using Rubin's rule <ref type="bibr" target="#b2">[3]</ref>. In our case, we generate multiple imputed datasets using the well-known Dropout <ref type="bibr" target="#b18">[19]</ref> approach: we randomly selects neurons in the fully connected layers and delete those neurons and all their connections. (The dropout probability p ∈ (0, 1) is a hyper-parameter to be chosen; the neurons to be dropped are chosen according to the Bernoulli distribution with parameter p.) In the training stage, we conduct joint optimization (equation <ref type="formula">(2)</ref>) using the dropout process. We then generate multiple outputs ot by sampling different dropout vectors R from the Bernoulli distributions. This yields multiple imputations (MI).</p><p>(To construct a single imputation (SI) we proceed in precisely the same way but set the dropout probability to 0. For comparisons, we normalize the final output by multiplying by p.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Overall Structure</head><p>We refer to the entire structure above as a Multi-directional Recurrent Neural Network (M-RNN). We use the notations M-RNN (MI) and M-RNN (SI) to clarify whether we are producing multiple or single imputations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets</head><p>We use five datasets, the characteristics of which are summarized in <ref type="table" target="#tab_0">Table 1</ref>: more detailed descriptions are below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MIMIC-III</head><p>The dataset MIMIC-III <ref type="bibr" target="#b19">[20]</ref> contains data for patients monitored in intensive care units (ICUs) of various hospitals. Within the entire MIMIC-III dataset, we only used the data for the 23,160 patients whose measurements are recorded by Metavision (post 2008). We used 20 vital signs (e.g. heart rate, respiratory rate, blood pressures, etc.) and 20 lab tests (e.g. creatinine, chloride, etc.) whose missing rates are minimum. For these patients we have 40 physiological data streams in all. Vital signs were sampled roughly every hour; lab tests were sampled roughly every 12 hours. Each patient was followed until either death (1,320 patients (5.7 %)) or discharge from ICU (21,840 patients (94.3 %)). Note that because lab tests are sampled only 1/12 as often as vital signs, in effect 11/12 of lab test data is missing -even if every lab test for every patient was actually conducted and recorded. For the purpose of prediction, we take the goal as predicting at each time t whether the patient will die within the next 24 hours. Hence we assign the label yt = 1 of the patient actually died within the 24 hrs following the time st and yt = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Deterioration</head><p>The dataset described in <ref type="bibr" target="#b20">[21]</ref> provides records for a cohort of 6,094 patients who were followed for potential clinical deterioration while hospitalized. Patients were monitored for 28 vital signs (heart rate, blood pressure, etc.) and 10 lab tests (creatinine, hemoglobin, etc.) so there are 38 physiological data streams in all. Vital signs were sampled roughly every 4 hours; lab tests were sampled roughly every 24 hours. Each patient was followed until either admission to ICU (306 patients (5.0 %)) or discharge from hospital <ref type="bibr" target="#b4">(5,</ref><ref type="bibr">788</ref> patients (95.0%)). Again, because lab tests are sampled only 1/6 as often as vital signs, in effect 5/6 of lab test data is missing -even if every lab test for every patient was actually conducted and recorded. For the purpose of prediction, we take the goal as predicting at each time stamp st whether the patient will be admitted to the ICU (experienced clinical deterioration) within the next 24 hours. Hence we assign the label yt = 1 if the patient was admitted to the ICU within the following 24 hours and yt = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">UNOS-Heart and UNOS-Lung</head><p>The UNOS (United Network for Organ Transplantation) dataset (available at https://www.unos.org/data/) provides yearly follow-up information for the entire U.S. cohort of 69,205 patients who received heart transplants and 32,986 patients who received lung transplants during the period 1985-2015. We view patients in the dataset as described by a total of 34 clinical features. (In fact, a total of 232 features are recorded in the UNOS dataset, but many features are not recorded for most patients. We therefore excluded the 198 features for which missing rates were higher than 80%; this is in keeping with standard medical statistical practice.) For each patient, a number of yearly follow-ups are recorded; the smallest number of yearly follow-ups is 1, the largest is 26; the median number of follow-ups for heart transplantation is 6 and the median number of follow-ups for lung transplantation is 4. (In the main text, we focus entirely on features of the patient, ignoring features of the donor. We do this for two reasons: (i) the features of the patient change over time but the features of the donor do not (because the donor is dead); (ii) the relevant features of the donor appear to be largely captured in the time series measurements of the patient. However, as we show in the Appendix, taking features of the donor into account seems to make little difference for either imputation or prediction.) For the purpose of prediction, we take the goal in each case as predicting at each follow up time st whether the patient will be dead one year later. Hence we assign the label yt = 1 if the patient actually died within the following year and yt = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Biobank</head><p>We used the UK Biobank dataset gathered from 21 assessment centers across England, Wales, and Scotland using standardized procedures from 2007 to 2014. (The UK Biobank protocol is available online.) UK Biobank recorded various patient information including baseline measurements, physical measurements, and evaluations of biological samples. For this paper, we excluded all the variables that were missing for more than 80% of the participants and all the static measurements and only used the 113 longitudinal measurements. Of the 4,096 total patients, we used only the data for the 3,902 patients who missed no admissions to assessment centers (and hence were assessed the maximum number of times, which was three) and for whom there are no missing measurements of these 113 variables; thus we have a complete dataset. For the purpose of prediction we take the goal as the correct prediction of diabetes, so we assign the label yt = 1 if diabetes is diagnosed at t and yt = 0 otherwise.</p><p>6 Results and Discussions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Imputation Accuracy on the Given Datasets</head><p>We begin by comparing the performance of our method (using both multiple imputations and single imputation) on the given datasets against 11 benchmarks with respect to the accuracy of imputing missing values. The benchmarks against which we compare are: the algorithms proposed in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>; Spline and Cubic Interpolation <ref type="bibr" target="#b0">[1]</ref> ; MICE <ref type="bibr" target="#b4">[5]</ref>; MissForest <ref type="bibr" target="#b5">[6]</ref>, EM <ref type="bibr" target="#b3">[4]</ref>; the matrix completion algorithm of <ref type="bibr" target="#b6">[7]</ref>;the Auto-Encoder algorithm proposed in <ref type="bibr" target="#b31">[32]</ref>; and the Markov chain Monte Carlo (MCMC) method <ref type="bibr" target="#b32">[33]</ref>. (The details of the implementations for the various benchmarks are presented in the Appendix.) As is common, we use root mean square error (RMSE) as the measure of performance. In each experiment, we use 5 cross-validation. <ref type="table" target="#tab_1">Table 2</ref> shows the mean RSME for our method and benchmarks, and the percentage improvement of RMSE for M-RNN (MI) over the benchmarks. (Note that we are unable to provide results for the EM algorithm on the UNOS-Heart and UNOS-Lung datasets because -at least for the implementation we use -the EM algorithm requires at least one patient for whom data is complete, and the UNOS-Heart and UNOS-Lung datasets do not contain any such patient.) As can be seen in <ref type="table" target="#tab_1">Table 2</ref>, M-RNN achieves better performance (smaller RMSE) than all of the benchmarks on all of the datasets (for all comparisons are possible). With a single exception (the comparison with MissForest on the UNOS-Lung dataset) the performance improvements are statistically significant at the 95% level (i.e., p &lt; 0.05), and many of the improvements are very large. For instance, for the Deterioration dataset, M-RNN using multiple imputations achieves RMSE of 0.0105 (95% CI: 0.0071-0.0138), while the best benchmark (Spline interpolation) achieves RMSE of 0.0215 (95% CI: 0.0178-0.0255); this represents an improvement of 51.2%.</p><p>The performance comparisons across datasets are revealing, if not necessarily surprising. The interpolation benchmarks (such as Spline, Cubic and RNN-based methods) work best on datasets, such as MIMIC-III and Deterioration, for which measurements were more frequent (and more highly correlated within each stream (see <ref type="table" target="#tab_0">Table 1</ref>)); the imputation benchmarks work best on datasets, such as UNOS-Heart and UNOS-Lung, for which measurements were less frequent but for which there were many streams of data (many dimensions). The improvement of our method over all benchmarks is larger for the MIMIC-III and Deterioration datasets because those datasets have many streams of frequently sampled data, so that our method gains a great deal from exploiting both the correlations within each data stream and the correlations across data streams. Conversely, the improvement of our method is smaller for the UNOS-Heart and UNOS-Lung datasets, because streams in those datasets are infrequently sampled to that there is less to be gained by exploiting the correlations within data streams. (The performances of the benchmarks for the Biobank dataset are mixed, and don't quite fit this same pattern, perhaps because Biobank is a small dataset (less than 4,000 patients with complete temporal data streams).)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Multiple Imputations vs. Single Imputation</head><p>As we have noted, the purpose of conducting multiple imputations is to reduce uncertainty/shrink confidence intervals (rather than to improve average performance). As is illustrated in the box-plot in <ref type="figure" target="#fig_1">Fig. 3(a)</ref> which shows the comparison of M-RNN with multiple imputations and M-RNN with a single imputation against the best benchmark (Cubic interpolation) on the MIMIC-III dataset, our multiple imputations do achieve this purpose. (For discussion of <ref type="figure" target="#fig_1">Fig. 3(b)</ref>, which illustrates the corresponding reduction in uncertainty for prediction, see below.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Combining models of interpolation and imputation</head><p>As we have already discussed, standard interpolation algorithms cannot capture the patterns across streams and standard imputation algorithms cannot capture the patterns within the streams. However, it is possible to combine a standard interpolation algorithm and standard imputation algorithm in an attempt to capture both patterns, and it might be thought that such a combination would be a fairer benchmark against which to compare our method. To put this idea to the test, we create a family of "joint algorithms" by first using an interpolation algorithm to interpolate the missing values, and then using the interpolated values as the initial points of an imputation algorithm to provide final imputed values. For this exercise, we use two standard interpolation methods (Cubic and Spline), and two standard imputation methods (MICE and MissForest) so that we have 4 interpolation-imputation combination models: Cubic + MICE, Cubic + MissForest, Spline + MICE, and Spline + MissForest. As <ref type="table" target="#tab_2">Table 3</ref> shows, however, the performances of these interpolation-imputation combination models are very similar to those of the performance of the simple imputation model that is used. Indeed, the largest RMSE performance improvement is only 0.0018. The reason for this is that imputation methods use algorithms that operate iteratively until they converge, so that their performance is rather robust to the initialization. Hence, although the interpolation part of the joint models captures some of the inter-stream information, the iterative imputation part ignores most of what is captured.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Source of Gains</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, our M-RNN consists of an Interpolation block and an Imputation block. To understand where the gains of our approach come from, we compare the performance of that is achieved when we use only the Interpolation block or only the Imputation block; the results are shown in <ref type="table" target="#tab_3">Table 4</ref>. The Interpolation block is intended to exploit the correlations within each data stream and the Imputation block is intended to exploit the correlations across streams, so it is to be expected that the largest gains of our M-RNN method should come from the Interpolation block for the datasets (MIMIC-III and Deterioration) which are frequently samples and have large temporal correlations, and should come from the Imputation block for the datasets (UNOS-Heart and UNOS-Lung) which are infrequently sampled but have many data streams. As shown in <ref type="table" target="#tab_3">Table 4</ref>, these intuitions are indeed supported by the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Additional Experiments</head><p>The experiments we have described above demonstrate that our method significantly outperforms a wide variety of benchmarks for the imputation of missing data on five somewhat representative datasets. However it is natural to ask how our method would compare in other circumstances. To get some understanding of this, we conducted four sets of experiments based on the MIMIC-III dataset: increasing the amount of missing data, reducing the number of data streams, reducing the number of samples, and reducing the number of measurements per patient. Within each set of experiments, we conducted 10 trials for each value of the parameter being studied (e.g. amount of missing data), and we report the average over these 10 trials. The results are described below and in <ref type="figure" target="#fig_3">Fig. 4</ref>. Although the results of these experiments are extremely suggestive, we caution the reader that these are only a specific set of   experiments and that one should be careful about drawing general conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Amount of Missing Data</head><p>To evaluate the performance of M-RNN in comparison to benchmarks in settings with more missing data, we constructed sub-samples of the MIMIC-III dataset by randomly removing 10%, 20%, 30%, 40%, 50% of the actual data and carrying out the same estimation exercise as above on the smaller datasets that remain. (Recall that in the original MIMIC-III dataset, 75% of the data is already missing; hence removing 50% of the data present leads to an artificial dataset in which 87.5% of the data is missing.) The graph in <ref type="figure" target="#fig_3">Fig. 4(a)</ref> shows the performance of M-RNN against the best benchmarks of each type for these smaller datasets. As can be seen, M-RNN continues to substantially outperform the benchmarks. Note that as the amount of missing data increases the improvement of M-RNN over the imputation benchmark(s) increases, but the improvement over the interpolation benchmarks decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Number of Data Streams</head><p>As we have noted, typical medical datasets contain many data streams (many feature dimensions). To evaluate the performance of M-RNN in comparison to benchmarks in settings with fewer data streams, we conducted experiments in which we reduced the number of data streams (feature dimensions) of MIMIC-III. In the original MIMIC-III dataset the number of data streams is D = 40;</p><p>we conducted experiments with D = 3, 5, 7, 10, 15, 20 data streams. (In each case, we conducted 10 trials in which we selected data streams at random; we report the average of these 10 trials.) As expected, the performance of M-RNN degrades when there are fewer data streams, but as <ref type="figure" target="#fig_3">Figure 4(b)</ref> shows, M-RNN still outperforms the benchmarks. (Note that interpolation methods are insensitive to the number of data streams because they operate only within each data stream separately.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Number of Samples</head><p>The original MIMIC-III dataset has N = 23, 160 samples (patients). To understand the performance of M-RNN in comparison to benchmarks in settings with fewer samples, we conducted experiments in which we used only subsets of all patients (samples) of sizes N = 500, 1000, 2000, 4000, 8000, 16000. Because M-RNN has to learn many parameters, it should come as no surprise that, as <ref type="figure" target="#fig_3">Fig. 4(c)</ref> shows, the performance of M-RNN degrades badly -and indeed is worse than that of (some) other benchmarks -when the number of samples is too small, but M-RNN outperforms all the benchmarks as soon as the number of training samples exceeds N = 7, 000. (However, one should not necessarily take the <ref type="figure">figure N = 7</ref>, 000 as representing a cut-off below which M-RNN should not be applied, because M-RNN outperforms the benchmarks on the Deterioration and Biobank datasets, which contain only 6,094 samples and 3,902 samples, respectively.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">Number of Measurements per Patient</head><p>We have already noted that, in our datasets, MIMIC-III and Deterioration have many (relatively frequent) measurements per patient, while the other datasets have only a few (and infrequent) measurements per patient and that this leads to differences in performance of M-RNN. To further explore this effect, we created subsets of the MIMIC-III dataset with T = 1, 3, 5, 10, 20, 30 measurements per patient. As might be expected, and as <ref type="figure" target="#fig_3">Fig. 4(d)</ref> shows, having fewer measurements per patient degrades the performance of interpolation based algorithms but has little effect on pure imputation based methods; the performance of M-RNN is also degraded, but to a much lesser extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Prediction Accuracy</head><p>As we have noted, there are many reasons for imputing missing data; one such is to improve predictive performance. We therefore compare our method against the same 11 benchmarks with respect to the accuracy of predicting labels. (See the description of the datasets in section 5 for labeling in each case.) For this purpose, we use Area Under the Receiver Operating Characteristic Curve (AUROC) as the measure of performance. To be fair to all methods of imputing missing values, we use the same predictive model (a simple 1-layer RNN) in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Prediction accuracy on the original datasets</head><p>In this subsection, we evaluate the effects of the imputations on the prediction of labels (outcomes), which in the cases at hand correspond to prognoses. <ref type="table" target="#tab_4">Table 5</ref> shows the mean and percentage performance gain of M-RNN (MI) in comparison with the benchmarks on all the datasets. M-RNN -which we have already shown to achieve the best imputation accuracy -also yields the best prediction accuracy. However, even in cases where the improvement in imputation accuracy is large and statistically significant, the improvements in prediction accuracy are sometimes smaller and not always statistically significant. For instance, on the Deterioration dataset, the AUROC of M-RNN (MI) is 0.7779 (95% CI: 0.7678-0.7868); the best benchmark is <ref type="bibr" target="#b24">[25]</ref> with AUROC of 0.7593 (95% CI: 0.7478-0.7702). Similarly, on the UNOS-Heart dataset, the AUROC of M-RNN (MI) is 0.6855 (95% CI: 0.6781-0.6913); the best benchmark is MissForest, with AUROC of 0.6740 (95% CI: 0.6651-0.6817).</p><p>It should be noted that, by using mean square error as the loss function, we have deliberately optimized M-RNN for imputation accuracy. If we want to optimize M-RNN for prediction accuracy we might do better by using a different loss function, such as cross-entropy. In <ref type="table" target="#tab_0">Table 10</ref> of the Appendix we report an experiment in which we have done precisely that; the short summary is that optimizing for prediction accuracy does in fact improve the predictive performance of M-RNN but the improvement is marginal.</p><p>The Appendix also reports other experiments that help further our understanding of the M-RNN algorithm. <ref type="table" target="#tab_8">Table 7</ref> demonstrates that using a different predictive model (random forest, logistic regression or Xgboost <ref type="bibr" target="#b33">[34]</ref>, rather than a 1-layer RNN) for prediction after imputation leads to results similar to those obtained above. <ref type="table" target="#tab_9">Tables 8 and 9</ref> demonstrate that accounting for donor features in the UNOS datasets makes little difference.  RNN-based <ref type="bibr" target="#b22">[23]</ref> 0.8381 (9.3%) 0.7558 (9.0%) 0.6505 (10.0%) 0.6557 (6.0%) 0.8802 (12.8%) <ref type="bibr" target="#b23">[24]</ref> 0.8402 (8.1%) 0.7551 (9.3%) 0.6574 (8.2%) 0.6561 (5.8%) 0.8748 (16.5%) <ref type="bibr" target="#b24">[25]</ref> 0.8410 (7.6%) 0.7593 (7.7%) 0.6583 (8.0%) 0.6520 (7.0%) 0.8826 (11.0%)  </p><formula xml:id="formula_7">Interpolation</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Prediction accuracy with various missing rates</head><p>As discussed above, we carried out experiments with increased rates of missing data in order to understand the implications for the accuracy of imputation. We also carried out experiments with increased rates of missing data in order to understand the implications for the accuracy of prediction. In this case, we begin with the Biobank dataset, which is complete, and randomly remove 10% to 90% of the measurements (with increments of 10%) to create multiple datasets with different missing rates. (In each case we use 80% of the data for training and 20% for testing.) As before, we use M-RNN and various benchmarks for imputing missing data and a 1-layer  RNN as the predictive model. (In this setting we are predicting a clinical diagnosis of diabetes.) In each setting, we conducted 10 trials, and report the performance in terms of AUROC. <ref type="figure" target="#fig_5">Fig. 5 (a)</ref> illustrates the impact (in terms of AUROC) of more and more missing data for M-RNN and various benchmarks. As <ref type="figure" target="#fig_5">Figure 5</ref> (a) shows, for M-RNN and all benchmarks, the prediction performance decreases as the amount of missing data increases. However, as <ref type="figure" target="#fig_5">Fig. 5 (b)</ref> shows, M-RNN continues to outperform the benchmarks; indeed, the performance gap between M-RNN and the benchmarks widens when more data is missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">The importance of specific features</head><p>To this point, we have treated all missing data as equally important and given the same weight to all errors. However, this is not always the right thing to do. In particular, it is clear that not all missing data is equally important for prediction. To understand the importance of missing data for purposes of prediction we conduct two experiments in parallel. For the first experiment (which we call Setting A: Purely Random Removal), we construct 5 sub-samples of the Deterioration dataset by randomly removing 10%, 20%, 30%, 40%, 50% of the measurements for randomly chosen features. For the second experiment (which we call Setting B: Correlated Random Removal) we first identify the four features that are most highly correlated with the label (in the Deterioration dataset the label means either Admission to the ICU or Discharge from Hospital); these four features are: O2 Device, Heart rates, Respiratory rate and Urea Nitrogen. We then construct 5 sub-samples of the Deterioration dataset by removing 10%, 20%, 30%, 40%, 50% of the measurements for these specific features. In both cases we repeat the exercise 10 times and report average results. We then compare the prediction performance of M-RNN (MI) with the best benchmarks; the results are shown visually in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p><p>Because the purpose of this exercise is to see the impact of missingness of more important features on prediction, we focus most of our attention on Setting B. <ref type="figure" target="#fig_6">Fig. 6(b)</ref> shows the AUROC for M-RNN and benchmarks for various rates of missing data in Setting B. As can be seen, M-RNN outperforms the best benchmarks for every sub-sample and the improvement in performance is greater for the sub-samples for which more data is missing. <ref type="figure" target="#fig_6">Fig. 6(a)</ref> shows the percentage improvement of M-RNN against the benchmarks (still in Setting B). The improvement in performance is statistically significant (p-value &lt; 0.05) when 30% or more of the measurements (of the most important features) are missing. <ref type="figure" target="#fig_6">Fig. 6(c)</ref> shows the prediction performance of M-RNN and the best benchmark in Settings A and B. As can be seen, the prediction performances of both M-RNN and the benchmark are worse when the more important data is missing -but the prediction performance of M-RNN is much less sensitive to the amount of data that is missing and to which data is missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Congeniality of the model</head><p>As <ref type="bibr" target="#b26">[27]</ref> has emphasized, an extremely desirable aspect of any imputation method is that it produce imputed values in a manner that is consistent and preserves the original relationships between features and labels; <ref type="bibr" target="#b26">[27]</ref> refers to this as congeniality. Congeniality of an imputation model can be evaluated with respect to a particular model of the feature-label relationships by computing the model parameters for the true complete data and the imputed data and measuring the difference between parameters according to some specified metric. Of course no imputation method can be expected to be perfectly congenial, but we argue that our method is more congenial -i.e. better preserves the relationships between features and labels -than benchmarks. To see this, we exploit the fact that the Biobank dataset is complete so that it is possible to delete some of the data, impute the deleted data, and compare the relationship between the actual (original) data and labels with the relationship between the imputed data and labels.</p><p>In our particular experiment, we delete 20% of the data and impute the missing data using our M-RNN and the 4 best benchmarks (the method of <ref type="bibr" target="#b24">[25]</ref>, Cubic Interpolation, MissForest and Matrix Completion). As a model of the feature-label relationship, we use a logistic regression. As a metric of the difference between the logistic regression parameters w for the actual data andŵ for the imputed data (which can be interpreted as a measure of the uncongeniality of the imputation) we report both the mean bias w −ŵ 1 and the root mean square error error w −ŵ 2. As can be seen in <ref type="table" target="#tab_7">Table 6</ref>, in comparison with the 4 best benchmarks, M-RNN achieves both smaller mean bias and small root mean square error between the original and imputed representations of feature-label relationship. (With the exception of MissForest, all the performance improvements of M-RNN are statistically significant at the 95% level.) Thus our method is more congenial (to the logistic regression model) than the benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The problem of reconstructing/estimating missing data is ubiquitous in many settings -especially in longitudinal medical datasetsand is of enormous importance for many reasons, including statistical analysis, diagnosis, prognosis and treatment. In this paper we have presented a new method, based on a novel deep learning architecture, for reconstructing/estimating missing data that exploits both the correlation within data streams and the correlation across data streams. We have demonstrated on the basis of a variety of real-world medical datasets that our method makes large and statistically significant improvements in comparison with state-of-the-art benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Predictions with Alternative Predictive Models  <ref type="table" target="#tab_4">Table 5</ref> compares the implied predictive performance of M-RNN with the benchmarks when imputation is followed by prediction using a particularly simple predictive model. We now compare the implied predictive performance of M-RNN with the benchmarks when imputation is followed by prediction other predictive models: in addition to the 1-layer RNN model used already, we use Random Forest <ref type="bibr" target="#b34">[35]</ref>, Logistic Regression and Xgboost <ref type="bibr" target="#b33">[34]</ref>. (For implementations, we use the randomForest package, the glm package and the xgboost package, all in R.) We restrict our attention to the Deterioration dataset and continue to use AUROC as the performance metric. <ref type="table" target="#tab_8">Table 7</ref> shows the mean and performance gain (%) (in terms of AUROC) of M-RNN in comparison with the benchmarks with various predictive models.</p><p>As can be seen in <ref type="table" target="#tab_8">Table 7</ref>, the different predictive models do not yield much different prediction accuracy, and no one predictive model seems to consistently lead to more or less accurate predictions. (Because the RNN imputation methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> can only be combined with an RNN predictive model, there are no RF, LR, XG results for these methods.) Note that prediction accuracy is not perfectly correlated with imputation accuracy, but the differences are not statistically significant. As before, M-RNN leads to the most accurate predictions when followed by each of the various predictive models; again, not all the differences are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Donor Features in UNOS Datasets</head><p>The UNOS organ transplant dataset records features of both the recipient and the donor. In the main text we ignored donor features; here we expand the analysis to include these features, and compare the results obtained using only recipient features with the results obtained using both recipient and donor features. <ref type="table" target="#tab_9">Table 8</ref> presents the comparison for imputations and <ref type="table" target="#tab_10">Table 9</ref> presents the comparisons for prediction. As noted in the main text, the differences are not large, presumably because the donor information is completely static and the relevant aspects are captured implicitly in the recipient data.</p><p>As can be seen in <ref type="table" target="#tab_9">Tables 8 and 9</ref>, the effects of donor features on imputation and prediction accuracy are not large. The RMSE of M-RNN (MI) improves from from 0.0479 to 0.0451 (5.8%) on the UNOS-Heart dataset and 0.0606 to 0.0579 (4.5%) on the UNOS-Lung dataset. In terms of prediction, AUROC of M-RNN (MI) improves from from 0.6855 to 0.7153 (9.5%) on the UNOS-Heart dataset and from 0.6762 to 0.6883 (3.7%) on UNOS-Lung dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction-oriented M-RNN</head><p>Finally, we report the results of an experiment in which we optimized M-RNN for prediction accuracy rather than imputation accuracy. To do this, we trained M-RNN to minimize the cross-entropy L = 1 N N n=1 [yn logŷn + (1 − yn) log(1 −ŷn)], rather than the mean square error. (We continue to evaluate performance in terms of AUROC.) As can be seen from <ref type="table" target="#tab_0">Table 10</ref>, doing this does improve   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-based</head><p>[23] 0.6830 (10.2%) 0.6505 (10.0%) 0.6575 (9.0%) 0.6557(6.0%) <ref type="bibr" target="#b23">[24]</ref> 0.6892 (8.4%) 0.6574 (8.2%) 0.6651 (6.9%) 0.6561(5.8%) <ref type="bibr" target="#b24">[25]</ref> 0.6962 (6.3%) 0.6583 (8.0%) 0.6728 (4.7%) 0.6520 (7.0%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolation</head><p>Spline 0.6817 (10.6%) 0.6477(10.7%) 0.6651 (6.9%) 0.6520 (7.0%) Cubic 0.6788 (11.4%) 0.6468 (11.0%) 0.6629 (7.5%) 0.6517 (7.0%) Imputation MICE 0.6785 (11.4%) 0.6397 (12.7%) 0.6567 (9.2%) 0.6509 (7.2%) MissForest 0.6981 (5.7%) 0.6740 (1.7%) 0.6754 (4.0%) 0.6587 (5.1%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Others</head><p>Matrix Completion 0.6969 (6.1%) 0.6712 (2.2%) 0.6745 (4.2%) 0.6579 (5.3%) Auto-encoder 0.6966 (6.2%) 0.6633 (6.6%) 0.6741 (4.4%) 0.6574 (5.5%) MCMC 0.6772 (11.8%) 0.6417 (12.2%) 0.6717 (5.1%) 0.6512 (7.2%)  <ref type="bibr" target="#b23">[24]</ref> 0.8402 (11.0%) 0.7551 (10.7%) 0.6574 (9.4%) 0.6561 (7.0%) 0.8748 (19.1%) <ref type="bibr" target="#b24">[25]</ref> 0.8410 (10.6%) 0.7593 (9.1%) 0.6583 (9.2%) 0.6520 (8.1%) 0.8826 (13.7%) the predictions of M-RNN, but the improvement is marginal and not statistically significant. (However, doing this does have the advantage that it creates an end-to-end prediction algorithm that does not require any preprocessing or imputation steps.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementations</head><p>For some of these algorithms, we are able to use off-the-shelf implementations. For Spline and Cubic Interpolation, we use the interp1 package in MATLAB; for MICE we use the mice package in R; for MissForest we use the MissForest package in R; for EM we use the Amelia package in R; for matrix completion we use the softImpute package in R.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>M-RNN Architecture. (Dropout is used for multiple imputations)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Box-plot comparisons between M-RNN (MI), M-RNN (SI) and the best benchmark. (a) RMSE comparison using MIMIC-III dataset, (b) AUROC comparison using MIMIC-III dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Imputation accuracy for the MIMIC-III dataset with various settings (a) Additional data missing at random (top-left), (b) Feature dimensions chosen at random (top-right), (c) Samples chosen at random (bottom-left), (d) Measurements chosen at random (bottom-right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>(a) The AUROC performance with various missing rates -upper line graph, (b) The AUROC gain from two most competitive benchmarks -lower bar graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>AUROC comparisons in Settings A and B using Deterioration dataset (a) AUROC improvement from the best benchmark (top) in Setting B, (b) AUROC comparisons with various missing rates in Setting B (middle), (c) AUROC comparison between Settings A and B (bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the datasets (Cont: Continuous, Cat: Categorical, Avg: Average)</figDesc><table><row><cell>Datasets</cell><cell>MIMIC-III</cell><cell cols="3">Deterioration UNOS-Heart UNOS-Lung</cell><cell>Biobank</cell></row><row><cell>Number of Patients</cell><cell>23,160</cell><cell>6,094</cell><cell>69,205</cell><cell>32,986</cell><cell>3,902</cell></row><row><cell>Number of Dimensions (Cont, Cat)</cell><cell>40 (31, 9)</cell><cell>38 (16, 22)</cell><cell>34 (10, 24)</cell><cell>34 (10, 24)</cell><cell>113 (67, 46)</cell></row><row><cell>Label (y = 1)</cell><cell>1,320 (5.7%)</cell><cell>306 (5.3%)</cell><cell>4,844 (7.0%)</cell><cell>2,276 (6.9%)</cell><cell>195 (5.0%)</cell></row><row><cell>Avg number of samples</cell><cell>24.3</cell><cell>34.3</cell><cell>6.2</cell><cell>4.0</cell><cell>3.0</cell></row><row><cell>Avg missing rate</cell><cell>75.0%</cell><cell>61.4%</cell><cell>59.1%</cell><cell>58.5%</cell><cell>0.0%</cell></row><row><cell>Avg Measurement freq.</cell><cell>1 hr / 12 hrs</cell><cell>4 hrs / 24 hrs</cell><cell>1 year</cell><cell>1 year</cell><cell>2.3 years</cell></row><row><cell>Avg Correlation within streams</cell><cell>0.4122</cell><cell>0.3436</cell><cell>0.1213</cell><cell>0.1157</cell><cell>0.2424</cell></row><row><cell>Avg Correlation across streams</cell><cell>0.3127</cell><cell>0.3454</cell><cell>0.0875</cell><cell>0.0897</cell><cell>0.0506</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison for missing data estimation</figDesc><table><row><cell>Category</cell><cell>Algorithm</cell><cell></cell><cell cols="4">Mean RMSE (% Gain of M-RNN (Multiple Imputations))</cell></row><row><cell></cell><cell></cell><cell>MIMIC-III</cell><cell>Deterioration</cell><cell>UNOS-Heart</cell><cell>UNOS-Lung</cell><cell>Biobank</cell></row><row><cell>M-RNN</cell><cell>M-RNN (MI) M-RNN (SI)</cell><cell>0.0141 (-) 0.0144 (-)</cell><cell>0.0105 (-) 0.0108 (-)</cell><cell>0.0479 (-) 0.0477 (-)</cell><cell>0.0606 (-) 0.0609 (-)</cell><cell>0.0637 (-) 0.0629 (-)</cell></row><row><cell></cell><cell>[23]</cell><cell cols="5">0.0337 (58.2%) 0.0258 (59.3%) 0.1352 (64.6%) 0.1343 (54.9%) 0.0812 (21.6%)</cell></row><row><cell>RNN-based</cell><cell>[24]</cell><cell cols="5">0.0295 (52.2%) 0.0241 (56.4%) 0.1179 (59.4%) 0.1264 (52.1%) 0.0801 (20.5%)</cell></row><row><cell></cell><cell>[25]</cell><cell cols="5">0.0292 (51.7%) 0.0233 (54.9%) 0.1057 (54.7%) 0.1172 (48.3%) 0.0778 (18.1%)</cell></row><row><cell>Interpolation</cell><cell>Spline Cubic</cell><cell cols="5">0.0735 (80.8%) 0.0215 (51.2%) 0.1102 (56.5%) 0.1199 (49.5%) 0.0845 (24.6%) 0.0279 (49.5%) 0.0223 (52.9%) 0.1072 (55.3%) 0.1177 (48.5%) 0.0887 (28.2%)</cell></row><row><cell></cell><cell>MICE</cell><cell cols="5">0.0611 (76.9%) 0.0319 (67.1%) 0.1147 (58.2%) 0.1151 (47.4%) 0.0915 (30.4%)</cell></row><row><cell>Imputation</cell><cell>MissForest</cell><cell cols="2">0.0293 (51.9%) 0.0264 (60.2%)</cell><cell>0.0489 (2.0%)</cell><cell>0.0652 (7.1%)</cell><cell>0.0892 (28.6%)</cell></row><row><cell></cell><cell>EM</cell><cell cols="2">0.0467 (69.8%) 0.0355 (70.4%)</cell><cell>-</cell><cell>-</cell><cell>0.0978 (34.9%)</cell></row></table><note>Others Matrix Completion 0.0311 (54.7%) 0.0264 (60.2%) 0.0974 (50.8%) 0.0942 (35.7%) 0.0886 (28.1%) Auto-encoder 0.0412 (66.0%) 0.0309 (65.0%) 0.0589 (18.7%) 0.0712 (14.9%) 0.0805 (20.9%) MCMC 0.0437 (67.7%) 0.0364 (71.2%) 0.1091 (56.1%) 0.1124 (46.1%) 0.0936 (31.9%)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison for joint interpolation/imputation algorithms</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell cols="3">Mean RMSE (% Gain from Imputation Algorithm)</cell></row><row><cell></cell><cell>MIMIC-III</cell><cell>Deterioration</cell><cell>UNOS-Heart</cell><cell>UNOS-Lung</cell><cell>Biobank</cell></row><row><cell>Spline + MICE</cell><cell cols="2">0.0602 (1.5%) 0.0320 (-0.3%)</cell><cell>0.1141 (0.5%)</cell><cell cols="2">0.1133 (1.7%) 0.0895 (2.2%)</cell></row><row><cell cols="2">Spline + MissForest 0.0291 (0.7%)</cell><cell>0.0259 (1.9%)</cell><cell cols="3">0.0491 (-0.4%) 0.0641 (1.4%) 0.0879 (4.1%)</cell></row><row><cell>Cubic + MICE</cell><cell>0.0605 (1.0%)</cell><cell>0.0315 (1.3%)</cell><cell>0.1137 (0.9%)</cell><cell cols="2">0.1138 (1.1%) 0.0901 (1.6%)</cell></row><row><cell cols="2">Cubic + MissForest 0.0289 (1.4%)</cell><cell>0.0261 (1.1%)</cell><cell cols="3">0.0493 (-0.8%) 0.0643 (1.4%) 0.0887 (3.2%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Source of Gain of M-RNN. (Performance degradation from original M-RNN)</figDesc><table><row><cell>Datasets</cell><cell cols="3">M-RNN (Mean RMSE; % Gain)</cell></row><row><cell cols="4">Only Interp Only Impute Interp + Impute</cell></row><row><cell>MIMIC-III</cell><cell>0.0191 (26.2 %)</cell><cell>0.0312 (54.8 %)</cell><cell>0.0141 (-)</cell></row><row><cell>Deterioration</cell><cell>0.0133 (21.1 %)</cell><cell>0.0295 (64.4 %)</cell><cell>0.0105 (-)</cell></row><row><cell>UNOS-Heart</cell><cell>0.0897 (46.6 %)</cell><cell>0.0531 (9.8 %)</cell><cell>0.0479 (-)</cell></row><row><cell>UNOS-Lung</cell><cell>0.0998 (39.3 %)</cell><cell>0.0734 (17.4 %)</cell><cell>0.0606 (-)</cell></row><row><cell>Biobank</cell><cell>0.0794 (19.8 %)</cell><cell>0.0778 (18.1 %)</cell><cell>0.0637 (-)</cell></row><row><cell>Missing Rate (%)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison for patient state prediction with a 1-layer RNN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Congeniality of imputation models</figDesc><table><row><cell>Algorithm</cell><cell>Mean Bias (||w −ŵ||1)</cell><cell>Root Mean Square Error (||w −ŵ||2)</cell></row><row><cell>M-RNN (MI)</cell><cell>0.0814 ± 0.0098</cell><cell>0.1229 ±0.0151</cell></row><row><cell>[25]</cell><cell>0.1097 ± 0.0104</cell><cell>0.1649 ± 0.0212</cell></row><row><cell cols="2">Cubic Interpolation 0.1169 ± 0.01075</cell><cell>0.1816 ± 0.0201</cell></row><row><cell>MissForest</cell><cell>0.0842 ±0.0103</cell><cell>0.1312 ± 0.0139</cell></row><row><cell>Matrix Completion</cell><cell>0.1001 ± 0.0125</cell><cell>0.1551 ± 0.0230</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison for patient state prediction using Deterioration dataset</figDesc><table><row><cell>Category</cell><cell>Algorithms</cell><cell cols="4">AUROC (% Gain of M-RNN (Multiple Imputations))</cell></row><row><cell></cell><cell></cell><cell>RNN</cell><cell cols="2">Random Forest Logistic Regression</cell><cell>Xgboost</cell></row><row><cell>M-RNN</cell><cell>M-RNN (MI) M-RNN (SI)</cell><cell>0.7779 (-) 0.7783 (-)</cell><cell>0.7568 (-) 0.7577 (-)</cell><cell>0.7657 (-) 0.7652 (-)</cell><cell>0.7619 (-) 0.7599 (-)</cell></row><row><cell></cell><cell>[23]</cell><cell>0.7558 (9.0%)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RNN-based</cell><cell>[24]</cell><cell>0.7551 (9.3%)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>[25]</cell><cell>0.7593 (7.7%)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Spline</cell><cell>0.7542 (9.6%)</cell><cell>0.7396 (6.6%)</cell><cell>0.7504 (6.1%)</cell><cell>0.7442 (6.9%)</cell></row><row><cell>Interpolation</cell><cell>Cubic</cell><cell>0.7569 (8.6%)</cell><cell>0.7348 (8.3%)</cell><cell>0.7475 (7.2%)</cell><cell>0.7406 (8.2%)</cell></row><row><cell></cell><cell>MICE</cell><cell>0.7571 (8.6%)</cell><cell>0.7315 (9.4%)</cell><cell>0.7422 (9.1%)</cell><cell>0.7358 (9.9%)</cell></row><row><cell>Imputation</cell><cell>MissForest</cell><cell>0.7578 (8.3%)</cell><cell>0.7331 (8.9%)</cell><cell>0.7459 (7.8%)</cell><cell>0.7403 (8.3%)</cell></row><row><cell></cell><cell>EM</cell><cell cols="2">0.7531 (10.0%) 0.7252 (11.5%)</cell><cell>0.7355 (11.4%)</cell><cell>0.7286 (12.3%)</cell></row><row><cell></cell><cell>Matrix Completion</cell><cell>0.7551 (9.3%)</cell><cell>0.7272 (10.9%)</cell><cell>0.7454 (8.0%)</cell><cell>0.7385 (8.9%)</cell></row><row><cell>Others</cell><cell>Auto-encoder</cell><cell cols="2">0.7488 (11.6%) 0.7256 (11.4%)</cell><cell>0.7343 (11.8%)</cell><cell>0.7279 (12.5%)</cell></row><row><cell></cell><cell>MCMC</cell><cell cols="2">0.7512 (10.7%) 0.7238 (11.9%)</cell><cell>0.7325 (12.4%)</cell><cell>0.7270 (12.8%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison for missing value estimation with/without donor features in UNOS dataset</figDesc><table><row><cell>Category</cell><cell>Algorithms</cell><cell cols="4">Mean RMSE (% Gain of M-RNN (Multiple Imputations))</cell></row><row><cell></cell><cell></cell><cell>Heart (with)</cell><cell>Heart (w/o)</cell><cell>Lung (with)</cell><cell>Lung (w/o)</cell></row><row><cell>M-RNN</cell><cell>M-RNN (MI) M-RNN (SI)</cell><cell>0.0451 (-) 0.0453 (-)</cell><cell>0.0479 (-) 0.0477 (-)</cell><cell>0.0579 (-) 0.0583 (-)</cell><cell>0.0606 (-) 0.0609 (-)</cell></row><row><cell></cell><cell>[23]</cell><cell cols="4">0.1352 (66.6%) 0.1352 (64.6%) 0.1343 (56.9%) 0.1343 (54.9%)</cell></row><row><cell>RNN-based</cell><cell>[24]</cell><cell cols="4">0.1179 (61.7%) 0.1179 (59.4%) 0.1264 (54.2%) 0.1264 (52.1%)</cell></row><row><cell></cell><cell>[25]</cell><cell cols="4">0.1057 (57.3%) 0.1057 (54.7%) 0.1172 (50.6%) 0.1172 (48.3%)</cell></row><row><cell>Interpolation</cell><cell>Spline Cubic</cell><cell cols="4">0.1102 (59.1%) 0.1102 (56.5%) 0.1199 (51.7%) 0.1199 (49.5%) 0.1072 (57.9%) 0.1072 (55.3%) 0.1177 (50.8%) 0.1177 (48.5%)</cell></row><row><cell>Imputation</cell><cell>MICE MissForest</cell><cell cols="4">0.1067 (57.7%) 0.1147 (58.2%) 0.1015 (43.0%) 0.1151 (47.4%) 0.0465 (3.0%) 0.0489 (2.0%) 0.0627 (7.7%) 0.0652 (7.1%)</cell></row><row><cell></cell><cell cols="5">Matrix Completion 0.0767 (41.2%) 0.0974 (50.8%) 0.0819 (29.3%) 0.0942 (35.7%)</cell></row><row><cell>Others</cell><cell>Auto-encoder</cell><cell cols="4">0.0544 (17.1%) 0.0589 (18.7%) 0.0668 (13.3%) 0.0712 (14.9%)</cell></row><row><cell></cell><cell>MCMC</cell><cell cols="4">0.0934 (51.7%) 0.1091 (56.1%) 0.1017 (43.1%) 0.1124 (46.1%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Performance comparison for label prediction (AUROC) with/without donor features in UNOS dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Performance comparison for prediction oriented M-RNN</figDesc><table><row><cell>Algorithms</cell><cell></cell><cell cols="3">AUROC (% Gain of M-RNN (Multiple Imputations))</cell><cell></cell></row><row><cell></cell><cell>MIMIC-III</cell><cell>Deterioration</cell><cell>UNOS-Heart</cell><cell>UNOS-Lung</cell><cell>Biobank</cell></row><row><cell>M-RNN: Predict</cell><cell>0.8578 (-)</cell><cell>0.7813 (-)</cell><cell>0.6897 (-)</cell><cell>0.6802 (-)</cell><cell>0.8987 (-)</cell></row><row><cell>M-RNN: Original</cell><cell>0.8531 (3.2%)</cell><cell>0.7779 (1.5%)</cell><cell>0.6855 (1.3%)</cell><cell>0.6762 (1.2%)</cell><cell>0.8955 (3.1%)</cell></row><row><cell>[23]</cell><cell cols="5">0.8381 (12.2%) 0.7558 (10.4%) 0.6505 (11.2%) 0.6557 (7.1%) 0.8802 (15.4%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t for x d t depends on the data with x d t removed, we writex d t = Φ(D − x d t ); but keep in mind that the construction uses only the data from stream d, not the data from other streams. We construct Φ using a bi-directional recurrent neural network (Bi-RNN). However, unlike a conventional Bi-RNN<ref type="bibr" target="#b12">[13]</ref>, the timing of inputs into the hidden layer is lagged in the forward direction and advanced in the</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t for x d t depends on the data with x d t removed, we writex d t = Ψ(D − x d t ); again, keep in mind that now we are using only data at time stamp st, not data from other time stamps. We construct the function Ψ to be independent of t, so we use fully connected layers; see the Imputation component ofFig 2.If we write zt = [xt, mt] then a more mathematical description is:xt = σ(W ht + α) ht = φ(U xt + V zt + β)where σ, φ are activation functions. The diagonal entries of U are zero because we do not use x d t to estimatex d t . We learn the functions Φ and Ψ jointly using the stacked networks of Bi-RNN and Fully Connected (FC) layers, using MSE as the objective function.Φ * , Ψ * = arg min Φ,Ψ L({Ψ {x d t , Φ {x d t , m d t , δ d t } T t=1 , m d t } D d=1 , x d t })(2)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We are grateful for comments from Katrina Poppe (University of Auckland) and Angela Wood (University of Cambridge). This work was supported by the Office of Naval Research (ONR) and the NSF (Grant number: ECCS1462245).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using Real Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Kreindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lumsden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">135</biblScope>
		</imprint>
	</monogr>
	<note>The effects of the irregular sample and missing data in time series analysis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wavelet variance analysis for gappy time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the Institute of Statistical Mathematics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="943" to="966" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple imputation for nonresponse in surveys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pattern classification with missing data: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>García-Laencina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Sancho-Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Figueiras-Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="282" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple imputation using chained equations: issues and guidance for practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Royston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="399" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Missforest-non-parametric missing value imputation for mixed-type data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Stekhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="118" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral regularization algorithms for learning large incomplete matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2287" to="2322" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal regularized matrix factorization for high-dimensional time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recommendations as treatments: debiasing learning and evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swaminatan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral regularization algorithms for learning large incomplete matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2287" to="2322" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Nonlinear Dynamical Systems Analysis for the Behavioral Sciences Using Real Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Kreindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lumsden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">135</biblScope>
		</imprint>
	</monogr>
	<note>The effects of the irregular sample and missing data in time series analysis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from clinical judgments: Semi-markov-modulated marked hawkes processes for risk prognosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international conference on machine learning</title>
		<meeting>the 34th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A gentle introduction to imputation of missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R T</forename><surname>Donders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Van Der Heijden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stijnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Moons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical epidemiology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1087" to="1091" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple imputation for missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Patrician</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in Nursing &amp; Health</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="84" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining multiple imputation and meta-analysis with individual participant data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Resche-Rigon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="4499" to="4514" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The use and reporting of multiple imputation in medical research-a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of internal medicine</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="586" to="593" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple imputation for missing data in epidemiological and clinical research: potential and pitfalls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Sterne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Royston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kenward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bmj</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page">2393</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Personalized risk scoring for critical care prognosis using mixtures of gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Uk biobank: bank on it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">9578</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Doctor ai: Predicting clinical events via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05942</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Directly modeling missing data in sequences with rnns: Improved classification of clinical time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wetzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04130</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01865</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple imputation for general missing data patterns in the presence of high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21689</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple-imputation inferences with uncongenial sources of input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="page" from="538" to="558" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for missing or asynchronous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gingras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A solution for missing data in recurrent neural networks with an application to blood glucose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="971" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speech recognition with missing data using recurrent neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1189" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with missing information imputation for medical examination data prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data and Smart Computing (BigComp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="317" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multiple imputation using deep denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gondara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A markov chain monte carlo algorithm for multiple imputation in large surveys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AStA Advances in Statistical Analysis</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="114" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
