<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
							<email>xuanyi.dong@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SUSTech-UTS Joint Centre of CIS</orgName>
								<orgName type="institution" key="instit2">Southern China University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SUSTech-UTS Joint Centre of CIS</orgName>
								<orgName type="institution" key="instit2">Southern China University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial landmark detection aims to localize the anatomically defined points of human faces. In this paper, we study facial landmark detection from partially labeled facial images. A typical approach is to (1) train a detector on the labeled images; (2) generate new training samples using this detector's prediction as pseudo labels of unlabeled images;</p><p>(3) retrain the detector on the labeled samples and partial pseudo labeled samples. In this way, the detector can learn from both labeled and unlabeled data to become robust.</p><p>In this paper, we propose an interaction mechanism between a teacher and two students to generate more reliable pseudo labels for unlabeled data, which are beneficial to semi-supervised facial landmark detection. Specifically, the two students are instantiated as dual detectors. The teacher learns to judge the quality of the pseudo labels generated by the students and filter out unqualified samples before the retraining stage. In this way, the student detectors get feedback from their teacher and are retrained by premium data generated by itself. Since the two students are trained by different samples, a combination of their predictions will be more robust as the final prediction compared to either prediction. Extensive experiments on 300-W and AFLW benchmarks show that the interactions between teacher and students contribute to better utilization of the unlabeled data and achieves state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial landmark detection aims to find some pre-defined anatomical keypoints of human faces <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b36">37]</ref>. These keypoints include the corners of a mouth, the boundary of eyes, the tip of a nose, etc <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref>. It is usually a prerequisite of a large number of computer vision tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3]</ref>. For example, facial landmark coordinates are required to align faces to ease the visualization * This work was accepted to IEEE ICCV 2019.  <ref type="figure">Figure 1</ref>. The interaction mechanism between teacher and students. Two student detectors learn to generate pseudo labels for unlabeled samples, among which qualified samples are selected by the teacher. These premium pseudo labeled data along with real labeled data is used for the retraining of the students detectors.</p><p>for users when people would like to sort their faces by time and see the changes over time <ref type="bibr" target="#b8">[9]</ref>. Other examples include face morphing <ref type="bibr" target="#b2">[3]</ref>, face replacement <ref type="bibr" target="#b38">[39]</ref>, etc. The main challenge in recent landmark detection literatures is how to obtain abundant facial landmark labels. The annotation challenge comes from two perspectives. First, a large number of keypoints are required for a single face image, e.g., 68 keypoints for each face in the 300-W dataset <ref type="bibr" target="#b34">[35]</ref>. To precisely depict the facial features for a whole dataset, millions of keypoints are usually required. Second, different annotators have a semantic gap. There is no universal standard for the annotation of the keypoints, so different annotators give different positions for the same keypoints. A typical way to reduce such semantic deviations among various annotators is to merge the labels from several annotators. This will further increase the costs of the whole annotation work.</p><p>Semi-supervised landmark detection can to some extent alleviate the expensive and sophisticated annotations by utilizing the unlabeled images. Typical approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref> for semi-supervised learning use self-training or similar paradigms to utilize the unlabeled samples. For example, the authors of <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref> adopt a heuristic unsupervised criterion to select the pseudo labeled data for the retraining procedure. This criterion is the loss of each pseudo labeled data, where its predicted pseudo label is treated as the ground truth to calculate the loss <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>. Since no extra supervision is given to train the criterion function, this unsupervised loss criterion has a high possibility of passing inaccurate pseudo labeled data to the retraining stage. In this way, these inaccurate data will mislead the optimization of the detector and make it easier to trap into a local minimum. A straightforward solution to this problem is to use multiple models and regularize each other by the co-training strategy <ref type="bibr" target="#b3">[4]</ref>. Unfortunately, even if co-training performs well in simple tasks such as classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>, in more complex scenarios such as detection, co-training requires extremely sophisticated design and careful tuning of many additional hyper-parameters <ref type="bibr" target="#b11">[12]</ref>, e.g., more than 10 hyper-parameters for three models in <ref type="bibr" target="#b27">[28]</ref>.</p><p>To better utilize the pseudo labeled data as well as avoid the complicated model tuning for landmark detection, we propose Teacher Supervises StudentS (TS 3 ). As illustrated in <ref type="figure">Figure 1</ref>, TS 3 is an interaction mechanism between one teacher network and two (or multiple) student networks. Two student detection networks learn to generate pseudo labels for unlabeled images. The teacher network learns to judge the quality of the pseudo labels generated from students. Consequently, the teacher can select qualified pseudo labeled samples and use them to retrain the students. TS 3 applies these steps in an iterative manner, where students gradually become more robust, and the teacher is adaptively updated with the improved students. Besides, two students can also encourage each other to advance their performances in two ways. First, predictions from two students can be ensembled to further improve the quality of pseudo labels. Second, two students can regularize each other by training on different samples. The interactions between the teacher and students as well as the students themselves help to provide more accurate pseudo labeled samples for retraining and the model does not need careful hyper-parameter tuning.</p><p>To highlight our contribution, we propose an easy-totrain interaction mechanism between teacher and students (TS 3 ) to provide more reliable pseudo labeled samples in semi-supervised facial landmark detection. To validate the performance of our TS 3 , we do experiments on 300-W, 300-VW, and AFLW benchmarks. TS 3 achieves state-of-the-art semi-supervised performance on all three benchmarks. In addition, using only 30% labels, our TS 3 achieves competitive results compared to supervised methods using all labels on 300-W and AFLW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We will first introduce some supervised facial landmark algorithms in Section 2.1. Then, we will compare our algorithm with semi-supervised learning algorithms and semisupervised facial landmark algorithm in Section 2.2. Lastly, we explain our algorithm in a meta learning perspective in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Supervised Facial Landmark Detection</head><p>Supervised facial landmark detection algorithms can be categorized into linear regression based methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b6">7]</ref> and heatmap regression based methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>. Linear regression based methods learn a function that maps the input face image to the normalized landmark coordinates <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b6">7]</ref>. Heatmap regression based methods produce one heatmap for each landmark, where the coordinate is the location of the highest response on this heatmap <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref>. All above algorithms can be readily integrated into our framework, serving as different student detectors.</p><p>These supervised algorithms require a large amount of data to train deep neural networks. However, it is tedious to annotate the precise facial landmarks, which need to average different annotations from multiple different annotators. Therefore, to reduce the annotation cost, it is necessary to investigate the semi-supervised facial landmark detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-supervised Facial Landmark Detection</head><p>Some early semi-supervised learning algorithms are difficult to handle large scale datasets due to the high complexity <ref type="bibr" target="#b7">[8]</ref>. Others exploit pseudo-labels of unlabeled data in the semi-supervised scenario <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>. Since most of these algorithms studied their effect on small-scale datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>, a question remains open: can they be used to improve large-scale semi-supervised landmark detection? In addition, those self-training or co-training approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12]</ref> simply leverage the confidence score or an unsupervised loss to select qualified samples. For example, Dong et al. <ref type="bibr" target="#b11">[12]</ref> proposed a model communication mechanism to select reliable pseudo labeled samples based on loss and score. However, such selection criterion does not reflect the real quality of a pseudo labeled sample. In contrast, our teacher directly learns to model the quality, and selected samples are thus more reliable.</p><p>There are only few of researchers study the semisupervised facial landmark detection algorithms. A recent work <ref type="bibr" target="#b15">[16]</ref> presented two techniques to improve landmark localization from partially annotated face images. The first technique is to jointly train facial landmark network with an attribute network, which predicts the emotion, head pose, etc. In this multi-task framework, the gradient from the attribute network can benefit the landmark prediction. The second technique is a kind of supervision without the need of manual labels, which enables the transformation invariant of landmark prediction. Compared to using the supervision from transformation, our approach leverages a progressive paradigm to learn facial shape information from unlabeled data. In this way, our approach is orthogonal to <ref type="bibr" target="#b15">[16]</ref>, and these two techniques can complement our approach to further boost the performance.</p><p>Radosavovic et al. <ref type="bibr" target="#b30">[31]</ref> applied the data augmentation to improve the quality of generated pseudo landmark labels. For an unlabeled image, they ensemble predictions from multiple transformations, such as flipping and rotation. This strategy can also be used to improve the accuracy of our pseudo labels and complement our approach. Since the data augmentation is not the focus of this paper, we did not apply their algorithms in our approach. Dong et al. <ref type="bibr" target="#b10">[11]</ref> proposed a self-supervised loss by exploiting the temporal consistence on unlabeled videos to enhance the detector. This is a video-based approach and not the focus of our work. Therefore, we do not discuss more with those video-based approach <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Meta Learning</head><p>In a meta learning perspective, our TS 3 learns a teacher network to learn which pseudo labeled samples are helpful to train student detectors. In this sense, we are related to some recent literature in "learning to learn" <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b44">45]</ref>. For example, Ren et al. <ref type="bibr" target="#b32">[33]</ref> learn to re-weight samples based on gradients of a model on the clean validation set. Xu et al. <ref type="bibr" target="#b44">[45]</ref> suggest using meta-learning to tune the optimization schedule of alternative optimization problems. Jiang et al. <ref type="bibr" target="#b17">[18]</ref> propose an architecture to learn data-driven curriculum on corrupted labels. Fan et al. <ref type="bibr" target="#b12">[13]</ref> leverage reinforcement learning to learn a policy to select good training samples for a single student model. These algorithms are designed in the supervised scenarios and can not easily be modified in semi-supervised scenario.</p><p>Difference with other teacher-student frameworks and generative adversarial networks (GAN). Our TS 3 learns to utilize the output (pseudo labels) of the student model qualified by the teacher model to do semi-supervised learning. Other teacher-student methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> aim to fit the output of the student model to that of the teacher model. The student and teacher in our work do similar jobs as the generator and discriminator in GAN <ref type="bibr" target="#b13">[14]</ref>, while we aim to predict/generate qualified pseudo labels in semi-supervised learning using a different training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we will first introduce the scenario of the semi-supervised facial landmark detection in Section 3.1.  <ref type="figure">Figure 2</ref>. A brief overview of the structure between the two student detection networks in our TS 3 . The first network is convolutional pose machine <ref type="bibr" target="#b40">[41]</ref> and the second is stacked hourglass <ref type="bibr" target="#b29">[30]</ref>.</p><p>We explain how to design our student detectors and the teacher network in Section 3.2. Lastly, we demonstrate our overall algorithm in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Semi-Supervised Scenario</head><p>We introduce some necessary notations for the presentation of the proposed method.</p><p>Let L = {(x 1 , y 1 ), (x 2 , y 2 ), ..., (x n l , y n l )} be the labeled data in the training set and U = {(x n l +1 ), (x n l +2 ), ..., (x n l +nu )} be the unlabeled data in the training set, where x i denotes the i-th image, and y i ∈ R 2×K denotes the ground-truth landmark label of x i . K is the number of the facial landmarks, and the k-th column of y i indicates the coordinate of the kth landmark. n l and n u denote the number of labeled data and unlabeled data, respectively. The semi-supervised facial landmark detection aims to learn robust detectors from both L and U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Teacher and Students Design</head><p>The Student Detectors. We choose the convolutional pose machine (CPM) <ref type="bibr" target="#b40">[41]</ref> and stacked hourglass (HG) <ref type="bibr" target="#b29">[30]</ref> models as our student detectors. These two landmark detection architectures are the cornerstone of many facial landmark detection algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b36">37]</ref>. Moreover, their architectures are quite different, and can thus complement each other to achieve a better detection performance compared to using two similar neural architectures. Therefore, we integrate these two detectors in our TS 3 approach. In this paragraph, we will give a brief overview of these two facial landmark detectors. We illustrate the structures of CPM and HG in <ref type="figure">Figure 2</ref>. Both CPM and HG are the heatmap regression based methods and utilize the cascaded structure. Formally, suppose there are M convolutional stages in CPM, the output of CPM is:  <ref type="figure">Figure 3</ref>. The illustration of our teacher network. The input of the teacher is the concatenation of the original RGB face image and the heatmap (pseudo label) predicted by the student detector. The output of teacher is a scalar, representing the quality of the input pseudo labeled face image. During training, we can calculate a detection loss using the ideal heatmap and the predicted heatmap. The teacher aims to fit the negative value of this detection loss by an L1 loss. During evaluation, a higher value of the quality represents a lower detection loss, which means this pseudo labeled image is reliable.</p><formula xml:id="formula_0">f 1 (x i |w 1 ) = {H m i |1 ≤ m ≤ M },<label>(1)</label></formula><p>where f 1 indicates the CPM student detector whose parameters are w 1 . x i is the RGB image of the i-th data-point and H m i ∈ R (K+1)×h ×w indicates the heatmap prediction of the m-th stage. h and w denote the spatial height and width of the heatmap. Similarly, we use f 2 indicates the HG student detector whose parameters are w 2 . The detection loss function of the CPM student is:</p><formula xml:id="formula_1">(f 1 (x i |w 1 ), y i ) = M m ||H m i − H * i || 2 F = M m ||H m i − p(y i )|| 2 F ,<label>(2)</label></formula><p>where p is a function taking the label y i ∈ R 2×K as inputs to generate the the ideal heatmap H * i ∈ R (K+1)×h ×w . Details of p can be found in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30]</ref>. During the evaluation, we take the argmax results over the first K channel of the last heatmap H M as the coordinates of landmarks, and the (K + 1)-th channel corresponding to the background will be omitted.</p><p>The Teacher Network. Since our student detectors are based on heatmap, the pseudo label is in the form of heatmap and ground truth label is the ideal heatmap. We build our teacher network using the structure of discriminators adopted in CycleGAN <ref type="bibr" target="#b45">[46]</ref>. As shown in <ref type="figure">Figure 3</ref>, the input of this teacher network is the concatenation of a face image and its heatmap prediction H M i 1 . The output of this teacher network is a scalar representing the quality of a pseudo labeled facial image. Since we train the teacher on 1 H M i will be resized into the same spatial size as its face image</p><formula xml:id="formula_2">Algorithm 1 The Algorithm Description of Our TS 3 Input: Labeled data L = {(x i , y i )|1 ≤ i ≤ n l } 1: Unlabeled data U = {(x u i )|n l + 1 ≤ i ≤ n u + n l } 2:</formula><p>Two student detectors f 1 with w 1 and f 2 with w 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>The teacher network g with parameters w g 4:</p><p>The selection ratio r and the maximum step S 5: Initialize the w 1 and w 2 by minimizing Eq. (2) on L 6: for i = 1; i ≤ S; i++ do <ref type="bibr">7:</ref> Predict H M i on both L and U using Eq. (5), and denote U with its pseudo labels as U 1 update the first student <ref type="bibr">8:</ref> Optimize teacher with w g by minimizing Eq. (4) on L with prediction H M i and ground truth label H * i 9:</p><p>Compute the quality scalar of each sample in U 1 using the optimized teacher via Eq. Retrain w 2 on L 2 = L ∪ L 2 ex by minimizing Eq. (2) 17: end for Output: Students with optimized parameters w 1 and w 2 the trustworthy labeled data, we could obtain a supervised detection loss by calculating ||H M i − H * i || 2 F . We consider the negative value of this detection loss as the ground truth label of the quality, because a high negative value of the detection loss indicates a high similarity between the predicted heatmap and the ideal heatmap. In another word, a higher quality scalar corresponds to a more accurate pseudo label.</p><p>Formally, denote the teacher network as g, we have:</p><formula xml:id="formula_3">g(x i H M i |w g ) = q i , (3) t (g(x i H M i |w g ), y i ) = |q + ||H M i − H * i || 2 F |,<label>(4)</label></formula><p>where the parameters of the teacher is w g . "x H" first resizes the tensor H into the same spatial shape as x and then concatenates the resized tensor with x to get a new tensor. This new tensor is regarded as pseudo labeled image and will be qualified by the teacher later. The teacher outputs a scalar q i representing the quality of the i-th sample associated with its pseudo label H M i . We optimize the teacher on the trustworthy labeled data by minimizing Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The TS 3 Algorithm</head><p>Our TS 3 aims to progressively improve the performance of the student detector. The key idea is to learn a teacher network that can teach students which pseudo labeled sample is reliable and can be used for training. In this procedure, we define the pseudo label of a facial image is as follows:</p><formula xml:id="formula_4">f (x i ) = 1 2 (f 1 (x i |w 1 ) + f 2 (x i |w 2 )) = { 1 2 (H (1,m) i + H (2,m) i )|1 ≤ m ≤ M }, = {H m i |1 ≤ m ≤ M },<label>(5)</label></formula><p>where H</p><formula xml:id="formula_5">(1,m) i</formula><p>indicates the heatmap prediction from the first student at the m-th stage for the i-th sample. H m i in Eq. (5) indicates the ensemble result from both two students detection networks. It will be used as the prediction during the inference procedure.</p><p>We show our overall algorithm in Algorithm 1. We first initialize the two detectors f 1 and f 2 on the labeled facial images L. Then, in the first round, our algorithm applies the following procedures: (1) generate pseudo labels on L via Eq. (5) and train the teacher network from scratch with these pseudo labels; (2) generate pseudo labels on U and estimate the quality of these pseudo labeled using the learned teacher; (3) select some high-quality pseudo labeled samples to retrain one student network from scratch. (4) repeat the first three steps to update another student detection network. In the next rounds, each student can be improved and generate more accurate pseudo labels. In this way, we will select more pseudo labeled samples when retraining the students. As the rounds go, students will gradually become better, and the teacher will also be adaptive with the improved students. Our interaction mechanism helps to obtain more accurate pseudo labels and select more reliable pseudo labeled samples. As a result, our algorithm achieves better performance in the semi-supervised facial landmark detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Can this algorithm generalize to other tasks? Our algorithm relies on the design of the teacher network. It requires the input pseudo label to be a structured prediction. Therefore, our algorithm is possible to be applied to tasks with structured predictions, such as segmentation and pose estimation, but is not suitable other tasks like classification.</p><p>Limitation. It is challenging for a teacher to judge the quality of a pseudo label for an image, especially when the spatial shape of this image becomes large. Therefore, in this paper, we use an input size of 64×64. If we increase the input size to 256×256, the teacher will fail and need to be modified accordingly. There are two main reasons: (1) the larger resolution requires a deeper architecture or di-lated convolutions for the teacher network and (2) the highresolution faces bring high-dimensional inputs, and consequently, the teacher needs much more training data. This drawback limits the extension of our algorithm to highresolution tasks, such as segmentation. We will explore to solve this problem in the future.</p><p>Further improvements. (1) In our algorithm, during the retraining procedure, a part of unlabeled samples are not involved during retraining. To utilize these unlabeled facial images, we could use self-supervised techniques such as <ref type="bibr" target="#b15">[16]</ref> to improve the detectors. <ref type="bibr" target="#b1">(2)</ref> In this framework, we use only two student detectors, while it is easy to integrate more student detectors. More student detectors are likely to improve the prediction accuracy, but this will introduce more computation costs. <ref type="formula">(3)</ref> The specifically designed data augmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref> is another direction to improve the accuracy and precision of the pseudo labels.</p><p>Will the teacher network over-fit to the labeled data? In Algorithm 1, since labeled data set L is used to optimize both teacher and students, the teacher's judgment could suffer from the over-fitting problem. Most of the students' predictions on the labeled data can be similar to the ground truth labels. In other words, most pseudo labeled samples on L are "correctly" labeled samples. If the teacher is optimized on L with those pseudo labels, it might only learn what a good pseudo labeled sample is, but overlook what a bad one is. It would be more reasonable to let students predict on the unseen validation set, and then train the teacher on this validation set. However, having an additional validation set during training is different from the typical setting of previous semi-supervised facial landmark detection. We would explore this problem in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Studies</head><p>We perform experiments on three benchmark datasets to investigate the behavior of the proposed method. The datasets and experiment settings are introduced in Section 4.1 and Section 4.2. We first compare the proposed semi-supervised facial landmark algorithm with other stateof-the-art algorithms in Sec. 4.3. We then perform ablation studies in Sec. 4.4 and visualize our results at last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The 300-W dataset <ref type="bibr" target="#b34">[35]</ref> annotates 68 landmarks from five facial landmark datasets, i.e., LFPW, AFW, HELEN, XM2VTS, and IBUG. Following the common settings <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27]</ref>, we regard all the training samples from LFPW, HE-LEN and the full set of AFW as the training set, in which there is 3148 training images. The common test subset consists of 554 test images from LFPW and HELEN. The challenging test subset consists of 135 images from IBUG to construct . The full test set the union of the common and challenging subsets, 689 images in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratio</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Common The AFLW dataset <ref type="bibr" target="#b20">[21]</ref> contains 21997 real-world images with 25993 faces in total. They provide at most 21 landmark coordinates for each face, but they exclude invisible landmarks. Faces in AFLW usually have a different head pose, expression, occlusion or illumination, and therefore it causes difficulties to train a robust detector. Following the same setting as in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>, we do not use the landmarks of two ears. There are two types of AFLW splits, i.e., AFLW-Full and AFLW-Frontal following <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b8">9]</ref>. AFLW-Full contains 20000 training samples and 4386 test samples. AFLW-Front uses the same training samples as in AFLW-Full, but only use the 1165 samples with the frontal face as the test set.</p><p>The 300-VW dataset <ref type="bibr" target="#b35">[36]</ref> is a video-based facial landmark benchmark. It contains 50 training videos with 95192 frames. Following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>, we report the results for the 49 inner points on the category C subset of the 300-VW test set, which has 26338 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>Training student detection networks. The first student detector is CPM <ref type="bibr" target="#b40">[41]</ref>. We follow the same model configuration as the base detector used in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref>, and the number of cascaded stages is set as three. Its number of parameters is 16.70 MB and its FLOPs is 1720.98 M. To train CPM, we apply the SGD optimizer with the momentum of 0.9 and the weight decay of 0.0005. For each stage, we train the CPM for 50 epochs in total. We start the learning rate of 0.00005, and reduce it by 0.5 at 20-th, 25-th, 30-th, and 40-th epoch.</p><p>The second student detector is HG <ref type="bibr" target="#b29">[30]</ref>. We follow the same model configuration as <ref type="bibr" target="#b5">[6]</ref> but use the number of cascaded stages of four to build our HG model, where the number of parameters is 24.97 MB and FLOPs is 1600.85 M. To train HG, we apply the RMSprop optimizer with the alpha of 0.99. For each stage, we train the HG for 110 epochs in total. We start the learning rate of 0.00025, and reduce it by 0.5 at 50-th, 70-th, 90-th, and 100-th.</p><p>For both of these two detectors, we use the batch size of eight on two GPUs. To generate the heatmap ground truth labels, we apply the Gaussian distribution with the sigma of 3. Each face image is first resized into the size of 64×64, and then randomly resized between the scale of 0.9 and 1.1. After the random resize operation, the face image will be randomly rotated with the maximum degree of 30, and then randomly cropped with the size of 64×64 2 . We set selection ratio r as 0.1 and the maximum step S as 6 based on crossvalidation.</p><p>Training the teacher network 3 . We build our teacher network using the structure of discriminators adopted in Cy-cleGAN <ref type="bibr" target="#b45">[46]</ref>. Given a 64×64 face image, we first resize the predicted heatmap into the same spatial size of 64×64. We use the Adam to train this teacher network. The initial learning rate is 0.01, and the batch size is 128. Random flip, random rotation, random scale and crop are applied as data argumentation.</p><p>Evaluation. Normalized Mean Error (NME) is usually applied to evaluate the performance for facial landmark predictions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b8">9]</ref>. For the 300-W dataset, we use the inter-ocular distance to normalize mean error following the same setting as in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9]</ref>. For the AFLW dataset, we use the face size to normalize mean error <ref type="bibr" target="#b26">[27]</ref>. Area Under the Curve (AUC) @ 0.08 error is also employed for evaluation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref>. When training on the partially labeled data, the sets of L and U are randomly sampled. During evaluation, we use Eq. (5) to obtain the final heatmap and follow <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30]</ref> to generate the coordinate of each landmark. We repeat each experiment three times and report the mean result. The codes will be public available upon the acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art</head><p>Comparisons on 300-W. We compare our algorithm with several state-of-the-art algorithms <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16]</ref>, as shown in <ref type="table">Table 1</ref>. In this table, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11]</ref> are very recent methods, which represent the state-of-the-art supervised facial landmark algorithms. By using 100% fa-Methods SDM <ref type="bibr" target="#b43">[44]</ref> LBF <ref type="bibr" target="#b33">[34]</ref> CCL <ref type="bibr" target="#b46">[47]</ref>   <ref type="table">Table 3</ref>. AUC @ 0.08 error on 300-VW category C. Note that all compared algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>  cial landmark labels on 300-W training set and unlabeled AFLW, our algorithm achieves competitive 3.49 NME on the 300-W common test set, which is competitive to other state-of-the-art algorithms. In addition, even though our approach utilizes two detectors, the number of parameters is much lower than SAN <ref type="bibr" target="#b8">[9]</ref>. The robust detection performance of ours can be mainly caused by two reasons. First, the proposed teacher network can effectively sample the qualified pseudo labeled data, which enables the model to exploit more useful information. Second, our framework leverages two advanced CNN architectures, which can complement each other. We also compare our TS 3 with a recent work on semisupervised facial landmark detection <ref type="bibr" target="#b15">[16]</ref> in <ref type="table">Table 1</ref>. When using 10% of labels, our TS 3 obtains a lower NME result on the challenging test set than RCN + [16] (5.64 NME vs. 6.32 NME). When using 20% of labels, our TS 3 is also superior to it (5.03 NME vs. 5.88 NME). Note that <ref type="bibr" target="#b15">[16]</ref> utilizes a transformation invariant auxiliary loss function. This auxiliary loss can also be easily integrated into our framework. Therefore, <ref type="bibr" target="#b15">[16]</ref> is orthogonal to our work, combining two methods can potentially achieve a better performance.</p><p>Comparisons on AFLW. We also show the NME comparison on the AFLW dataset in <ref type="table" target="#tab_2">Table 2</ref>. Compared to semi-supervised facial landmark detection algorithm <ref type="bibr" target="#b15">[16]</ref>, we achieve a similar performance. RCN + <ref type="bibr" target="#b15">[16]</ref> can learn transformation invariant information from a large amount of unlabeled images, while ours does not consider this information as it is not our focus. On the AFLW-Full test set, using 20% annotation, our framework achieves 1.99 NME, which is competitive to other supervised algorithms. On the AFLW-Front test set, using only 10% annotation, our <ref type="figure">Figure 4</ref>. We compare three different algorithms, which can train two detectors in a progressive manner: SPL <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref>, SPaCo <ref type="bibr" target="#b27">[28]</ref>, and our TS 3 . All these algorithms iteratively improve detectors one round by another round. The x-axis shows the results of the first five rounds. The y-axis indicates the NME results on the 300-W full test set. framework achieves competitive NME results to <ref type="bibr" target="#b8">[9]</ref>. The above results demonstrate our framework can train a robust detector with much less annotation effort.</p><p>Comparisons on 300-VW. We experiment our algorithm to leverage a large amount of unlabeled facial video frames on 300-VW. We use the labeled 300-W training set and the unlabeled 300-VW training set to train our TS 3 . We evaluate the learned detectors on the 300-VW C test subset w.r.t. AUC @ 0.08. Some video-based facial landmark detection algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref> utilize the labeled 300-VW training data to improve the base detectors. Compared with them, without using any label on 300-VW, our TS 3 obtains a higher AUC result than them, i.e., 59.65 vs. 59.39, as shown in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>The key contribution of our TS 3 lies on two components: (1) the teacher supervising the training data selection of students. (2) the complementary effect of two students. In this subsection, we validate the contribution of these two com- ponents to the final detection performance.</p><p>The effect of the teacher. Compared to other progressive pseudo label generation strategies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>, our designed teacher can sample pseudo labeled with higher quality. In <ref type="figure">Figure 4</ref>, we show the detection results after the first five training rounds (only 10% labels are used). We use SPL <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref> to separately train CPM and HG, and then ensemble them together as Eq. <ref type="bibr" target="#b4">(5)</ref>. We use SPaCo <ref type="bibr" target="#b27">[28]</ref> to jointly optimize CPM and HG in a co-training strategy. To make a fair comparison, at each round, we control the number of pseudo labels is the same across these three algorithms. From <ref type="figure">Figure 4</ref>, several conclusions can be made: (1) TS 3 obtains the lowest NME, because the quality of selected pseudo labels is better than others. (2) SPL falls into a local trap at round 4 and results in a higher error at round 5 , whereas SPaCo and our TS 3 not. This could be caused by that the interaction between two students can help regularize each other. (3) Our TS 3 converges faster than SPaCo and achieves better results. The pseudo labeled data selection in SPaco is a heuristic unsupervised criterion, whereas our criterion is a supervised teacher. Since no extra supervision is given in SPaCo, their criterion might induce inaccurate pseudo labeled samples. Besides, as discussed in Section 3.4, our TS 3 can utilize validation set to further improve the performance by avoid over-fitting, but the compared methods may not effectively utilize validation set.</p><p>The effect of the interaction between students. From <ref type="table" target="#tab_4">Table 4</ref>, we show the ablative studies on the complementary effect of multiple students. In these experiments, we use the same teacher structure, while "CPM" and "HG" are trained without the interaction between students. Using 10% labels, CPM achieves 8.28 NME, and HG achieves 6.25 NME on 300-W. Leveraging from their mutual benefits, our TS 3 can boost the performance to 5.64, which is higher than CPM by about 30% and than HG by 9%. Under different portion of annotations, we can conclude similar observations. This ablation study demonstrates the contribution of student interaction to the final performance. Note that, our algorithm can be readily applied to multiple students without introducing additional hyper-parameters. In contrast, the number of hyper-parameters in other co-training strategies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref> is quadratic to the number of detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>On the 300-W training set, we train our TS 3 using only 10% labeled facial images, and we show some qualitative results of the 300-W test set in <ref type="figure" target="#fig_3">Figure 5</ref>. The first row shows seven raw input facial images. The second row shows the ground truth background heatmaps, and the third row shows the faces with ground truth landmarks of these images. We visualize the predicted background heatmap in the fourth row and the predicted coordinates in the fifth row. As we can see, the predicted landmarks of our TS 3 are very close to the ground truth. These predictions are already robust enough, and human may not be able to distinguish the difference between our predictions (the third line) and the ground truth (the fifth line).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an interaction mechanism between a teacher and multiple students for semi-supervised facial landmark detection. The students learn to generate pseudo labels for the unlabeled data, while the teacher learns to judge the quality of these pseudo labeled data. After that, the teacher can filter out unqualified samples; and the students get feedback from the teacher and improve itself by the qualified samples. The teacher is adaptive along with the improved students. Besides, multiple students can not only regularize each other but also be ensembled to predict more accurate pseudo labels. We empirically demonstrate that the proposed interaction mechanism achieves state-of-the-art performance on three facial landmark benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(3) 10 : 1 ex 11 : 14 :</head><label>1011114</label><figDesc>Pickup the top r × i × |U| samples from U 1 , named as L Retrain w 1 on L 1 = L ∪ L 1 ex by minimizing Eq. (2) 12: Predict H M i on both L and U using Eq. (5), and denote U with its pseudo labels as U 2 update the second student 13: Optimize teacher with w g by minimizing Eq. (4) on L with H M i and H * i Compute the quality scalar of each sample in U 2 using Eq. (3) 15: Pickup the top r × i × |U| samples from U 2 , named as L 2 ex 16:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results on images in the 300-W test set. We train our TS 3 with 314 labeled facial images and 2834 unlabeled facial images in the 300-W training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of NME normalized by face size on the AFLW dataset. † indicates that SBR<ref type="bibr" target="#b10">[11]</ref> used additional unlabeled video data during training. The ratio number in the brackets represents the portion of the labels that we use. Compared to the semi-supervised algorithm<ref type="bibr" target="#b15">[16]</ref>, our TS 3 obtains a similar NME result (2.19 vs. 2.17). Compared to supervised algorithms which use 100% labels, our TS 3 obtains competitive NME when using only 20% labels.</figDesc><table><row><cell>Two-Stage [27] SBR [9] † SAN [9] DSRN [29]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of the NME results on the 300-W test sets for different configuration and models. CPM and HG indicate using only one CPM student or only one HG student in our framework. When using a single detector, we use the heatmap of the last stage in Eq. (1) as prediction. When using two students (TS 3 ), we use H M i in Eq. (5) as prediction. "Ratio" indicates the proportion of labeled data in our semi-supervised setting.</figDesc><table><row><cell cols="5">Ratio Method Common Challenging Full</cell></row><row><cell></cell><cell>CPM</cell><cell>6.86</cell><cell>14.69</cell><cell>8.28</cell></row><row><cell>10%</cell><cell>HG</cell><cell>5.16</cell><cell>11.28</cell><cell>6.25</cell></row><row><cell></cell><cell>TS 3</cell><cell>4.67</cell><cell>9.26</cell><cell>5.64</cell></row><row><cell></cell><cell>CPM</cell><cell>5.36</cell><cell>11.31</cell><cell>6.68</cell></row><row><cell>20%</cell><cell>HG</cell><cell>5.84</cell><cell>10.15</cell><cell>6.68</cell></row><row><cell></cell><cell>TS 3</cell><cell>4.31</cell><cell>7.97</cell><cell>5.03</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Different input image resolution can cause different detection performance. We choose 64×64 to ease the training of our teacher network.<ref type="bibr" target="#b2">3</ref> Model codes are publicly available on GitHub: https://github.com/D-X-Y/landmark-detection</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ronan Collobert, and Jason Weston. Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLT</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D face alignment problem? (and a dataset of 230,000 3D facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Face alignment by explicit shape regression. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09717</idno>
		<title level="m">Network pruning via transformable architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervision-by-Registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to teach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Poseinvariant face alignment with a single cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">and Georgios Tzimiropoulos. Synergy between face alignment and tracking via discriminative global consensus optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Muhammad Haris Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Selfpaced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Teacher and student joint learning for compact facial landmark detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong Joo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wissam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hak</forename><surname>Baddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Tae</forename><surname>Gu Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prototype propagation networks (PPN) for weakly-supervised few-shot learning on category graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring disentangled feature representation beyond face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage reinitialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjing</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-paced co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zina</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vassilis Athitsos, and Heng Huang. Direct shape regression networks for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omnisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Face alignment via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The first facial landmark tracking in-the-wild challenge: Benchmark and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grigoris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Christian Theobalt, and Matthias Nießner. Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Facial landmark detection with tweaked convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanggeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent 3d-2d dual learning for large-pose facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AutoLoss: Learning discrete schedules for alternate optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
