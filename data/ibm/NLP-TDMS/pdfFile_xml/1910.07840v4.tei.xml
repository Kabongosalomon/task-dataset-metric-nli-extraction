<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">END-TO-END SPEECH ENHANCEMENT BASED ON DISCRETE COSINE TRANSFORM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Geng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">END-TO-END SPEECH ENHANCEMENT BASED ON DISCRETE COSINE TRANSFORM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speech enhancement</term>
					<term>DCT</term>
					<term>U-net</term>
					<term>real spectrum</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous speech enhancement methods focus on estimating the short-time spectrum of speech signals due to its shortterm stability. However, these methods often only estimate the clean magnitude spectrum and reuse the noisy phase when resynthesize speech signals, which is unlikely a valid short-time Fourier transform (STFT). Recently, DNN based speech enhancement methods mainly joint estimation of the magnitude and phase spectrum. These methods usually give better performance than magnitude spectrum estimation but need much larger computation and memory overhead. In this paper, we propose using the Discrete Cosine Transform (DCT) to reconstruct a valid short-time spectrum. Under the U-net structure, we enhance the real spectrogram and finally achieve perfect performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The goal of speech enhancement is to separate target speech from the background noise to improve the intelligibility and quality of speech. As a fundamental task in signal processing, speech enhancement has a wide range of applications. Such as improving the quality of mobile communications in noisy environments, hearing aids and providing robustness for automatic speech and speaker recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Traditional speech enhancement approaches include spectral subtraction <ref type="bibr" target="#b2">[3]</ref>, Wiener filtering <ref type="bibr" target="#b4">[4]</ref>, statistical model-based methods <ref type="bibr" target="#b5">[5]</ref>, and nonnegative matrix factorization <ref type="bibr" target="#b6">[6]</ref>. All these models are based on prior knowledge and assumptions of underlying properties of speech and noise, which may not always hold. In recent years, deep learning-based methods have started to attract much attention in the source separation research community by modeling the nonlinear relationship between the mixture and clean speech signals. Typical speech enhancement systems operate in the time Frequency(T-F) domain, only enhancing the magnitude response and leaving the phase in noisy conditions <ref type="bibr" target="#b8">[7]</ref>. This may be because there is no clear structure in phase spectrogram, which makes estimating the clean phase from the noisy phase difficult. These methods can be divided into two categories, namely mask based approaches and spectral mapping approaches. Common mask functions include Ideal Binary Mask(IBM) <ref type="bibr" target="#b9">[8]</ref> and Ideal Ratio Mask(IRM) <ref type="bibr" target="#b0">[1]</ref>, which show better performance than direct spectral mapping.</p><p>Recently, some research has shown the importance of phase when spectrograms are resynthesized back into timedomain waveforms <ref type="bibr" target="#b11">[9]</ref>. One major approach is to use an end-to-end model that takes audio as the raw waveform inputs without using any explicit T-F representation <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b13">11]</ref>. Since raw waveforms inherently contain phase information, it is expected to achieve phase estimation naturally. Another method is estimating the magnitude spectrum and phase spectrum simultaneously <ref type="bibr" target="#b14">[12]</ref><ref type="bibr" target="#b15">[13]</ref><ref type="bibr" target="#b17">[14]</ref>. But estimating phase spectrum is not easy, the result in <ref type="bibr" target="#b14">[12]</ref> shows that separately enhancing the magnitude response and phase response offers little to no improvement over ratio mask alone. However, later studies that defined a complex IRM(cIRM) to jointly enhance magnitude and phase show better performance than IRM <ref type="bibr" target="#b15">[13]</ref>. Recently, <ref type="bibr" target="#b18">[15]</ref> proposed elementary complex building blocks for complex-valued deep neural networks, which put all arithmetic in the complex domain. Based on these blocks, <ref type="bibr" target="#b17">[14]</ref> proposed a new architecture which combined the advantages of both deep complex networks and the U-net <ref type="bibr" target="#b22">[18]</ref>, and finally achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATION TO PRIOR WORK</head><p>Recently deep learning-based method seems more suitable for speech enhancement due to its nonlinear model capability. Many studies have attempted to solve the phase estimation problem because of its importance and difficulty. The research in <ref type="bibr" target="#b19">[16]</ref> tries to minimize a loss defined in the frequency domain to solve the problem that the combination of noisy phase and estimated magnitude is unlikely a valid STFT. But the result shows that this frequency loss using both the magnitude and phase information does not give an as good performance as using only the magnitude information, which is unexpected. Another method tries to define a cIRM that jointly enhances the magnitude and phase spectrum of noisy speech <ref type="bibr" target="#b15">[13]</ref>. However, it only lets the mask in the complex domain, while the whole network structure is real-valued, which could not represent spectral patterns. Based on complex component proposed by <ref type="bibr" target="#b18">[15]</ref>, <ref type="bibr" target="#b17">[14]</ref> and <ref type="bibr" target="#b20">[17]</ref> separately combine it with U-net <ref type="bibr" target="#b22">[18]</ref> and Feed-forward network. These methods are reasonable therefore achieving the best performance. However, complex blocks such as complex convolution and complex batch normalization all need much more computation and memory overhead than that of real components. Besides, complex batch normalization reduces the correlation between the real part and the imaginary part of the complex spectrum. In theory, we could recover the complex spectral only from its real part or imaginary part after symmetric transformation, which could prove the internal relations between them.</p><p>To escape the phase estimation problem, we use DCT to reconstruct a real-valued short-term spectrum. Then we use the common real-valued U-net to estimate the clean spectrum. Under this structure, we finally achieve comparable performance to complex networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SYSTEM OVERVIEWS</head><p>Given the input sequence, we first use DCT to extract the real spectrogram. Next, we treat each real spectrogram as the input to the U-net architecture. At the output of the U-net, we estimate the mask function and then multiply it to the noisy input to achieve the enhanced spectrogram. Finally, we use inverse DCT to recover the estimated clean speech waveform. The whole framework is shown in <ref type="figure" target="#fig_0">Fig.1</ref>. Below we will detail our approach, starting with the theory of DCT, followed by the design of the U-net structure. Finally, we will introduce the mask function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.Discrete cosine transform</head><p>We have proposed to derive Conjugate Symmetric Sequence (n) e x from the original signal (n)</p><p>x to solve the phase estimation problem, where (n) e x is defined as</p><formula xml:id="formula_0">* 1 (n) [ (n) ( n)] 2 e x x x   <label>(1)</label></formula><p>It can be proved that the discrete-time Fourier transform (DTFT) of (n) e x is real-valued and equals to the real part of the complex spectrum of (n)</p><p>x . Then we put the real spectrogram into the network to estimate the enhanced spectrogram. Finally, we could recover (n)</p><p>x from (n) e x in the time domain. In later studies, we found that the DTFT of (n) e x is theoretically equivalent to the DCT of (n)</p><p>x , which we could obtain the real spectrum in an easier way. The DCT of (n)</p><p>x is defined as</p><formula xml:id="formula_1">1 0 2 (2n 1) (k) (k) (n)cos( ) 2 N c n k Xx NN        0,1, , 1 kN  (2)</formula><p>and its inverse DCT is  is the even symmetric sequence of (n)</p><formula xml:id="formula_2">1 0 2 (2n 1) (n) (k) (k)cos( ) 2 N c k k xX NN        0,1, , 1 nN  (3) where 1 0 (k) 2 1 1 1 k kN           <label>(4)</label></formula><p>x . We denote (k) es X as the 2N points DFT of (n) es x , when 0 k N 1    , we have</p><formula xml:id="formula_3">2 2 (k) (k) (k) k j N es c N X e X    <label>(11)</label></formula><p>This shows that (k) c X is the spectrum of (n) es x , which is the even symmetric sequence derived from (n)</p><p>x . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.U-net structure</head><p>The U-net is a well-known architecture composed as a convolutional autoencoder with skip-connections, originally proposed for medical imaging in the computer vision community <ref type="bibr" target="#b22">[18]</ref>. The U-net consists of two stages, namely the encoder stage and the decoder stage. In the encoder stage, we use strided convolution to realize the subsample and in decoder using strided transpose convolution to realize the upsample. After each convolutional layer, we use batch normalization to normalize the layer output. For the activation function, we choose parametric RELU <ref type="bibr" target="#b23">[19]</ref> whose negative x-axis slope is trainable:</p><formula xml:id="formula_4">0 (x) 0 xx f ax x      <label>(12)</label></formula><p>Description of encoder and decoder block is in <ref type="figure" target="#fig_3">Fig.3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.Real-valued mask function</head><p>Although it is possible to directly estimate the spectrogram of a clean speech signal, it has been shown that better performance can be achieved by applying a weighting mask to the mixture spectrogram <ref type="bibr" target="#b12">[10]</ref>. The definition of mask function is , , , Where rIRM is restricted in the range [-K, K], and C is used to control the steepness.</p><formula xml:id="formula_5">tf tf tf S M Y <label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.Dataset</head><p>Voice Bank-DEMAND <ref type="bibr" target="#b24">[20]</ref>: This dataset has been used in recent several denoising works which we choose as baselines.</p><p>To generate the training set, we choose 28 speakers (14 male and 14 female) from the Voice Bank corpus <ref type="bibr" target="#b25">[21]</ref> and 10 types of noise data from the DEMAND <ref type="bibr" target="#b27">[22]</ref>. The signal-tonoise (SNR) values used for training were: 15dB, 10dB, 5dB and 0dB. The test set was chosen from two other speakers (one male and one female), mixed with 5 other noise types.</p><p>The SNR values for test set are 17.5dB, 12.5dB, 7.5dB and 2.5dB. We resample them to 16kHz and normalize the wave amplitude to [-0.5, 0.5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.Experimental setups</head><p>Given the input sequence, we first frame each signal using the hamming window of size 1024 with a frame shift of 64 samples. Next, using DCT to extract the real spectrogram, which is the input of the U-net. The structure of the U-net is in <ref type="figure">Fig.4</ref>. It has ten blocks: five encoder layers and five decoder layers. We add skip-connection to each layer. The convolution kernels are set to be independent of each other by initializing the weight tensor as an orthogonal matrix. After each convolution layer, batch normalization and parametric ReLU are used. In the very last layer the batch normalization is not used and mask function is applied instead. We choose K=2 and C=0.5 for rIRM. We train the https://github.com/BYRTIMO/END-TO-END-SPEECH-ENHANCEMENT-BASED-ON-DISCRETE-COSINE-TRANSFORM network by Adam <ref type="bibr" target="#b28">[23]</ref> optimizer where the learning rate is 1e-3 and beta1 and beta2 are 0 and 0.999, respectively. Epsilon is set to 1e-8 for numerical stability. The batch size is 16. The loss function is the same as in <ref type="bibr" target="#b17">[14]</ref>.  <ref type="figure">Fig.4</ref>.Discription of the U-net structure, the encoder and decoder blocks are described in <ref type="figure" target="#fig_3">Fig.3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.Experimental results</head><p>For a fair model capability comparison, we choose SEGAN <ref type="bibr" target="#b13">[11]</ref>, Wavenet <ref type="bibr" target="#b14">[12]</ref> and DCUnet <ref type="bibr" target="#b17">[14]</ref> as the baselines because they use the same Voice Bank-DEMAND corpus. The SEGAN is based on Generative Adversarial Network whose Generator is the U-net structure. The Wavenet denoising network is modified on <ref type="bibr" target="#b29">[24]</ref>, which is a Generative model for raw audio synthesis. The SEGAN and Wavenet are end-to-end structures directly denoising on the raw waveform. The DCUnet is the state-of-the-art denoising network that combines both the U-net and deep complex networks. For a fair comparison, we choose DCUnet-10(RMRn) and DCUnet-10(cRMCN). The network of RMRn is absolutely the same as ours, but its input is magnitude spectrogram and the ground truth phase was given during training and test. While the cRMCN is a complex-valued network, whose channels per layer are 2 times that of RMRn and ours.</p><p>The evaluation metrics we used are Perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b30">[25]</ref> and three other composite scores, which are CSIG for signal distortion, CBAK for background noise intrusiveness and COVL for overall signal quality <ref type="bibr" target="#b32">[26]</ref>. PESQ values in the range <ref type="bibr">[-0.5, 4.5]</ref> and the other three in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">5]</ref>. All metrics with higher scores mean better performance. The comparison results show in <ref type="table" target="#tab_3">Table 1</ref>. We see that our method is better than SEGAN and Wavenet by a large margin. Compared with RMRn, our method performs better in all metrics and could achieve comparable performance to cRMCN. However, compared with the complex-valued network, our real-valued network is simpler and requires less computation and memory overhead. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.Multiple noise test</head><p>In this section, we test a situation where a speech signal is contaminated with multiple noises. For comparison, we use the Wiener filter method as our baseline. First, we add blue, pink, violet, white noise on a clean utterance selected from the TIMIT database sequentially. The SNR value is 10dB. Then we use both the Wiener filter and our model to filter the noisy speech. The filter results are shown in <ref type="figure" target="#fig_5">Fig.5</ref>. As can be seen from the picture (d), the Wiener filter perfectly filters out blue and violet noise, as well as white noise at high frequencies, but can not filter out pink noise and low-frequency white noise. This is because the Wiener filter estimates the noise spectrum from the blue noise, whose spectrum is mainly at high frequencies. From the picture (c), we could see that our model could denoise all types of noise. However, at the switching point of blue/violet noise and pink noise (the area in the red box), our model can not filter the noise effectively. This may be because the noisy spectrum is not continuous in that place. Next, we will try to solve the problem of noise tracking in speech enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION AND CONCLUSION</head><p>In this paper, we proposed using DCT to reconstruct a valid short-term spectrogram in the real domain, successfully escaping the phase estimation problem in speech enhancement. The experiment result shows that our method could achieve comparable performance to DCUnet. Besides, we test a situation where a speech signal is contaminated with multiple noises. And the result shows that at switching point of noise types, our method could not filter the noise effectively. Next, we will address the noise tracking problem in speech enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of speech enhancement framework with the U-net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>n)) N represents a cyclic shift operation. The relationship between (n)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The relationship between (n) x and (n) es x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Description of encoder and decoder blocks.t F and f F denote the convolution filter size along the time and frequency axis, respectively. t S and f S denote the stride size of the convolution filter. C denotes the number of output channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Multiple noises test results. (a) is the clean speech spectrogram. (b) is the noisy speech spectrogram, blue, pink, violet and white noises are added sequentially. (c) is the denoised spectrogram based on our model. (d) is the denoised spectrogram based on the Wiener filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>:</figDesc><table><row><cell></cell><cell>in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>out</cell></row><row><cell></cell><cell></cell><cell cols="5">Convolution</cell><cell>transpose</cell><cell>Convolution</cell></row><row><cell></cell><cell></cell><cell cols="2">Filter</cell><cell cols="3">size:</cell><cell>(Ft,Ff)</cell><cell>Filter</cell><cell>size:</cell><cell>(Ft,Ff)</cell></row><row><cell></cell><cell></cell><cell cols="4">Stride:</cell><cell>(St,Sf)</cell><cell>Stride:</cell><cell>(St,Sf)</cell></row><row><cell></cell><cell>Enc</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dec</cell></row><row><cell></cell><cell></cell><cell cols="3">#Output</cell><cell cols="2">channel:</cell><cell>C</cell><cell>#Output</cell><cell>channel:</cell><cell>C</cell></row><row><cell>F:</cell><cell>(Ft,</cell><cell>Ff)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F:</cell><cell>(Ft,</cell><cell>Ff)</cell></row><row><cell>S:</cell><cell>(St,</cell><cell>Sf)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>S:</cell><cell>(St,</cell><cell>Sf)</cell></row><row><cell></cell><cell>C</cell><cell>Batch</cell><cell cols="4">normalization</cell><cell>Batch</cell><cell>normalization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C</cell></row><row><cell></cell><cell></cell><cell cols="5">parametric</cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>parametric</cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>out</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results with different methods</figDesc><table><row><cell></cell><cell>PESQ</cell><cell>CSIG</cell><cell>CBAK</cell><cell>COVL</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>3.35</cell><cell>2.44</cell><cell>2.63</cell></row><row><cell>SEGAN</cell><cell>2.16</cell><cell>3.48</cell><cell>2.94</cell><cell>2.8</cell></row><row><cell>Wavenet</cell><cell></cell><cell>3.62</cell><cell>3.23</cell><cell>2.98</cell></row><row><cell>DCUnet-10(RMRn)</cell><cell>2.51</cell><cell>3.71</cell><cell>3.23</cell><cell>3.01</cell></row><row><cell>DCUnet-10(cRMCN)</cell><cell>2.72</cell><cell>3.74</cell><cell>3.6</cell><cell>3.22</cell></row><row><cell>Ours</cell><cell>2.7</cell><cell>3.9</cell><cell>3.29</cell><cell>3.29</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">/2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech enhancement using long short-term memory based recurrent neural networks for noise robust speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolboek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop (SLT). IEEE</title>
		<imprint>
			<biblScope unit="page" from="305" to="311" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancement and bandwidth compression of noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A V. ; R</forename><forename type="middle">E</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Sorace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaughn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1997-09-16" />
			<biblScope unit="volume">1979</biblScope>
			<biblScope unit="page">668</biblScope>
		</imprint>
	</monogr>
	<note>High-speed digital-to-RF converter</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimummean square error short-time spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Supervised and unsupervised speech enhancement using nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2140" to="2151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L R</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards scaling up classification-based speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1381" to="1390" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The importance of phase in speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wójcicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="465" to="494" />
		</imprint>
	</monogr>
	<note>speech communication</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SEGAN: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09452</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Complex ratio masking for joint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">/2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5220" to="5224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03107</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09792</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A New Framework for Supervised Speech Enhancement in the Time Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">/Interspeech</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1136" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring Deep Complex Networks for Complex Spectrogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2019" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6885" to="6889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The voice bank corpus: Design, collection and data analysis of a large regional accent speech database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">/2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation</title>
		<editor>O-COCOSDA/CASLRE</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Meetings Acoust</title>
		<meeting>Meetings Acoust</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A generative model for raw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M P</forename><surname>Hollier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Proceedings (Cat. No. 01CH37221)</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Loizou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
