<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CKConv: Continuous Kernel Convolution For Sequential Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Kuzina</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hoogendoorn</surname></persName>
						</author>
						<title level="a" type="main">CKConv: Continuous Kernel Convolution For Sequential Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional neural architectures for sequential data present important limitations. Recurrent networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional networks are unable to handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that all these problems can be solved by formulating convolutional kernels in CNNs as continuous functions. The resulting Continuous Kernel Convolution (CKConv) allows us to model arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularlysampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a much faster and simpler manner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recurrent Neural Networks (RNNs) have long governed tasks handling sequential data. Their main ingredients are recurrent units: network components endowed with a recurrence property which grants RNNs the ability to be unrolled for arbitrarily many steps, and thus to handle sequences of arbitrary size. In practice, however, the effective memory horizon of RNNs, i.e., the number of steps the network can retain information from, has proven to be surprisingly small. This is most notably due to the vanishing gradients problem <ref type="bibr" target="#b21">(Hochreiter, 1991;</ref><ref type="bibr" target="#b6">Bengio et al., 1994)</ref>. In fact, it is the same recurrent nature of RNNs that grants them the ability to be unrolled for arbitrarily many steps which is responsible for inducing vanishing gradients <ref type="bibr" target="#b37">(Pascanu et al., 2013)</ref>. This, in 1 Vrije Universiteit Amsterdam, Amsterdam, The Netherlands 2 University of Amsterdam, Amsterdam, The Netherlands. Correspondence to: David. W. <ref type="bibr">Romero &lt;d.w.romeroguzman@vu.nl&gt;.</ref> turn, hinders learning from the far past, and thereby induces a small effective memory horizon.</p><p>Convolutional networks (CNNs) have proven a strong alternative to recurrent architectures for several tasks as long as relevant input dependencies fall within their memory horizon, e.g., <ref type="bibr" target="#b11">Conneau et al. (2016)</ref>. CNNs avoid the training instability and vanishing / exploding gradients characteristic of RNNs by circumventing Back-Propagation Through Time (BPTT) altogether. However, these architectures parameterize their convolutional kernels as a sequence of independent weights. As a consequence, their memory horizon must be defined a priori, and the extent of the memory horizon is directly attached to a proportional growth of the model size.</p><p>Inspired by works modeling convolutional kernels as continuous functions, e.g., <ref type="bibr">Wu et al. (2019)</ref>, we propose to replace the conventional discrete convolutional kernel formulation in sequence modeling architectures with a continuous one. Thanks to this formulation, our proposed Continuous Kernel Convolutions (CKConvs) 1 enjoy the following properties:</p><p>• Analogously to RNNs, CKConvs are able to consider arbitrarily large memory horizons within a single operation.</p><p>• Contrary to RNNs, CKConvs do not rely on any form of recurrency. Hence, Continuous Kernel Convolutional Networks (CKCNNs) do not suffer from vanishing / exploding gradients or small effective memory horizons.</p><p>• Contrary to conventional CNNs, CKCNNs detach the memory horizon -often referred to as receptive fieldfrom (i) the depth of the network, (ii) the dilation factor used, and (iii) the parameter count of the architecture.</p><p>• CKCNNs do not make use of Back-Propagation Through Time. Consequently, CKCNNs can be trained in parallel.</p><p>• Since continuous kernels can be evaluated at arbitrary positions, CKConvs easily handle irregularly sampled data as well as data sampled at different sampling rates.</p><p>We demonstrate that these properties are not only theoretical but present in practice as well. With a practical and simple implementation, CKCNNs obtain outstanding results for a large series of tasks encompassing stress tests, real applications with continuous and discrete data, as well as nonuniformly sampled datasets and irregularly sampled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Lessen the vanishing and exploding gradient problem.</p><p>Vanishing and exploding gradients are a long-standing problem for recurrent networks <ref type="bibr" target="#b21">(Hochreiter, 1991)</ref>. We identify three classes of methods that aim to alleviate this problem:</p><p>The first class, exemplified by gating mechanisms, aims to preserve information from the far past and enhance gradient flow by updating their hidden representation only when update gates are activated, e.g., LSTMs <ref type="bibr" target="#b22">(Hochreiter &amp; Schmidhuber, 1997)</ref>, GRUs <ref type="bibr" target="#b10">(Chung et al., 2014)</ref>, gated convolutions <ref type="bibr" target="#b14">(Dauphin et al., 2017)</ref>. The second class, exemplified by unitary recurrent units, parameterizes recurrent connections to have unitary eigenvalues, e.g., <ref type="bibr" target="#b0">Arjovsky et al. (2016)</ref>; <ref type="bibr" target="#b33">Mhammedi et al. (2017)</ref>; <ref type="bibr" target="#b29">Lezcano-Casado &amp; Martínez-Rubio (2019)</ref>. This is motivated by the fact that recurrent connections with eigenvalues other than one are responsible for vanishing and exploding gradients in recurrent architectures <ref type="bibr" target="#b37">(Pascanu et al., 2013;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2016)</ref>. The third class is given by convolutional networks (CNNs), e.g., <ref type="bibr" target="#b35">Oord et al. (2016)</ref>; <ref type="bibr" target="#b12">Dai et al. (2017)</ref>; <ref type="bibr" target="#b3">Bai et al. (2018a)</ref>. CNNs elude exploding and vanishing gradients by avoiding recurrent connections and Back-Propagation Through Time altogether. Our method belongs to this class and thus does not suffer from vanishing or exploding gradients. However, as our continuous kernel parameterization allows for arbitrarily large memory horizons, CKConvs are able to handle arbitrarily large sequences under a fixed parameter budget.</p><p>Implicit neural representations. Implicit neural representations aim to represent data by encoding it in the weights of a neural network <ref type="bibr" target="#b36">(Park et al., 2019;</ref><ref type="bibr" target="#b32">Mescheder et al., 2019;</ref><ref type="bibr">Sitzmann et al., 2020)</ref>. By doing so, implicit representations exhibit numerous advantages over conventional (discrete) ones, e.g., memory efficiency, analytic differentiability, etc.</p><p>Since we model convolutional kernels as continuous functions and parameterize them via neural networks, our approach can be understood as implicitly representing the convolutional kernels of a conventional CNN. Different is the fact that these convolutional kernels are not known a priori but learned as part of the optimization task of the CNN. However, making the connection between implicit neural representations and continuous kernel formulations explicit brings substantial insights for the construction of these kernels. In particular, it motivates the use of sine nonlinearities to parameterize the convolutional kernels, which leads to significant improvements over the ReLU, LeakyReLU, and Swish nonlinearities used so far for this purpose (Sec. 4.3).</p><p>Continuous kernel formulation. Continuous formulations to convolutional kernels were introduced as a powerful alternative to handle irregularly sampled 3D data <ref type="bibr">(Schütt et al., 2017;</ref><ref type="bibr">Simonovsky &amp; Komodakis, 2017;</ref><ref type="bibr">Wang et al., 2018;</ref><ref type="bibr">Wu et al., 2019)</ref>. As discrete convolutions learn independent weights for specific relative positions, they cannot handle irregularly sampled data effectively. After its introduction, most subsequent work has focused on 3D point-cloud applications, e.g., <ref type="bibr">Thomas et al. (2018);</ref><ref type="bibr">Shi et al. (2019)</ref>; <ref type="bibr" target="#b31">Mao et al. (2019)</ref>; <ref type="bibr" target="#b23">Hu et al. (2020)</ref>; <ref type="bibr" target="#b18">Fuchs et al. (2020)</ref>.</p><p>Our paper introduces a new flavor of applications for which continuous kernels are advantageous. To the best of our knowledge, we are first in observing the potential of continuous kernel convolutions for sequence modeling and exploit its relation to recurrent architectures. Though our formulation is similar to those proposed before, we exploit its connection to implicit neural representations to provide a practical kernel parameterization with much better reconstruction properties than previously proposed ones (Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Convolution Operation</head><p>In this section we introduce the convolution operation and outline kernel parameterizations often used in practice.</p><p>Notation. We denote by [n] the set {0, 1, 2, . . . , n}. Bold capital and lowercase letters depict vectors and matrices, e.g., x, W, sub-indices are used to index vectors, e.g., x = {x c } N C c=1 , parentheses are used for time indexing, e.g., x(τ ) is the value of x at time-step τ , and sequences are depicted by calligraphic letters, e.g., X = {x(τ )} N X τ =0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Centered and Causal Convolutions</head><p>Let x ∶ R → R Nc and ψ ∶ R → R Nc be a vector valued signal and kernel on R, such that x = {x c } N C c=1 and ψ = {ψ c } N C c=1 . The convolution is defined as:</p><formula xml:id="formula_0">(x * ψ)(t) = N C c=1 R x c (τ )ψ c (t − τ ) dτ.</formula><p>(1)</p><p>In practice, however, the input signal x is gathered via some sampling procedure. Consequently, the convolution is effectively performed between the input signal described as a sequence of finite length X = {x(τ )} N X τ =0 and a convolutional kernel K = {ψ(τ )} N X τ =0 described in the same way:</p><formula xml:id="formula_1">(x * ψ)(t) = N C c=1 N X 2 τ =− N X 2 x c (τ )ψ c (t − τ ).<label>(2)</label></formula><p>Here, any values x(τ ) falling outside of X are padded by a constant value often defined as zero <ref type="figure" target="#fig_0">(Fig. 1a</ref>).</p><p>The convolutional kernel is commonly centered around the point of calculation t. This can be undesirable for sequence modeling, as future input values {x(t − τ )} −1 τ = −N X 2 are considered during the operation. This is solved by providing a causal formulation to the convolution: a formulation in which the convolution at time-step t only depends on input values at time-steps (t − τ ) ≤ t ( <ref type="figure" target="#fig_0">Fig. 1b)</ref>: This is easily implemented via asymmetrical padding. We consider causal convolutions as default in this document, yet our analyses are valid for centered convolutions too.</p><formula xml:id="formula_2">(x * ψ)(t) = N C c=1 t τ =0 x c (τ )ψ c (t − τ ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convolutions with Discrete Kernels</head><p>By a large margin, the most used parameterization of convolutional kernels ψ is by describing them as a finite sequence of N K + 1 independent weights K = {ψ(τ )} N K τ =0 with equal number of channels to that of the input x ( <ref type="figure" target="#fig_0">Fig. 1</ref>). As these weights are independent of one another, N K is usually much smaller than the input length as to keep the parameter count of the model tractable: N K ≪ N X . As a result, the convolution operation is reduced to:</p><formula xml:id="formula_3">(x * ψ)(t) = N C c=1 N K τ =0 x c (τ )ψ c (t − τ ),<label>(4)</label></formula><p>where values x(τ ) falling outside of X are equally padded. This parameterization presents the following limitations:</p><p>• The memory horizon N K must be defined a priori.</p><p>• Since N K ≪ N X , this parameterization implicitly assumes that the mapping (x * ψ)(t) at position t is only dependent on the input at positions (t − τ ) up to τ = N K steps in the past. Consequently, no functions depending on inputs x(t − τ ) for τ &gt; N K can be modeled.</p><p>• The most general selection of N K is given by a global memory horizon: N K = N X . Unfortunately, as discrete convolutional kernels are modeled as a sequence of independent weights, this incurs an extreme growth of the model size and rapidly becomes statistically unfeasible.</p><p>Dilated Convolution. In order to alleviate the limitations mentioned previously, it has been proposed to dilate the sequence parameterizing the convolutional kernel K by a factor η. That is, to stretch the distances between filter positions ψ(τ ), ψ(τ + 1) by a factor η as to cover a larger memory horizon without any additional parameters:</p><formula xml:id="formula_4">(x * η ψ)(t) = N C c=1 N K τ =0 x c (ητ )ψ c (t − ητ ).<label>(5)</label></formula><p>Though this formulation alleviates some of the limitations outlined before, it introduces some other important ones:</p><p>• Dilated convolutions are unable to model functions depending on input values within x(ητ ) and x(η(τ + 1)).</p><p>• Several authors propose to use dilated convolutions with varying dilation factors as a function of the network depth, e.g., <ref type="bibr" target="#b35">Oord et al. (2016)</ref>. By carefully selecting the dilation factor at every layer, one can guarantee that some kernel hits each input within the memory horizon of the network. Due to the extreme sparsity of this formulation, however, it is difficult to estimate the effective amount of processing applied to the input. Furthermore, this structure ties together the memory horizon, the depth, and the layerwise dilation factors of the network, which effectively constrains the flexibility of the neural architecture design.</p><p>In contrast to (dilated) discrete convolutions, our proposed formulation allows handling arbitrarily long sequences with arbitrarily large, dense memory horizons in a single layer and under a fixed parameter budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Continuous Kernel Convolution</head><p>In this section, we introduce our approach. First, we analyze its properties and illustrate its connection to recurrent units. Subsequently, we discuss concrete parameterizations of continuous convolutional kernels, illustrate its connection to implicit neural representations and empirically show that our final kernels are able to fit complex nonlinear functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>We formulate the convolutional kernel ψ as a continuous function parameterized by a small neural network MLP ψ . MLP ψ receives a relative position (t−τ ) as input and outputs the value of the convolutional kernel at that position ψ(t−τ ). Hence, we can construct an arbitrarily large convolutional kernel K={ψ(t − τ )} N K τ =0 by providing an equally large sequence of relative positions {t − τ } N K τ =0 to MLP ψ <ref type="figure" target="#fig_1">(Fig. 2</ref>). For N K = N X , we can sample a convolutional kernel of equal size to the input sequence X. In other words, we are able to construct global memory horizons without modifying the structure of the network or adding more parameters.</p><p>The Continuous Kernel Convolution (CKConv) is given by:</p><formula xml:id="formula_5">(x * ψ)(t) = N C c=1 t τ =0 x c (τ )MLP ψ c (t − τ ).<label>(6)</label></formula><p>Irregularly sampled data. CKConvs are able to handle irregularly sampled and partially observed data natively. To this end, it is sufficient to sample MLP ψ at positions for which the input signal is known and perform the convolution operation with the sampled kernel. This formulation holds as long as the input is not very non-uniformly sampled. In such cases, an inverse density function over the samples can be incorporated to provide an unbiased estimation of the convolution response (see <ref type="bibr">Appx. A.1, Wu et al. (2019)</ref>).</p><p>Data sampled at different sampling rates. In addition, CKConvs can also be used to process data sampled at differ- ent sampling rates. Consider the convolution (x * ψ) sr1 between an input signal x and a continuous convolutional kernel ψ both sampled at a sampling rate sr 1 . Now, if the convolution receives the same input signal sampled at a different sampling rate sr 2 , one can simply sample the convolutional kernel at the sampling rate sr 2 to perform an "equivalent" convolution: (x * ψ) sr2 . As shown in Appx.A.2, it holds that:</p><formula xml:id="formula_6">(x * ψ) sr2 (t) ≈ sr 2 sr 1 (x * ψ) sr1 (t).<label>(7)</label></formula><p>That is, convolutions calculated at different sampling rates sr 1 and sr 2 are approximately equal up to a normalization factor sr2 sr1 . Hence, CKCNNs (i) can be deployed at sampling rates different than those seen during training, and (ii) can be trained on data with varying temporal resolutions.</p><p>We note that the features listed in this section are hardly attainable by regular architectures, with an exception being those that provide a continuous-time interpretation to RNNs, e.g., <ref type="bibr" target="#b19">Gu et al. (2020)</ref>; <ref type="bibr" target="#b24">Kidger et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Linear Recurrent Units Are CKConvs</head><p>Interesting insights can be obtained by drawing connections between convolutions and recurrent units. In particular, we can show that linear recurrent units can be described as a CKConv with a particular family of convolutional kernels: exponential functions. Besides providing a generalization to recurrent units, this equality provides a fresh and intuitive view to the analysis of vanishing and exploding gradients.</p><p>Recurrent unit. Given an input sequence X = {x(τ )} N X τ =0 , a recurrent unit is constructed as:</p><formula xml:id="formula_7">h(τ ) = σ(Wh(τ − 1) + Ux(τ )) (8) y(τ ) = softmax(Vh(τ )),<label>(9)</label></formula><p>where U, W, V parameterize the input-to-hidden, hiddento-hidden and hidden-to-output connections of the unit. h(τ ),ỹ(τ ) depict the hidden representation and the output at time-step τ , and σ represents a point-wise non-linearity.</p><p>We are able to represent a linear recurrent unit, i.e., a recurrent unit with σ=Id, as a convolutional operation. To see this, consider the definition of the unit hidden representation h(τ ) (Eq. 8) unrolled for t steps. We obtain that:</p><formula xml:id="formula_8">h(t) = W t+1 h(−1) + t τ =0 W τ Ux(t − τ ),<label>(10)</label></formula><p>where h(−1) is the initial state of the hidden representation. We see that in fact it corresponds to a convolution between an input signal x and a convolutional kernel ψ given by: 2,3</p><formula xml:id="formula_9">x = [x(0), x(1), ..., x(t − 1), x(t)] (11) ψ = [U, WU, ..., W t−1 U, W t U] (12) h(t) = t τ =0 x(τ )ψ(t − τ ) = t τ =0 x(t − τ )ψ(τ ).<label>(13)</label></formula><p>Drawing this equality yields some important insights:</p><p>The cause of the exploding and vanishing gradients.</p><p>The correspondence in Eqs. 11-13 intuitively depicts the root of the exploding and vanishing gradient problem. It stems from sequence elements x(t − τ ) τ steps back in the past being multiplied with an effective convolutional weight ψ(τ )=W τ U. For eigenvalues of W other than one, the resulting convolutional kernel ψ can only represent functions that either grow or decrease exponentially as a function of the sequence length (Figs. 3a, 3b). As a result, the contribution of input values in the past either rapidly fades away or governs the updates of the model parameters.</p><p>Since exponentially growing gradients lead to divergence, the eigenvalues of W for converging architectures are often smaller than 1. This explains why the effective memory horizon of recurrent networks is so small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear recurrent units are a subclass of CKConvs.</head><p>Linear recurrent units can be described as a convolution between the input and a very specific class of convolutional kernels: exponential functions (Eq. 12). In general, however, convolutional kernels are not restricted to this functional class. This can be seen in conventional (discrete) convolu-  tions, whose kernels are able to model complex functions within their memory horizon. Unfortunately, discrete convolutions use a predefined, small kernel size, and thus possess a restricted memory horizon. This is equivalent to imposing an effective magnitude of zero to all input values outside the memory horizon ( <ref type="figure" target="#fig_3">Fig. 3c</ref>). CKConvs, on the other hand, are able to define arbitrary large memory horizons. For memory horizons of size equal to the input length, CKConvs are able to model complex functions upon the entire input ( <ref type="figure" target="#fig_3">Fig. 3d</ref>).</p><p>In conclusion, we illustrate that CKConvs are also a generalization of (linear) recurrent architectures which allows for parallel training and enhanced expressivity. This statement also holds for nonlinear recurrent units, in which case it is sufficient to stack CKConvs and point-wise non-linearities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Continuous Convolutional Kernel MLP ψ</head><p>So far we have illustrated the advantages of a continuous convolutional kernel formulation without providing a concrete parameterization to MLP ψ . In this section, we formalize this parameterization, analyze its properties and show empirically that it is able to fit very complex functions.</p><p>Overview. Let {∆τ i =(t − τ i )} N i=0 be a sequence of relative positions. The convolutional kernel MLP ψ is parameterized by a conventional L-layer neural network:</p><formula xml:id="formula_10">h (1) (∆τ i ) = σ w (1) ∆τ i + b (1) (14) h (l) (∆τ i ) = σ W (l) h (l−1) (∆τ i ) + b (l)<label>(15)</label></formula><formula xml:id="formula_11">ψ(∆τ i ) = W (L) h (L−1) (∆τ i ) + b (L) ,<label>(16)</label></formula><p>where σ is a pointwise non-linearity, e.g., ReLU. As stated in Sec. 2, our approach can be thought of as providing implicit neural representations to the unknown convolutional kernels ψ of a conventional convolutional architecture.</p><p>The importance of initialization. There is an important distinction between implicit neural representations and conventional neural applications regarding the assumed distribution of the input. Conventional applications assume the distribution of the input features to be centered around the origin. This is orthogonal to implicit neural representations, where the spatial distribution of the output, i.e., the value of the function being implicitly represented, is uniformly distributed. Consequently, conventional initialization techniques lead to poor performance ("ReLU 0-Init", <ref type="figure" target="#fig_5">Fig. 5</ref>).</p><p>For ReLU networks, function approximation is equivalent to an approximation via a max-spline basis <ref type="bibr" target="#b5">(Balestriero &amp; Baraniuk, 2018)</ref>. The expressiveness of such an approximation is determined by the number of knots the basis provides, i.e., places where a non-linearity bends the space. Naturally, the better the placing of these knots at initialization, the faster the approximation may converge. For applications for which the data values are centered around zero, placing the knots at initialization around zero is a good inductive bias. 4 However, for a spatially uniform distributed input, the knots should be uniformly distributed as well ( <ref type="figure" target="#fig_4">Fig. 4</ref>).</p><p>An improved initialization scheme. For ReLU layers y=max{0, Wx + b}, knots appear at the point where 0=Wx + b. In order to place knots at x=0, it is sufficient to set the bias to zero: b=0. For uniformly distributed knots in the range [x min , x max ], one must solve the ReLU equation for uniformly distributed points in that range: 0=Wx unif +b.</p><p>It results that b= − Wx unif , for arbitrary values of W.</p><p>In multilayered networks (Eqs. 14-16), the approximation problem can be understood as reconstructing the target function in terms of a basis h (L−1) . Consequently, the expressivity of the network is determined by the number of knots in h (L−1) . In theory, each ReLU layer is able to divide the linear regions of the previous layer in exponentially many sub-regions <ref type="bibr" target="#b34">(Montufar et al., 2014;</ref><ref type="bibr">Serra et al., 2018)</ref>, or equivalently, to induce an exponential layer-wise increase in the number of knots. For the first layer, the positions of the knots are described by the bias term. For subsequent layers, these positions also depend on W (l) . Unfortunately, as depicted by <ref type="bibr" target="#b20">Hanin &amp; Rolnick (2019)</ref>, slight modifications of {W (l) , b (l) } can strongly simplify the landscape of the linear regions, and thus the knots <ref type="figure">(Fig. 6</ref>). More importantly, <ref type="bibr" target="#b20">Hanin &amp; Rolnick (2019)</ref> show that the number of linear regions at initialization is actually equal to a constant times the number of neurons in the network (with a constant very close to one in their experiments). In addition, they show that this behavior barely changes throughout training. and a periodic basis (right). As the target function is defined uniformly on a given interval, uniformly initializing the knots of the spline basis provides faster and better approximations. If all the knots are initialized at zero, the best approximation at initialization is given by a straight line. Periodic bases periodically bend space. As a consequence, they can be tuned much easier in order to better approximate the target function at arbitrary points in space.</p><p>We observe that finding an initialization with an exponential number of knots is a cumbersome and unstable procedure. In fact, this is not always possible, and, whenever possible, this initialization strongly restricts the values the weights W (l) can assume. Consequently, based on the findings of <ref type="bibr" target="#b20">Hanin &amp; Rolnick (2019)</ref>, we utilize an initialization procedure with which the total number of knots is equal to the number of neurons of the network. This is obtained by replicating the initialization procedure of the first layer throughout the network: For randomly initialized weights W (l) , the bias term b (l) is given by the equality</p><formula xml:id="formula_12">b (l) =−W (l) h (l) (x unif ), where x unif is a vector of uniformly distributed points in the range [x min , x max ].</formula><p>Interestingly, we observe that this simple initialization strategy consistently outperforms the standard initialization for a large range of target functions ("ReLU Unif-Init", <ref type="figure" target="#fig_5">Fig. 5</ref>). Unfortunately, however, ReLU networks still show large difficulties in representing very nonlinear and non-smooth functions.</p><p>In Appx. C.1 <ref type="figure" target="#fig_7">(Fig. 7)</ref>, we illustrate that other popular alternatives, i.e., LeakyReLU, Swish, exhibit the same behavior.</p><p>The miracle of periodic bases. Recently, Sitzmann et al.</p><p>(2020) proposed to replace ReLU by Sine in order to learn implicit neural representations. Intriguingly, this slight modification allows our kernels to approximate any provided function to near perfection -even a sequence of random values!-("Sine", <ref type="figure" target="#fig_5">Fig. 5</ref>). A possible explanation of these astonishing results can be given via our prior analysis:</p><p>Periodic bending of the space. A Sine layer is given by:</p><formula xml:id="formula_13">y = Sin(ω 0 [Wx + b]),</formula><p>where ω 0 works as a prior on the variability of the target function.</p><p>Orthogonal to ReLU layers, Sine layers periodically bend the space. As a result, the same y value is obtained for all bias values b ′ i =b i +n2π W i,∶ −1 , ∀n ∈ Z. This is important from a spline approximation perspective. While for ReLU layers a unique value of b exists that bends the space at a desired position, infinitely many values of b do so for Sine ones. Resultantly, Sine layers are much more robust to parameter selection, and can be tuned to benefit pattern approximation at arbitrary -or even multiple-positions in ReLU networks fail to model very discontinuous or non-linear functions. Though improvements result by imposing a uniform distribution on the knots at initialization, these approximations remain poor. Contrarily, Sine networks quickly and seamlessly approximate very complex non-linear discontinuous functions. space ( <ref type="figure" target="#fig_4">Fig. 4</ref>). We conjecture that this behavior leads to much more reliable approximations and faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An exponentially big Fourier basis.</head><p>It is not surprising for a (large) basis of phase-shifted sinusoidal functions to be able to approximate arbitrary functions with high fidelity. This result was first observed over two centuries ago by <ref type="bibr" target="#b17">Fourier (1807)</ref> and lies at the core of the well-known Fourier transform, which states that any integrable function can be described as a linear combination of an infinite basis of phase-shifted sinusoidal functions. <ref type="bibr">Sitzmann et al. (2020)</ref> proposed an initialization of {W (l) } that allows for the construction of deep Sine networks able to periodically divide the space into exponentially many regions as a function of depth. Intuitively, approximations via Sine networks can be seen in terms of an exponentially large Fourier-like basis. We conjecture that is this exponential growth combined with the periodicity of sine which allows for astonishingly good approximations: the more terms in a Fourier transform, the better the approximation becomes.</p><p>Interestingly, our experiments suggest that a uniformly distributed initialization of the bias term b ∼ U(−1, 1) also leads to better and faster convergence for Sine networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We validate our approach across a large variety of tasks and against a large variety of existing models. In particular, we benchmark its ability to handle long-term dependencies as well as non-uniformly sampled datasets and irregularly sampled data. A complete description of the datasets used as well as additional experiments and ablation studies can be found in the Appendix <ref type="figure">(Appx. B, C)</ref>.</p><p>Experimental details. We parameterize all our convolutional kernels as a 3-layered MLP with Sine nonlinearities. We use weight normalization in MLP ψ : <ref type="bibr">Kingma, 2016)</ref>, which allows for separate control of the magnitude g i and the direction Vi,∶ Vi,∶ of the mappings, and consistently leads to better and faster convergence. All our networks follow the structure shown in <ref type="figure" target="#fig_8">Fig. 8</ref> and vary only in the number of blocks and channels. Specifications on the architectures and hyperparameters used are given in Appx. D. We leverage the convolution theorem to speed up convolution operations in our networks:</p><formula xml:id="formula_14">W i,∶ = g i Vi,∶ Vi,∶ (Salimans &amp;</formula><formula xml:id="formula_15">(f * ψ)= F −1 F{f }⋅F{ψ} , with F the Fourier transform.</formula><p>Stress experiments. First, we validate that the memory horizon of shallow CKCNNs is not restricted by architectural choices. To this end, we evaluate if a shallow CKCNN is able to solve the Copy Memory and the Adding Problem tasks <ref type="bibr" target="#b22">(Hochreiter &amp; Schmidhuber, 1997)</ref> for sequences of varying sizes in the range [100, 6000].</p><p>Our results (Tab. 1) show that a shallow CKCNN solves both problems for all sequence lengths considered without structural modifications. This is of large contrast to recurrent networks, which could not solve the copy problem at all and could solve the sum problem up to 200 steps. TCNs with k=7, n=7, on the other hand, were able to solve both tasks for up to 1000 steps. Longer sequences were out of reach as their architecture constraints their memory horizon a priori.</p><p>Discrete sequences. One might think that the inherent continuous nature of our kernels could restrict their applicability to tasks of the same nature, e.g., time-series. However, as depicted in Sec. 4.3, Sine nonlinearities allow our convolutional kernels to faithfully model complex non-continuous functions. Consequently, we validate the applicability of CKConvs for discrete sequence modeling tasks: sMNIST, pMNIST <ref type="bibr" target="#b27">(Le et al., 2015)</ref> and sCIFAR10 <ref type="bibr">(Trinh et al., 2018)</ref>.</p><p>Our results (Tab. 2) show that shallow CKCNNs outperform strong recurrent and convolutional models. A small CK-CNN (100K params.) obtains state-of-the-art on sMNIST with a model 80× smaller (99.31). A wider CKCNN (1M params) increases this result to 99.32. In addition, we see an improvement of 0.8% in pMNIST over the best model of size ≤100K, and a wider shallow CKCNN achieves stateof-the-art on pMNIST (98.54). For sCIFAR10, a small CKCNN obtains similar results to a self-attention model 5× bigger. Nevertheless, a wider variant only slightly improves accuracy and falls behind the state-of-the-art in this dataset.</p><p>Time-series modeling. Next, we evaluate CKCNNs on time-series data. To this end, we consider the CharacterTrajectories (CT) <ref type="bibr" target="#b2">(Bagnall et al., 2018)</ref> and the Speech Commands (SC) (Warden, 2018) datasets. We follow <ref type="bibr" target="#b24">Kidger et al. (2020)</ref> to obtain a balanced SC classification dataset with precomputed mel-frequency cepstrum coefficients. In <ref type="table">Table 1</ref>. Evaluation on stress tasks. states that problem has been properly solved. i.e., 100% acc. for Copy Memory, and loss ≤ 1e-4 for Adding Problem (predicting always 1 yields a loss of ∼ 0.17).  Testing at different sampling rates. We now consider the case where a network is trained with data at a sampling rate sr 1 , and tested with data at a different sampling rate sr 2 following the sampling procedure of <ref type="bibr" target="#b19">Gu et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL</head><p>Our results (Tab. 5) show that the performance of CKCNNs remains relatively stable even for large sampling rate fluctuations. This is of large contrast to most previous continuoustime models, which catastrophically fail upon these changes. CKCNNs outperform HiPPO <ref type="bibr" target="#b19">(Gu et al., 2020)</ref> and set a new state-of-the-art in this setting. Depending on the sampling procedure, additional care may be needed to account for spatial displacements of our kernels (see Appx. D.2).</p><p>Irregularly-sampled data. To conclude, we explore the applicability of CKCNNs for irregularly-sampled data. To this end, we follow the methodology of <ref type="bibr" target="#b24">Kidger et al. (2020)</ref> and drop 30%, 50% and 70% of the data for the CT dataset, and include an additional channel in the input to indicate if values at that position are known. In addition, we provide results under the same methodology for the SC raw dataset. We omit the SC dataset as the preprocessed bins are calcu- lated with overlapping windows and thus information from dropped points might still be present.</p><p>Our results (Tab. 4) show that CKCNNs exhibit stable performance for varying quantities of missing data, and perform better than several models explicitly developed to this end. Though NCDEs <ref type="bibr" target="#b24">(Kidger et al., 2020)</ref> perform slightly better than our method, our method is much faster (Sec. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Is depth important? Shallow global memory horizons.</p><p>Our results are obtained with CKCNNs built with two residual blocks only. Additional experiments (Appx. C.2) indicate that our models do not benefit from larger depth, which suggests that CKCNNs do not rely on very deep features. Though further analysis is required to draw consistent conclusions, it is intriguing to explore if either the current parameterization of CKCNNs does not allow exploiting depth properly, or if it is indeed sufficient to equip neural networks with global memory horizons even if this happens in a shallow manner. We consider both outcomes very interesting for the understanding of CKCNNs and deep learning in general.</p><p>Selection of ω 0 . We note that the performance of CKCNNs is very susceptible to the selection of ω 0 . For instance, performance on pMNIST may vary from 98.54 to 65.22 for values of ω 0 in [1, 100]. ω 0 acts as a prior on the variability of the target function. However, it is not obvious which value of ω 0 is optimal for the internal (unknown) features of a network. Learning layer-wise ω 0 values yielded suboptimal results, and better results were obtained by using a predefined ω 0 value across all layers. Interestingly, we observe that kernel approximation (Sec. 4.3) benefits from large values of ω 0 ≥ 1000. However, for all other experiments, the optimal value of ω 0 is always smaller than 70. The rationale behind these observations is not fully understood and its analysis is an important topic for future research. Speed of computation. CKCNNs can be executed in parallel, and thus can be much faster than recurrent networks. This difference becomes more pronounced in comparison to neural ODEs, which require at least 5× longer than RNNs <ref type="bibr" target="#b24">(Kidger et al., 2020)</ref>. Our approach relies on computing convolutions with very large convolutional kernels. While we strongly alleviate its computational complexity via the convolution theorem, further benefits may result from other tools such as low-rank approximation.</p><p>MLPs parameterizing spatial functions should use sine nonlinearities. Our findings indicate that Sine is much better suited to describe continuous functions via MLPs than all other nonlinearities considered (Sec. 4.3, Appx. C.1). This motivates replacing common nonlinearities in models using MLPs to describe continuous spatial functions, such as convolutional models with continuous kernels, e.g., Schütt et al. <ref type="formula" target="#formula_1">(2017)</ref>, as well as self-attention models with positional encodings, e.g., <ref type="bibr" target="#b40">Romero &amp; Cordonnier (2020)</ref>.</p><p>High-frequency components. Interestingly, our kernels often contain frequency components higher than the resolution of the grid used during training ( <ref type="figure">Fig. 9)</ref>. As a result, transitions to finer resolutions benefit from smoothing (see Appx. D.3). Nevertheless, we believe that, if tuned properly, these high-frequency components might prove advantageous for tasks such as super-resolution and compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Outlook and Future Work</head><p>We are intrigued about the potential of CKCNNs for tasks in which (global) long-term interactions play a crucial role, e.g., audio, video, reinforcement learning, (autoregressive) generative modeling, etc. The usage of CKConvs for 2D and 3D data is also interesting. In particular, CKConvs provide a convenient way to study the effect of long-range interactions as no changes to the network are required for varying neighborhoods. In addition, we are excited about structural developments of CKConvs, e.g., attentive CKConvs, as well as further understanding their underlying dynamics (Sec. 6). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>CKConv: Continuous Kernel Convolution For Sequential Data <ref type="figure">Figure 6</ref>. The sensitivity of networks with layer-wise exponential growing to slight changes. Taken from <ref type="bibr" target="#b20">Hanin &amp; Rolnick (2019)</ref>. The sawtooth function with 2 n teeth (left) can be easily expressed via a ReLU network with 3n + 4 neurons (bottom). However, a slight perturbation of the network parameters (Gaussian noise of stddev. 0.1) greatly simplifies the linear regions captured by the network, and thus the distribution of the knots in the basis (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Properties of CKConvs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Irregularly Sampled Data</head><p>CKConvs can readily handle irregularly sampled and partially observed data. This is a result of the convolutional kernel MLP ψ being able to be sampled at arbitrary positions. For very non-uniformed sampled inputs, however, the corresponding sampling of the convolutional kernel can provide a biased estimation of the operation. To overcome this problem, one can follow the strategy proposed by Wu et al. <ref type="formula" target="#formula_1">(2019)</ref>, which we summarize here for completeness.</p><p>For very non-uniformly sampled inputs, the continuous convolution (x * ψ)(t) = ∫ R x(τ )ψ(t − τ ) dτ , must be reformulated to account for the distribution of samples in the input. Specifically, it is rewritten as:</p><formula xml:id="formula_16">(x * ψ)(t) = R s(τ )x(τ )ψ(t − τ ) dτ,<label>(17)</label></formula><p>where s(τ ) depicts the inverse sample density of the input at point τ . Intuitively, s(τ ) controls the contribution of points x(τ ) in the output response: if several points are close to one another, their contribution should be smaller than the contribution of points in regions where the sample distribution is much sparser. This provides a Monte Carlo estimate of (x * ψ) from biased samples. In particular, one has that:</p><formula xml:id="formula_17">f (τ ) dτ = f (τ ) p(τ ) p(τ ) dτ ≈ i f (τ i ) p(τ i ) , for τ i ∼ p(τ ).</formula><p>With s(τ ) = 1 p(τ ) , we see that Eq. 17 indeed provides an unbiased estimation of the convolution operation (x * ψ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Data Sampled at Different Sampling Rates</head><p>In addition, CKConvs are readily able to handle data sampled at different sampling rates. In particular, the continuous kernel convolution between an input signal x and a continuous convolutional kernel ψ calculated at sampling rates sr 1 : (x * ψ) sr1 , and sr 2 : (x * ψ) sr2 , are approximately equal up to a normalization factor given by sr2 sr1 :</p><formula xml:id="formula_18">(x * ψ) sr2 (t) ≈ sr 2 sr 1 (x * ψ) sr1 (t).</formula><p>Consequently, CKCNNs (i) can be deployed at sampling rates different than those seen during training, and (ii) can be trained on data with varying spatial resolutions. The later is important for tasks in which data can be given at different resolutions such as super-resolution and segmentation.</p><p>Proof. To prove the previous statement, we start with the continuous definition of the convolution:</p><formula xml:id="formula_19">(x * ψ)(t) = R x(τ )ψ(t − τ ) dτ,</formula><p>where we assume for simplicity and without loss of generality that the functions x, ψ are scalar-valued.</p><p>In practice, an integral on a continuous function f ∶ R → R cannot be computed on finite time. Consequently, it is often approximated via a Riemann integral defined on a finite grid {τ sr,i } N sr i=1 obtained by sampling τ at a sampling rate sr:</p><formula xml:id="formula_20">f (τ ) dτ ≈ N sr i=1 f (τ sr,i )∆ sr ,</formula><p>where ∆ sr = 1 sr depicts the distance between sampled points. For two sampling rates sr 1 , sr 2 , the convolution can be approximated through the corresponding Riemann integrals:</p><formula xml:id="formula_21">R x(τ )ψ t − τ ) dτ ≈ N sr1 i=1 x(τ sr1,i )ψ t − τ sr1,i )∆ sr1 ≈ N sr2 i=1 x(τ sr2,i )ψ t − τ sr2,i )∆ sr2</formula><p>Hence, we have that both approximations are approximately equal to the continuous integral. By equating both approximations, we obtain that:</p><formula xml:id="formula_22">N sr2 i=1 x(τ sr2,i )ψ t − τ sr2,i )∆ sr2 ≈ N sr1 i=1 x(τ sr1,i )ψ t − τ sr1,i )∆ sr1 N sr2 i=1 x(τ sr2,i )ψ t − τ sr2,i ) (x * ψ)sr 2 (t) 1 sr2 ≈ N sr1 i=1 x(τ sr1,i )ψ t − τ sr1,i ) (x * ψ)sr 1 (t) 1 sr1 (x * ψ) sr2 (t) ≈ sr2</formula><p>sr1 (x * ψ) sr1 (t) which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Description</head><p>Copy Memory Problem. The copy memory task consists of sequences of length T +20, for which the first 10 values are chosen randomly among the digits {1, ..., 8}, the subsequent T −1 digits are set to zero, and the last 11 entries are filled with the digit 9. The goal is to generate an output of the same size of the input filled with zeros everywhere except for the last 10 values, for which the model is expected to predict the first 10 elements of the input sequence.</p><p>The Adding Problem. The adding problem consists of input sequences of length T and depth 2. The first dimension is filled with random values in [0, 1], whereas the second dimension is set to zeros except for two elements marked by 1. The objective is to sum the random values for which the second dimension is equal to 1. Simply predicting the sum to be 1 results in a MSE of about 0.1767.</p><p>Sequential and Permuted MNIST. The MNIST dataset <ref type="bibr" target="#b28">(LeCun et al., 1998)</ref> consists of 70K gray-scale 28×28 handwritten digits divided into training and test sets of 60K and 10K samples, respectively. The sequential MNIST dataset (sMNIST) presents MNIST images as a sequence of 784 pixels for digit classification. Consequently, good predictions require the model to preserve long-term dependencies up to 784 steps in the past: much longer dependencies than most language modelling tasks <ref type="bibr" target="#b4">(Bai et al., 2018b)</ref>.</p><p>The permuted MNIST dataset (pMNIST) additionally permutes the order or the sMNIST sequences at random. Consequently, models can no longer rely on on local features to perform classification. As a result, the classification problem becomes more difficult and the importance of long-term dependencies more pronounced.</p><p>Sequential CIFAR10. The CIFAR10 dataset <ref type="bibr" target="#b26">(Krizhevsky et al., 2009)</ref> consists of 60K real-world 32 × 32 RGB images uniformly drawn from 10 classes divided into training and test sets of 50K and 10K samples, respectively. Analogously to the sMNIST dataset, the sequential CIFAR10 (sCIFAR10) dataset presents CIFAR10 images as a sequence of 1024 pixels for image classification. This dataset is more difficult than sMNIST, as (i) even larger memory horizons are required to successfully solve the task, and (ii) more complex structures as well as intra-class variations are present in the images <ref type="bibr">(Trinh et al., 2018)</ref>.</p><p>CharacterTrajectories. The CharacterTrajectories dataset is part of the UEA time series classification archive <ref type="bibr" target="#b2">(Bagnall et al., 2018)</ref>. It consists of 2858 time series of length 182 and 3 channels representing the x, y positions and the pen tip force while writing a Latin alphabet character in a single stroke. The goal is to classify which of the different 20 characters was written using the time series data.</p><p>Speech Commands. The Speech Commands dataset (War-den, 2018) consists of 105809 one-second audio recordings of 35 spoken words sampled at 16kHz. Following <ref type="bibr" target="#b24">Kidger et al. (2020)</ref>, we extract 34975 recordings from ten spoken words to construct a balanced classification problem. We refer to this dataset as SC raw. In addition, we utilize the preprocessing steps of <ref type="bibr" target="#b24">Kidger et al. (2020)</ref> and extract mel-frequency cepstrum coefficients from the raw data. The resulting dataset, named SC, consists of time series of length 161 and 20 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>In this section, we perform an ablative study of our approach. Specifically, we analyze the effect of multiple components of our network, and provide additional comparisons with alternative architectures. Specifications on the architectures and hyperparameters used are given in Appx. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Using Sine Non-Linearities Over Popular Alternatives</head><p>As shown in Sec. 4.3, Sine nonlinearities provide astonishing improvements over equivalent networks with ReLU nonlinearities for function reconstruction. In this section, we provide additional experiments to highlight the suitability of Sine nonlinearities over other popular alternatives both for function approximation and other complementary tasks.</p><p>The same neural architectures are used across all experiments and vary only in the nonlinearity used in the MLP ψ 's. We find that nonlinearities other than Sine benefit from layer normalization and thus we incorporate it in these variants.</p><p>Case I: Function Approximation via MLP ψ . First, we evaluate the problem of function approximation in Sec. 4.3, <ref type="figure" target="#fig_5">Fig. 5</ref>, for nonlinearities other than ReLU and Sine. In particular, we approximate several functions with a MLP ψ network which varies only in the type of nonlinearity used: ReLU, Sine <ref type="bibr">(Sitzmann et al., 2020)</ref>, <ref type="bibr">LeakyReLU (Xu et al., 2015)</ref> and Swish <ref type="bibr" target="#b38">(Ramachandran et al., 2017)</ref>.</p><p>Our results <ref type="figure" target="#fig_7">(Fig. 7)</ref>, illustrate that Sine provides astonishing approximation capabilities over all other nonlinearities considered. In particular, we observe that Sine is the only nonlinearity able to reconstruct very nonlinear and very nonsmooth functions, while all other alternatives fail poorly.</p><p>Case II: CKCNNs with non-Sine MLP ψ . Next, we consider the case in which CKCNNs with non-Sine MLP ψ 's are used to solve the tasks considered in Sec. 5. In particular, we train CKCNNs on sMNIST, pMNIST, SC and SC raw for different non-linearities: ReLU, Sine, LeakyReLU and Swish. We utilize the same backbone architecture used in the main text for the corresponding dataset.</p><p>Our results (Tab. 6) show that Sine non-linearities outperform CKCNNs using any of the other non-linearities.   <ref type="bibr" target="#b18">Fuchs et al. (2020)</ref>, as well as self-attention models with positional encodings, e.g., <ref type="bibr" target="#b13">Dai et al. (2019)</ref>; <ref type="bibr" target="#b39">Ramachandran et al. (2019)</ref>; <ref type="bibr" target="#b40">Romero &amp; Cordonnier (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Going Deeper with CKCNNs</head><p>The experimental results shown in Sec. 5 are obtained with shallow CKCNNs composed of 2 residual blocks only. An interesting question is whether going deeper can be used to improve the performance of CKCNNs. To analyze this, we compare deep and shallow CKCNNs with the same architecture for equal width, and equal number of parameters.</p><p>Our results (Tab. 7) indicate that deep CKCNNs do not provide improvements over shallow CKCNNs. In fact, deep CKCNNs of fixed size underperform their shallow counterparts. This is an interesting results as shallow CKCNNs do not strongly rely on deep-wise compositionality of features, which is largely considered indispensable in deep learning.</p><p>Analysis of the results. The dynamics governing these results are not yet fully understood. However, our findings may lead to two different conclusions, both of which we consider interesting for the development and understanding of CKCNNs and deep learning in general:</p><p>Outcome I: Deep CKCNNs. The first possible outcome is that the current parameterization of our models does not correctly leverage depth. In this case, efforts to construct proper deep CKCNNs will likely lead to performance improvements over the current architectures, and thus have the potential to advance the state-of-the-art further.</p><p>Outcome II: Depth is not necessary when global memory horizons are provided with shallow networks. The first possible outcome is that deep learning in fact does not have to be deep, but that, on the contrary, it is sufficient to provide global memory horizons even if this happens in a shallow manner. This possible outcome is exciting as depth is largely considered indispensable within the community.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Details</head><p>In this section, we provide extended details over our implementation as well as the exact architectures and optimization schemes used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. General Remarks</head><p>Our models follow the structure shown in <ref type="figure" target="#fig_8">Fig. 8</ref> and vary only in the number of channels. We use layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> in our backbone network, and use the Adam optimizer <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref> across all our experiments. Our code is implemented in PyTorch and is publicly available at github.com/dwromero/ckconv. We utilize wandb <ref type="bibr" target="#b7">(Biewald, 2020)</ref> to log our results, and use NVIDIA TITAN RTX GPUs throughout our experiments.</p><p>Continuous Convolutional Kernel MLP ψ . Our convolutional kernels are parameterized by a conventional 3-layer MLP with 32 hidden units and Sine nonlinearities:</p><formula xml:id="formula_23">1 → 32 → 32 → N Cin × N Cout ,</formula><p>where N Cin , N Cout correspond to the input and output channels of the corresponding convolutional layer. We utilize weight normalization (Salimans &amp; Kingma, 2016) across all our MLP ψ networks and select a hidden size of 32 based on empirical evidence as well as findings from previous works, e.g., <ref type="bibr" target="#b16">Finzi et al. (2020)</ref>.</p><p>Normalized relative positions. The MLPs parameterizing our convolutional kenels receive relative positions as input. However, considering unitary step-wise relative positions, i.e., 0, 1, 2, ... , N, can be problematic from a numerical stability perspective as N may grow very large, e.g., N =16000 for the SC raw dataset. Consequently, we follow good practices from works modelling continuous functions via MLPs and utilize normalized relative positions as input. That is, we map the largest unitary step-wise relative positions seen during training [0, N ] to a uniform linear space in [−1, 1].</p><p>Hyperparameter tuning. We tune the hyperparameters of our models via the bayes method given in wandb Sweeps. We perform tuning on a validation dataset until a predefined computational budget is exhausted. Further improvements upon our results may be obtained by leveraging more sophisticated tuning methods as well as additional runs.</p><p>Selecting ω 0 . CKCNNs are very susceptible to the value of ω 0 . In order to obtain a reasonable ω 0 , we first perform a random search on a large interval ω 0 ∈ [0, 3000]. After a few runs, we stop the random search and select the subinterval in which the validation accuracy is most promising. Next, we restart the random search on this sub-interval and repeat the process until a ω 0 value is obtained, for which the validation accuracy is sufficiently high. Surprisingly, we found optimal values of ω 0 to be always enclosed in the interval [1, 70] even for very long sequences as in SC raw.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Accounting for Spatial Displacements of the Sampled Convolutional Kernels</head><p>We follow the sampling procedure of <ref type="bibr" target="#b19">Gu et al. (2020)</ref> throughout our test sampling rate discrepancy experiments. Specifically, for a sequence seq of length N , subsampling by a factor n is performed by running seq[::n]. That is, by taking the n-th element of the sequence starting from its first element. For example, for a sequence of length N = 182, different values of n would yield the following sequences:</p><p>(n = 1) → <ref type="figure" target="#fig_0">[1, 2, 3, ... , 180, 181</ref>, 182] (n = 2) → <ref type="figure" target="#fig_0">[1, 3, 5, ... , 177, 179, 181]  (n = 4) → [1, 5, 9, ... , 173, 177, 181]  (n = 8) → [1, 9, 17, ... , 161, 169</ref>, 177]</p><p>Recall that MLP ψ takes normalized relative positions in [−1, 1] as input, which are computed based on the max input length seen during training. However, some of these subsampling transitions change the max value of the sequence, e.g., for (n = 8) the maximum is given by 177, whereas for (n = 1) this value corresponds to 182. Consequently, a naive approach would consider the last position in each subsampled sequence to correspond to the maximum normalized relative position 1. This effectively induces an spatial displacement, and a re-scaling of the sampled convolutional kernel used during training.</p><p>This misalignment is automatically handled under the hood in our CKConv implementation. Nevertheless, we highlight this subtle phenomenon to prevent it in future applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Dealing with High-Frequency Components</head><p>Interestingly, our experiments revealed that our continuous kernels often contain frequency components of frequency higher than the resolution of the sampling grid used during training <ref type="figure">(Fig. 9</ref>). As these high-frequency components are not observed during training, we observe that they hurt performance when evaluated at higher resolutions.</p><p>In order to neutralize their influence, we filter these components before performing the convolution by means of blurring. This is performed by applying a convolution upon the convolutional kernel with a Gaussian filter g of length 2 srtest sr train + 1 and parameters µ=0, σ=0.5: g − srtest sr train , g − srtest sr train + 1 , ..., 0, ..., g srtest sr train − 1 , g srtest sr train</p><p>Note that blurring is only used when the test sampling rate is higher than the train sampling rate, as opposed to the normalization factor srtest sr train discussed in Eq. 7, Appx. A.2, which is applied whenever the sampling rates differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Hyperparameters and Experimental Details</head><p>In this section, we provide further specifications of the hyperparameter configurations with with our models are trained. An overview of these hyperparameters is provided in Tab. 8.</p><p>Copy Memory. We set the number of channels of our CK-CNN as to roughly match the number of parameters of the GRU and TCN networks of <ref type="bibr" target="#b3">Bai et al. (2018a)</ref>. This is obtained with 10 hidden channels at every layer. We observe that the time to convergence grew proportional to the length of the sequence considered. Whereas for sequences of length 100 convergence was shown after as few as 10 epochs, for sequences of length 6000 approximately 250 epochs were required. The maximum number of epochs is set to 50, 50, 100, 200 and 300 for sequences of size 100, 200, 1000, 3000 and 6000. We observe that different values of ω 0 are optimal for different sequence lengths. The optimal ω 0 values found are <ref type="bibr">19.20, 34.71, 68.69, 43.65 and 69.97</ref> for the corresponding sequence lengths.</p><p>Adding Problem. We set the number of channels of our CKCNN as to roughly match the number of parameters of the GRU and TCN networks of <ref type="bibr" target="#b3">Bai et al. (2018a)</ref>. This is obtained with 25 hidden channels at every layer. Similarly to the Copy Memory task, we observe that the time to convergence grew proportional to the length of the sequence considered. Interestingly, this task was much easier to solve for our models, with convergence for sequences of length 6000 observed after 38 epochs. The maximum number of epochs is set to 20, 20, 30, 50 and 50 for sequences of size  100, 200, 1000, 3000 and 6000. We observe that different values of ω 0 are optimal for different sequence lengths. The optimal ω 0 values found are 14.55, 18.19, 2.03, 2.23 and 4.3 for the corresponding sequence lengths.</p><p>sMNIST, pMNIST and sCIFAR10. We construct two models of different sizes for these datasets: CKCNN and CKCNN-Big. The first is constructed to obtain a parameter count close to 100K. The second model, is constructed to obtain a parameter count close to 1M. The parameters utilized for these datasets are summarized in Tab. 8. Despite our efforts, we observed that our models heavily overfitted sCIFAR10. Combinations of weight decay, dropout and weight dropout were not enough to counteract overfitting.</p><p>CT, SC and SC raw. The parameters utilized for classification on these datasets are summarized in Tab. 8. For hyperparameters regarding experiments with irregularly-sampled data please refer to Tab. 9. Any non-specified parameter value in Tab. 9 can be safely consider to be the one listed for corresponding dataset in Tab. 8. <ref type="figure">Figure 9</ref>. High-frequency components in Sine CKConvs. We observe that continuous kernels parameterized by Sine networks often contain frequency components of frequency higher than the resolution of the grid used during training. Here, for instance, the kernel looks smooth on the training grid. However, several high-frequency components appear when sampled on a finer grid. Though this may be a problematic phenomenon, we believe that, if tuned properly, these high-frequency components can prove advantageous to model fine details in tasks such as super-resolution and compression cheaply.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Centered and causal convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Continuous kernel convolution. The continuous convolutional kernel, parameterized by an small neural network MLP ψ , receives a sequence of relative positions {∆τi=(t − τi)} N i=0 as input and outputs the value of the kernel at those positions {ψ(∆τi)} N i=0 . Consequently, arbitrarily large convolutional kernels can be constructed by providing an equally large sequence of relative positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Functional family of recurrent units, discrete convolutions and CKConvs. For eigenvalues of W, λ≠1, recurrent units are only able to describe exponentially decreasing or increasing functions(Figs. 3a, 3b). Discrete convolutions can describe arbitrary functions within their memory horizon but are zero otherwise(Fig. 3c). Conversely, CKConvs are able to define arbitrary long memory horizons, and thus are able to describe arbitrary functions upon the entire input sequence(Fig. 3d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>An step function approximated via a spline basis (left)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Function approximation via ReLU and Sine networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Function approximation via ReLU, LeakyReLU, Swish and Sine MLP ψ networks. All network variants perform a decent job in approximating simple functions. However, for very non-linear, non-smooth functions, all networks using nonlinearities other than Sine provide very poor approximations. Interestingly, the uniform knot initialization proposed in Sec. 4.3 provides consistent improvements for all network variants. However, despite this improvement, the approximation results remain very poor. Contrarily, Sine networks quickly and seamlessly approximate all functions. All network configurations are equal up to the non-linearities used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Graphical description of continuous kernel convolutional networks. Dot-lined blocks depict optional blocks, and blocks without borders depict variables. KernelNet blocks use Sine nonlinearities. We replace spatial convolutions by FFTConv, which leverages the convolution theorem to speed up computations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Test accuracies on sMNIST, pMNIST and sCIFAR10.</figDesc><table><row><cell>MODEL</cell><cell cols="4">SIZE SMNIST PMNIST SCIFAR10</cell></row><row><cell>TCN (Bai et al., 2018a)</cell><cell>70K</cell><cell>99.0</cell><cell>97.2</cell><cell>-</cell></row><row><cell>LSTM (Bai et al., 2018a)</cell><cell>70K</cell><cell>87.2</cell><cell>85.7</cell><cell>-</cell></row><row><cell>GRU (Bai et al., 2018a)</cell><cell>70K</cell><cell>96.2</cell><cell>87.3</cell><cell>-</cell></row><row><cell>IndRNN (Li et al., 2018)</cell><cell>83K</cell><cell>99.0</cell><cell>96.0</cell><cell>-</cell></row><row><cell>DilRNN (Chang et al., 2017)</cell><cell>44K</cell><cell>98.0</cell><cell>96.1</cell><cell>-</cell></row><row><cell>HiPPO (Gu et al., 2020)</cell><cell>0.5M</cell><cell>-</cell><cell>98.30</cell><cell>-</cell></row><row><cell>r-LSTM (Trinh et al., 2018)</cell><cell>0.5M</cell><cell>98.4</cell><cell>95.2</cell><cell>72.2</cell></row><row><cell cols="2">Self-Att. (Trinh et al., 2018) 0.5M</cell><cell>98.9</cell><cell>97.9</cell><cell>62.2</cell></row><row><cell>TrellisNet (Bai et al., 2018b)</cell><cell>8M</cell><cell>99.20</cell><cell>98.13</cell><cell>73.42</cell></row><row><cell>CKCNN (2-Blocks)</cell><cell>98K</cell><cell>99.31</cell><cell>98.00</cell><cell>62.25</cell></row><row><cell>CKCNN-Big (2-Blocks)</cell><cell>1M</cell><cell>99.32</cell><cell>98.54</cell><cell>63.74</cell></row></table><note>addition, we evaluate the ability of CKCNNs to model long- term dependencies by training on the raw SC dataset. We denote this as SC raw (sample length = 16k). Our results (Tab. 3) show that shallow CKCNNs outperform recent continuous-time models, e.g., NCDEs, for both the CT and SC datasets. In addition, CKCNNs obtain promising results on SC raw, which validates their ability to handle very-long-term dependencies. In fact, CKCNNs trained on SC raw outperform several Neural ODE models trained on the preprocessed data (SC).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Test accuracies on CT, SC and SC raw. Test accuracies for irregularly sampled datasets.</figDesc><table><row><cell>MODEL</cell><cell>SIZE</cell><cell>CT</cell><cell>SC</cell><cell cols="2">SC RAW</cell></row><row><cell>GRU-ODE</cell><cell>89K</cell><cell>-</cell><cell cols="2">47.9</cell><cell>-</cell></row><row><cell>GRU-∆t</cell><cell>89K</cell><cell>-</cell><cell cols="2">43.3</cell><cell>-</cell></row><row><cell>GRU-D</cell><cell>89K</cell><cell>-</cell><cell cols="2">32.4</cell><cell>-</cell></row><row><cell>ODE-RNN</cell><cell>88K</cell><cell>-</cell><cell cols="2">65.9</cell><cell>-</cell></row><row><cell>NCDE</cell><cell>89K</cell><cell>-</cell><cell cols="2">89.8</cell><cell>-</cell></row><row><cell>CKCNN</cell><cell cols="4">100K 99.53 95.27</cell><cell>71.66</cell></row><row><cell>MODEL</cell><cell></cell><cell cols="4">DROP PERCENTAGE (0%) (30%) (50%) (70%)</cell></row><row><cell cols="5">CHARACTERTRAJECTORIES</cell></row><row><cell cols="2">GRU-ODE (De Brouwer et al., 2019)</cell><cell>-</cell><cell></cell><cell>92.6</cell><cell>86.7</cell><cell>89.9</cell></row><row><cell cols="2">GRU-∆t (Kidger et al., 2020)</cell><cell>-</cell><cell></cell><cell>93.6</cell><cell>91.3</cell><cell>90.4</cell></row><row><cell cols="2">GRU-D (Che et al., 2018)</cell><cell>-</cell><cell></cell><cell>94.2</cell><cell>90.2</cell><cell>91.9</cell></row><row><cell cols="2">ODE-RNN (Rubanova et al., 2019)</cell><cell>-</cell><cell></cell><cell>95.4</cell><cell>96.0</cell><cell>95.3</cell></row><row><cell cols="2">NCDE (Kidger et al., 2020)</cell><cell>-</cell><cell></cell><cell>98.7</cell><cell>98.8</cell><cell>98.6</cell></row><row><cell cols="2">CKCNN (2-Blocks)</cell><cell cols="2">99.53</cell><cell>98.83</cell><cell>98.6</cell><cell>98.14</cell></row><row><cell></cell><cell cols="4">SPEECHCOMMADS RAW</cell></row><row><cell cols="2">CKCNN (2-Blocks)</cell><cell cols="2">71.66</cell><cell>63.46</cell><cell>60.55</cell><cell>57.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Results for different train test sampling rates. Fractions depict proportional frequencies w.r.t. the original one of the dataset.</figDesc><table><row><cell></cell><cell cols="4">CKCNN (2-BLOCKS) -SIZE=100K</cell><cell></cell><cell></cell></row><row><cell cols="3">DATASET TRAIN FREQ.</cell><cell></cell><cell cols="2">TEST FREQ.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>1 2</cell><cell>1 4</cell><cell>1 8</cell><cell>1 16</cell></row><row><cell></cell><cell>1</cell><cell cols="5">99.53 99.30 99.53 95.57 76.92</cell></row><row><cell>CT</cell><cell>1 2</cell><cell cols="5">98.83 99.07 98.37 95.80 80.42</cell></row><row><cell></cell><cell>1 4</cell><cell cols="5">96.74 98.83 99.30 94.41 84.84</cell></row><row><cell></cell><cell>1 8</cell><cell cols="5">48.02 53.61 85.08 99.30 84.61</cell></row><row><cell></cell><cell>1</cell><cell cols="5">71.66 65.96 52.11 40.33 30.87</cell></row><row><cell>SC RAW</cell><cell>1 2</cell><cell cols="5">71.24 72.06 69.48 64.61 28.95</cell></row><row><cell></cell><cell>1 4</cell><cell cols="5">70.33 70.61 69.47 69.37 37.34</cell></row><row><cell></cell><cell>1 8</cell><cell cols="5">42.84 44.31 56.81 66.44 23.23</cell></row><row><cell cols="6">MODEL COMPARISON -CHARACTER TRAJECTORIES</cell><cell></cell></row><row><cell cols="7">MODEL GRU-D ODE-RNN LMU NCDE HIPPO CKCNN</cell></row><row><cell>1 → 1 2</cell><cell>23.1</cell><cell>41.8</cell><cell>44.7</cell><cell>6.0</cell><cell>88.8</cell><cell>99.30</cell></row><row><cell>1 2 → 1</cell><cell>25.5</cell><cell>31.5</cell><cell>11.3</cell><cell>13.1</cell><cell>90.1</cell><cell>98.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Simonovsky, M. and Komodakis, N. Dynamic edgeconditioned filters in convolutional neural networks on graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3693-3702, 2017.</figDesc><table><row><cell cols="2">Sitzmann, V., Martel, J., Bergman, A., Lindell, D., and Wet-</cell></row><row><cell cols="2">zstein, G. Implicit neural representations with periodic David W. Romero is financed as part of the Efficient Deep activation functions. Advances in Neural Information Learning (EDL) programme (grant number P16-25), partly funded by the Dutch Research Council (NWO) and Semiotic Processing Systems, 33, 2020.</cell></row><row><cell cols="2">Labs. Anna Kuzina is funded by the Hybrid Intelligence Thomas, N., Smidt, T., Kearnes, S., Yang, L., Li, L.,</cell></row><row><cell cols="2">Center, a 10-year programme funded by the Dutch Ministry Kohlhoff, K., and Riley, P. Tensor field networks:</cell></row><row><cell cols="2">of Education, Culture and Science through the Netherlands Rotation-and translation-equivariant neural networks for</cell></row><row><cell cols="2">Organisation for Scientific Research. Erik J. Bekkers is 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.</cell></row><row><cell cols="2">financed by the research programme VENI (grant number</cell></row><row><cell cols="2">17290) funded by the Dutch Research Council. All authors Trinh, T. H., Dai, A. M., Luong, M.-T., and Le, Q. V.</cell></row><row><cell cols="2">sincerely thank everyone involved in funding this work. Learning longer-term dependencies in rnns with auxiliary</cell></row><row><cell cols="2">This work was carried out on the Dutch national e-losses. arXiv preprint arXiv:1803.00144, 2018.</cell></row><row><cell cols="2">infrastructure with the support of SURF Cooperative. Wang, S., Suo, S., Ma, W.-C., Pokrovsky, A., and Urtasun,</cell></row><row><cell cols="2">R. Deep parametric continuous convolutional neural net-</cell></row><row><cell cols="2">works. In Proceedings of the IEEE Conference on Com-</cell></row><row><cell cols="2">puter Vision and Pattern Recognition, pp. 2589-2597,</cell></row><row><cell>2018.</cell><cell></cell></row><row><cell>Warden, P.</cell><cell>Speech commands: A dataset for</cell></row><row><cell cols="2">limited-vocabulary speech recognition. arXiv preprint</cell></row><row><cell cols="2">arXiv:1804.03209, 2018.</cell></row><row><cell cols="2">Wu, W., Qi, Z., and Fuxin, L. Pointconv: Deep convolu-</cell></row><row><cell cols="2">tional networks on 3d point clouds. In Proceedings of</cell></row><row><cell cols="2">the IEEE Conference on Computer Vision and Pattern</cell></row><row><cell cols="2">Recognition, pp. 9621-9630, 2019.</cell></row><row><cell cols="2">Xu, B., Wang, N., Chen, T., and Li, M. Empirical evaluation</cell></row><row><cell cols="2">of rectified activations in convolutional network. arXiv</cell></row><row><cell cols="2">preprint arXiv:1505.00853, 2015.</cell></row><row><cell></cell><cell>Salimans, T. and Kingma, D. P. Weight normalization: A</cell></row><row><cell></cell><cell>simple reparameterization to accelerate training of deep</cell></row><row><cell></cell><cell>neural networks. arXiv preprint arXiv:1602.07868, 2016.</cell></row><row><cell></cell><cell>Schütt, K., Kindermans, P.-J., Felix, H. E. S., Chmiela, S.,</cell></row><row><cell></cell><cell>Tkatchenko, A., and Müller, K.-R. Schnet: A continuous-</cell></row><row><cell></cell><cell>filter convolutional neural network for modeling quantum</cell></row><row><cell></cell><cell>interactions. In Advances in neural information process-</cell></row><row><cell></cell><cell>ing systems, pp. 991-1001, 2017.</cell></row><row><cell></cell><cell>Serra, T., Tjandraatmadja, C., and Ramalingam, S. Bound-</cell></row><row><cell></cell><cell>ing and counting linear regions of deep neural networks.</cell></row><row><cell></cell><cell>In International Conference on Machine Learning, pp.</cell></row><row><cell></cell><cell>4558-4566. PMLR, 2018.</cell></row></table><note>Shi, S., Wang, Z., Shi, J., Wang, X., and Li, H. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. arXiv preprint arXiv:1907.03670, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Test accuracies of CKCNNs with multiple MLP ψ nonlinearities. Model size = 100K.</figDesc><table><row><cell>NON-LINEARITY</cell><cell>SMNIST</cell><cell cols="2">DATASET PMNIST</cell><cell>SC</cell><cell>SC RAW</cell></row><row><cell>RELU</cell><cell>81.21</cell><cell>59.15</cell><cell cols="2">94.97</cell><cell>49.15</cell></row><row><cell>LEAKYRELU</cell><cell>80.57</cell><cell>55.85</cell><cell cols="2">95.03</cell><cell>38.67</cell></row><row><cell>SWISH</cell><cell>85.20</cell><cell>61.77</cell><cell cols="2">93.43</cell><cell>62.23</cell></row><row><cell>SINE</cell><cell>99.31</cell><cell>98.00</cell><cell cols="2">95.27</cell><cell>71.66</cell></row><row><cell cols="6">Analysis of the results. Our findings indicate Sine is much</cell></row><row><cell cols="6">better suited to describe continuous functions via MLPs than</cell></row><row><cell cols="6">all other popular nonlinearities considered. We consider this</cell></row><row><cell cols="6">result to be of large interest to the deep learning community.</cell></row><row><cell cols="6">In particular, it motivates replacing popular nonlinearities</cell></row><row><cell cols="6">by Sine for applications in which MLPs are used to describe</cell></row><row><cell cols="6">continuous positional functions. This family encompasses</cell></row><row><cell cols="6">models with continuous types of convolutions, e.g., Schütt</cell></row><row><cell cols="6">et al. (2017); Thomas et al. (2018); Finzi et al. (2020);</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Test accuracies of CKCNNs of different depth and width.</figDesc><table><row><cell></cell><cell></cell><cell>PMNIST</cell><cell></cell><cell></cell></row><row><cell>DEPTH</cell><cell cols="4">FIXED WIDTH SIZE ACC.(%) SIZE ACC.(%) FIXED SIZE</cell></row><row><cell>2 Blocks</cell><cell>98k</cell><cell>99.21</cell><cell>98k</cell><cell>99.21</cell></row><row><cell>4 Blocks</cell><cell>225k</cell><cell>99.26</cell><cell>95k</cell><cell>99.19</cell></row><row><cell>8 Blocks</cell><cell>480k</cell><cell>99.29</cell><cell>105k</cell><cell>99.12</cell></row><row><cell cols="2">16 Blocks 990k</cell><cell>99.19</cell><cell>107k</cell><cell>99.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Hyperparameter specifications of the best performing CKCNN models. Hyperparameter values for the classification and varying sampling rate tasks. For hyperparameters w.r.t. irregularly-sampled data please see Tab. 9.</figDesc><table><row><cell>PARAMS.</cell><cell cols="2">COPY MEMORY ADDING PROBLEM</cell><cell>SMNIST Small / Big</cell><cell>PMNIST Small / Big</cell><cell>SCIFAR10 Small / Big</cell><cell>CT  †</cell><cell>SC</cell><cell>SC RAW  †</cell></row><row><cell>Epochs</cell><cell>See Appx. D.4</cell><cell>See Appx. D.4</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>300</cell></row><row><cell>Batch Size</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>32</cell><cell>64</cell><cell>32</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Learning Rate</cell><cell>5e-4</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell># Blocks</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Hidden Size</cell><cell>10</cell><cell>25</cell><cell>30 / 100</cell><cell>30 / 100</cell><cell>30 / 100</cell><cell>30</cell><cell>30</cell><cell>30</cell></row><row><cell>ω0</cell><cell>See Appx. D.4</cell><cell>See Appx. D.4</cell><cell>31.09 / 30.5</cell><cell>43.46 / 42.16</cell><cell>25.67</cell><cell>21.45</cell><cell>30.90</cell><cell>39.45</cell></row><row><cell>Dropout</cell><cell>-</cell><cell>-</cell><cell>0.1 / 0.2</cell><cell>-</cell><cell>0.2 / 0.3</cell><cell>0.1</cell><cell>0.2</cell><cell>-</cell></row><row><cell>Input Dropout</cell><cell>-</cell><cell>-</cell><cell>0.1 / 0.2</cell><cell>0.1 / 0.2</cell><cell>0.0 / 0.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Weight Dropout</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-/ 0.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Weight Decay</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-/ 1e-4</cell><cell>-</cell><cell>-</cell><cell>1e-4</cell></row><row><cell>Scheduler</cell><cell>-</cell><cell>-</cell><cell>Plateau</cell><cell>Plateau</cell><cell>Plateau</cell><cell>Plateau</cell><cell>Plateau</cell><cell>Plateau</cell></row><row><cell>Patience</cell><cell>-</cell><cell>-</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>15</cell><cell>20</cell></row><row><cell>Scheduler Decay</cell><cell>-</cell><cell>-</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Model Size</cell><cell>15.52K</cell><cell>70.59K</cell><cell cols="5">98.29K / 1.03M 98.29K / 1.03M 100.04K / 1.04M 100.67K 118.24K</cell><cell>98.29K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Hyperparameter values for experiments on irregularly sampled data. Non-listed parameters correspond to those in Tab. 8.</figDesc><table><row><cell>PARAMS.</cell><cell cols="6">CT (30%) (50%) (70%) (30%) (50%) (70%) SC RAW</cell></row><row><cell>ω0</cell><cell>11.46</cell><cell>23.10</cell><cell>23.10</cell><cell>35.66</cell><cell>31.70</cell><cell>25.29</cell></row><row><cell>Dropout</cell><cell>0.2</cell><cell>0.2</cell><cell>0.0</cell><cell>0.1</cell><cell>0</cell><cell>0</cell></row><row><cell>Weight Decay</cell><cell>0.0</cell><cell>0.001</cell><cell>0.001</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at github.com/dwromero/ckconv</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We discard h(−1) as it only describes the initialization of h. 3 Though it is likely that this results has been observed before, we were not able to find a reference for it. This is perhaps a result of this equality being disregarded due to its non-practical significance for conventional discrete kernel parameterizations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is why b=0 is common in regular initialization schemes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge Gabriel Dernbach for interesting analyses on the knot distribution of ReLU networks. We thank Emiel van Krieken and Ali el Hasouni as well for interesting questions and motivating comments at the beginning of this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The uea multivariate time series classification archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Southam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00075</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06682</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Max</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06576</idno>
		<title level="m">Affine spline insights into deep learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
		<ptr target="https://www.wandb.com/.Softwareavailablefromwandb.com" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural networks for raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7379" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12880</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mémoire sur la propagation de la chaleur dans les corps solides, présenté le 21 décembre 1807à l&apos;institut national-nouveau bulletin des sciences par la société philomatique de paris. i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fourier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paris: First European Conference on Signal Analysis and Prediction</title>
		<imprint>
			<date type="published" when="1807" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Se (3)-transformers: 3d roto-translation equivariant attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hippo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07669</idno>
		<title level="m">Recurrent memory with optimal polynomial projections</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09021</idno>
		<title level="m">Complexity of linear regions in deep networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">91</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lezcano-Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martínez-Rubio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08428</idno>
		<title level="m">Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1578" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient orthogonal parametrisation of recurrent neural networks using householder reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mhammedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hellicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2401" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepsdf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Group equivariant stand-alone self-attention for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00977</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5320" to="5330" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
