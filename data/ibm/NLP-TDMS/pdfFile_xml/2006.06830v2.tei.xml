<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Augmentation for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhao</surname></persName>
							<email>tzhao2@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yozen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<postCode>90405</postCode>
									<settlement>Santa Monica</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
							<email>lneves@snap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<postCode>90405</postCode>
									<settlement>Santa Monica</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
							<email>oliver.woodford@snap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<postCode>90405</postCode>
									<settlement>Santa Monica</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
							<email>mjiang2@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Shah</surname></persName>
							<email>nshah@snap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<postCode>90405</postCode>
									<settlement>Santa Monica</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Augmentation for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode classhomophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAUG graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAUG improves performance across GNN architectures and datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data driven inference has received a significant boost in generalization capability and performance improvement in recent years from data augmentation techniques. These methods increase the amount of training data available by creating plausible variations of existing data without additional ground-truth labels, and have seen widespread adoption in fields such as computer vision (CV) (DeVries and Taylor 2017; <ref type="bibr" target="#b8">Cubuk et al. 2019;</ref><ref type="bibr" target="#b57">Zhao et al. 2019;</ref><ref type="bibr" target="#b19">Ho et al. 2019)</ref>, and natural language processing (NLP) <ref type="bibr" target="#b12">(Fadaee, Bisazza, and Monz 2017;</ref><ref type="bibr" target="#b37">Ş ahin and Steedman 2019)</ref>. Such augmentations allow inference engines to learn to generalize better across those variations and attend to signal over noise. At the same time, graph neural networks (GNNs) <ref type="bibr" target="#b16">(Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b24">Kipf and Welling 2016a;</ref><ref type="bibr" target="#b41">Veličković et al. 2017;</ref><ref type="bibr" target="#b49">Xu et al. 2018a;</ref><ref type="bibr" target="#b53">Zhang et al. 2019a;</ref><ref type="bibr" target="#b7">Chen, Ma, and Xiao 2018;</ref><ref type="bibr" target="#b56">Zhang, Cui, and Zhu 2018;</ref><ref type="bibr" target="#b50">Xu et al. 2018b</ref>) have emerged as a rising approach for datadriven inference on graphs, achieving promising results on tasks such as node classification, link prediction and graph representation learning.</p><p>Despite the complementary nature of GNNs and data augmentation, few works present strategies for combining the two. One major obstacle is that, in contrast to other data, where structure is encoded by position, the structure of graphs is encoded by node connectivity, which is irregular. The hand-crafted, structured, data augmentation operations used frequently in CV and NLP therefore cannot be applied. Furthermore, this irregularity does not lend itself to easily defining new augmentation strategies. The most obvious approaches involve adding or removing nodes or edges. For node classification tasks, adding nodes poses challenges in labeling and imputing features and connectivity of new nodes, while removing nodes simply reduces the data available. Thus, edge addition and removal appears the best augmentation strategy for graphs. But the question remains, which edges to change.</p><p>Three relevant approaches have recently been proposed. DROPEDGE <ref type="bibr" target="#b36">(Rong et al. 2019</ref>) randomly removes a fraction of graph edges before each training epoch, in an approach reminiscent of dropout <ref type="bibr" target="#b39">(Srivastava et al. 2014)</ref>. This, in principle, robustifies test-time inference, but cannot benefit from added edges. In approaches more akin to denoising or prefiltering, ADAEDGE  iteratively add (remove) edges between nodes predicted to have the same (different) labels with high confidence in the modified graph. This ad-hoc, two-stage approach improves inference in general, but is prone to error propagation and greatly depends on training size. Similarly, BGCN <ref type="bibr" target="#b55">(Zhang et al. 2019b</ref>) iteratively trains an assortative mixed membership stochastic block model with predictions of GCN to produce multiple denoised graphs, and ensembles results from multiple GCNs. BGCN also bears the risk of error propagation. Present work. Our work studies new techniques for graph data augmentation to improve node classification. Section 3 introduces motivations and considerations in augmentation via edge manipulation. Specifically, we discuss how facilitating message passing by removing "noisy" edges and adding "missing" edges that could exist in the original graph can benefit GNN performance, and its relation to intra-class and inter-class edges. <ref type="figure">Figure 1</ref> demonstrates, on a toy dataset (a), that while randomly modifying edges (b) can lead to lower test-time accuracy, strategically choosing ideal edges <ref type="figure">Figure 1</ref>: GCN performance (test micro-F1) on the original Zachary's Karate Club graph in (a), and three augmented graph variants in <ref type="bibr">(b-d)</ref>, evaluated on both original (O) and modified (M ) graph settings. Black, solid-blue, dashed-blue edges denote original graph connectivity, newly added, and removed edges respectively. While random graph modification (b) hurts performance, our proposed GAUG augmentation approaches (c) demonstrate significant relative performance improvements, narrowing the gap to omniscient, class-aware modifications <ref type="bibr">(d)</ref>.</p><p>to add or remove given (unrealistic) omniscience of node class labels (d) can substantially improve it.</p><p>Armed with this insight, Section 4 presents our major contribution: the proposed GAUG framework for graph data augmentation. We show that neural edge predictors like GAE <ref type="bibr" target="#b25">(Kipf and Welling 2016b)</ref> are able to latently learn class-homophilic tendencies in existent edges that are improbable, and nonexistent edges that are probable. GAUG leverages this insight in two approaches, GAUG-M and GAUG-O, which tackle augmentation in settings where edge manipulation is and is not feasible at inference time. GAUG-M uses an edge prediction module to fundamentally modify an input graph for future training and inference operations, whereas GAUG-O learns to generate plausible edge augmentations for an input graph, which helps node classification without any modification at inference time. In essence, our work tackles the problem of the inherent indeterminate nature of graph data and provides graph augmentations, which can both denoise structure and also mimic variability. Moreover, its modular design allows augmentation to be flexibly applied to any GNN architecture. <ref type="figure">Figure 1</ref>(c) shows GAUG-M and GAUG-O achieves marked performance improvements over (a-b) on the toy graph.</p><p>In Section 5, we present and discuss an evaluation of GAUG-O across multiple GNN architectures and datasets, demonstrating a consistent improvement over the state-ofthe-art, and quite large in some scenarios. Our proposed GAUG-M (GAUG-O) shows up to 17% (9%) absolute F1 performance improvements across datasets and GNN architectures without augmentation, and up to 16% (9%) over baseline augmentation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Other Related Work</head><p>As discussed above, relevant literature in data augmentation for graph neural networks is limited <ref type="bibr" target="#b36">(Rong et al. 2019;</ref><ref type="bibr" target="#b6">Chen et al. 2019;</ref><ref type="bibr" target="#b55">Zhang et al. 2019b)</ref>. We discuss other related works in tangent domains below. Graph Neural Networks. GNNs enjoy widespread use in modern graph-based machine learning due to their flexibility to incorporate node features, custom aggregations and inductive operation, unlike earlier works which were based on embedding lookups <ref type="bibr" target="#b35">(Perozzi, Al-Rfou, and Skiena 2014;</ref><ref type="bibr" target="#b43">Wang, Cui, and Zhu 2016;</ref><ref type="bibr" target="#b40">Tang et al. 2015)</ref>. Many GNN variants have been developed in recent years, following the initial idea of convolution based on spectral graph theory <ref type="bibr" target="#b4">(Bruna et al. 2013)</ref>. Many spectral GNNs have since been developed and improved by <ref type="bibr" target="#b9">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b24">Kipf and Welling 2016a;</ref><ref type="bibr" target="#b18">Henaff, Bruna, and LeCun 2015;</ref><ref type="bibr" target="#b29">Li et al. 2018;</ref><ref type="bibr" target="#b27">Levie et al. 2018;</ref><ref type="bibr">Ma et al. 2020)</ref>. As spectral GNNs generally operate (expensively) on the full adjacency, spatial-based methods which perform graph convolution with neighborhood aggregation became prominent <ref type="bibr" target="#b16">(Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b41">Veličković et al. 2017;</ref><ref type="bibr" target="#b32">Monti et al. 2017;</ref><ref type="bibr" target="#b13">Gao, Wang, and Ji 2018;</ref><ref type="bibr" target="#b33">Niepert, Ahmed, and Kutzkov 2016)</ref>, owing to their scalability and flexibility <ref type="bibr" target="#b51">(Ying et al. 2018)</ref>. Several works propose more advanced architectures which add residual connections to facilitate deep GNN training <ref type="bibr" target="#b50">(Xu et al. 2018b;</ref><ref type="bibr" target="#b28">Li et al. 2019;</ref><ref type="bibr" target="#b42">Verma et al. 2019</ref>). More recently, task-specific GNNs were proposed in different fields such as behavior modeling <ref type="bibr" target="#b58">Zhao et al. 2020;</ref><ref type="bibr" target="#b52">Yu et al. 2020)</ref>.</p><p>Data Augmentation. Augmentation strategies for improving generalization have been broadly studied in contexts outside of graph learning. Traditional point-based classification approaches widely leveraged oversampling, undersampling and interpolation methods <ref type="bibr" target="#b5">(Chawla et al. 2002;</ref><ref type="bibr" target="#b2">Barandela et al. 2004)</ref>. In recent years, variants of such techniques are widely used in natural language processing (NLP) and computer vision (CV). Replacement approaches involving synonym-swapping are common in NLP <ref type="bibr" target="#b54">(Zhang, Zhao, and LeCun 2015)</ref>, as are text-variation approaches (Kafle, Yousefhussien, and Kanan 2017) (i.e. for visual questionanswering). Backtranslation methods <ref type="bibr" target="#b37">(Sennrich, Haddow, and Birch 2016;</ref><ref type="bibr" target="#b48">Xie et al. 2019;</ref><ref type="bibr" target="#b11">Edunov et al. 2018)</ref> have also enjoyed success. In CV, historical image transformations in the input space, such as rotation, flipping, color space transformation, translation and noise injection <ref type="bibr" target="#b38">(Shorten and Khoshgoftaar 2019)</ref>, as well as recent methods such as cutout and random erasure (DeVries and Taylor 2017; <ref type="bibr" target="#b59">Zhong et al. 2017</ref>) have proven useful. Recently, augmentation via photorealistic generation through adversarial networks shows promise in several applications, especially in medicine <ref type="bibr" target="#b1">(Antoniou, Storkey, and Edwards 2017;</ref><ref type="bibr" target="#b14">Goodfellow et al. 2014</ref>). Most-related to our work is liter-ature on meta-learning based augmentation in CV <ref type="bibr" target="#b26">(Lemley, Bazrafkan, and Corcoran 2017;</ref><ref type="bibr" target="#b8">Cubuk et al. 2019;</ref><ref type="bibr" target="#b34">Perez and Wang 2017)</ref>, which aim to learn neural image transformation operations via an augmentation network, using a loss from a target network. While our work is similar in motivation, it fundamentally differs in network structure, and tackles augmentation in the much-less studied graph context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Data Augmentation via Edge Manipulation</head><p>In this section, we introduce our key idea of graph data augmentation by manipulating G via adding and removing edges over the fixed node set. We discuss preliminaries, practical and theoretical motivations, and considerations in evaluation under a manipulated-graph context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Let G = (V, E) be the input graph with node set V and edge set E. Let N = |V| be the number of nodes. We denote the adjacency matrix as A ∈ {0, 1} N ×N , where A ij = 0 indicates node i and j are not connected. We denote the node feature matrix as X ∈ R N ×F , where F is the dimension of the node features and X i: indicates the feature vector of node i (the ith row of X). We define D as the diagonal degree matrix such that D ii = j A ij . Graph Neural Networks. In this work, we use the wellknown graph convolutional network (GCN) <ref type="bibr" target="#b24">(Kipf and Welling 2016a)</ref> as an example when explaining GNNs in the following sections; however, our arguments hold straightforwardly for other GNN architectures. Each GCN layer (GCL) is defined as:</p><formula xml:id="formula_0">H (l+1) = f GCL (A, H (l) ; W (l) ) = σ(D − 1 2ÃD − 1 2 H (l) W (l) ),<label>(1)</label></formula><p>whereÃ = A + I is the adjacency matrix with added selfloops,D is the diagonal degree matrixD ii = jÃ ij , and σ(·) denotes a nonlinear activation such as ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motivation</head><p>Practical reasons. Graphs aim to represent an underlying process of interest. In reality, a processed or observed graph may not exactly align with the process it intended to model (e.g. "which users are actually friends?" vs. "which users are observed to be friends?") for several reasons. Many graphs in the real world are susceptible to noise, both adversarial and otherwise (with exceptions, like molecular or biological graphs). Adversarial noise can manifest via spammers who pollute the space of observed interactions. Noise can also be induced by partial observation: e.g. a friend recommendation system which never suggests certain friends to an enduser, thus preventing link formation. Moreover, noise can be created in graph preprocessing, by adding/removing selfloops, removing isolated nodes or edges based on weights. Finally, noise can occur due to human errors: in citation networks, a paper may omit (include) citation to a highly (ir)relevant paper by mistake. All these scenarios can produce a gap between the "observed graph" and the so-called "ideal graph" for a downstream inference task (in our case, node classification).</p><p>Enabling an inference engine to bridge this gap suggests the promise of data augmentation via edge manipulation. In the best case, we can produce a graph G i (ideal connectivity), where supposed (but missing) links are added, and unrelated/insignificant (but existing) links removed. <ref type="figure">Figure  1</ref> shows this benefit realized in the ZKC graph: strategically adding edges between nodes of the same group (intraclass) and removing edges between those in different groups (inter-class) substantially improves node classification test performance, despite using only a single training example per class. Intuitively, this process encourages smoothness over same-class node embeddings and differentiates otherclass node embeddings, improving distinction. Theoretical reasons. Strategic edge manipulation to promote intra-class edges and demote inter-class edges makes class differentiation in training trivial with a GNN, when done with label omniscience. Consider a scenario of extremity where all possible intra-class edges and no possible interclass edges exists, the graph can be viewed as k fully connected components, where k is the number of classes and all nodes in each component have the same label. Then by Theorem 1 (proof in Appendix A.1), GNNs can easily generate distinct node representations between distinct classes, with equivalent representations for all same-class nodes. Under this "ideal graph" scenario, learned embeddings can be effortlessly classified. Theorem 1. Let G = (V, E) be a undirected graph with adjacency matrix A, and node features X be any block vector in R N ×F . Let f : A, X; W → H be any GNN layer with a permutation-invariant neighborhood aggregator over the target node and its neighbor nodes u ∪ N (u) (e.g. Eq. 1) with any parameters W, and H = f (A, X; W) be the resulting embedding matrix. Suppose G contains k fully connected components. Then we have: 1. For any two nodes i, j ∈ V that are contained in the same connected component, H i: = H j: . 2. For any two nodes i, j ∈ V that are contained in different connected components S a , S b ⊆ V, H i: = H j: when W is not all zeros and v∈Sa X v: = ε u∈S b X u: , ∀ε ∈ R.</p><p>This result suggests that with an ideal, class-homophilic graph G i , class differentiation in training becomes trivial. However, it does not imply such results in testing, where node connectivity is likely to reflect G and not G i . We would expect that if modifications in training are too contrived, we risk overfitting to G i and performing poorly on G due to a wide train-test gap. We later show techniques (Section 4) for approximating G i with a modified graph G m , and show empirically that these modifications in fact help generalization, both when evaluating on graphs akin to G m and G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modified and Original Graph Settings for</head><p>Graph Data Augmentation  <ref type="figure">Figure 2</ref>: GAUG-M uses an edge-predictor module to deterministically modify a graph for future inference. Neural edge-predictors (e.g. GAE) can learn class-homophilic tendencies, promoting intra-class and demoting inter-class edges compared to random edge additions (a-b) and removals (c-d) respectively, leading to node classification performance (test micro-F1) improvements (green).</p><p>training. Graph data augmentation is notably different, since typically |S| = 1 for node classification, unlike the image setting where |S| 1. However, we propose two strategies with analogous, but distinct formalisms: we can either (1) apply one or multiple graph transformation operation f : G → G m , such that G m replaces G for both training and inference, or (2) apply many transformations f i :</p><formula xml:id="formula_1">G → G i m for i = 1 . . . N , such that G ∪ {G i m } N i=1</formula><p>may be used in training, but only G is used for inference. We call (1) the modified-graph setting, and (2) the original-graph setting, based on their inference scenario.</p><p>One might ask: when is each strategy preferable? We reason that the answer stems from the feasibility of applying augmentation during inference to avoid a train-test gap. The modified-graph setting is thus most suitable in cases where a given graph is unchanging during inference. In such cases, one can produce a single G m , and simply use this graph for both training and testing. However, when inferences must be made on a dynamic graph (i.e. for large-scale, latencysensitive applications) where calibrating new graph connectivity (akin to G) with G m during inference is infeasible (e.g. due to latency constraints), augmentation in the originalgraph setting is more appropriate. In such cases, test statistics on G m may be overly optimistic as performance indicators. In practice, these loosely align with transductive and inductive contexts in prior GNN literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed GAUG Framework</head><p>In this section, we introduce the GAUG framework, covering two approaches for augmenting graph data in the aforementioned modified-graph and original-graph settings respectively. Our key idea is to leverage information inherent in the graph to predict which non-existent edges should likely exist, and which existent edges should likely be removed in G to produce modified graph(s) G m to improve model performance. As we later show in Section 5, by leveraging this label-free information, we can consistently realize improvements in test/generalization performance in semi-supervised node classification tasks across augmentation settings, GNN architectures and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GAUG-M for Modified-Graph Setting</head><p>We first introduce GAUG-M, an approach for augmentation in the modified-graph setting which includes two steps: (1) we use an edge predictor function to obtain edge probabilities for all possible and existing edges in G. The role of the edge predictor is flexible and can generally be replaced with any suitable method. (2) Using the predicted edge probabilities, we deterministically add (remove) new (existing) edges to create a modified graph G m , which is used as input to a GNN node-classifier.</p><p>The edge predictor can be defined as any model f ep : A, X → M, which takes the graph as input, and outputs an edge probability matrix M where M uv indicates the predicted probability of an edge between nodes u and v. In this work, we use the graph auto-encoder (GAE) (Kipf and Welling 2016b) as the edge predictor module due to its simple architecture and competitive performance. GAE consists of a two layer GCN encoder and an inner-product decoder:</p><formula xml:id="formula_2">M = σ ZZ T , where Z = f (1) GCL A, f (0) GCL (A, X) . (2)</formula><p>Z denotes the hidden embeddings learned by the encoder, M is the predicted (symmetric) edge probability matrix produced by the inner-product decoder, and σ(·) is an elementwise sigmoid function. Let |E| denote the number of edges in G. Then, using the probability matrix M, GAUG-M deterministically adds the top i|E| non-edges with highest edge probabilities, and removes the j|E| existing edges with least edge probabilities from G to produce G m , where i, j ∈ [0, 1]. This is effectively a denoising step. <ref type="figure">Figure 2</ref> shows the change in intra-class and interclass edges when adding/removing using GAE-learned edge probabilities and their performance implications compared to a random perturbation baseline on CORA: adding (removing) by learned probabilities results in a much steeper growth (slower decrease) of intra-class edges and much slower increase (steeper decrease) in inter-class edges compared to random. Notably, these affect classification performance (micro-F1 scores, in green): random addition/removal hurts performance, while learned addition consistently improves performance throughout the range, and learned removal improves performance over part of the range (until ∼20%). Importantly, these results show that while we are generally not able to produce the ideal graph G i without omniscience (as discussed in Section 3.2), such capable edge predictors can latently learn to approximate class-homophilic information in graphs and successfully promote intra-class and demote inter-class edges to realize performance gains in practice.</p><p>GAUG-M shares the same time and space complexity as its associated GNN architecture during training/inference, while requiring extra disk space to save the dense O(N 2 ) <ref type="figure">Figure 3</ref>: GAUG-O is comprised of three main components: (1) a differentiable edge predictor which produces edge probability estimates, (2) an interpolation and sampling step which produces sparse graph variants, and (3) a GNN which learns embeddings for node classification using these variants. The model is trained end-to-end with both classification and edge prediction losses. edge probability matrix M for manipulation. Note that M's computation can be trivially parallelized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GAUG-O for Original-Graph Setting</head><p>To complement the above approach, we propose GAUG-O for the original-graph setting, where we cannot benefit from graph manipulation at inference time. GAUG-O is reminiscent of the two-step approach in GAUG in that it also uses an edge prediction module for the benefit of node classification, but also aims to improve model generalization (test performance on G) by generating graph variants {G i m } N i=1 via edge prediction and hence improve data diversity. GAUG-O does not require discrete specification of edges to add/remove, is end-to-end trainable, and utilizes both edge prediction and node-classification losses to iteratively improve augmentation capacity of the edge predictor and classification capacity of the node classifier GNN. <ref type="figure">Figure 3</ref> shows the overall architecture: each training iteration exposes the node-classifier to a new augmented graph variant.</p><p>Unlike GAUG-M's deterministic graph modification step, GAUG-O supports a learnable, stochastic augmentation process. As such, we again use the graph auto-encoder (GAE) for edge prediction. To prevent the edge predictor from arbitrarily deviating from original graph adjacency, we interpolate the predicted M with the original A to derive an adjacency P. In the edge sampling phase, we sparsify P with Bernoulli sampling on each edge to get the graph variant adjacency A . For training purposes, we employ a (soft, differentiable) relaxed Bernoulli sampling procedure as a Bernoulli approximation. This relaxation is a binary special case of the Gumbel-Softmax reparameterization trick <ref type="bibr" target="#b31">(Maddison, Mnih, and Teh 2016;</ref><ref type="bibr" target="#b22">Jang, Gu, and Poole 2016)</ref>. Using the relaxed sample, we apply a straight-through (ST) gradient estimator <ref type="bibr" target="#b3">(Bengio, Léonard, and Courville 2013)</ref>, which rounds the relaxed samples in the forward pass, hence sparsifying the adjacency. In the backward pass, gradients are directly passed to the relaxed samples rather than the rounded values, enabling training. Formally,</p><formula xml:id="formula_3">A ij = 1 1 + e −(log Pij +G)/τ + 1 2 , where P ij = αM ij + (1 − α)A ij<label>(3)</label></formula><p>where A is the sampled adjacency matrix, τ is the temperature of Gumbel-Softmax distribution, G ∼ Gumbel(0, 1) is a Gumbel random variate, and α is a hyperparameter mediating the influence of edge predictor on the original graph. The graph variant adjacency A is passed along with node features X to the GNN node classifier. We then backpropagate using a joint node-classification loss L nc and edgeprediction loss L ep</p><formula xml:id="formula_4">L = L nc + βL ep , where L nc = CE(ŷ, y) and L ep = BCE(σ(f ep (A, X)), A)<label>(4)</label></formula><p>where β is a hyperparameter to weight the reconstruction loss, σ(·) is an elementwise sigmoid, y,ŷ denote groundtruth node class labels and predicted probabilities, and BCE/CE indicate standard (binary) cross-entropy loss. We train using L ep in addition to L nc to control potentially excessive drift in edge prediction performance. The nodeclassifier GNN is then directly used for inference, on G.</p><p>During training, GAUG-O has a space complexity of O(N 2 ) in full-batch setting due to backpropagation through all entries of the adjacency matrix. Fortunately, we can easily adapt the graph mini-batch training introduced by Hamilton et al. <ref type="bibr" target="#b16">(Hamilton, Ying, and Leskovec 2017)</ref> to achieve an acceptable space complexity of O(M 2 ), where M is the batch size. Appendix C.1 further details (pre)training, minibatching, and implementation choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we evaluate the performance of GAUG-M and GAUG-O across architectures and datasets, and over alternative strategies for graph data augmentation. We also showcase their abilities to approximate class-homophily via edge prediction and sensitivity to supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We evaluate using 6 benchmark datasets across domains: citation networks (CORA, CITESEER (Kipf and Welling 2016a)), protein-protein interactions (PPI <ref type="bibr" target="#b16">(Hamilton, Ying, and Leskovec 2017)</ref>), social networks (BLOGCATALOG, FLICKR <ref type="bibr" target="#b21">(Huang, Li, and Hu 2017)</ref>), and air traffic (AIR-USA <ref type="bibr" target="#b46">(Wu, He, and Xu 2019)</ref>). Statistics for each dataset are shown in <ref type="table" target="#tab_1">Table 1</ref>, with more details in Appendix B. We follow the semi-supervised setting in most GNN literature <ref type="bibr" target="#b24">(Kipf and Welling 2016a;</ref><ref type="bibr" target="#b41">Veličković et al. 2017)</ref> for train/validation/test splitting on CORA and CITESEER,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>We show comparative results against current baselines in Table 2.  across datasets): GAUG-M improves 4.6% (GCN), 4.8% (GSAGE), 10.9% (GAT) and 5.7% (JK-NET). GAUG-O improves 4.1%, 2.1%, 6.3% and 4.9%, respectively. We note that augmentation especially improves GAT performance, as self-attention based models are sensitive to connectivity. Improvements across datasets. GAUG also achieves improvements over all 6 datasets (averaged across architectures): GAUG-M improves 2.4%, 1.0%, 3.1%, 5.5%, 19.2%, 7.9% for each dataset (left to right in <ref type="table" target="#tab_2">Table 2</ref>). <ref type="figure" target="#fig_0">Figure 4</ref> shows GAUG-M (with GCN) classification performance heatmaps on 4 datasets when adding/removing edges according to various i, j (Section 4.1). Notably, while improvements(red) over original GCN on G differ over i, j and by dataset, they are feasible in all cases. These improvements are not necessarily monotonic with edge addition(row) or removal(column), and can encounter transitions. Empirically, we notice these boundaries correspond to excessive class mixing (addition) or graph shattering (removal). GAUG-O improves 1.6%, 2.5%, 11.5%, 3.6%, 2.2%, 4.7%. We note that both methods achieves large improvements in social data (BLOGCATALOG and FLICKR) where noisy edges may be prominent due to spam or bots (supporting intuition from Section 3.2): <ref type="figure" target="#fig_0">Figure 4</ref>(c) shows substantial edge removal significantly helps performance. Improvements over alternatives. GAUG also outperforms augmentation over BGCN, ADAEDGE, and DROPE-DGE (averaged across datasets/architectures): GAUG-M improves 9.3%, 4.8%, and 4.1% respectively, while GAUG-O improves 4.9%, 2.7%, and 2.0% respectively. We reason that GAUG-M outperforms BGCN and ADAEDGE by avoiding iterative error propagation, as well as directly manipulating edges based on the graph, rather than indirectly through classification results. GAUG-O outperforms DROPEDGE via learned denoising via addition and removal, rather than random edge removal. Note that some baselines have worse performance than vanilla GNNs, as careless augmentation/modification on the graph can hurt performance by removing critical edges and adding incorrect ones.</p><p>Promoting class-homophily. <ref type="figure">Figure 5a</ref> shows (on CORA) that the edge predictor in GAUG-O learns to promote intraclass edges and demote inter-class ones, echoing results from <ref type="figure">Figure 2</ref> on GAUG-M, facilitating message passing and improving performance. <ref type="figure">Figure 5b</ref> shows that L nc decreases and validation F1 improves over the first few epochs, while L ep increases to reconcile with supervision from L nc . Later on, the L nc continues to decrease while intra-class ratio increases (overfitting). Sensitivity to supervision. <ref type="figure" target="#fig_1">Figure 6</ref> shows that both GAUG is especially powerful under weak supervision, producing large F1 improvements with few labeled samples. Moreover, augmentation helps achieve equal performance w.r.t standard methods with fewer training samples. Naturally, improvements shrink in the presence of more supervision. GAUG-M tends towards slightly larger outperformance compared to GAUG-O with more training nodes, since inference benefits from persistent graph modifications in the former but not the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Data augmentation for facilitating GNN training has unique challenges due to graph irregularity. Our work tackles this problem by utilizing neural edge predictors as a means of exposing GNNs to likely (but nonexistent) edges and limiting exposure to unlikely (but existent) ones. We show that such edge predictors can encode class-homophily to promote intra-class edges and inter-class edges. We propose the GAUG graph data augmentation framework which uses these insights to improve node classification performance in two inference settings. Extensive experiments show our proposed GAUG-O and GAUG-M achieve up to 17% (9%) absolute F1 performance improvements across architectures and datasets, and 15% (8%) over augmentation baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Impact</head><p>We do not foresee ethical concerns posed by our method, but concede that both ethical and unethical applications of graph-based machine learning techniques may benefit from the improvements induced by our work. Care must be taken, in general, to ensure positive ethical and societal consequences of machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Micro-F1 and Accuracy</head><p>Definition 2. Micro-F1 score is mathematically equivalent to accuracy for classification tasks when every data point is guaranteed to be assigned to exactly one class (one ground truth label for each data point.)</p><p>Proof. The micro-F 1 score is defined as following <ref type="bibr" target="#b17">(Han, Pei, and Kamber 2011)</ref>:</p><p>micro-precision = T P/(T P + F P ) micro-recall = T P/(T P + F N ) micro-F 1 = 2 · micro-precision · micro-recall micro-precision + micro-recall <ref type="formula">(7)</ref> where T P , T N , F P , and F N are the number of true positives, true negatives, false positives and false negatives for all classes.</p><p>As each data object only has one label, for each misclassified data object, one F P case for the predicted class and one F N case for the ground truth class are created at the same time. Therefore, micro-precision and micro-recall will always be the same and we then have:</p><formula xml:id="formula_5">micro-precision = micro-recall = micro-F 1 (8)</formula><p>Accuracy is defined as the number of correct predictions divided by the number of total cases <ref type="bibr" target="#b17">(Han, Pei, and Kamber 2011)</ref>. Since the number of correct predictions is the same as T P and the number of incorrect predictions is F P , accuracy can also be calculated as following:</p><formula xml:id="formula_6">Accuracy = T P/(T P + F P )<label>(9)</label></formula><p>Thus we have:</p><formula xml:id="formula_7">Accuracy = micro-precision = micro-F 1 (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Dataset Details</head><p>In this section, we provide some additional, relevant dataset details. The preprocessed files of all datasets used in this work can be found at https://tinyurl.com/gaug-data, including graph adjacency matrix, node features, node labels, train/validation/test node ids and predicted edge probabilities for each dataset. Citation networks. CORA and CITESEER are citation networks which are used as benchmarks in most GNN-related prior works <ref type="bibr" target="#b24">(Kipf and Welling 2016a;</ref><ref type="bibr" target="#b41">Veličković et al. 2017;</ref><ref type="bibr" target="#b36">Rong et al. 2019;</ref><ref type="bibr" target="#b6">Chen et al. 2019)</ref>. In these networks, the nodes are papers published in the field of computer science; the features are bag-of-word vectors of the corresponding paper title; the edges represent the citation relation between papers; the labels are the category of each paper. Protein-protein interaction network. PPI is the combination of multiple protein-protein interaction networks from different human tissue. The node feature contains positional gene sets, motif gene sets and immunological signatures. Gene ontology sets are used as labels (121 in total) (Hamilton, <ref type="bibr" target="#b16">Ying, and Leskovec 2017)</ref>. The original graph provided by <ref type="bibr" target="#b16">(Hamilton, Ying, and Leskovec 2017)</ref> contains total of 295 connected components in various sizes, so in this work we took the top 3 largest connected components, forming a graph with 10,076 nodes. Social networks. BLOGCATALOG is an online blogging community where bloggers can follow each other, hence forming a social network. The features for each user are generated by the keywords in each bloggers description and the labels are selected from predefined categories of blogger interests <ref type="bibr" target="#b21">(Huang, Li, and Hu 2017)</ref>. FLICKR is an image and video sharing platform, where users can also follow each other, hence forming a social network. The userspecified list of interest tags are used as user features and the groups that users joined are used as labels <ref type="bibr" target="#b21">(Huang, Li, and Hu 2017)</ref>. Air traffic network. AIR-USA is the airport traffic network in the USA, where each node represents an airport and edge indicates the existence of commercial flights between the airports. The node labels are generated based on the label of activity measured by people and flights passed the airports <ref type="bibr" target="#b46">(Wu, He, and Xu 2019)</ref>. The original graph does not have any features, so we used one-hot degree vectors as node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details and Hyperparameter Tuning</head><p>All experiments were conducted on a virtual machine on Google Cloud 2 with 15 vCPUs, 15 Gb of RAM and one NVIDIA Tesla v100 GPU card (16 Gb of RAM at 32Gbps speed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Notes for effectively training GAUG-O</head><p>Pretraining the Edge Predictor and Node Classifier.</p><p>Since the graph structure of the GNN node classifier largely depends on the edge predictor, we pretrain both components of GAUG-O to achieve more stable joint training. Otherwise, a randomly initialized edge predictor can generate very unlikely edge probabilities M, which stunt training. Empirically, we find that pretraining the edge predictor is more important in producing good performance compared to the node classifier, and excessive pretraining of the node classifier can lead to overfitting and poor optimizer performance. Learning Rate Warmup for the Edge Predictor. Since the edge predictor is not only trained using L ep , but also by L nc , we adapt the learning rate warmup schema <ref type="bibr" target="#b15">(Goyal et al. 2017)</ref> for the edge predictor to avoid effective undoing of initial pretraining. Specifically, we initialize the edge predictor's learning rate at zero and gradually increase following a sigmoid curve. This empirically helps avoid sudden drift in edge prediction from L nc , and improves results. We also incorporate a parameter that narrows down a section of the sigmoid curve to specify how rapid the learning rate warms up.</p><p>Mini-batch Training. For graphs that are too large and exceed GPU memory in full-batch training with GAUG-O, we follow the mini-batch training algorithm proposed by <ref type="bibr" target="#b16">Hamilton, Ying, and Leskovec (2017)</ref>. For each batch, we first randomly sample a group of seed nodes from the training nodes. We then populate the batch with the union of seed nodes and their k-hop neighbors to form a subgraph, where k is the number of layers in GNNs. Lastly, the subgraph representing the mini-batch, together with their node features is fed as input to the model. L ep is calculated from the adjacency matrix of this subgraph; L nc is calculated only with the predictions on seed nodes. Note that the sampled graph for each mini-batch would be subtly different from the one during full-batch training, as the model cannot sample any new edges between seed nodes and target nodes outside of the extended subgraph. Nevertheless, this slight difference does not affect the training much as most of the sampled new edges are within the subgraph (within a few hops).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hyperparameters and Search Space</head><p>In this section, we describe the parameters of all methods along with the search space of all hyperparameters. All methods were implemented in Python 3.7.6 with PyTorch. Our implementation can be found at https:// github.com/zhao-tong/GAug. We further include code for ADAEDGE ) and DROPEDGE <ref type="bibr" target="#b36">(Rong et al. 2019</ref>) for comparisons. The best hyperparameter choices and searching scripts can be found in the supplementary material.</p><p>Original graph neural network architectures. All original GNN architectures are implemented in DGL 3 with Adam optimizer. We search through the basic parameters such as learning rate and the choice of aggregators (and number of layers only for JK-NET) to determine the default settings of each GNN. By default, GCN, GSAGE and GAT have 2 layers, and JK-NET has 3 layers due to its unique design. GCN, GSAGE and JK-NET have hidden size 128, and GAT has a hidden size of 16 for each head (have use 8 heads). GCN, GSAGE and JK-NET have learning rates of 1e−2 and GAT has best performance with learning rate of 5e−3. All methods have weight decay of 5e−4. GCN, GSAGE and JK-NET use feature dropout of 0.5, while GAT uses both feature dropout and attention dropout of 0.6. For GSAGE, we use the GCN-style aggregator. For JK-NET, we use GSAGE layer with GCN-style aggregator as neighborhood aggregation layers and concatenation for the final aggregation layer. To make fair comparisons, these parameters are fixed for all experiments and our hyperparameter searches only search over the new parameters introduced by baselines and our proposed methods. ADAEDGE. We implement ADAEDGE ) based on the above mentioned GNNs and the provided pseudo-code in their paper in PyTorch, since the author-implemented code was unavailable. We tune the following hyperparameters over ranges: order ∈ {add f irst, remove f irst}, num + ∈ {0, 1, . . . , |E| − 1}, num − ∈ {0, 1, . . . , |E| − 1}, conf + ∈ [0.5, 1], conf − ∈ [0.5, 1]. DROPEDGE. We also implement DROPEDGE <ref type="bibr" target="#b36">(Rong et al. 2019</ref>) (adapting the authors' code for easier comparison) based on the above mentioned GNNs, where the GNNs randomly remove p|E| of the edges and redo the normalization on the adjacency matrix before each training epoch, where p is searched in the range of [0, 0.99]. BGCN. The BGCN model consists of two parts: an assortative mixed membership stochastic block model (MMSBM) and a GNN. For MMSBM, we use the code package 4 provided by the authors <ref type="bibr" target="#b55">(Zhang et al. 2019b)</ref>. For GNNs, we use the above mentioned implementations. We follow the training process provided in the authors' code package. GAUG-M. As described in Section 4.1, GAUG-M has two hyperparameters i and j, which are both searched within the range of {0, 0.01, 0.02, . . . , 0.8}. GAUG-O. We tune the following hyperparameters over search ranges for GAUG-O. The influence of the edge predictor on the original graph: α ∈ {0, 0.01, 0.02, . . . , 1}; the weight for L ep when training the model: β ∈ {0, 0.1, 0.2, . . . , 4}; the temperature for the relaxed Bernoulli sampling: temp ∈ {0.1, 0.2, . . . , 2}; number of pretrain epochs for both edge predictor and node classifier: n pretrain ep , n pretrain nc ∈ {5, 10, 15, . . . , 300}; the parameter for warmup: warmup ∈ {0, 1, . . . , 10}.</p><p>Mini-batch training. For vanilla GNNs, we follow the standard mini-batch training proposed by <ref type="bibr" target="#b16">Hamilton, Ying, and Leskovec (2017)</ref> and <ref type="bibr" target="#b51">Ying et al. (2018)</ref>. For GNNs with GAUG-M, both the VGAE edge predictor and GNN also followed the same mini-batch training. For GNNs with GAUG-O, we used mini-batch training as described in Appx. C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Discussion and Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Ablation Study for GAUG-O</head><p>We extensively study the effect of various design choices in GAUG-O to support our decisions. Here, we compare the results of the 4 GNN architectures in combination with baseline and GAUG-O applied with different graph sampling and training choices on CORA. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. No Sampling: Instead of interpolating M and A and subsequently sampling, we avoid the sampling step and feed the fully dense adjacency into the GNN classifier (every edge is used for convolution, with different weights). This shows decreased performance compared to sampling-based solution, likely because the sampling removes noise from many, very weak edges. Rounding: Instead of sampling the graph, we deterministically round the edge probabilities to 0 or 1, using the same ST gradient estimator in the backward pass. Rounding creates the same decision boundary for edge and no-edge at each epoch. We observe that it hurts performance compared 4 https://github.com/huawei-noah/BGCN  to sampling, likely due to reduced diversity during model training.</p><p>No Edge-Prediction Loss: Instead of training GAUG-O with a positive β (coefficient for L ep ), we set β = 0. Without controlling drift of pre-trained edge predictor by combining its loss(L ep ) with node classification loss(L nc ) in training, we risk generating unrealistic graphs which arbitrarily deviate from the original graph, and producing instability in training. We find that removing this loss term leads to empirical performance decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 GAUG with Mini-batch Training</head><p>In order to better show the scalability of the proposed methods with mini-batch training, <ref type="table" target="#tab_6">Table 5</ref> shows that the proposed GAUG methods are able to perform well when using mini-batch training. More specifically, on PUBMED, augmentation (via GAUG-M) achieves 2.2% improvement, while on OGBN-ARXIV, augmentation (via GAUG-O) achieves 4.8% improvement. All three methods are trained in the mini-batch setting with the same batch size. Note that the performance using mini-batch training is generally not as good as full-batch training (even for vanilla GNNs), hence mini-batch training is only recommended when graphs are too large to fit in GPU as a whole. Similar to CORA and CITESEER, both of the two large graph datasets are citation networks: PUBMED is a commonly used GNN benchmark <ref type="bibr" target="#b24">(Kipf and Welling 2016a)</ref>, and OGBN-ARXIV is a standard benchmark provided by the   (g) JI <ref type="figure">Figure 7</ref>: GAUG-M with GCN on CORA with different edge prediction heuristics Open Graph Benchmark 5 <ref type="bibr" target="#b20">(Hu et al. 2020)</ref>. <ref type="table" target="#tab_5">Table 4</ref> summarizes their statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Evaluating GAUG-M on original vs. modified graph</head><p>Although GAUG-M is designed for the modified-graph setting, it is still possible to do inference on G while training the model on G m . As previously mentioned in Section 3.3, inference with GAUG-M on G would result in a train-test gap, which would affect the test performance. <ref type="table" target="#tab_7">Table 6</ref> presents the inference results of GAUG-M with G m and G (GAUG-M-O makes inference on G). We can observe that both variants of GAUG-M show performance improvements over the original GNNs across different architectures and datasets. Moreover, inference with GAUG-M on G m has equal or better performance in almost cases, which aligns with our in-  Index (JI). The first two are GCN based neural auto-encoder models and the later two are edge prediction methods based on local neighborhoods, which are commonly used and celebrated in network science literature. It is noticeable that even with the same dataset, the performance heatmaps are characteristically different when using various edge prediction methods, demonstrating the relative importance of edge predictor to GAUG-M's augmentation performance. Moreover, it supports our findings on the importance strategic edge addition and removal to improve performance of graph augmentation/regularization based methods -as <ref type="table" target="#tab_2">Table 2</ref> shows, careless edge addition/removal can actually hurt performance. We do not show results when equipping GAUG-O with these different edge predictors, since GAUG-O requires the edge predictor to be differentiable for training (hence our choice of GAE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Classification Performance with Deeper GNNs</head><p>In <ref type="table" target="#tab_9">Table 7</ref> we show the performance of our proposed GAUG framework with different number of layers on the CORA dataset. As mentioned in Appendix C.2, GCN, GSAGE and GAT have 2 layers by default, while JK-NET has 3 layers due to it's unique design. From <ref type="table" target="#tab_9">Table 7</ref> we can observe that when increasing the number of layers, most GNNs perform worse except for JK-NET, which is specifically designed for deep GNNs. GAUG-M shows stable performance improvements over all GNN architectures with different depth; GAUG-O shows performance improvements on GCN, GSAGE and JK-NET with different depth. Our results suggest that augmentation can be a tool which facilitates deeper GNN training, as performance improvements for the common practical GNN implementations (GCN and GSAGE) demonstrate quite large performance improvements when compared to standard implementations (e.g. 52.4 point absolute F1 improvement for GCN, 16.4 point absolute F1 improvement for GSAGE at 8 layers). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 GAUG's sensitivity to supervision</head><p>As previously mentioned in Section 5, which we showed that GAUG is especially powerful under weak supervision, in <ref type="figure" target="#fig_3">Figure 8</ref> we detail the sensitivity to supervision for each GNN combined with GAUG-M and GAUG-O. We can observe a larger separation between original (dotted) and GAUG (solid) in each plot when training samples decrease, indicating larger performance gain under weak supervision.</p><p>In most settings, test F1 score of GAUG-M and GAUG training with 35 training nodes is at par or better than baseline training with 2 times as many <ref type="formula">(70)</ref> nodes. This suggest that GAUG is an especially appealing option in cases of weak supervision, like in anomaly detection and other imbalanced learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 Embedding Visualization</head><p>To further illustrate the case for graph data augmentation in addition to <ref type="figure">Fig. 1</ref> and Section 3.2, we plot the features and embeddings to understand how edge manipulation can contribute to lower-effort (and in the fully class-homophilic/ideal scenario, effortless) classification. In <ref type="figure" target="#fig_4">Figure 9</ref>, we randomly initialize 2-D node features for the Zachary's Karate Club graph from normal distribution N (0, 1) ( <ref type="figure" target="#fig_4">Figure 9a</ref>), and show the results of applying a single graph convolution layer (Eq. 1) with randomly initialized weights. When the input graph is "ideal" (all intraclass edges exist, and no inter-class edges exist) all sameclass points project to the same embedding, making discriminating the classes trivial <ref type="figure" target="#fig_4">(Figure 9e</ref>). It is obvious that the original feature and embeddings <ref type="figure" target="#fig_4">(Figure 9b</ref>) are harder for a classifier to separate than <ref type="figure" target="#fig_4">Figure 9c</ref>, which the graph was modified by adding (removing) intra(inter)-class edges (simulating the scenario which GAUG-M achieves). <ref type="figure" target="#fig_4">Figure  9d</ref> shows superimposed points resulting from 100 different modifications of the graph (simulating GAUG-O), illustrating the much clearer decision boundary between red-class and green-class points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Classification (test) performance heatmaps of GAUG-M on various datasets when adding/dropping edges. Red-white-blue indicate outperformance, at-par, and underperformance w.r.t. GCN on G. Pixel (0, 0) indicates G, and x (y) axes show % edges added (removed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>GAUG augmentation especially improves performance under weak supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>GAUG improves performance under weak supervision with each GNN (GCN, GSAGE, GAT and JK-NET, left to right) and across augmentation settings (GAUG-M on top, GAUG-O on bottom). Relative improvement is clear even with many training nodes, but is larger with few training nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Embeddings after one GCN layer. (c) and (e) show that augmentation can produce more clear decision boundaries between red and green nodes, compared to raw features (a), and naive GCN on raw features (b). (e) shows the effortless classification possible in the ideal graph scenario, where all same-class nodes have the same embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary statistics and experimental setup for the six evaluation datasets.</figDesc><table><row><cell></cell><cell cols="2">CORA CITESEER</cell><cell>PPI</cell><cell cols="3">BLOGCATALOG FLICKR AIR-USA</cell></row><row><cell># Nodes</cell><cell>2,708</cell><cell>3,327</cell><cell>10,076</cell><cell>5,196</cell><cell>7,575</cell><cell>1,190</cell></row><row><cell># Edges</cell><cell>5,278</cell><cell>4,552</cell><cell>157,213</cell><cell>171,743</cell><cell>239,738</cell><cell>13,599</cell></row><row><cell># Features</cell><cell>1,433</cell><cell>3,703</cell><cell>50</cell><cell>8,189</cell><cell>12,047</cell><cell>238</cell></row><row><cell># Classes</cell><cell>7</cell><cell>6</cell><cell>121</cell><cell>6</cell><cell>9</cell><cell>4</cell></row><row><cell># Training nodes</cell><cell>140</cell><cell>120</cell><cell>1,007</cell><cell>519</cell><cell>757</cell><cell>119</cell></row><row><cell># Validation nodes</cell><cell>500</cell><cell>500</cell><cell>2,015</cell><cell>1,039</cell><cell>1,515</cell><cell>238</cell></row><row><cell># Test nodes</cell><cell>1,000</cell><cell>1,000</cell><cell>7,054</cell><cell>3,638</cell><cell>5,303</cell><cell>833</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>GAUG performance across GNN architectures and six benchmark datasets.<ref type="bibr" target="#b36">Rong et al. 2019</ref>) (original-graph) evaluating on G m and G, respectively. We also show results of proposed GAUG methods on large graphs<ref type="bibr" target="#b20">(Hu et al. 2020)</ref> in Appendix D.2 to show their ability of mini-batching. We report test micro-F1 scores over 30 runs, employing Optuna<ref type="bibr" target="#b0">(Akiba et al. 2019)</ref> for efficient hyperparameter search. Note that for</figDesc><table><row><cell>GNN Arch.</cell><cell>Method</cell><cell>CORA</cell><cell>CITESEER</cell><cell>PPI</cell><cell>BLOGC</cell><cell>FLICKR</cell><cell>AIR-USA</cell></row><row><cell></cell><cell>Original</cell><cell>81.6±0.7</cell><cell>71.6±0.4</cell><cell>43.4±0.2</cell><cell>75.0±0.4</cell><cell>61.2±0.4</cell><cell>56.0±0.8</cell></row><row><cell></cell><cell>+BGCN</cell><cell>81.2±0.8</cell><cell>72.4±0.5</cell><cell>-</cell><cell>72.0±2.3</cell><cell>52.7±2.8</cell><cell>56.5±0.9</cell></row><row><cell>GCN</cell><cell>+ADAEDGE</cell><cell>81.9±0.7</cell><cell>72.8±0.7</cell><cell>43.6±0.2</cell><cell>75.3±0.3</cell><cell>61.2±0.5</cell><cell>57.2±0.8</cell></row><row><cell></cell><cell>+GAUG-M</cell><cell>83.5±0.4</cell><cell>72.3±0.4</cell><cell>43.5±0.2</cell><cell>77.6±0.4</cell><cell>68.2±0.7</cell><cell>61.2±0.5</cell></row><row><cell></cell><cell>+DROPEDGE</cell><cell>82.0±0.8</cell><cell>71.8±0.2</cell><cell>43.5±0.2</cell><cell>75.4±0.3</cell><cell>61.4±0.7</cell><cell>56.9±0.6</cell></row><row><cell></cell><cell>+GAUG-O</cell><cell>83.6±0.5</cell><cell>73.3±1.1</cell><cell>46.6±0.3</cell><cell>75.9±0.2</cell><cell>62.2±0.3</cell><cell>61.4±0.9</cell></row><row><cell></cell><cell>Original</cell><cell>81.3±0.5</cell><cell>70.6±0.5</cell><cell>40.4±0.9</cell><cell>73.4±0.4</cell><cell>57.4±0.5</cell><cell>57.0±0.7</cell></row><row><cell></cell><cell>+BGCN</cell><cell>80.5±0.1</cell><cell>70.8±0.1</cell><cell>-</cell><cell>73.2±0.2</cell><cell>58.1±0.3</cell><cell>53.5±0.3</cell></row><row><cell>GSAGE</cell><cell>+ADAEDGE</cell><cell>81.5±0.6</cell><cell>71.3±0.8</cell><cell>41.6±0.8</cell><cell>73.6±0.4</cell><cell>57.7±0.7</cell><cell>57.1±0.5</cell></row><row><cell></cell><cell>+GAUG-M</cell><cell>83.2±0.4</cell><cell>71.2±0.4</cell><cell>41.1±1.0</cell><cell>77.0±0.4</cell><cell>65.2±0.4</cell><cell>60.1±0.5</cell></row><row><cell></cell><cell>+DROPEDGE</cell><cell>81.6±0.5</cell><cell>70.8±0.5</cell><cell>41.1±1.0</cell><cell>73.8±0.4</cell><cell>58.4±0.7</cell><cell>57.1±0.5</cell></row><row><cell></cell><cell>+GAUG-O</cell><cell>82.0±0.5</cell><cell>72.7±0.7</cell><cell>44.4±0.5</cell><cell>73.9±0.4</cell><cell>56.3±0.6</cell><cell>57.1±0.7</cell></row><row><cell></cell><cell>Original</cell><cell>81.3±1.1</cell><cell>70.5±0.7</cell><cell>41.5±0.7</cell><cell>63.8±5.2</cell><cell>46.9±1.6</cell><cell>52.0±1.3</cell></row><row><cell></cell><cell>+BGCN</cell><cell>80.8±0.8</cell><cell>70.8±0.6</cell><cell>-</cell><cell>61.4±4.0</cell><cell>46.5±1.9</cell><cell>54.1±3.2</cell></row><row><cell>GAT</cell><cell>+ADAEDGE</cell><cell>82.0±0.6</cell><cell>71.1±0.8</cell><cell>42.6±0.9</cell><cell>68.2±2.4</cell><cell>48.2±1.0</cell><cell>54.5±1.9</cell></row><row><cell></cell><cell>+GAUG-M</cell><cell>82.1±1.0</cell><cell>71.5±0.5</cell><cell>42.8±0.9</cell><cell>70.8±1.0</cell><cell>63.7±0.9</cell><cell>59.0±0.6</cell></row><row><cell></cell><cell>+DROPEDGE</cell><cell>81.9±0.6</cell><cell>71.0±0.5</cell><cell>45.9±0.3</cell><cell>70.4±2.4</cell><cell>50.0±1.6</cell><cell>52.8±1.7</cell></row><row><cell></cell><cell>+GAUG-O</cell><cell>82.2±0.8</cell><cell>71.6±1.1</cell><cell>44.9±0.9</cell><cell>71.0±1.1</cell><cell>51.9±0.5</cell><cell>54.6±1.1</cell></row><row><cell></cell><cell>Original</cell><cell>78.8±1.5</cell><cell>67.6±1.8</cell><cell>44.1±0.7</cell><cell>70.0±0.4</cell><cell>56.7±0.4</cell><cell>58.2±1.5</cell></row><row><cell></cell><cell>+BGCN</cell><cell>80.2±0.7</cell><cell>69.1±0.5</cell><cell>-</cell><cell>65.7±2.2</cell><cell>53.6±1.7</cell><cell>55.9±0.8</cell></row><row><cell>JK-NET</cell><cell>+ADAEDGE</cell><cell>80.4±1.4</cell><cell>68.9±1.2</cell><cell>44.8±0.9</cell><cell>70.7±0.4</cell><cell>57.0±0.3</cell><cell>59.4±1.0</cell></row><row><cell></cell><cell>+GAUG-M</cell><cell>81.8±0.9</cell><cell>68.2±1.4</cell><cell>47.4±0.6</cell><cell>71.9±0.5</cell><cell>65.7±0.8</cell><cell>60.2±0.6</cell></row><row><cell></cell><cell>+DROPEDGE</cell><cell>80.4±0.7</cell><cell>69.4±1.1</cell><cell>46.3±0.2</cell><cell>70.9±0.4</cell><cell>58.5±0.7</cell><cell>59.1±1.1</cell></row><row><cell></cell><cell>+GAUG-O</cell><cell>80.5±0.9</cell><cell>69.7±1.4</cell><cell>53.1±0.3</cell><cell>71.0±0.6</cell><cell>55.7±0.5</cell><cell>60.4±1.0</cell></row></table><note>and a 10/20/70% split on other datasets due to varying choices in prior work. We evaluate GAUG-M and GAUG- O using 4 widely used GNN architectures: GCN (Kipf and Welling 2016a), GSAGE (Hamilton, Ying, and Leskovec 2017), GAT (Veličković et al. 2017) and JK-NET (Xu et al. 2018b). We compare our GAUG-M (modified-graph) and GAUG-O (original-graph) performance with that achieved by standard GNN performance, as well as three state-of- the-art baselines: ADAEDGE (Chen et al. 2019) (modified- graph), BGCN (Zhang et al. 2019b) (modified-graph), and DROPEDGE (classification tasks which every object is guaranteed to be assigned to exactly one ground truth class (all datasets except PPI), micro-F1 score is mathematically equivalent to accuracy (proof in Appendix A.2). Our implementation is made publicly available 1 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>is organized per architecture (row), per dataset (column), and original-graph and modified-graph settings (within-row). Note that results of BGCN on PPI are missing due to CUDA out of memory error when running the code package from the authors. We bold best-performance per architecture and dataset, but not per augmentation setting for visual clarity. In short, GAUG-O and GAUG-M consistently improve over GNN architectures, datasets and alternatives, with a single exception for GAT on PPI, on which DROPE-</figDesc><table /><note>DGE performs the best. Improvement across GNN architectures. GAUG achieves improvements over all 4 GNN architectures (averaged</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of GAUG-O on CORA 6±0.7 81.3±0.5 81.3±1.1 78.0±1.5 +GAUG-O 83.6±0.5 82.0±0.5 82.2±0.8 80.5±0.9 +GAUG-O No Sampling 82.8±0.9 81.2±0.8 77.8±2.2 76.9±1.4 +GAUG-O Rounding 82.5±0.5 81.4±0.5 81.3±1.1 79.5±1.3 +GAUG-O No Lep 82.8±0.8 81.5±1.1 81.9±0.8 79.5±1.0</figDesc><table><row><cell>Setting</cell><cell>GCN</cell><cell>GSAGE</cell><cell>GAT</cell><cell>JK-NET</cell></row><row><cell>Original (GCN)</cell><cell>81.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Summary statistics for the large datasets.</figDesc><table><row><cell></cell><cell cols="2">PUBMED OGBN-ARXIV</cell></row><row><cell># Nodes</cell><cell>19,717</cell><cell>169,343</cell></row><row><cell># Edges</cell><cell>44,338</cell><cell>1,166,243</cell></row><row><cell># Features</cell><cell>500</cell><cell>128</cell></row><row><cell># Classes</cell><cell>3</cell><cell>40</cell></row><row><cell># Training nodes</cell><cell>60</cell><cell>90,941</cell></row><row><cell># Validation nodes</cell><cell>500</cell><cell>29,799</cell></row><row><cell># Test nodes</cell><cell>1000</cell><cell>48,603</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>GAUG performance with mini-batch training.</figDesc><table><row><cell>Methods</cell><cell cols="2">PUBMED OGBN-ARXIV</cell></row><row><cell>GCN</cell><cell>78.5±0.5</cell><cell>68.1±0.3</cell></row><row><cell cols="2">GCN + GAUG-M 80.2±0.3</cell><cell>68.2±0.3</cell></row><row><cell>GCN + GAUG-O</cell><cell>79.3±0.4</cell><cell>71.4±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>GAUG-M performance on original and modified graphs.</figDesc><table><row><cell cols="2">Backbone Method</cell><cell>CORA</cell><cell>CITESEER</cell><cell>PPI</cell><cell>BLOGC</cell><cell>FLICKR</cell><cell>AIR-USA</cell></row><row><cell></cell><cell>original</cell><cell>81.6±0.7</cell><cell>71.6±0.4</cell><cell cols="3">43.4±0.2 75.0±0.4 61.2±0.4</cell><cell>56.0±0.8</cell></row><row><cell>GCN</cell><cell>+GAUG-M</cell><cell>83.5±0.4</cell><cell>72.3±0.4</cell><cell cols="3">43.5±0.2 77.6±0.4 68.2±0.7</cell><cell>61.2±0.5</cell></row><row><cell></cell><cell cols="2">+GAUG-M-O 83.1±0.5</cell><cell>72.8±0.5</cell><cell cols="3">43.5±0.2 75.6±0.4 61.6±0.6</cell><cell>58.1±0.6</cell></row><row><cell></cell><cell>Original</cell><cell>81.3±0.5</cell><cell>70.6±0.5</cell><cell cols="3">40.5±0.9 73.4±0.4 57.4±0.5</cell><cell>57.0±0.7</cell></row><row><cell>GSAGE</cell><cell>+GAUG-M</cell><cell>83.2±0.4</cell><cell>71.2±0.4</cell><cell cols="3">41.1±1.0 77.0±0.4 65.2±0.4</cell><cell>60.1±0.5</cell></row><row><cell></cell><cell cols="2">+GAUG-M-O 82.4±0.5</cell><cell>71.6±0.3</cell><cell cols="3">41.1±1.4 74.3±0.3 58.1±0.5</cell><cell>58.9±0.5</cell></row><row><cell></cell><cell>Original</cell><cell>81.3±1.1</cell><cell>70.5±0.7</cell><cell cols="3">41.5±0.7 63.8±5.2 49.6±1.6</cell><cell>52.0±1.3</cell></row><row><cell>GAT</cell><cell>+GAUG-M</cell><cell>82.1±1.0</cell><cell>71.5±0.5</cell><cell cols="3">42.8±0.9 70.8±1.0 63.7±0.9</cell><cell>59.0±0.6</cell></row><row><cell></cell><cell cols="2">+GAUG-M-O 82.0±0.9</cell><cell>71.3±0.7</cell><cell cols="3">46.3±0.2 71.0±1.3 48.5±1.9</cell><cell>53.4±1.1</cell></row><row><cell></cell><cell>Original</cell><cell>78.8±1.5</cell><cell>67.6±1.8</cell><cell cols="3">44.1±0.7 70.0±0.4 56.7±0.4</cell><cell>58.2±1.5</cell></row><row><cell>JK-NET</cell><cell>+GAUG-M</cell><cell>81.8±0.9</cell><cell>68.2±1.4</cell><cell cols="3">47.4±0.6 71.9±0.5 65.7±0.8</cell><cell>60.2±0.6</cell></row><row><cell></cell><cell cols="2">+GAUG-M-O 80.6±1.0</cell><cell>68.3±1.4</cell><cell cols="3">48.6±0.5 71.0±0.4 57.0±0.4</cell><cell>60.2±0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>GAUG performance for deeper GNNs. 6±0.7 81.3±0.5 81.3±1.1 78.0±1.5 +GAUG-M 83.5±0.4 83.2±0.4 82.1±1.0 81.8±0.9 +GAUG-O 83.6±0.5 82.0±0.5 82.2±0.8 80.5±0.9 4 layers Original 74.7±2.7 78.9±1.4 79.8±1.0 79.6±1.6 +GAUG-M 78.9±1.0 81.5±0.8 81.4±0.8 81.9±1.2</figDesc><table><row><cell></cell><cell></cell><cell cols="3"># of layers Method</cell><cell>GCN</cell><cell cols="2">GSAGE</cell><cell>GAT</cell><cell>JK-NET</cell></row><row><cell></cell><cell></cell><cell cols="2">default</cell><cell cols="6">Original 81.+GAUG-O 80.6±0.9 80.1±1.0 75.3±2.1 80.9±0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original</cell><cell cols="5">57.2±9.6 77.7±1.3 77.8±1.5 79.7±1.0</cell></row><row><cell></cell><cell></cell><cell cols="2">6 layers</cell><cell cols="6">+GAUG-M 74.2±2.4 80.7±1.1 79.2±0.8 82.0±0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+GAUG-O</cell><cell cols="5">79.8±1.0 79.8±0.7 13.6±2.8 80.9±0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original</cell><cell cols="5">25.0±4.7 61.7±9.9 65.0±6.4 79.2±1.5</cell></row><row><cell></cell><cell></cell><cell cols="2">8 layers</cell><cell cols="6">+GAUG-M 56.4±5.7 78.1±2.0 77.9±1.5 82.1±0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>+GAUG-O</cell><cell cols="5">77.4±1.9 76.7±1.5 13.0±0.0 81.1±1.1</cell></row><row><cell>Test Micro F1</cell><cell>0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88</cell><cell>35 140 340 540 740 940 1140 # Nodes in training GCN-Orig GCN-GAug-M Test Micro F1</cell><cell>0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88</cell><cell cols="2">35 140 340 540 740 940 1140 # Nodes in training GSage-Orig GSage-GAug-M Test Micro F1</cell><cell>0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88</cell><cell cols="3">35 140 340 540 740 940 1140 # Nodes in training GAT-Orig GAT-GAug-M Test Micro F1</cell><cell>0.74 0.76 0.78 0.80 0.82 0.86 0.84</cell><cell># Nodes in training 35 140 340 540 740 940 1140 JKNet-Orig JKNet-GAug-M</cell></row><row><cell>Test Micro F1</cell><cell>0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88</cell><cell>35 140 340 540 740 940 1140 # Nodes in training GCN-Orig GCN-GAug-O Test Micro F1</cell><cell>0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88</cell><cell cols="2">35 140 340 540 740 940 1140 # Nodes in training GSage-Orig GSage-GAug-O Test Micro F1</cell><cell>0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88</cell><cell cols="3">35 140 340 540 740 940 1140 # Nodes in training GAT-Orig GAT-GAug-O Test Micro F1</cell><cell>0.74 0.76 0.78 0.80 0.82 0.86 0.84</cell><cell># Nodes in training 35 140 340 540 740 940 1140 JKNet-Orig JKNet-GAug-O</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Prior CV literature<ref type="bibr" target="#b45">(Wang, Wang, and Lian 2019)</ref> considers image data augmentation a two-step process: (1) applying a transformation f : S → T to input images S to generate variants T , and (2) utilizing S ∪ T for model</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/zhao-tong/GAug</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://cloud.google.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.dgl.ai/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://ogb.stanford.edu/ tuition in Section 3.3. This suggests that GAUG-M actually improves training in a way that helps generalization even on the original graph by better parameter inference, despite modifying the graph during training to achieve this.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Theorem 1</head><p>We first reproduce the definition of a permutation-invariant neighborhood aggregator <ref type="bibr" target="#b49">(Xu et al. 2018a)</ref> in the context of graph convolution:</p><p>it is invariant to the order of the target node and its neighbor nodes u ∪ N (u), i.e. let {x 1 , x 2 , . . . ,</p><p>Next, we prove Theorem 1:</p><p>Proof. LetÃ be the adjacency matrix with added self loops, i.e.,Ã = A + I. Here we denote the calculation process of a GNN layer with a permutation-invariant neighborhood aggregator as:</p><p>where σ denotes a nonlinear activation (e.g. ReLU) and A denotes the normalized adjacency matrix according to the design of different GNN architectures. For example, in GCN layer (Kipf and Welling 2016a),Ā =D − 1 2ÃD − 1 2 ; in GSAGE layer with GCN aggregator <ref type="bibr" target="#b16">(Hamilton, Ying, and Leskovec 2017)</ref>,Ā is the row L1-normalized A.</p><p>For any two nodes i, j ∈ V that are contained in the same fully connected component S ⊆ V, i and j are only connected to all other nodes in S by definition. HenceÃ iv = A jv = 1, ∀v ∈ S andÃ iu =Ã ju = 0, ∀u / ∈ S, that is,Ã i: =Ã j: . Moreover, as the degrees of all nodes in the same fully connected component are the the same, we havē A i: =Ā j: . Thus by Equation 5, H i: = H j: .</p><p>On the other hand, for any two nodes i, j ∈ V that are contained in different fully connected components S a , S b ⊆ V respectively (a = b), i and j are not connected and do not share any neighbors by definition. As all nodes in S a have the same degree, all nonzero entries inÃ i: would have the same positive valueā. Similarly, all the nonzero entries iñ A j: also have the same positive valueb. Then, by Equation 5, the embeddings of i and j after the GNN layer will respectively be</p><p>From the above equation, we can observe that H i: = H j: when W is not all zeros and v∈Sa X v: =b a u∈S b X u: .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD</title>
		<meeting>the 25th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<title level="m">Data augmentation generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The imbalanced training sample problem: Under or over sampling?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barandela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Valdovinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR international workshops on SPR and SSPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMOTE: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03211</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on CVPR</title>
		<meeting>the IEEE conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding Back-Translation at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on EMNLP</title>
		<meeting>the 2018 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00440</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD</title>
		<meeting>the 24th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Data mining: concepts and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Population based augmentation: Efficient learning of augmentation policy schedules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05393</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<title level="m">Open graph benchmark: Datasets for machine learning on graphs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Label informed attributed network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on WSDM</title>
		<meeting>the Tenth ACM International Conference on WSDM</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="731" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data Augmentation for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousefhussien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="198" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Smart augmentation learning an optimal data augmentation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corcoran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Ieee Access 5</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICCV</title>
		<meeting>the IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In 32th AAAI</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01777</idno>
		<idno>arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A Unified View on Graph Neural Networks as Graph Signal Denoising</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on CVPR</title>
		<meeting>the IEEE Conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04621</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD</title>
		<meeting>the 20th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data Augmentation via Dependency Tree Morphing for Low-Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Ş Ahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09460</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Improving Neural Machine Translation Models with Monolingual Data</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th WWW</title>
		<meeting>the 24th WWW</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11715</idno>
		<title level="m">Graphmix: Regularized training of graph neural networks for semi-supervised learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD</title>
		<meeting>the 22nd ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Calendar Graph Neural Networks for Modeling Time Structures in Spatiotemporal User Behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD</title>
		<meeting>the 26th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A survey on face data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11685</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DEMO-Net: Degree-specific graph neural networks for node and graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD</title>
		<meeting>the 25th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised Data Augmentation for Consistency Training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD</title>
		<meeting>the 24th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Identifying referential intention with heterogeneous contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD</title>
		<meeting>the 25th ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Character-level Convolutional Networks for Text Classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bayesian graph convolutional neural networks for semisupervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ustebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5829" to="5836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<title level="m">Deep learning on graphs: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on CVPR</title>
		<meeting>the IEEE conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Error-Bounded Graph Anomaly Loss for GNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Random erasing data augmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GAUG-M performance under different edge predictors Although we use GAE as the edge prediction model of choice due to its strong performance in link prediction, GAUG-M can be generally equipped with any edge prediction module. In Figure 7 we show classification performance heatmaps of GAUG-M (with GCN) on CORA, when adding/removing edges according to different heuristics. Specifically, graph auto-encoder (GAE) (Kipf and Welling 2016b), variational graph auto-encoder (VGAE) (Kipf and Welling</title>
	</analytic>
	<monogr>
		<title level="m">the Local Leicht-Holme-Newman Index (LLHN), the Resource Allocation Index (RA), CAR-based Indices (CAR), Local Naive Bayes (LNB) and the Jaccard</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
