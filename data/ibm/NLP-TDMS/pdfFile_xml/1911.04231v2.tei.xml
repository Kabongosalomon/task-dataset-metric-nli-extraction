<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianran</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a novel data-driven method for robust 6DoF object pose estimation from a single RGBD image. Unlike previous methods that directly regressing pose parameters, we tackle this challenging task with a keypointbased approach. Specifically, we propose a deep Hough voting network to detect 3D keypoints of objects and then estimate the 6D pose parameters within a least-squares fitting manner. Our method is a natural extension of 2Dkeypoint approaches that successfully work on RGB based 6DoF estimation. It allows us to fully utilize the geometric constraint of rigid objects with the extra depth information and is easy for a network to learn and optimize. Extensive experiments were conducted to demonstrate the effectiveness of 3D-keypoint detection in the 6D pose estimation task. Experimental results also show our method outperforms the state-of-the-art methods by large margins on several benchmarks. Code and video are available at https://github.com/ethnhe/PVN3D.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this paper, we study the problem of 6DoF pose estimation, i.e. recognize the 3D location and orientation of an object in a canonical frame. It is an important component in many real-world applications, such as robotic grasping and manipulation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b57">56]</ref>, autonomous driving <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b54">53]</ref>, augmented reality <ref type="bibr" target="#b32">[31]</ref> and so on. 6DoF estimation has been proven a quite challenging problem due to variations of lighting, sensor noise, occlusion of scenes and truncation of objects. Traditional methods like <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b30">30]</ref> used hand-crafted features to extract the correspondence between images and object mesh models. Such empirical human-designed features would suffer from limited performance with changing illumination conditions  and scenes with heavy occlusion. More recently, with the explosive growth of machine learning and deep learning techniques, Deep Neural Network (DNN) based methods have been introduced into this task and reveal promising improvements. <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b53">52]</ref> proposed to regress rotation and translation of objects directly with DNNs. However, these methods usually had poor generalization due to the nonlinearity of the rotation space explained by <ref type="bibr" target="#b38">[37]</ref>. Instead, recent works utilized DNNs to detect 2D keypoints of an object, and computed 6D pose parameters with Perspectiven-Point (PnP) algorithms <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b48">47]</ref>. Although these two-stage approaches performed more stable, most of them were built on top of the 2D projection. Errors that are small in projection can be large in real 3D space. Also, different keypoints in 3D space may be overlapped after 2D projection, making them hard to be distinguished. Moreover, geometric constraint information of rigid objects would be arXiv:1911.04231v2 [cs.CV] <ref type="bibr" target="#b24">24</ref> Mar 2020 partially lost due to projection.</p><p>On the other hand, with the development of inexpensive RGBD sensors, more and more RGBD datasets are available. The extra depth information allows 2D algorithms to be extend into 3D space with better performance, like Point-Fusion <ref type="bibr" target="#b54">[53]</ref>, Frustum pointnets <ref type="bibr" target="#b40">[39]</ref> and VoteNet <ref type="bibr" target="#b39">[38]</ref>. Towards this end, we extend 2D-keypoint-based approaches to 3D keypoint to fully utilize geometric constraint information of rigid objects and significantly improved the accuracy of 6DoF estimation. More specifically, we develop a deep 3D keypoints Hough voting neural network to learn the point-wise 3D offset and vote for 3D keypoints, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Our key observation is a simple geometric property that positional relationship between two points of a rigid object in 3D space is fixed. Hence, given a visible point on the object surface, its coordinate and orientation can be obtained from depth images and its translation offset to selected keypoint is also fixed and learnable. Meanwhile, learning point-wise Euclidean offset is straightforward for network and easier to optimize.</p><p>To handle scenes with multiple objects, we also introduce an instance semantic segmentation module into the network and jointly optimized with keypoint voting. We find that jointly training these tasks boosts the performance of each other. Specifically, semantic information improves translation offset learning by identifying which part a point belongs to and the size information contained in translation offsets helps the model to distinguish objects with similar appearance but different size.</p><p>We further conduct experiments on YCB-Video and LineMOD datasets to evaluate our method. Experimental results show that our approach outperforms current stateof-the-art methods by a significant margin.</p><p>To summarize, the main contributions of this work are as follows:</p><p>• A novel deep 3D keypoints Hough voting network with instance semantic segmentation for 6DoF Pose Estimation of single RGBD image.</p><p>• State-of-the-art 6DoF pose estimation performance on YCB and LineMOD datasets.</p><p>• An in-depth analysis of our 3D-keypoint-based method and comparison with previous approaches, demonstrating that 3D-keypoint is a key factor to boost performance for 6DoF pose estimation. We also show that jointly training 3D-keypoint and semantic segmentation can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Holistic Methods</head><p>Holistic methods directly estimate the 3D position and orientation of objects in a given image. Classical template-based methods construct rigid templates and scan through the image to compute the best matched pose <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">17]</ref>. Such templates are not robust to clustered scenes. Recently, some Deep Neural Network (DNN) based methods are proposed to directly regress the 6D pose of cameras or objects <ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b13">14]</ref>. However, non-linearity of the rotation space making the data-driven DNN hard to learn and generalize. To address this problem, some approaches use postrefinement procedure <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b51">50]</ref> to refine the pose iteratively, others discrete the rotation space and simplify it to be a classification problem <ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b46">45]</ref>. For the latter approach, post-refinement processes are still required to compensate for the accuracy sacrificed by the discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Keypoint-based Methods</head><p>Current keypoint-based methods first detect the 2D keypoints of an object in the images, then utilize a PnP algorithm to estimate the 6D pose. Classical methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b1">2]</ref> are able to detect 2D keypoint of objects with rich texture efficiently. However, they can not handle texture-less objects. With the development of deep learning techniques, some neural-network-based 2D keypoints detection methods are proposed. <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b20">20]</ref> directly regress the 2D coordinate of the keypoints, while <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b35">34]</ref> use heatmaps to locate the 2D keypoints. To better deal with truncated and occluded scenes, <ref type="bibr" target="#b38">[37]</ref> proposes a pixel-wise voting network to vote for the 2D keypoints location. These 2D keypoint based methods aim to minimize the 2D projection errors of objects. However, errors that are small in projection may be large in the real 3D world. <ref type="bibr" target="#b47">[46]</ref> extracts 3D keypoints from two views of synthetic RGB images to recover 3D poses. Nevertheless, they only utilize the RGB images, on which geometric constraint information of rigid objects partly lost due to projection, and different keypoints in 3D space may be overlapped and hard to be distinguished after projected to 2D. The advent of cheap RGBD sensors enables us to do everything in 3D with captured depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dense Correspondence Methods</head><p>These approach utilize Hough voting scheme <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b11">12]</ref> to vote for final results with per-pixel prediction. They either use random forest <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">32]</ref> or CNNs <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b52">51]</ref> to extract feature and predict the corresponding 3D object coordinates for each pixel and then vote for the final pose results. Such dense 2D-3D correspondence making these methods robust to occluded scenes, while the output space is quite large. PVNet <ref type="bibr" target="#b38">[37]</ref> uses per-pixel voting for 2D Keypoints to combine the advantages of Dense methods and keypoint-based methods. We further extend this method to 3D keypoints with extra depth information and fully utilize geometric constraints of rigid objects. <ref type="figure">Figure 2</ref>. Overview of PVN3D. The Feature Extraction module extracts the per-point feature from an RGBD image. They are fed into module MK, MC and MS to predict the translation offsets to keypoints, center point and semantic labels of each point respectively. A clustering algorithm is then applied to distinguish different instances with the same semantic label and points on the same instance vote for their target keypoints. Finally, a least-square fitting algorithm is applied to the predicted keypoints to estimate 6DoF pose parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Given an RGBD image, the task of 6DoF pose estimation is to estimate the rigid transformation that transforms an object from its object world coordinate system to the camera world coordinate system. Such transformation consists of a 3D rotation R ∈ SO(3) and a translation t ∈ R 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>To tackle this task, we develop a novel approach based on a deep 3D Hough voting network, as shown in <ref type="figure">Figure 2</ref>. The proposed method is a two-stage pipeline with 3D keypoint detection followed by a pose parameters fitting module. More specifically, taking an RGBD image as input, a feature extraction module would be used to fuse the appearance feature and geometry information. The learned feature would be fed into a 3D keypoint detection module M K which was trained to predict the per-point offsets w.r.t keypoints. Additionally, we include an instance segmentation module for multiple objects handling where a semantic segmentation module M S predicts the per-point semantic label, and a center voting module M C predicts the per-point offsets to object center. With the learned per-point offset, the clustering algorithm <ref type="bibr" target="#b6">[7]</ref> is applied to distinguish different instances with the same semantic label and points on the same instance vote for their target keypoints. Finally, a least-square fitting algorithm is applied to the predicted keypoints to estimate 6DoF pose parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Algorithm</head><p>The goal of our learning algorithm is to train a 3D keypoint detection module M K for offset prediction as well as a semantic segmentation module M S and center voting module M C for instance-level segmentation. This naturally makes training our network multi-task learning, which is achieved by a supervised loss we designed and several training details we adopt.</p><p>3D Keypoints Detection Module. As shown in <ref type="figure">Figure  2</ref>, with the per-point feature extracted by the feature extraction module, a 3D keypoint detection module M K is used to detect the 3D keypoints of each object. To be specific, M K predicts the per-point Euclidean translation offset from visible points to target keypoints. These visible points, together with the predicted offsets then vote for the target keypoints. The voted points are then gathered by clustering algorithms and centers of clusters are selected as the voted keypoints.</p><p>We give a deeper view of M K as follows. Given a set of visible seed points {p i } N i=1 and a set of selected keypoints {kp j } M j=1 belonging to the same object instance I, we denote p i = [x i ; f i ] with x i the 3D coordinate and f i the extracted feature. We denote kp j = [y j ] with y j the 3D coordinate of the keypoint. M K absorbs feature f i of each seed point and generates translation offset {of j i } M j=1 for them, where of j i denotes the translation offset from the i th seed point to the j th keypoint. Then the voted keypoint can be denoted as vkp j i = x i + of j i . To supervise the learning of of j i , we apply an L1 loss:</p><formula xml:id="formula_0">L keypoints = 1 N N i=1 M j=1 ||of j i − of j * i ||I(p i ∈ I) (1)</formula><p>where of j * i is the ground truth translation offset; M is the total number of selected target keypoints; N is the total number of seeds and I is an indicating function equates to 1 only when point p i belongs to instance I, and 0 otherwise.</p><p>Instance Semantic Segmentation Module. To handle scenes with multi objects, previous methods <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b54">53,</ref><ref type="bibr" target="#b40">39]</ref> utilize existing detection or semantic segmentation architecture to pre-process the image and obtain RoIs (regions of interest) containing only single objects. Then build the pose estimation models with the extracted ROIs as input to simplify the problem. However, as we have formulated the pose estimation problem to first detect keypoints of objects with a translation offsets to keypoints learning module, we believe that the two tasks can enhance the performance of each other. On the one hand, the semantic segmentation module forces the model to extract global and local features on instance to distinguish different objects, which helps to locate a point on the object and does good to the keypoint offset reasoning procedure. On the other hand, size information learned for the prediction of offsets to the keypoints helps distinguish objects with similar appearance but different in size. Under such observation, we introduce a pointwise instance semantic segmentation module M S into the network and jointly optimized it with module M K .</p><p>To be specific, given the per-point extracted feature, the semantic segmentation module M S predicts the per-point semantic labels. We supervise this module with Focal Loss <ref type="bibr" target="#b29">[29]</ref>:</p><formula xml:id="formula_1">L semantic = − α(1 − q i ) γ log(q i ) where q i = c i · l i<label>(2)</label></formula><p>with α the α-balance parameter, γ the focusing parameter, c i the predicted confidence for the i th point belongs to each class and l i the one-hot representation of ground true class label. Meanwhile, the center voting module M C is applied to vote for centers of different object so as to distinguish different instance. We propose such module under the inspiration of CenterNet <ref type="bibr" target="#b9">[10]</ref> but further extend the 2D center point to 3D. Compared to 2D center points, different center points in 3D won't suffer from occlusion due to camera projection in some viewpoints. Since we can regard the center point as a special keypoint of an object, module M C is similar to the 3D keypoint detection module M K . It takes in the per-point feature but predicts the Euclidean translation offset ∆x i to the center of objects it belongs to. The learning of ∆x i is also supervised by an L1 loss:</p><formula xml:id="formula_2">L center = 1 N N i=1 ||∆x i − ∆x * i ||I(p i ∈ I)<label>(3)</label></formula><p>where N denotes the total number of seed points on the object surface and ∆x * i is the ground truth translation offset from seed p i to the instance center. I is an indication function indicating whether point p i belongs to that instance.</p><p>Multi-task loss. We supervise the learning of module M K , M S and M C jointly with a multi-tasks loss:</p><formula xml:id="formula_3">L multi-task =λ 1 L keypoints + λ 2 L semantic + λ 3 L center<label>(4)</label></formula><p>where λ 1 , λ 2 and λ 3 are the weights for each task. Experimental results shows that jointly training these tasks boosts the performance of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Implementation</head><p>Network Architecture. The first part in <ref type="figure">Figure 2</ref> is a feature extraction module. In this module, a PSPNet <ref type="bibr" target="#b56">[55]</ref> with an ImageNet <ref type="bibr" target="#b7">[8]</ref> pretrained ResNet34 <ref type="bibr" target="#b15">[16]</ref> is applied to extract the appearance information in RGB images. A PointNet++ <ref type="bibr" target="#b41">[40]</ref> extracts the geometry information in point clouds and their normal maps. They are further fused by a DenseFusion block <ref type="bibr" target="#b51">[50]</ref> to obtain the combined feature for each point. After the process of this module, each point p i has a feature f i ∈ R C of C dimension. The following module M K , M S and M C are composed of shared Multi-Layer Perceptrons (MLPs) shown in <ref type="figure">Figure 2</ref>. We sample N = 12288 points (pixels) for each frame of RGBD image and set λ 1 = λ 2 = λ 3 = 1.0 in Formula 4.</p><p>Keypoint Selection. The 3D keypoints are selected from 3D object models. In 3D object detection algorithms <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b54">53,</ref><ref type="bibr" target="#b39">38]</ref>, eight corners of the 3D bounding box are selected. However, These bounding box corners are virtual points that are far away from points on the object, making point-based networks difficult to aggregate scene context in the vicinity of them. The longer distance to the object points results in larger localization errors, which may do harm to the compute of 6D pose parameters. Instead, points selected from the object surface will be quite better. Therefore, we follow <ref type="bibr" target="#b38">[37]</ref> and use the farthest point sampling (FPS) algorithm to select keypoints on the mesh. Specifically, we initial the selection procedure by adding the center point of the object model in an empty keypoint set. Then update it by adding a new point on the mesh that is farthest to all the selected keypoints repeatedly, until M keypoints are obtained.</p><p>Least-Squares Fitting. Given two point sets of an object, one from the M detected keypoints {kp j } M j=1 in the camera coordinate system, and another from their corresponding points {kp j } M j=1 in the object coordinate system, the 6D pose estimation module computes the pose parameters (R, t) with a least-squares fitting algorithm <ref type="bibr" target="#b0">[1]</ref>, which finds R and t by minimizing the following square loss:</p><formula xml:id="formula_4">L least-squares = M j=1 ||kp j − (R · kp j + t)|| 2<label>(5)</label></formula><p>where M is the number of selected keypoints of a object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our method on two benchmark datasets. YCB-Video Dataset contains 21 YCB [4] objects of varying shape and texture. 92 RGBD videos of the subset of objects were captured and annotated with 6D pose and instance semantic mask. The varying lighting conditions, significant image noise, and occlusions make this dataset chal- lenging. We follow <ref type="bibr" target="#b53">[52]</ref> and split the dataset into 80 videos for training and another 2,949 keyframes chosen from the rest 12 videos for testing. Following <ref type="bibr" target="#b53">[52]</ref>, we add the synthetic images into our training set. A hole completion algorithm <ref type="bibr" target="#b25">[25]</ref> is also applied to improve the quality of depth images. LineMOD Dataset <ref type="bibr" target="#b18">[18]</ref> consists of 13 low-textured objects in 13 videos, annotated 6D pose and instance mask. The main challenge of this dataset is the cluttered scenes, texture-less objects, and lighting variations. We follow prior works <ref type="bibr" target="#b53">[52]</ref> to split the training and testing set. Also, we follow <ref type="bibr" target="#b38">[37]</ref> and add synthesis images into our training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We follow <ref type="bibr" target="#b53">[52]</ref> and evaluate our method with the average distance ADD and ADD-S metric <ref type="bibr" target="#b53">[52]</ref>. The average distance ADD metric <ref type="bibr" target="#b19">[19]</ref> evaluates the mean pair-wise distance between object vertexes transformed by the predicted </p><p>where x is a vertex of totally m vertexes on the object mesh O. The ADD-S metric is designed for symmetric objects and the mean distance is computed based on the closest point distance:</p><formula xml:id="formula_6">ADD-S = 1 m x1∈O min x2∈O ||(Rx 1 + t) − (R * x 2 + t * )||<label>(7)</label></formula><p>For evaluation, we follow <ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b51">50]</ref> and compute the ADD-S AUC, the area under the accuracy-threshold curve, which is obtained by varying the distance threshold in evaluation. The ADD(S) <ref type="bibr" target="#b19">[19]</ref> AUC is computed in a similar way but calculate ADD distance for non-symmetric objects and ADD-S distance for symmetric objects. <ref type="table">Table 1</ref> shows the evaluation results for all the 21 objects in the YCB-Video dataset. We compare our model with other single view methods. As shown in the Table, our model without any iterative refinement procedure (PVN3D) surpasses all other approaches by a large margin, even when they are iterative refined. On the ADD(S) metric, our model outperforms PoseCNN+ICP [52] by 6.4% RGB RGBD PoseCNN DeepIM <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b53">52]</ref> PVNet <ref type="bibr" target="#b38">[37]</ref> CDPN <ref type="bibr" target="#b27">[27]</ref> Implicit ICP <ref type="bibr" target="#b46">[45]</ref> SSD-6D ICP <ref type="bibr" target="#b22">[22]</ref> Point-Fusion <ref type="bibr" target="#b51">[50]</ref> DF(perpixel) <ref type="bibr" target="#b51">[50]</ref> DF(iterative) <ref type="bibr">[</ref>     <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b53">52]</ref> suffer from poor detection results. We also report evaluation results with ground truth segmentation in <ref type="table">Table 2</ref>, which shows that our PVN3D still achieves the best performance. Some Qualitative results are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. <ref type="table" target="#tab_2">Table 3</ref> demonstrates the evaluation results on LineMOD dataset. Our model also achieves the best performance. Robust to Occlusion Scenes. One of the biggest advantages of our 3D-keypoint-based method is that it's robust to occlusion naturally. To explored how different methods are influenced by different degrees of occlusion, we follow <ref type="bibr" target="#b51">[50]</ref> and calculate the percentage of invisible points on the object surface. Accuracy of ADD-S &lt; 2cm under different invisible surface percentage is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The performance of different approaches is very close when 50% of points are invisible. However, with the percentage of invisible part increase, DenseFusion and PoseCNN+ICP fall faster comparing with ours. <ref type="figure" target="#fig_3">Figure 3</ref> shows that our model performs well even when objects are heavily occluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on YCB-Video &amp; LineMOD Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this part, we explore the influence of different formulation for 6DoF pose estimation and the effect of keypoint selection methods. We also probe the effect of multi-task learning.</p><p>Comparisons to Directly Regressing Pose. To compare our 3D keypoint based formulation with formulations that directly regressing the 6D pose parameters [R, t] of an object, we simply modify our 3D keypoint voting module M K to directly regress the quaternion rotation R and the translation parameters t for each point. We also add a confidence header following DenseFusion <ref type="bibr" target="#b51">[50]</ref> and select the pose with the highest confidence as the final proposed pose. We supervise the training process using ShapeMatch-Loss <ref type="bibr" target="#b53">[52]</ref> with confidence regularization term <ref type="bibr" target="#b51">[50]</ref> following DenseFusion.</p><p>Experimental results in <ref type="table" target="#tab_3">Table 4</ref> shows that our 3D keypoint formulation performs quite better.</p><p>To eliminate the influence of different network architecture, we also modify the header of DenseFusion(per-pixel) to predict the per-point translation offset and compute the 6D pose following our keypoint voting and least-squares fitting procedure. <ref type="table" target="#tab_3">Table 4</ref> reveals that the 3D keypoint formulation, DF(3D KP) in the Table, performs better than the RT regression formulation, DF(RT). That's because the 3D keypoint offset search space is smaller than the non-linearity of rotation space, which is easier for neural networks to learn, enabling them to be more generalizable.</p><p>Comparisons to 2D Keypoints. In order to contrast the influence of 2D and 3D keypoints, we project the voted 3D keypoints back to 2D with the camera intrinsic parameters. A PnP algorithm with Random Sample Consensus (RANSAC) is then applied to compute the 6D pose parameters. <ref type="table" target="#tab_3">Table 4</ref> shows that algorithms with 3D keypoint formulation, denoted as Ours(3D KP) in the table, outperforms 2D keypoint, denoted Ours(2D KP) in the table, by 13.7% under ADD-S metric. That's because PnP algorithms aim to minimize the projection error. However, pose estimation  errors that are small in projection may be quite large in the 3D real world.</p><p>To compare the influence between 2D and 3D center point in our instance semantic segmentation module, we also project our voted 3D center point to 2D in the Instance Semantic Segmentation module(Ours(2D KPC)). We apply a similar Mean Shift algorithm to cluster the voted 2D center points to distinguish different instance, finding that in occlusion scenes, different instances are hard to be differentiated when their centers are close to each other after projected on 2D, while they are far away from each other and can be easily differentiated in 3D real world. Note that other existing 2D keypoints detection approaches, such as heatmap <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b35">34]</ref> and vector voting <ref type="bibr" target="#b38">[37]</ref> models may also suffer from overlapped keypoints. By definition, centers of most objects in our daily life won't be overlapped as they usually lie within the object while they may be overlapped after projected to 2D. In a word, the object world is in 3D, we believe that building models on 3D is quite important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons to Dense Correspondence Exploring.</head><p>We modify our 3D keypoint offset module M K to output the corresponding 3D coordinate of each point in the object coordinate system and apply the least-squares fitting algorithm to computes the 6DoF pose. An L1 loss similar to Formula 3 is applied to supervise the training of the corresponding 3D coordinate. Evaluation results are shown as Ours(corr) in Tabel 4, which shows that our 3D keypoints formulation still performs quite better. We believe that regressing object coordinates is more difficult than keypoint detection. Because the model has to recognize each point of a mesh in the image and memorize its coordinate in the object coordinate system. However, detecting keypoints on objects in the camera system is easier since many keypoints are visible and the model can aggregate scene context in the vicinity of them.</p><p>Effect of 3D Keypoints Selection. In this part, we select 8 corners of the 3D bounding box and compares them with points selected from the FPS algorithm. Different number of keypoints generated by FPS are also taken into consideration. <ref type="table" target="#tab_4">Table 5</ref> shows that keypoints selected by the FPS algorithm on the object enable our model to perform better. That's because the bounding box corners are virtual points that are far away from points on the object. Therefore, point-based networks are difficult to aggregate scene context in the vicinity of these virtual corner points. Also, 8 keypoints selected from FPS algorithm is a good choice for our network to learn. More keypoints may better eliminate errors when recovering pose in the least-squares fitting module, but harder for the network to learn as the output  <ref type="bibr" target="#b53">[52]</ref> and Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> are confused by the two objects. In (d), our semantic segmentation module MS , trained separately, can not distinguish them well either. In (e), jointly training MS with the keypoints offset voting module MK performs better. In (f), with the voted center and Mean-Shift clustering algorithm, our model can distinguish them well.  space is bigger. Selecting 8 keypoints is a good trade-off. Effect of Multi-task learning. In this part, we discuss how the joint learning of semantic segmentation and keypoint (center) translation offset boosts the performance. In <ref type="table" target="#tab_6">Table 6</ref>, we explore how semantic segmentation enhances keypoint offset learning. We remove semantic segmentation and center voting modules M S , M C , and train our keypoint voting module M K individually. During inference time, the instance semantic segmentation predicted by Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> (M K +MRC) and the ground truth (M K +GT) are applied. Experimental results show that jointly trained with semantic segmentation (M K,S +GT) boosts the performance of keypoint offset voting and improves the accuracy of 6D pose estimation by 0.7% on ADD(S) metric. We believe that the semantic module extracts global and local features to distinguish different objects. Such features also help the model to recognize which part of an object a point belongs to and improve offset prediction.</p><formula xml:id="formula_7">M K +MRC M K +GT M K,S +GT M K,S,C M K,S,</formula><p>In <ref type="table">Table 7</ref>, we explore how keypoint and center point offset learning improve the instance semantic segmentation result. Point mean intersection over union (mIoU) is used as evaluation metric. We report the results of the challenging large clamp and extra-large clamp in YCB-Video dataset. They look same in appearance but are different in size, as shown in <ref type="figure" target="#fig_5">Figure 5</ref>. We trained Mask R-CNN (ResNeXt-50-FPN) <ref type="bibr" target="#b14">[15]</ref> with the recommended setting as a simple baseline and found it was completely confused by the two objects. With extra depth information, our semantic segmentation module (PVN3D(M S )), trained individually, didn't perform well either. However, jointly trained with our keypoint offset voting module (PVN3D(M S,K )), the mIoU was improved by 9.2% on the extra-large clamp. With voted centers obtained from the center voting module M C , we can split up objects with the Mean-Shift clustering algorithm and assign points to its closest object cluster. The mIoU of the extra-large clamp is further improved by 18.3% in this way. Some qualitative results are shown in <ref type="figure" target="#fig_5">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel deep 3D keypoints voting network with instance semantic segmentation for 6DoF pose estimation, which outperforms all previous approaches in several datasets by large margins. We also show that jointly training 3D keypoint with semantic segmentation can boost the performance of each other. We believe the 3D keypoint based approach is a promising direction to explore for the 6DoF pose estimation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>A.1. Architecture Details.</p><p>The image embedding network in the Feature Extraction module consists of a standard ResNet-34 encoder pretrained with ImageNet and followed by 4 up-sampling layers as a decoder. The output RGB feature is of 128 channels. The point cloud embedding network is a PointNet++ <ref type="bibr" target="#b41">[40]</ref> with Multi-scale Grouping (MSG), the output of which is also of 128 channels. In the DenseFuion block, for each sampled point, the 128 channels RGB feature is concatenated with the corresponding 128 channels point cloud feature and is fed into shared MLPs to raise the feature channels to 1024. The 128 channels RGB and point cloud feature are also fed into shared MLPs and raised to 256 channels respectively. They are then all concatenated together to form the 1792 (128*2+256*2+1024) channels RGBD features. The 3D keypoint offset voting module M K consists of shared MLPs (1792-1024-512-128-N kp *3) reducing the 1792 channels RGBD features to N kp 3D keypoints offset. The semantic segmentation module M S consists of shared MLPs (1792-1024-512-128-N cls ) with N cls the number of object classes. The center voting module M C also consists of shared MLPs (1792-1024-512-128-3). During training, Adam optimization algorithm is employed, with a minibatch size of 24. We applied cyclical learning rates during training, the range of which is from 1e-5 to 1e-3.</p><p>A.2. Implementation: Models for LineMOD dataset.</p><p>In the LineMOD dataset, though there are multi objects in one scene, only the label of one target object in each scene is provided, making it hard to train a single model for all objects. Therefore, we follow PVNet <ref type="bibr" target="#b38">[37]</ref> and trained models separately for each object. It means our semantic segmentation module M S only predicts two semantic labels for each object, label of the target object and the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Implementation: PVN3D+ICP</head><p>We introduce our implementation of PVN3D+ICP as follows. During training and inference of PVN3D, we randomly sample 12288 points (pixels) from the whole scene as input. Only these points are labeled with the semantic label by our instance semantic segmentation module, which is not enough for a good performance of the ICP algorithm. Therefore, for each unlabeled point in the whole point cloud scene, we find its closest labeled point and assigned that label to it. Points with the same instance semantic labels are then selected. To eliminate the effect of noise points, a MeanShift <ref type="bibr" target="#b6">[7]</ref> clustering algorithm is also applied. The biggest cluster of points is then selected as the input of the ICP algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. More Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Instance semantic segmentation</head><p>We provide more results of instance semantic segmentation in <ref type="table" target="#tab_8">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Visualization of detected keypoints</head><p>The visualization of some detected keypoints is shown in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 Evaluation on the Occlusion LineMOD Dataset</head><p>The Occlusion LineMOD dataset <ref type="bibr" target="#b2">[3]</ref> is additionally annotated from the LineMOD datasets, objects of which are heavily occluded, making it harder for pose estimation. Evaluation results in <ref type="table">Table 9</ref> show that our PVN3D outperforms previous methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.4 Inference time</head><p>It takes 0.17 seconds for network forward propagation and 0.02 seconds for pose estimation of each object during inference. The overall runtime is 5 FPS on the LineMOD dataset.   <ref type="table">Table 9</ref>. Quantitative evaluation of 6D Pose on ADD(S) <ref type="bibr" target="#b19">[19]</ref> metric on the Occlusion LineMOD dataset. Objects with bold name are symmetric.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work is supported by The National Key Research and Development Program of China (2018YFC0831700).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Pipeline of PVN3D: With an input RGBD image (a), we use a deep Hough voting network to predict the per-point translation offset to the selected keypoint (b). Each point on the same object votes for the selected keypoint and the center of the cluster is selected as a predicted keypoint (c). A least-squares fitting method is then applied to estimate 6D pose parameters (d)-(e). The model transformed by estimated pose parameters is shown in Figure (f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6D pose [R, t] and the ground true pose [R * , t * ]: ADD = 1 m x∈O ||(Rx + t) − (R * x + t * )||</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results on the YCB-Video dataset. Points on different meshes in the same scene are in different colors. They are projected back to the image after being transformed by the predicted pose. We compare our PVN3D without any iterative refinement procedure to DenseFusion with iterative refinement (2 iterations). Our model distinguishes the challenging large clamp and extra-large clamp and estimates their poses well. Our model is also robust in heavily occluded scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Performance of different approaches under increasing levels of occlusion on the YCB-Video dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of semantic segmentation on the challenging YCB-Video dataset. (a) shows the ground truth label. Different objects are labeled in different colors, with large clamp colored green and extra-large clamp colored orange. In (b)-(c), the simple baselines PoseCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Table 2</head><label>12</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Without Iterative Refinement</cell><cell></cell><cell></cell><cell></cell><cell cols="3">With Iterative Refinement</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">PoseCNN[52]</cell><cell cols="2">DF(per-pixel)[50]</cell><cell cols="2">PVN3D</cell><cell cols="2">PoseCNN+ICP[52]</cell><cell cols="2">DF(iterative)[50]</cell><cell cols="2">PVN3D+ICP</cell></row><row><cell></cell><cell></cell><cell>ADDS</cell><cell cols="2">ADD(S) ADDS</cell><cell cols="2">ADD(S) ADDS</cell><cell cols="2">ADD(S) ADDS</cell><cell>ADD(S)</cell><cell>ADDS</cell><cell cols="2">ADD(S) ADDS</cell><cell>ADD(S)</cell></row><row><cell cols="2">002 master chef can</cell><cell>83.9</cell><cell>50.2</cell><cell>95.3</cell><cell>70.7</cell><cell>96.0</cell><cell>80.5</cell><cell>95.8</cell><cell>68.1</cell><cell>96.4</cell><cell>73.2</cell><cell>95.2</cell><cell>79.3</cell></row><row><cell>003 cracker box</cell><cell></cell><cell>76.9</cell><cell>53.1</cell><cell>92.5</cell><cell>86.9</cell><cell>96.1</cell><cell>94.8</cell><cell>92.7</cell><cell>83.4</cell><cell>95.8</cell><cell>94.1</cell><cell>94.4</cell><cell>91.5</cell></row><row><cell>004 sugar box</cell><cell></cell><cell>84.2</cell><cell>68.4</cell><cell>95.1</cell><cell>90.8</cell><cell>97.4</cell><cell>96.3</cell><cell>98.2</cell><cell>97.1</cell><cell>97.6</cell><cell>96.5</cell><cell>97.9</cell><cell>96.9</cell></row><row><cell cols="2">005 tomato soup can</cell><cell>81.0</cell><cell>66.2</cell><cell>93.8</cell><cell>84.7</cell><cell>96.2</cell><cell>88.5</cell><cell>94.5</cell><cell>81.8</cell><cell>94.5</cell><cell>85.5</cell><cell>95.9</cell><cell>89.0</cell></row><row><cell cols="2">006 mustard bottle</cell><cell>90.4</cell><cell>81.0</cell><cell>95.8</cell><cell>90.9</cell><cell>97.5</cell><cell>96.2</cell><cell>98.6</cell><cell>98.0</cell><cell>97.3</cell><cell>94.7</cell><cell>98.3</cell><cell>97.9</cell></row><row><cell cols="2">007 tuna fish can</cell><cell>88.0</cell><cell>70.7</cell><cell>95.7</cell><cell>79.6</cell><cell>96.0</cell><cell>89.3</cell><cell>97.1</cell><cell>83.9</cell><cell>97.1</cell><cell>81.9</cell><cell>96.7</cell><cell>90.7</cell></row><row><cell>008 pudding box</cell><cell></cell><cell>79.1</cell><cell>62.7</cell><cell>94.3</cell><cell>89.3</cell><cell>97.1</cell><cell>95.7</cell><cell>97.9</cell><cell>96.6</cell><cell>96.0</cell><cell>93.3</cell><cell>98.2</cell><cell>97.1</cell></row><row><cell>009 gelatin box</cell><cell></cell><cell>87.2</cell><cell>75.2</cell><cell>97.2</cell><cell>95.8</cell><cell>97.7</cell><cell>96.1</cell><cell>98.8</cell><cell>98.1</cell><cell>98.0</cell><cell>96.7</cell><cell>98.8</cell><cell>98.3</cell></row><row><cell cols="2">010 potted meat can</cell><cell>78.5</cell><cell>59.5</cell><cell>89.3</cell><cell>79.6</cell><cell>93.3</cell><cell>88.6</cell><cell>92.7</cell><cell>83.5</cell><cell>90.7</cell><cell>83.6</cell><cell>93.8</cell><cell>87.9</cell></row><row><cell>011 banana</cell><cell></cell><cell>86.0</cell><cell>72.3</cell><cell>90.0</cell><cell>76.7</cell><cell>96.6</cell><cell>93.7</cell><cell>97.1</cell><cell>91.9</cell><cell>96.2</cell><cell>83.3</cell><cell>98.2</cell><cell>96.0</cell></row><row><cell>019 pitcher base</cell><cell></cell><cell>77.0</cell><cell>53.3</cell><cell>93.6</cell><cell>87.1</cell><cell>97.4</cell><cell>96.5</cell><cell>97.8</cell><cell>96.9</cell><cell>97.5</cell><cell>96.9</cell><cell>97.6</cell><cell>96.9</cell></row><row><cell cols="2">021 bleach cleanser</cell><cell>71.6</cell><cell>50.3</cell><cell>94.4</cell><cell>87.5</cell><cell>96.0</cell><cell>93.2</cell><cell>96.9</cell><cell>92.5</cell><cell>95.9</cell><cell>89.9</cell><cell>97.2</cell><cell>95.9</cell></row><row><cell>024 bowl</cell><cell></cell><cell>69.6</cell><cell>69.6</cell><cell>86.0</cell><cell>86.0</cell><cell>90.2</cell><cell>90.2</cell><cell>81.0</cell><cell>81.0</cell><cell>89.5</cell><cell>89.5</cell><cell>92.8</cell><cell>92.8</cell></row><row><cell>025 mug</cell><cell></cell><cell>78.2</cell><cell>58.5</cell><cell>95.3</cell><cell>83.8</cell><cell>97.6</cell><cell>95.4</cell><cell>94.9</cell><cell>81.1</cell><cell>96.7</cell><cell>88.9</cell><cell>97.7</cell><cell>96.0</cell></row><row><cell>035 power drill</cell><cell></cell><cell>72.7</cell><cell>55.3</cell><cell>92.1</cell><cell>83.7</cell><cell>96.7</cell><cell>95.1</cell><cell>98.2</cell><cell>97.7</cell><cell>96.0</cell><cell>92.7</cell><cell>97.1</cell><cell>95.7</cell></row><row><cell>036 wood block</cell><cell></cell><cell>64.3</cell><cell>64.3</cell><cell>89.5</cell><cell>89.5</cell><cell>90.4</cell><cell>90.4</cell><cell>87.6</cell><cell>87.6</cell><cell>92.8</cell><cell>92.8</cell><cell>91.1</cell><cell>91.1</cell></row><row><cell>037 scissors</cell><cell></cell><cell>56.9</cell><cell>35.8</cell><cell>90.1</cell><cell>77.4</cell><cell>96.7</cell><cell>92.7</cell><cell>91.7</cell><cell>78.4</cell><cell>92.0</cell><cell>77.9</cell><cell>95.0</cell><cell>87.2</cell></row><row><cell>040 large marker</cell><cell></cell><cell>71.7</cell><cell>58.3</cell><cell>95.1</cell><cell>89.1</cell><cell>96.7</cell><cell>91.8</cell><cell>97.2</cell><cell>85.3</cell><cell>97.6</cell><cell>93.0</cell><cell>98.1</cell><cell>91.6</cell></row><row><cell>051 large clamp</cell><cell></cell><cell>50.2</cell><cell>50.2</cell><cell>71.5</cell><cell>71.5</cell><cell>93.6</cell><cell>93.6</cell><cell>75.2</cell><cell>75.2</cell><cell>72.5</cell><cell>72.5</cell><cell>95.6</cell><cell>95.6</cell></row><row><cell cols="2">052 extra large clamp</cell><cell>44.1</cell><cell>44.1</cell><cell>70.2</cell><cell>70.2</cell><cell>88.4</cell><cell>88.4</cell><cell>64.4</cell><cell>64.4</cell><cell>69.9</cell><cell>69.9</cell><cell>90.5</cell><cell>90.5</cell></row><row><cell>061 foam brick</cell><cell></cell><cell>88.0</cell><cell>88.0</cell><cell>92.2</cell><cell>92.2</cell><cell>96.8</cell><cell>96.8</cell><cell>97.2</cell><cell>97.2</cell><cell>92.0</cell><cell>92.0</cell><cell>98.2</cell><cell>98.2</cell></row><row><cell>ALL</cell><cell></cell><cell>75.8</cell><cell>59.9</cell><cell>91.2</cell><cell>82.9</cell><cell>95.5</cell><cell>91.8</cell><cell>93.0</cell><cell>85.4</cell><cell>93.2</cell><cell>86.1</cell><cell>96.1</cell><cell>92.3</cell></row><row><cell></cell><cell></cell><cell>DF(p.p.)</cell><cell>PVN3D</cell><cell>DF(iter.)</cell><cell cols="2">PVN3D+ICP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>large clamp</cell><cell>ADD-S</cell><cell>87.7</cell><cell>93.9</cell><cell>90.3</cell><cell>96.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>extra large clamp</cell><cell>ADD-S</cell><cell>75.0</cell><cell>90.1</cell><cell>74.9</cell><cell>93.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ALL</cell><cell>ADD-S ADD(S)</cell><cell>93.3 84.9</cell><cell>95.7 91.9</cell><cell>94.8 89.4</cell><cell>96.4 92.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>. Quantitative evaluation of 6D Pose (ADD-S AUC [52], ADD(S) AUC [19]) on the YCB-Video Dataset. Symmetric objects' names are in bold.w/o iter. ref.w/ iter. ref.. Quantitative evaluation results on the YCB-Video dataset with ground truth instance semantic segmentation result.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative evaluation of 6D Pose on ADD(S)<ref type="bibr" target="#b19">[19]</ref> metric on the LineMOD dataset. Objects with bold name are symmetric.</figDesc><table><row><cell></cell><cell>DF(RT)[50]</cell><cell>DF(3D KP)[50]</cell><cell>Ours(RT)</cell><cell>Ours(2D KPC)</cell><cell>Ours(2D KP)</cell><cell>PVNet[37]</cell><cell>Ours(Corr)</cell><cell>Ours(3D KP)</cell></row><row><cell>ADD-S</cell><cell>92.2</cell><cell>93.1</cell><cell>92.8</cell><cell>78.2</cell><cell>81.8</cell><cell>-</cell><cell>92.8</cell><cell>95.5</cell></row><row><cell>ADD(S)</cell><cell>86.9</cell><cell>87.9</cell><cell>87.3</cell><cell>73.8</cell><cell>77.2</cell><cell>73.4</cell><cell>88.1</cell><cell>91.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative evaluation of 6D Poses on the YCB-Video dataset with different formulations. All with our predicted segmentation.</figDesc><table><row><cell></cell><cell>VoteNet[38]</cell><cell>BBox 8</cell><cell>FPS 4</cell><cell>FPS 8</cell><cell>FPS 12</cell></row><row><cell>ADD-S</cell><cell>89.9</cell><cell>94.0</cell><cell>94.3</cell><cell>95.5</cell><cell>94.5</cell></row><row><cell>ADD(S)</cell><cell>85.1</cell><cell>90.2</cell><cell>90.5</cell><cell>91.8</cell><cell>90.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Effect of different keypoint selection methods of PVN3D. Results of VoteNet<ref type="bibr" target="#b39">[38]</ref>, another 3D bounding box detection approach are added as a simple baseline to compare with our BBox8.</figDesc><table><row><cell>and exceeds DF(iterative) [50] by 5.7%. With iterative re-</cell></row><row><cell>finement, our model (PVN3D+ICP) achieves even better</cell></row><row><cell>performance. Note that one challenge of this dataset is to</cell></row><row><cell>distinguish the large clamp and extra-large clamp, on which</cell></row><row><cell>previous methods</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Performance of PVN3D with different instance semantic segmentation on all objects in the YCB-Video dataset. MK, MS and MC denote keypoint offset module, semantic segmentation and center point offset module of PVN3D respectively. +MRC and +GT denotes inference with segmentation result of Mask R-CNN and ground truth segmentation respectively.</figDesc><table><row><cell></cell><cell>PoseCNN</cell><cell>Mask R-</cell><cell>PVN3D</cell><cell>PVN3D</cell><cell>PVN3D</cell></row><row><cell></cell><cell>[52]</cell><cell>CNN[15]</cell><cell>(M S )</cell><cell>(M S,K )</cell><cell>(M S,K,C )</cell></row><row><cell>large clamp</cell><cell>43.1</cell><cell>48.4</cell><cell>58.6</cell><cell>62.5</cell><cell>70.2</cell></row><row><cell>extra-large</cell><cell>30.4</cell><cell>36.1</cell><cell>41.5</cell><cell>50.7</cell><cell>69.0</cell></row><row><cell>clamp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Table 7. Instance semantic segmentation results (mIoU(%)) of dif-</cell></row><row><cell cols="6">ferent methods on the YCB-Video dataset. Jointly training seman-</cell></row><row><cell cols="6">tic segmentation module with keypoint offset module (MS,K) ob-</cell></row><row><cell cols="6">tains size information from the offset module and performs better,</cell></row><row><cell cols="6">especially on large clamp and extra-large clamp. With the cen-</cell></row><row><cell cols="6">ter voting module MC and the Mean-Shift clustering algorithm,</cell></row><row><cell cols="5">further improvement of performance is obtained.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Instance semantic segmentation results (mIoU(%)) of different approaches.</figDesc><table><row><cell></cell><cell>PoseCNN</cell><cell>Oberweger</cell><cell>Hu et al.</cell><cell>Pix2Pose</cell><cell>PVNet [37]</cell><cell>DPOD [54]</cell><cell>PVN3D</cell></row><row><cell></cell><cell>[52]</cell><cell>[34]</cell><cell>[20]</cell><cell>[35]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ape</cell><cell>9.6</cell><cell>12.1</cell><cell>17.6</cell><cell>22.0</cell><cell>15.8</cell><cell>-</cell><cell>33.9</cell></row><row><cell>can</cell><cell>45.2</cell><cell>39.9</cell><cell>53.9</cell><cell>44.7</cell><cell>63.3</cell><cell>-</cell><cell>88.6</cell></row><row><cell>cat</cell><cell>0.9</cell><cell>8.2</cell><cell>3.3</cell><cell>22.7</cell><cell>16.7</cell><cell>-</cell><cell>39.1</cell></row><row><cell>driller</cell><cell>41.4</cell><cell>45.2</cell><cell>62.4</cell><cell>44.7</cell><cell>65.7</cell><cell>-</cell><cell>78.4</cell></row><row><cell>duck</cell><cell>19.6</cell><cell>17.2</cell><cell>19.2</cell><cell>15.0</cell><cell>25.2</cell><cell>-</cell><cell>41.9</cell></row><row><cell>eggbox</cell><cell>22.0</cell><cell>22.1</cell><cell>25.9</cell><cell>25.2</cell><cell>50.2</cell><cell>-</cell><cell>80.9</cell></row><row><cell>glue</cell><cell>38.5</cell><cell>35.8</cell><cell>39.6</cell><cell>32.4</cell><cell>49.6</cell><cell>-</cell><cell>68.1</cell></row><row><cell>holepuncher</cell><cell>22.1</cell><cell>36.0</cell><cell>21.3</cell><cell>49.5</cell><cell>39.7</cell><cell>-</cell><cell>74.7</cell></row><row><cell>average</cell><cell>24.9</cell><cell>27.0</cell><cell>27.0</cell><cell>32.0</cell><cell>40.8</cell><cell>47.3</cell><cell>63.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Least-squares fitting of two 3-d point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Blostein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="698" to="700" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on advanced robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The moped framework: Object recognition and pose estimation for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recovering 6d object pose and predicting next-bestview in the crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3583" to="3592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08189</idno>
		<title level="m">Centernet: Keypoint triplets for object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">aware object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1275" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative mixture-of-templates for viewpoint classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="408" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cullnet: Calibrated and pose aware confidence scores for object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE con</title>
		<meeting>the IEEE con</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Visualization of 3D keypoints in YCB-Video dataset. Blue points are sampled point clouds from the scene. Red points are the ground truth 3D keypoints and green points are predicted 3D keypoints. ference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Figure 6</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient response maps for realtime detection of textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="858" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmentationdriven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparing images using the hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Klanderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Rucklidge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="850" to="863" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 15th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">independent object class detection using 3d feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schertler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
		<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pose estimation for augmented reality: a hands-on survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Spindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2633" to="2651" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global hypothesis generation for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rothganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="259" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2686" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Depthencoded hough voting for joint object detection and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="658" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discovery of latent 3d keypoints via end-toend geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2059" to="2070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10790</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for categorylevel 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00199</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dpod: 6d pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Single image 3d object detection and pose estimation for grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lecce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3936" to="3943" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
