<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Specifying Object Attributes and Relations in Interactive Scene Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Ashual</surname></persName>
							<email>oron.ashual@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="laboratory">Lior Wolf Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Specifying Object Attributes and Relations in Interactive Scene Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a method for the generation of images from an input scene graph. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the scene graph, have higher visual quality, and support more complex scene graphs. In addition, the embedding scheme supports multiple and diverse output images per scene graph, which can be further controlled by the user. We demonstrate two modes of per-object control: (i) importing elements from other images, and (ii) navigation in the object space, by selecting an appearance archetype.</p><p>Our code is publicly available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>David Marr has defined vision as the process of discovering from images what is present in the world, and where it is <ref type="bibr" target="#b14">[15]</ref>. The combination of what and where captures the essence of an image at the semantic level and therefore, also plays a crucial role when defining the desired output of image synthesis tools.</p><p>In this work, we employ scene graphs with per-object location and appearance attributes as an accessible and easyto-manipulate way for users to express their intentions, see <ref type="figure">Fig. 1</ref>. The what aspect is captured hierarchically: objects are defined as belonging to a certain class (horse, tree, boat, etc.) and as having certain appearance attributes. These attributes can be (i) selected from a predefined set obtained by clustering previously seen attributes, or (ii) copied from a sample image. The where aspect, is captured by what is often called a scene graph, i.e., a graph where the scene objects are denoted as nodes, and their relative position, such as "above" or "left of", are represented as edge types.</p><p>Our method employs a dual encoding for each object in the image. The first part encodes the object's placement and captures a relative position and other global image features, as they relate to the specific object. It is generated based on the scene graph, by employing a graph convolution net-work, followed by the concatenation of a random vector z. The second part encodes the appearance of the object and can be replaced, e.g., by importing it from the same object as it appears in another image, without directly changing the other objects in the image. This copying of objects between images is done in a semantic way, and not at the pixel level.</p><p>In the scene graph that we employ, each node is equipped with three types of information: (i) the type of object, encoded as a vector of a fixed dimension, (ii) the location attributes of the objects, which denote the approximate location in the generated image, using a coarse 5 × 5 grid and its size, discretized to ten values, and (iii) the appearance embedding mentioned above. The edges denote relations: "right of", "left of", "above", "below", "surrounding", and "inside". The method is implemented within a convenient user interface, which supports a dynamic placement of objects and the creation of a scene graph. The edge relations are inferred automatically, given the relative position of the objects. This eliminates the need for mostly unnecessary user intervention. Rendering is done in real time, supporting the creation of novel scenes in an interactive way, see <ref type="figure">Fig. 1</ref>.</p><p>The neural network that we employ has multiple subparts, as can be seen in <ref type="figure">Fig. 2</ref>: (i) A graph convolutional network that converts the input scene graph to a per-object embedding to their location. (ii) A CNN that converts the location embedding of each object to an object's mask. (iii) A parallel network that converts the location embedding to a bounding box location, where the object mask is placed. (iv) An appearance embedding CNN that converts image information into an embedding vector. This process is done off-line and when creating a new image, the vectors can be imported from other images, or selected from a set of archetypes. (v) A multiplexer that combines the object masks and the appearance embedding information, to create a one multidimensional tensor, where different groups of layers denote different objects. (vi) An encoder-decoder residual network that creates the output image.</p><p>Our method is related to the recent work of <ref type="bibr" target="#b8">[9]</ref>, who create images based on scene graphs. Their method also uses a graph convolutional network to obtain masks, a mul- <ref type="figure">Figure 1</ref>. An example of the image creation process. (top row) the schematic illustration panel of the user interface, in which the user arranges the desired objects. (2nd row) the scene graph that is inferred automatically based on this layout. (3rd row) the layout that is created from the scene graph. (bottom row) the generated image. Legend for the GUI colors in the top row: purple -adding an object, green -resizing it, red -replacing its appearance. (a) A simple layout with a sky object, a tree and a grass object. All object appearances are initialized to a random archetype appearance. tiplexer that combines the layout information and a subsequent encoder-decoder architecture for obtaining the final image. There are, however, important differences: (i) by separating the layout embedding from the appearance embedding, we allow for much more control and freedom to the object selection mechanism, (ii) by adding the location attributes as input, we allow for an intuitive and more direct user control, (iii) the architecture we employ enables better quality and higher resolution outputs, (iv) by adding stochasticity before the masks are created, we are able to generate multiple results per scene graph, (v) this effect is amplified by the ability of the users to manipulate the resulting image, by changing the properties of each individual object, (vi) we introduce a mask discriminator, which plays a crucial role in generating plausible masks, (vii) another novel discriminator captures the appearance encoding in a counterfactual way, and (viii) we introduce feature matching based on the discriminator network and (ix) a perceptual loss term to better capture the appearance of an object, even if the pose or shape of that object has changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>Image generation techniques based on GANs <ref type="bibr" target="#b2">[3]</ref> are constantly improving in resolution, visual quality, the diversity of generated images, and the ability to cover the entire visual domain presented during training. In this work, we address conditional image generation, i.e., the creation of images that match a specific input. Earlier work in conditional image generation includes class based image generation <ref type="bibr" target="#b15">[16]</ref>, which generates an image that matches a given textual description <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>. In many cases, the conditioning signal is a source image, in which case the problem is often referred to as image translation. Pix2pix <ref type="bibr" target="#b6">[7]</ref> is a fully supervised method that requires pairs of matching samples from the two domains. The Pix2pixHD architecture that was recently presented by <ref type="bibr" target="#b24">[25]</ref> is highly influential and many recent video or image mapping works employ elements of it, including our work.</p><p>Image generation based on scene graphs was recently presented in <ref type="bibr" target="#b8">[9]</ref>. A scene graph representation is often used for retrieval based on text <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>, and a few datasets include this information, e.g., COCO-stuff <ref type="bibr" target="#b1">[2]</ref> and the visual <ref type="figure">Figure 2</ref>. The architecture of our composite network, including the subnetworks G, M, B, A, R, and the process of creating the layout tensor t. The scene graph is passed to the network G to create the layout embedding ui of each object. The bounding box bi is created from this embedding, using network B. A random vector zi is concatenated to ui, and the network M computes the mask mi. The appearance information, as encoded by the network A, is then added to create the tensor t with c + d5 channels, c being the number of classes. The autoencoder R generates the final image p from this tensor. genome <ref type="bibr" target="#b11">[12]</ref>. Also related is the synthesis of images from a given input layout of bounding boxes (and not one that is inferred by a network from a scene graph), which was very recently studied by [28] for small 64x64 images. In another line of work, images are generated to match input sentences, without constructing the scene graph as an intermediate representation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Recently, an interactive tool was introduced based on the novel notion of GAN dissection <ref type="bibr" target="#b0">[1]</ref>. This tool allows for the manipulation of well-localized neurons that control the occurrence of specific objects across the image using a drawing interface. By either adding or reducing the activations of these neurons, objects can be added, expanded, reduced or removed. The manipulation that we offer here is both more semantic (less related to specific locations and more to spatial relations between the objects), and more precise, in the sense that we provide full control over the exact instance of the object and not just of the desired class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Each object i in the input scene graph is associated with a single node n i = [o i , l i ], where o i ∈ R d1 is a learned encoding of the object class and l i ∈ {0, 1} d2+d3 is a location vector. The object class embedding o i is one of c possible embedding vectors, c being the number of classes, and o i is set according to the class of object i, denoted c i . The embedding size d 1 is set arbitrarily to 128. The first d 2 = 25 bits of l i denote a coarse image location using a 5 × 5 grid, and the rest denotes the size of the object, using a scale of d 3 = 10 values. The edge information e ij ∈ R d1 exists for a subset of the possible pairs of nodes, and encodes, using a learned embedding, the relations between the nodes. In other words, the values of e ij are taken from a learned dictionary with six possible values, each associated with one type of pairwise relation.</p><p>The location of each generated object is given as a pseudo-binary mask m i (output of a sigmoid) and a bound-</p><formula xml:id="formula_0">ing box b i = [x 1 , y 1 , x 2 , y 2 ] ∈ [0, 1] 4</formula><p>, which encodes the coordinates of the bounding box as a ratio of the image dimensions. The mask, but not the bounding box, is also determined by a per-object random vector z i ∼ N(0, 1) d4 to create a variation in the generated masks, where d 4 = 64 was set arbitrarily, without testing other values.</p><p>The method employs multiple ways of embedding input information. The class identity and every inter-object relation, both taking a discrete value, are captured by embedding vectors of dimension d 1 , which are learned as part of the end-to-end training. The object appearance a i ∈ R d5 of object i seen during training, is obtained by applying a CNN A to a (ground truth) cropped image I i of that object, resized to a fixed resolution of 64 × 64. d 5 was set arbitrarily to 32 to reflect that it has less information than that of the entire object, which is embedded in R d1 .</p><p>The way in which the data flows through the subnetworks, as depicted in <ref type="figure">Fig. 2</ref>, is captured by the equations:</p><formula xml:id="formula_1">u i = G({n i }, {e ij }) (1) m i = M (u i , z i ) (2) b i = B(u i )<label>(3)</label></formula><formula xml:id="formula_2">a i = A(I i ) (4) t = T ({c i , m i , b i , a i }) (5) p = R(t) (6)</formula><p>where G is the graph convolutional network <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref> that generates the per-object layout embedding, M and B are the networks that generate the object's mask and its bounding box, respectively, T is the fixed (unlearned) function that maps the various object embeddings to a tensor t. Finally, R is the encoder-decoder network that outputs an image p ∈ R H×W ×3 based on t. The exact architecture of each network is provided in the appendix.</p><p>The function T constructs the tensor t as a sum of per-</p><formula xml:id="formula_3">object tensors t i ∈ R H×W ×(d5+c)</formula><p>, where c is the number of objects. First, the mask m i is shifted and scaled, according to the bounding box b i , resulting in a mask m HW i of size H ×W . Then, a first tensor t 1 i of size H ×W ×d 5 is formed as the tensor product of m HW i and a i . Similarly, a second tensor t 2 i ∈ R H×W ×c is formed as the tensor product of m HW i and the one hot vector of length c encoding class c i . The tensor t i is a concatenation of the two tensors t 1 i and t 2 i along the third dimension.</p><p>For performing adversarial training of the appearance embedding network A, we create two other tensors: t and t . The first one is obtained by employing the ground truth bounding box b i of object i and the ground truth segmentation mask m i . The second one is obtained by incorporating the same ground truth bounding box and mask in a counterfactual way, by replacing a i with a k , where a k is an appearance embedding of an object image I k of a different object from the same class c i , i.e., a k = A(I k ), c i = c k and</p><formula xml:id="formula_4">t = T ({c i , m i , b i , a i }) (7) t = T ({c i , m i , b i , a k })<label>(8)</label></formula><p>During training, in half of the training samples, the location and size information vectors l i are zeroed, in order to allow the network to generate layouts, even when this information is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training Loss Terms</head><p>The loss used to optimize the networks contains multiple terms, which is not surprising, given the need to train five networks (not including the adversarial discriminators mentioned below) and two vector embeddings (o i and e ij ). <ref type="bibr" target="#b8">(9)</ref> where in our experiments we set λ 1 = λ 2 = λ 6 = λ 7 = 10, λ 3 = λ 4 = 1, λ 5 = 0.1.</p><formula xml:id="formula_5">L = L Rec +λ 1 L box +λ 2 L perceptual +λ 3 L D-mask +λ 4 L D-image + λ 5 L D-object + λ 6 L FM-mask + λ 7 L FM-image</formula><p>The reconstruction loss L Rec is the L1 difference between the reconstructed image p and the ground truth training image. The box loss L box is the MSE between the computed b i (summed over all objects) and the ground truth bounding box b i . Note that unlike <ref type="bibr" target="#b8">[9]</ref>, we do not employ a mask loss, since our mask contains a stochastic element (Eq. 2). The perceptual loss <ref type="bibr" target="#b7">[8]</ref> compares the generated image with the ground truth training image p , using the activations F u of the VGG network <ref type="bibr" target="#b20">[21]</ref> at layer u in a set of predefined layers U .</p><formula xml:id="formula_6">L perceptual = u∈U 1 u ||F u (p) − F u (p )|| 1</formula><p>Our method employs three discriminators D mask , D object , and D image . The mask discriminator employs a Least Squares GAN (LS-GAN <ref type="bibr" target="#b13">[14]</ref>) and is conditioned on the object's class c i . Recall that m i is the real mask of object i and m i , the generated mask, which depends on a random variable z i . The GAN loss associated with the mask discriminator is given by</p><formula xml:id="formula_7">L D−mask = [log D mask (m i , c i )]+ E z∼N (0,1) 64 [log(1 − D mask (M (u i , z), c i )] (10)</formula><p>For the purpose of training D mask , we minimize −L D−mask . The second discriminator D image , is used for training in an adversarial manner three networks R, M , and A. The loss of L D-image is a compound loss that is given as</p><formula xml:id="formula_8">L D-image = L real − L fake-image − L fake-layout + L alt-appearance where L real = log D image (t , p ) (11) L fake-image = log(1 − D image (t , p)) (12) L fake-layout = log(1 − D image (t, p )) (13) L alt-appearance = log(1 − D image (t , p ))<label>(14)</label></formula><p>The goal of the compound loss is to make sure that the generated image p, given a ground truth layout tensor t is indistinguishable from the real image p , and that this is true, even if the layout tensor t is based on estimated bounding boxes and masks (unlike t ). In addition, we would like the ground truth image to be a poor match for a counterfactual appearance vector, as given in t .</p><p>Following <ref type="bibr" target="#b24">[25]</ref> we use a multi-scale LS-GAN with two scales. In other words, L D-image is computed at the full scale and at half scale (using two different discriminators), and both terms are summed up to obtain the actual L D-image .</p><p>The third discriminator, D object , guarantees that the generated objects, one by one, look real. For this purpose, we crop p using the bounding boxes b i to create object images I i . Recall that I i are ground truth crops of images, obtained from the ground truth image p , using the ground truth bounding boxes b .</p><formula xml:id="formula_9">L D-object = n i=1 log D object (I i ) − log D object (I i )<label>(15)</label></formula><p>D object maximizes this loss during training. The mask feature matching loss L FM-mask and the image feature matching loss L FM-image are similar to the perceptual loss, i.e., they are based on the L1 difference in the activation. However, instead of the VGG loss, the discriminators are used, as in <ref type="bibr" target="#b18">[19]</ref>. In these losses, all layers are used. L FM-mask compares the activations of the generated mask m i and the real mask m i (the discriminator D mask also takes the class c i as input). The other feature matching loss L FM-image compares the activations of D image (t, p) with those of the ground truth layout tensor and image D image (t , p ). <ref type="figure">Figure 3</ref>. Image generation based on a given scene graph. Each row is a different example. (a) the scene graph, (b) the ground truth image, from which the layout was extracted, (c) our results when we used the ground truth layout of the image, similar to [28], (d) our method's results, where the appearance attributes present a random archetype and the location attributes coarsely describe the ground truth bounding box, (e) our results when we use the ground truth image to generate the appearance attributes, and the location attributes are zeroed li = 0, (f) our results where li = 0, and the appearance attributes are sampled from the archetypes, and (g) the results of <ref type="bibr" target="#b8">[9]</ref>.</p><formula xml:id="formula_10">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generating the archetypes</head><p>The GUI enables the user to select from preexisting object appearances, as well as copying the appearance vec-tor a i from another image. The existing object appearances are given as 100 archetypes per object class. These are obtained, by applying the learned network A to all objects in a (a) <ref type="figure">Figure 4</ref>. The diversity obtained when keeping the location attributes li fixed at zero and sampling different appearance archetypes. (a) the scene graph, (b) the ground truth image, from which the layout was extracted, (c-g) generated images.</p><formula xml:id="formula_11">(b) (c) (d) (e) (f) (g)</formula><p>given class in the training set and employing k-means clustering, in order to obtain 100 class means.</p><p>In the GUI, the archetypes are presented linearly along a slider. The order along the slider is obtained by applying a 1-D t-SNE <ref type="bibr" target="#b23">[24]</ref> embedding to the 100 archetypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inferring the scene graph from the layout panel</head><p>The GUI lets the users place objects on a schematic layout, see <ref type="figure">Fig. 1</ref>. Each object is depicted as a string in one of ten different font sizes, in order to capture the size element of l i . The location in the layout determines the 5 × 5 grid placement, which is encoded in the other part of l i . Note, however, that the locations and sizes are provided as indications of the structure of the graph layout and not as absolute locations (or scene layout). The generating network maintains freedom in the object placements to match the semantic properties of the objects in the scene.</p><p>The coarse placement by the user is more intuitive and less laborious than specifying a scene graph. To avoid adding unwanted work for the users, the edge labels are inferred, based on the relative position and size of the objects. An object which is directly to the left of another object, for example, is labeled "left of". The order in which objects are inserted to the layout determines the reference directing. If object i is inserted before object j, then "i is to the left of j" and not "j is to the right of i". The inside and surrounding relations are determined similarly, by considering objects of different sizes, whose centers are nearby.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training details</head><p>All networks are trained using ADAM <ref type="bibr" target="#b10">[11]</ref> solver with beta1 = 0.5 for 1 million iterations. The learning rate was set to 1e −4 for all components except L D-mask , where we set it to a smaller learning rate of 1e −5 . The different learning rates help us to stabilize the mask network. We use batch sizes of 32, 16, 4 in our 64 × 64, 128 × 128, 256 × 256 resolutions respectively. Notice that since each image contains up to 8 objects, each batch contains up to 8 × 32 = 256 different objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compare our results with the state of the art methods of <ref type="bibr" target="#b8">[9]</ref> and [28], using various metrics from the literature. In addition, we perform an ablation analysis to study the relative contribution of various aspects of our method. Our experiments are conducted on the COCO-Stuff dataset <ref type="bibr" target="#b1">[2]</ref>, which, using the same split as the previous works, contains approximately 25,000 train images, 1000 validation, and 2000 test images.</p><p>We employ two modes of experiments: either using the ground truth (GT) layout or the inferred layout. The first mode is the only one suitable for the method of <ref type="bibr">[29]</ref>. Whenever possible, we report the statistics reported in the previous work. Some of the statistics reported for <ref type="bibr" target="#b8">[9]</ref> are computed by us, based on the published model. We report results for three resolutions 64 2 , 128 2 , and 256 2 . The literature reports numerical results only for the first resolution. While <ref type="bibr" target="#b8">[9]</ref> presents visual results for 128x128, our attempts <ref type="figure">Figure 6</ref>. Duplicating an object's appearance in the generated image. Images are created based on the scene graph, such that the appearance is taken from one of five unrelated images. In this example, the sky's appearance is generated from the reference image, while all other objects use the same random appearance archetype.</p><p>to train their method using the published code on this resolution, resulted in sub-par performance, despite some effort. We, therefore, prefer not to provide these non-competitive baseline numbers. The code of <ref type="bibr">[29]</ref> is not yet available.</p><p>We employ multiple acceptable literature evaluation metrics for evaluating the generated images. The inception score <ref type="bibr" target="#b18">[19]</ref> measures both the quality of the generated images and their diversity. As has been done in previous works, a pre-trained inception network <ref type="bibr" target="#b21">[22]</ref> is employed in order to obtain the network activations used to compute the score. Larger inception scores are better. The FID <ref type="bibr" target="#b4">[5]</ref> measures the distance between the distribution of the generated images and that of the real test images, both modelled as a multivariate Gaussian. Lower FID scores are better. Less common, but relevant to our task, is the classification accuracy score, used by <ref type="bibr">[29]</ref>. A ResNet-101 model <ref type="bibr" target="#b3">[4]</ref> is trained to classify the 171 objects available in the training datasets, after cropping and resizing them to a fixed size of 224x224 pixels. On the test image, we report the accuracy of this classifier applied to the object images that are generated, using the bounding box of the image's layout. A higher accuracy means that the method creates more realistic, or at least identifiable, objects.</p><p>We also report a diversity score <ref type="bibr" target="#b26">[27]</ref>, which is based on the perceptual similarity <ref type="bibr" target="#b9">[10]</ref> between two images. This score is used to measure the distance between pairs of images that are generated given the same input. Ideally, the user would be able to obtain multiple, diverse, alternative outputs to choose from. Specifically, the activations of AlexNet <ref type="bibr" target="#b12">[13]</ref> are used together with the LPIPS visual similarity metric <ref type="bibr" target="#b26">[27]</ref>. A higher diversity score is better.</p><p>In addition, we also report three scores for evaluating the quality of the bounding boxes. The IoU score is the ratio between the area of the ground truth bounding box that is also covered by the generated bounding box (the intersection), and the area covered by either box (the union). We also report recall scores at two different thresholds. R@0.5 measures the ratio of object bounding boxes with an IoU of at least 0.5, and similarly for R@0.3.</p><p>Tab. 1 compares our method with the baselines and the real test images using the inception, FID, and classification accuracy scores. We make sure not to use information that the baseline method of <ref type="bibr" target="#b8">[9]</ref> is not using and use zero location attributes and appearance attributes that are randomly sam-  <ref type="table">Table 2</ref>. The diversity score of <ref type="bibr" target="#b26">[27]</ref>. The results of <ref type="bibr" target="#b8">[9]</ref> are computed by us and are considerably higher than those reported for the same method by <ref type="bibr">[28]</ref>. The results of [28] are from their paper.  [29] employs bounding boxes and not masks. However, we follow the same comparison (to masked based methods) given in their paper.</p><p>As can be seen, our method obtains a significant lead in all these scores over the baseline methods, whenever such a comparison can be made. This is true both when the ground truth layout is used and when the layout is generated. As expected, the ground truth layout obtains better scores.</p><p>Sample results of our 256x256 model are shown in <ref type="figure">Fig. 3</ref>, using test images from the COCO-stuff datasets. Each row presents the scene layout, the ground truth image from which the layout was extracted, our method's results, where the object attributes present a random archetype and the location attributes are zeroed (l i = 0), our results when using the ground truth layout of the image (including masks and bounding boxes), our results where the appearance attributes of each object are copied from the ground truth image and the location vectors are zero, and our results where the location attributes coarsely describe the objects' locations and the appearance attributes are randomly selected from the archetypes. In addition, we present the result of the baseline method of <ref type="bibr" target="#b8">[9]</ref> at the 64x64 resolution for which a model was published.</p><p>As can be seen, our model produces realistic results across all settings, which are more pleasing than the baseline method. Using ground truth location and appearance attributes, the resulting image better matches the test image.</p><p>Tab. 2 reports the diversity of our method in comparison to the two baseline methods. The source of stochasticity we employ (the random vector z i used in Eq. 2) produces a higher diversity than the two baseline methods (which also include a random element), even when not changing the location vector l 1 or appearance attributes a i . Varying either one of these factors adds a sizable amount of diversity. In the experiments of the table, the location attributes, when varied, are sampled using per-class Gaussian distribution that fit to the location vectors of the training set images. <ref type="figure">Fig. 4</ref> presents samples obtained when sampling the appearance attributes. In each case, for all i, l i = 0 and the object's appearance embedding a i is sampled uniformly between the archetypes. This results in a considerable visual diversity. <ref type="figure" target="#fig_1">Fig. 5</ref> presents results in which the appearance is fixed to the mean appearance vector for all objects of that class and the location attribute vectors l i are sampled from the Gaussian distributions mentioned above. In almost all cases, the generated images are visually pleasing. In some cases, the location attributes sampled are not compatible with a realistic image. Note, however, that in our method, the default value for l i is zero and not a random vector.</p><p>Tab. 3 presents a comparison with the method of <ref type="bibr" target="#b8">[9]</ref>, regarding the placement accuracy of the bounding boxes. Even when not using the location attribute vectors l i , our bounding box placement better matches the test images. As expected, adding the location vectors improves the results.</p><p>The ability of our method to copy the appearance of an existing image object is demonstrated in <ref type="figure">Fig. 6</ref>. In this example, we generate the same test scene graph, while varying a single object in accordance with five different options extracted from images unseen during training. Despite the variability of the appearance that is presented in the five sources, the generated images mostly maintain their visual quality. These results are presented at a resolution of 256x256, which is the default resolution for our GUI. At this resolution, the system processes a graph in 16.3ms.</p><p>User study Following <ref type="bibr" target="#b8">[9]</ref>, we perform a user study to compare with the baseline method the realism of the generated image, the adherence to the scene graph, as well as to verify that the objects in the scene graph appear in the output image. The user study involved n = 20 computer graphics and computer vision students. Each student was shown the output images for 30 random test scene-graphs from the COCO-stuff dataset and was asked to select the preferable method, according to two criteria: "which image is more realistic" and "which image better reflects the scene graph". In addition, the list of objects in the scene graph was presented, and the users were asked to count the number of objects that appear in each of the images. The two images, one for the method of <ref type="bibr" target="#b8">[9]</ref> and one for our method, were presented in a random order. To allow for a fair comparison, the appearance archetypes were selected at random, the location vectors were set to zero for all objects, and we have used images from our 64 × 64 resolution model. The results, listed in Tab. 4, show that our method significantly outperforms the baseline method in all aspects tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation analysis</head><p>The relative importance of the various losses is approximated, by removing it from the method and training the 128x128 model. For this study, we use both the inception and the FID scores. The results are reported in Tab. 5. As can be seen, removing each of the losses results in a noticeable degradation. Removing the perceptual loss is extremely detrimental. Out of the three discriminators, removing the mask discriminator is the most damaging, since, due to the random component z i , we do not have a direct loss on the mask. Finally, replacing our image discriminator with the one in <ref type="bibr" target="#b8">[9]</ref>, results in some loss of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present an image generation tool in which the input consists of a scene graph with the potential addition of location information. Each object is associated both with a location embedding and with an appearance embedding. The latter can be extracted from another image, allowing for a duplication of existing objects to a new image, where their layout is drastically changed. In addition to the dual encoding, our method presents both a new architecture and new loss terms, which leads to an improved performance over the existing baselines. A. Network architecture</p><p>The graph convolutional network used is the same as the one used in <ref type="bibr" target="#b8">[9]</ref>, which was modified in order to support the added node information. We concatenate the embedding of the objects to the location attributes creating vectors in R 128+35 . These vectors, together with the relation embedding is feed-forward into a fully-connected layer which results in a vector embedding in R 128 for each of the objects and each of the relations. The network then follows the architecture of <ref type="bibr" target="#b8">[9]</ref>, using a shared symmetric function to calculate the object vector from all the relations it participate in. Our graph convolution has overall five layers.</p><p>To describe the rest of the networks, we follow a semiconventional shorthand notation for defining architectures. Let C k denote a Convolution layer of k filters with a kernel size of 7x7 and a stride of 1, followed by instance normalization <ref type="bibr" target="#b22">[23]</ref> and a ReLU activation function. Similarly, we use D k to a layer which uses a stride of 2, reflection padding, and k filters. In addition, we use B k to denote a 3x3 upsample-convolution-BatchNormalization-ReLU layer with k filters and a stride and padding of 1. We use V k to define Residual blocks with two 3x3 convolutional layers, both with k filters. Moreover, U k denotes a layer with k filters of size 3x3 and a fractional stride of 0.5, followed by instance normalization. CB k denotes a stride-2 k-filter, 4x4 convolution followed by batch normalization. GA denotes a global average pooling layer. Fully connected layers with k hidden units followed by a ReLU activation are denoted by L k . The ReLU is not applied to the L k layer, if it is the top layer.</p><p>The discriminators call for an even more elaborate terminology. Let C i−k−o denote a 3x3 Convolution layer with i input channels and k output filters, a stride of o and a padding of 1. In addition, LR denotes Leaky-ReLU with negative slope of 0.2, IN denotes Instance Normalization, and AP k denotes a 3x3 Average Pool stride 2 and padding of 1. Also, let C k−s denote a Convolution layer with k filters, stride of s, kernel size of 4x4, and a padding of size 2. D k−s denotes a 4x4 Convolution-InstanceNorm-LeakyReLU layer with k filters, a stride of s padding of 2 and a LeakyReLU with a negative slope of 0.2.</p><p>The different components of the networks can be described as:  </p><formula xml:id="formula_12">M</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) A giraffe is added. (c) The giraffe is enlarged. (d) The appearance of the sky is changed to a different archetype. (e) A small sheep is added. (f) An airplane is added. (g) The tree is enlarged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>The diversity obtained when keeping the appearance vectors fixed and sampling from the location distribution. (a) the scene graph, (b) the ground truth image from which the layout was extracted, (c-g) generated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>D mask C 1 − 1 D 1 (</head><label>111</label><figDesc>64−2 , LR, C * (c+64)−128−1 , IN , LR, C 128−1−1 , AP D image C (c+32)−2 , LR, D 64−2 , D 128−2 , D 256−1 , C 512−object CB 64 ,CB 128 ,C4256  , GA, L 1024 , L * Concatenating the c i data in D image is done at the third layer)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The inception score of<ref type="bibr" target="#b8">[9]</ref> for the complete pipeline is taken from their paper. The other scores are not reported there. The inception score for[28]   is the one reported by the authors.</figDesc><table><row><cell>Reso-</cell><cell>Method</cell><cell>Inception a</cell><cell>FID</cell><cell>Accu-</cell></row><row><cell>lution b</cell><cell></cell><cell></cell><cell></cell><cell>racy</cell></row><row><cell></cell><cell>Real Images</cell><cell cols="2">16.3 ± 0.4 0</cell><cell>54.5</cell></row><row><cell></cell><cell>[9] GT Layout</cell><cell>7.3 ± 0.1</cell><cell>86.5</cell><cell>33.9</cell></row><row><cell>64x64</cell><cell>[28] GT Layout</cell><cell>9.1 ± 0.1</cell><cell>c</cell><cell>d</cell></row><row><cell></cell><cell cols="3">Ours GT Layout 10.3 ± 0.1 48.7</cell><cell>46.1</cell></row><row><cell></cell><cell>[9]</cell><cell>6.7 ± 0.1</cell><cell cols="2">103.4 28.8</cell></row><row><cell></cell><cell>Ours</cell><cell>7.9 ± 0.2</cell><cell>65.3</cell><cell>43.3</cell></row><row><cell></cell><cell>Real Images</cell><cell>24.2± 0.9</cell><cell>0</cell><cell>59.3</cell></row><row><cell>128x128</cell><cell cols="3">Ours GT Layout 12.5 ± 0.3 59.5</cell><cell>44.6</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">10.4 ± 0.4 75.4</cell><cell>42.8</cell></row><row><cell></cell><cell>Real Images</cell><cell cols="2">30.7 ± 1.2 0</cell><cell>62.4</cell></row><row><cell>256x256</cell><cell cols="3">Ours GT Layout 16.4 ± 0.7 65.2</cell><cell>45.3</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">14.5 ± 0.7 81.0</cell><cell>42.2</cell></row></table><note>ab [9] and [28] report numerical results only for a resolution of 64x64.c Not reported and cannot be computed due to lack of code/results.d The accuracy reported by [28] is incompatible (different classifiers). Table 1. A quantitative comparison using various image generation scores. In order to support a fair comparison, our model does not use location attributes and employs random appearance attributes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of predicted bounding boxes</figDesc><table><row><cell>User Study</cell><cell></cell><cell>[9]</cell><cell>Ours</cell></row><row><cell>More realistic output</cell><cell></cell><cell>16.7%</cell><cell>83.3%</cell></row><row><cell cols="2">Better adherence to scene graph</cell><cell>19.3%</cell><cell>80.7%</cell></row><row><cell>Ratio of observed objects</cell><cell></cell><cell cols="2">27.31% 45.38%</cell></row><row><cell>among all COCO objects</cell><cell></cell><cell></cell></row><row><cell>Ratio of observed objects</cell><cell></cell><cell cols="2">46.49% 65.23%</cell></row><row><cell>among all COCO stuff</cell><cell></cell><cell></cell></row><row><cell cols="3">Table 4. User study results</cell></row><row><cell>Model</cell><cell cols="2">Inception</cell><cell>FID</cell></row><row><cell>Full method</cell><cell cols="2">10.4 ± 0.4</cell><cell>75.4</cell></row><row><cell>No Lperceptual</cell><cell cols="2">6.2 ± 0.1</cell><cell>125.1</cell></row><row><cell>No LD-mask</cell><cell cols="2">5.2 ± 0.1</cell><cell>183.6</cell></row><row><cell>No LD-image</cell><cell cols="2">7.4 ± 0.2</cell><cell>122.5</cell></row><row><cell>No LD-object</cell><cell cols="2">8.7 ± 0.1</cell><cell>94.5</cell></row><row><cell>Using Dimage of [9]</cell><cell cols="2">8.1 ± 0.3</cell><cell>114.2</cell></row><row><cell cols="3">Table 5. Ablation Study</cell></row><row><cell>pled (see 3.2).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>[ 28 ]</head><label>28</label><figDesc>Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. CoRR, abs/1811.11389, 2018. 3, 5, 7, 9 [29] Han Zhao, Shanghang Zhang, Guanhang Wu, Jo ao P. Costeira, José M. F. Moura, and Geoffrey J. Gordon. Multiple source domain adaptation with adversarial learning. In ICLR workshop, 2018. 7, 8, 9</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>B 192 , B 192 , B 192 , B 192 , B 192 , B 192 , C 1 , Sigmoid activation B L 128 , L 512 , L 4 A CB 64 , CB 128 , CB 256 , GA, L 192 , L 64 , L 32 R C c+32 , D 128 , D 256 , D 512 , D 1024 , V 1024 , V 1024 , V 1024 , V 1024 , V 1024 , V 1024 , V 1024 , V 1024 , V 1024 , U 512 , U 256 , U 128 , U 64 , C 3</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC CoG 725974).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Bolei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10597</idno>
		<title level="m">Gan dissection: Visualizing and understanding generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Inferring semantic layout for hierarchical textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1801.05091</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Henry Holt and Co., Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic image retrieval via active grounding of visual situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">H</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Conser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">M</forename><surname>Witte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
