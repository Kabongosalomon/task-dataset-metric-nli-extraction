<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Information Lab</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Information Lab</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We focus on multi-modal fusion for egocentric action recognition, and propose a novel architecture for multimodal temporal-binding, i.e. the combination of modalities within a range of temporal offsets. We train the architecture with three modalities -RGB, Flow and Audio -and combine them with mid-level fusion alongside sparse temporal sampling of fused representations. In contrast with previous works, modalities are fused before temporal aggregation, with shared modality and fusion weights over time. Our proposed architecture is trained end-to-end, outperforming individual modalities as well as late-fusion of modalities.</p><p>We demonstrate the importance of audio in egocentric vision, on per-class basis, for identifying actions as well as interacting objects. Our method achieves state of the art results on both the seen and unseen test sets of the largest egocentric dataset: EPIC-Kitchens, on all metrics using the public leaderboard.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the availability of multi-sensor wearable devices (e.g. GoPro, Google Glass, Microsoft Hololens, Magi-cLeap), egocentric audio-video recordings have become popular in many areas such as extreme sports, health monitoring, life logging, and home automation. As a result, there has been a renewed interest from the computer vision community on collecting large-scale datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35]</ref> as well as developing new or adapting existing methods to the firstperson point-of-view scenario <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>In this work, we explore audio as a prime modality to provide complementary information to visual modalities (appearance and motion) in egocentric action recognition. While audio has been explored in video understanding in general <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b33">34]</ref> the egocentric domain in particular offers rich sounds resulting from the interactions between hands and objects, as well as the close proximity of the wearable microphone to the undergoing action. Audio is a prime discriminator for some actions (e.g. 'wash', 'fry') as well as objects within actions (e.g. 'put plate' vs 'put bag'). At times, the temporal progression (or change) of sounds can separate visually ambiguous actions (e.g. 'open tap' vs 'close tap'). Audio can also capture actions that are out of the wearable camera's field of view, but audible (e.g. 'eat' can be heard but not seen). Conversely, other actions are sound-less (e.g. 'wipe hands') and the wearable sensor might capture irrelevant sounds, such as talking or music playing in the background. The opportunities and challenges of incorporating audio in egocentric action recognition allow us to explore new multi-sensory fusion approaches, particularly related to the potential temporal asynchrony between the action's appearance and the discriminative audio signal -the main focus of our work.</p><p>While several multi-modal fusion architectures exist for action recognition, current approaches perform temporal aggregation within each modality before modalities are fused <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref> or embedded <ref type="bibr" target="#b22">[23]</ref>. Works that do fuse inputs before temporal aggregation, e.g. <ref type="bibr" target="#b9">[10]</ref>, do so with inputs synchronised across modalities. In <ref type="figure" target="#fig_0">Fig. 1, we</ref> show an example of 'breaking an egg into a pan' from the EPIC-Kitchens dataset. The distinct sound of cracking the egg, the motion of separating the egg and the change in appearance of the egg occur at different frames/temporal positions within the video. Approaches that fuse modalities with synchronised input would thus be limited in their ability to learn such actions. In this work, we explore fusing inputs within a Temporal Binding Window (TBW) <ref type="figure" target="#fig_0">(Fig 1)</ref>, allowing the model to train using asynchronous inputs from the various modalities. Evidence in neuroscience and behavioural sciences points at the presence of such a TBW in humans <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41]</ref>. The TBW offers a "range of temporal offsets within which an individual is able to perceptually bind inputs across sensory modalities" <ref type="bibr" target="#b38">[39]</ref>. This is triggered by the gap in the biophysical time to process different senses <ref type="bibr" target="#b24">[25]</ref>. Interestingly, the width of the TBW in humans is heavily task-dependant, shorter for simple stimuli such as flashes and beeps and intermediate for complex stimuli such as a hammer hitting a nail <ref type="bibr" target="#b40">[41]</ref>.</p><p>Combining our explorations into audio for egocentric action recognition, and using a TBW for asynchronous modality fusion, our contributions are summarised as follows. First, an end-to-end trainable mid-level fusion Temporal Binding Network (TBN) is proposed 1 . Second, we present the first audio-visual fusion attempt in egocentric action recognition. Third, we achieve state-of-the-art results on the EPIC-Kitchens public leaderboards on both seen and unseen test sets. Our results show (i) the efficacy of audio for egocentric action recognition, (ii) the advantage of mid-level fusion within a TBW over late fusion, and (iii) the robustness of our model to background or irrelevant sounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We divide the related works into three groups: works that fuse visual modalities (RGB and Flow) for action recognition (AR), works that fuse modalities for egocentric AR in particular, and finally works from the recent surge in interest of audio-visual correspondence and source separation. Visual Fusion for AR: By observing the importance of spatial and temporal features for AR, two-stream (appearance and motion) fusion has become a standard technique <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. Late fusion, first proposed by Simonyan and Zisserman <ref type="bibr" target="#b35">[36]</ref>, combines the streams' independent predictions. Feichtenhofer et al. <ref type="bibr" target="#b9">[10]</ref> proposed mid-level fusion of the spatial and temporal streams, showing optimal results by combining the streams after the last convolutional layer. In <ref type="bibr" target="#b6">[7]</ref>, 3D convolution for spatial and motion streams was proposed, followed by late fusion of modalities. All these approaches do not model the temporal progression of actions, a problem addressed by <ref type="bibr" target="#b41">[42]</ref>. Temporal Segment Networks (TSN) <ref type="bibr" target="#b41">[42]</ref> perform sparse temporal sampling followed by temporal aggregation (averaging) of softmax scores across samples. Each modality is trained independently, with late fusion of modalities by averaging their predictions. Follow-up works focus on pooling for temporal aggregation, still training modalities independently <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">45]</ref>. Modality fusion before temporal aggregation was proposed <ref type="bibr" target="#b0">1</ref> Code at: http://github.com/ekazakos/temporal-binding-network in <ref type="bibr" target="#b17">[18]</ref>, where the appearance of the current frame is fused with 5 uniformly sampled motion frames, and vice versa, using two temporal models (LSTM). While their motivation is similar to ours, their approach focuses on using predefined asynchrony offsets between two modalities. In contrast, we relax this constraint and allow fusion from any random offset within a temporal window, which is more suitable for scaling up to many modalities.</p><p>Fusion in Egocentric AR: Late fusion of appearance and motion has been frequently used in egocentric AR <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>, as well as extended to additional streams aimed at capturing egocentric cues <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. In <ref type="bibr" target="#b20">[21]</ref>, the spatial stream segments hands and detects objects. The streams are trained jointly with a triplet loss on objects, actions and activities, and fused through concatenation. <ref type="bibr" target="#b36">[37]</ref> uses head motion features, hand masks, and saliency maps, which are stacked and fed to both a 2D and a 3D ConvNet, and combined by late fusion. All previous approaches have relied on small-scale egocentric datasets, and none utilised audio for egocentric AR.</p><p>Audio-Visual Learning: Over the last three years, significant attention has been paid in computer vision to an underutilised and readily available source of information existing in video: the audio stream <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b33">34]</ref>. These fall in one of four categories: i) audio-visual representation learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, ii) sound-source localisation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>, iii) audio-visual source separation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> and (iv) visual-question answering <ref type="bibr" target="#b0">[1]</ref>. These approaches attempt fusion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref> or embedding into a common space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>. Several works sample the two modalities with temporal shifts, for learning better synchronous representations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. Others sample within a 1s temporal window, to learn a correspondence between the modalities, e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Of these works, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> note this audio-visual representation learning could be used for AR, by pretraining on the self-supervised task and then fine-tuning for AR.</p><p>Fusion for AR using three modalities (appearance, motion and audio) has been explored in <ref type="bibr" target="#b42">[43]</ref>, employing latefusion of predictions, and <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> using attention to integrate local features into a global representation. Tested on UCF101, <ref type="bibr" target="#b42">[43]</ref> shows audio to be the least informative modality for third person action recognition (16% accuracy for audio compared to 80% and 78% for spatial and motion). A similar conclusion was made for other third-person datasets (AVA <ref type="bibr" target="#b11">[12]</ref> and Kinetics <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>).</p><p>In this work, we show audio to be a competitive modality for egocentric AR on EPIC-Kitchens, achieving comparable performance to appearance. We also demonstrate that audio-visual modality fusion in egocentric videos improves the recognition performance of both the action and the accompanying object. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Temporal Binding Network</head><p>Our goal is to find the optimal way to fuse multiple modality inputs while modelling temporal progression through sampling. We first explain the general notion of temporal binding of multiple modalities in Sec 3.1, then detail our architecture in Sec 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multimodal Temporal Binding</head><p>Consider a sequence of samples from one modality in a video stream, m i = (m i1 , m i2 , · · · , m iT /ri ) where T is the video's duration and r i is the modality's framerate (or frequency of sampling). Input samples are first passed through unimodal feature extraction functions f i . To account for varying representation sizes and frame-rates, most multi-modal architectures apply pooling functions G to each modality in the form of average pooling or other temporal pooling functions (e.g. maximum or VLAD <ref type="bibr" target="#b14">[15]</ref>), before attempting multimodal fusion.</p><p>Given a pair of modalities m 1 and m 2 , the final class predictions for a video are hence obtained as follows:</p><formula xml:id="formula_0">y = h G(f 1 (m 1 )), G(f 2 (m 2 ))<label>(1)</label></formula><p>where f 1 and f 2 are unimodal feature extraction functions, G is a temporal aggregation function, h is the multimodal fusion function and y is the output label for the video. In such architectures (e.g. TSN <ref type="bibr" target="#b41">[42]</ref>), modalities are tempo-rally aggregated for a prediction before different modalities are fused; this is typically referred to as 'late fusion'. Conversely, multimodal fusion can be performed at each time step as in <ref type="bibr" target="#b9">[10]</ref>. One way to do this would be to synchronise modalities and perform a prediction at each timestep. For modalities with matching frame rates, synchronised multi-modal samples can be selected as (m 1j , m 2j ), and fused according to the following equation:</p><formula xml:id="formula_1">y = h G(f sync (m 1j , m 2j ))<label>(2)</label></formula><p>where f sync is a multimodal feature extractor that produces a representation for each time step j, and G then performs temporal aggregation over all time steps. When frame rates vary, and more importantly so do representation sizes, only approximate synchronisation can be attempted,</p><formula xml:id="formula_2">y = h G(f sync (m 1j , m 2k )) : k = jr 2 r 1<label>(3)</label></formula><p>We refer to this approach as 'synchronous fusion' where synchronisation is achieved or approximated.</p><p>In this work, however, we propose fusing modalities within temporal windows. Here modalities are fused within a range of temporal offsets, with all offsets constrained to lie within a finite time window, which we henceforth refer to as a temporal binding window (TBW). Formally,</p><formula xml:id="formula_3">y = h G(f tbw (m 1j , m 2k )) : k ∈ jr 2 r 1 −b , jr 2 r 1 +b</formula><p>(4) where f tbw is a multimodal feature extractor that combines inputs within a binding window of width ±b. Interestingly, as the number of modalities increases, say from two to three modalities, the TBW representation allows fusion of modalities each with different temporal offsets, yet within the same binding window ±b:</p><formula xml:id="formula_4">y = h G(f tbw (m 1j , m 2k , m 3l ) : k ∈ jr 2 r 1 − b , jr 2 r 1 + b : l ∈ jr 3 r 1 − b , jr 3 r 1 + b (5)</formula><p>This formulation hence allows a large number of different inputs combinations to be fused. This is different from proposals that fuse inputs over predefined temporal differences (e.g. <ref type="bibr" target="#b17">[18]</ref>). Sampling within a temporal window allows fusing modalities with various temporal shifts, up to the temporal window width ±b. This: 1) enables straightforward scaling to multiple modalities with different frame rates, 2) allows training with a variety of temporal shifts, accommodating, say, different speeds of action performance and 3) provides a natural form of data augmentation.</p><p>With the basic concept of a TBW in place, we now describe our proposed audio-visual fusion model, TBN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">TBN with Sparse Temporal Sampling</head><p>Our proposed TBN architecture is shown in <ref type="figure" target="#fig_1">Fig 2 (left)</ref>. First, the action video is divided into K segments of equal width. Within each segment, we select a random sample of the first modality ∀k ∈ K : m 1k . This ensures the temporal progression of the action is captured by sparse temporal sampling of this modality, as with previous works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref>, while random sampling within the segment offers further data for training. The sampled m 1k is then used as the centre of a TBW of width ±b. The other modalities are selected randomly from within each TBW (Eq. 5). In total, the input to our architecture in both training and testing is K × M samples from M modalities.</p><p>Within each of the K TBWs, we argue that the complementary information in audio and vision can be better exploited by combining the internal representations of each modality before temporal aggregation, and hence we propose a mid-level fusion. A ConvNet (per modality) extracts mid-level features, which are then fused through concatenating the modality features and feeding them to a fullyconnected layer, making multi-modal predictions per TBW. We backpropagate all the way to the inputs of the ConvNets. <ref type="figure" target="#fig_2">Fig 3 details</ref> the proposed TBN block. The predictions, for each of these unified multimodal representations, are then aggregated for video-level predictions. In the proposed architecture, we train all modalities simultaneously. The con-  <ref type="figure" target="#fig_1">Fig. 2</ref>. We model the problem of learning both verbs and nouns as a multi-task learning problem, by adding two output FC layers, one that predicts verbs and the other nouns (as in <ref type="bibr" target="#b7">[8]</ref>). Best viewed in colour.</p><p>volutional weights for each modality are shared over the K segments. Additionally, mid-level fusion weights and class prediction weights are also shared across the segments.</p><p>To avoid biasing the fusion towards longer or shorter action lengths, we calculate the window width b relative to the action video length. Our TBW is thus of variable width, where the width is a function of the length of the action. We note again that b can be set independently of the number of segments K, allowing the temporal windows to overlap. This is detailed in Sec. 4.1.</p><p>Relation to TSN. In <ref type="figure" target="#fig_1">Fig 2,</ref> we contrast the TBN architecture (left) to an extended version of the TSN architecture (right). The extension is to include the audio modality, since the original TSN only utilises appearance and motion streams. There are two key differences: first, in TSN each modality is temporally aggregated independently (across segments), and the modalities are only combined by late fusion (e.g. the RGB scores of each segment are temporally aggregated, and the flow scores of each segment are temporally aggregated, individually). Hence, it is not possible to benefit from combining modalities within a segment which is the case for TBN. Second, in TSN, each modality is trained independently first after which predictions are combined in inference. In the TBN model instead, all modalities are trained simultaneously, and their combination is also learnt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset: We evaluate the TBN architecture on the largest dataset in egocentric vision: EPIC-Kitchens <ref type="bibr" target="#b7">[8]</ref>, which contains 39, 596 action segments recorded by 32 participants performing non-scripted daily activities in their native kitchen environments. In EPIC-Kitchens, an action is defined as a combination of a verb and a noun, e.g. 'cut cheese'. There are in total 125 verb classes and 331 noun classes, though these are heavily-imbalanced. The test set is divided in two splits: Seen Kitchens (S1) where sequences from the same environment are in both training, and Unseen Kitchens (S2) where the complete sequences for 4 participants are held out for testing. Importantly, EPIC-Kitchens sequences have been captured using a head-mounted Go-Pro with the audio released as part of the dataset. No previous baseline on using audio for this dataset is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>RGB and Flow: We use the publicly available RGB and computed optical flow with the dataset <ref type="bibr" target="#b7">[8]</ref>. Audio Processing: We extract 1.28s of audio, convert it to single-channel, and resample it to 24kHz. We then convert it to a log-spectrogram representation using an STFT of window length 10ms, hop length 5ms and 256 frequency bands. This results in a 2D spectrogram matrix of size 256 × 256, after which we compute the logarithm. Since many egocentric actions are very short (&lt; 1.28s), we extract 1.28s of audio from the untrimmed video, allowing the audio segment to extend beyond the action boundaries. Training details: We implement our model in Py-Torch <ref type="bibr" target="#b30">[31]</ref>. We use Inception with Batch Normalisation (BN-Inception) <ref type="bibr" target="#b13">[14]</ref> as a base architecture, and fuse the modalities after the average pooling layer. We chose BN-Inception as it offers a good compromise between performance and model-size, critical for our proposed TBN that trains all modalities simultaneously, and hence is memoryintensive. Compared to TSN, the three modalities have 10.78M, 10.4M and 10.4M parameters, with only one modality in memory during training. In contrast, TBN has 32.64M paramaters.</p><p>We train using SGD with momentum <ref type="bibr" target="#b32">[33]</ref>, a batch size of 128, a dropout of 0.5, a momentum of 0.9, and a learning rate of 0.01. Networks are trained for 80 epochs, and the learning rate is decayed by a factor of 10 at epoch 60. We initialise the RGB and the Audio streams from ImageNet. While for the Flow stream, we use stacks of 10 interleaved horizontal and vertical optical flow frames, and use the pretrained Kinetics <ref type="bibr" target="#b6">[7]</ref> model, provided by the authors of <ref type="bibr" target="#b41">[42]</ref>.</p><p>Note that our network is trained end-to-end for all modalities and TBWs. We train with K = 3 segments over the M = 3 modalities, with b = T , allowing the temporal window to be as large as the action segment. We test using 25 evenly spaced samples for each modality, as with the TSN basecode for direct comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>This section is organised as follows. First, we show and discuss the performance of single modalities, and compare them with our proposed TBN, with a special focus on the efficacy of the audio stream. Second, we compare different <ref type="figure">Figure 4</ref>: Verb (left) and noun (right) classes' performances using single modalities for top-performing 32 verb and 41 noun classes, using single modality accuracy. For each, we consider whether the accuracy is high for Flow, Audio or RGB, or for two or all of these modalities. It can be clearly seen that noun classes can be predicted with high accuracy using RGB alone, whereas for many verbs, Flow and Audio are also important modalities. mid-level fusion techniques. And finally, we investigate the effect of the TBW width on both training and testing. Single-modality vs multimodal fusion performance: We examine the overall performance of each modality individually in <ref type="table" target="#tab_0">Table 1</ref>. Although it is clear that RGB and optical flow are stronger modalities than audio, an interesting find is that audio performs comparably to RGB on some of the metrics (e.g. top-1 verb accuracy), signifying the relevance of audio on recognising egocentric actions. While as expected optical flow outperforms RGB in S2, interestingly for S1, the RGB and Flow modalities perform comparatively, and in some cases RGB performs better. This matches the expectation that Flow is more invariant to the environment.</p><p>To obtain a better analysis of how these modalities perform, we examine the accuracy of individual verb and noun classes on S1, using single modalities. <ref type="figure">Fig 4 plots</ref> topperforming verb and noun classes, into a Venn diagram. For each class, we consider the accuracy of individual modalities. If all modalities perform comparably (within 0.15), we plot that class in the intersection of the three circles. On the other hand, if one modality is clearly better than the others (more than 0.15), we plot the class in the outer part of the modality's circle. For example, for the verb 'close', we have per-class accuracy of 0.23, 0.47 and 0.42 for RGB, Flow and Audio respectively. We thus note that this class performs best for two modalities: Flow and Audio, and plot it in the intersection of these two circles.</p><p>From this plot, many verb and noun classes perform comparably for all modalities (e.g. 'wash', 'peel' and 'fridge', 'sponge'). This suggests all three modalities contain useful information for these tasks. A distinctive difference, however, is observed in the importance of individual modalities for verbs and nouns. Verb classes are strongly related to the temporal progression of actions, making Flow  more important for verbs than nouns. Conversely, noun classes can be predicted with high accuracy using RGB alone. Audio, on the other hand, is important for both nouns and verbs, particularly for some verbs such as 'turn-on', and 'spray'. For nouns, Audio tends to perform better for objects with distinctive sounds (e.g. 'switch', 'extractor fan') and materials that sound when manipulated (e.g. 'foil').</p><p>In <ref type="table">Table.</ref> 1, we compare single modality performance to the performance over the three modalities. Single modalities are trained as in TSN, as TBN is designed to bind multiple modalities. We find that the fusion method outperforms single modalities, and that audio is a significantly informative modality across the board. Per-class accuracies, for individual modalities as well as for TBN trained on all three modalities, can be seen in <ref type="figure">Figure 6</ref>. The advantage of the fusion method is more pronounced for verbs (where we expect motion and audio to be more informative) than nouns, and more for particular noun classes than others, such as 'pot', 'kettle', 'microwave', and particular verb classes eg. 'spray' (fusion 0.54, RGB 0.09, Flow 0, Audio 0.3). This suggests that the mixture of complementary and redundant information captured in a video is highly dependant on the action itself, yielding the fusion method to be more useful for some classes than for others. We also note that the fusion   <ref type="table" target="#tab_0">Table 1</ref>. An increase of 5% (S1) and 4% (S2) in top-5 action recognition accuracy with the addition of audio demonstrates the importance of audio for egocentric action recognition. <ref type="figure" target="#fig_3">Fig 5 shows</ref> the confusion matrix with the utilisation of audio for the largest-15 verb classes (in S1). Studying the difference <ref type="figure" target="#fig_3">(Fig 5 right)</ref> clearly demonstrates an increase (blue) in confidence along the diagonal, and a decrease (red) in confusion elsewhere. Audio with irrelevant sounds: In the recorded videos for EPIC-Kitchens, background sounds irrelevant to the observed actions have been captured by the wearable sensor. These include music or TV playing in the background, ongoing washing machine, coffee machine or frying sounds while actions take place. To quantify the effect of these sounds, we annotated the audio in the test set, and report that 14% of all action segments in S1, and 46% of all action segments in S2 contain other audio sources. We refer to these as actions containing 'irrelevant' sounds, and independently report the results in  <ref type="table">Table 3</ref>: Comparison of mid-level fusion techniques for the TBN architecture. <ref type="figure">Figure 6</ref>: Per-class accuracies for the S1 test set for verbs (top) and nouns (bottom) for fusion and single modalities. We select verb classes with more than 10 samples, and noun classes with more than 30 samples. The classes are presented in the order of number of samples per class, from left to right. For most classes the fusion method provides significant performance gains over single modality classification (largest improvements shown in bold). Best viewed in colour.</p><p>'irrelevant' S2 (comparing to 'rest'), validating that irrelevant sounds are not the source of confusion, but that this set of action segments is more challenging even in the visual modalities. This demonstrates the robustness of our network to noisy and unconstrained audio sources. Comparison of fusion strategies: <ref type="figure" target="#fig_1">As Fig 2 indicates</ref>, TBN performs mid-level fusion on the modalities within the binding window. Here we describe three alternative mid-level fusion strategies, and then compare their performances.</p><p>(i) Concatenation, where the feature maps of each modality are concatenated, and a fully-connected layer is used to model the cross-modal relations.</p><formula xml:id="formula_5">f concat tbw = φ(W [m 1j , m 2k , m 3l ] + b)<label>(6)</label></formula><p>where φ is a non-linear activation function. When used within TBWs, shared weights f tbw are to be learnt between modalities within a range of temporal shifts.</p><p>(ii) Context gating was used in <ref type="bibr" target="#b21">[22]</ref>, aiming to recalibrate the strength of the activations of different units with a selfgating mechanism:</p><formula xml:id="formula_6">f context tbw = σ(W h + b z ) • h<label>(7)</label></formula><p>where • is element-wise multiplication. We apply context gating on top of our multi-modal fusion with concatenation, so h in <ref type="formula" target="#formula_6">(7)</ref> is equivalent to (6). <ref type="figure">Figure 7</ref>: Effect of TBW width for verbs (left) and nouns (right) in the S1 test set.</p><p>(iii) Gating fusion was introduced in <ref type="bibr" target="#b3">[4]</ref>, where a gate neuron takes as input the features from all modalities to learn the importance of one modality w.r.t. all modalities.</p><formula xml:id="formula_7">h i = φ(W i m ij + b i ) ∀i (8) z i = σ(W zi [m 1j , m 2k , m 3l ] + b zi ) ∀i (9) f gating tbw = z 1 • h 1 + z 2 • h 2 + z 3 • h 3 ,<label>(10)</label></formula><p>In <ref type="table">Table 3</ref>, we compare the various fusion strategies. We find that the simplest method, concatenation (Eq. 6) generally outperforms more complex fusion approaches. We believe this shows modality binding within a temporal binding window to be robust to the mid-level fusion method.  <ref type="table">Table 4</ref>: Results on the EPIC-Kitchens for seen (S1) and unseen (S2) test splits. At the time of submission, our method outperformed all previous methods on all metrics, and in particular by 11%, 5% and 4% on top-1 verb, noun and action classification on S1. Our method achieved second ranking in the 2019 challenge. Screenshots of the leaderboard at submission and challenge conclusion are in the supplementary material.</p><p>The effect of TBW width: Here, we investigate the effect of the TBW width in training and testing. We varied TBW width in training with b ∈ { T 6 , T 3 , T }, by training three TBN models for each respective window width. We noted little difference in performance. As changing b in training is expensive and performance is subject to the particular optimisation run, we opt for a more conclusive test by focusing on varying b in testing for a single model.</p><p>In testing, we vary</p><formula xml:id="formula_8">b ∈ { T 60 , T 30 , T 25 , T 15 , T 10 , T 5 , T 3 }.</formula><p>This corresponds, in average, to varying the width of TBW on the S1 test set between 60ms and 1200ms. We additionally run with synchrony b ∼ 0. In each case we sample a single TBW, to solely assess the effect of the window size. We repeat this experiment for 100 runs and report mean and standard deviation in <ref type="figure">Fig. 7</ref>, where we compare results for verb and noun classes separately. The figure shows that best performance is achieved for b ∈ T 30 , T 20 , that is on average b ∈ [120ms ± 190ms, 180ms ± 285ms]. TBWs of smaller width show a clear drop in performance, with synchrony comparable to b = T 60 . Note that the 'Sync' baseline provides only approximate synchronisation of modalities, as modalities have different sampling rates (RGB 60fps, flow 30fps, audio 24000kHz). The model shows a degree of robustness for larger TBWs.</p><p>Note that in <ref type="figure">Fig. 7</ref>, we compare widths on a single temporal window in testing. When we temporally aggregate multiple TBWs, the effect of the TBW width is smoothed, and the model becomes robust to TBW widths. Comparison with the state-of-the-art: We compare our work to the baseline results reported in <ref type="bibr" target="#b7">[8]</ref> in <ref type="table">Table 4</ref> on all metrics. First we show that a late fusion with an additional audio stream, outperforms the baseline on top-1 verb accuracy by 7% on S1 and also 7% on S2. Second, we show that our TBN single model, improves these results even further (9%, 10% and 11% on top-1 verb, noun and action accuracy on S1, and 6%, 5% and 6% on S2 respectively). Finally we report results of an Ensemble of five TBNs, where each one is trained with a different TBW width. The ensemble shows additional improvement of up to 3% on top-1 metrics.</p><p>We compare TBN with Attention Clusters <ref type="bibr" target="#b18">[19]</ref>, a previous effort to utilise RGB, Flow, and Audio for action recognition, using pre-extracted features. We use the authors available implementation, and fine-tuned features (TSN, BN-Inception), from the global avg pooling layer (1024D), to provide a fair comparison to TBN, and follow the implementation choices from <ref type="bibr" target="#b18">[19]</ref>. The method from <ref type="bibr" target="#b18">[19]</ref> performs significantly worse than the baseline, as pre-extracted video features are used to learn attention weights.</p><p>At the time of submission, our TBN Ensemble results demonstrated an overall improvement over all state-of-theart, published or anonymous, by 11% on top-1 verb for both S1 and S2. Our method was also ranked 2nd in the 2019 EPIC-Kitchens Action Recognition challenge. Details of the public leaderboard are provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have shown that the TBN architecture is able to flexibly combine the RGB, Flow and Audio modalities to achieve an across the board performance improvement, compared to individual modalities. In particular, we have demonstrated how audio is complementary to appearance and motion for a number of classes; and the pre-eminence of appearance for noun (rather than verb) classes. The performance of TBN significantly exceeds TSN trained on the same data; and provides state-of-the-art results on the public EPIC-Kitchens leaderboard.</p><p>Further avenues for exploration include a model that learns to adjust TBWs over time, as well as implementing class-specific temporal binding windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>This additional material includes a description of the qualitative examples in the supplementary video in App. A. This is followed by the leaderboard results in App B. We further analyse per-class results with and without audio in App. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Qualitative results</head><p>We show selected qualitative results on a held-out validation set, from the publicly available training videos. We hold-out <ref type="bibr">14 (untrimmed)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action Recognition Challenge -Public Leaderboard Results</head><p>In <ref type="figure" target="#fig_4">Fig. 8</ref>, we show results for Ours (TBN, Single Model) and Ours (TBN, Ensemble), as they appeared on the public leaderboard of the EPIC-Kitchens -Action recognition challenge on CodaLab at the time of submission (March 22nd 2019). As noted in the paper, the single model TBN outperforms all other submissions by a clear margin, on both test sets S1 and S2, and the results are further improved using an ensemble of TBNs trained with different TBW widths.</p><p>As the challenge concluded, our model (TBN Ensemble) is ranked 2nd in the leaderboard. A snapshot of the leaderboard for the 2019 challenge is available at https://epic-kitchens.github.io/2019#results. <ref type="figure" target="#fig_5">Fig 9.</ref> It shows the confusion matrices without and with the utilisation of audio for the largest-15 verb and noun classes (in S1). The first confusion matrix show TBN (RGB+Flow), and the second shows TBN (RGB+Flow+Audio). Studying the difference <ref type="figure" target="#fig_3">(Fig 5 right)</ref> clearly demonstrates an increase (blue) in confidence along the diagonal, and a decrease (red) in confusion elsewhere. <ref type="table">Table 5</ref> shows a comparison of the performance of the top largest classes against the less represented classes, for individual modalities, and our proposed TBN. The classes are ranked by the number of examples in training, and the results are reported separately for the top-10% classes versus the rest which we refer to as tail classes. The effect of fusion is more evident on the tail classes, 63% improvement on tail vs. 34% improvement on top-10% for verbs, and 50% improvement on tail vs. 15% improvement on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Per-class Multi-modal Fusion Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A complete version of Fig 5 is available in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Flow Audio Fusion Verb</head><p>Top-10% 41.90 49. <ref type="bibr" target="#b28">29</ref>   <ref type="table">Table 5</ref>: Comparison of mean class accuracy for the top 10% of classes when ranked by class size and the remaining (tail) classes. It can be seen that fusion clearly has a greater effect on the tail classes, where single modalities perform very poorly.</p><p>top-10% for nouns. This finding shows that fusion in TBN decreases the effect of the class-imbalance. Furthermore, it is important to note that audio outperforms RGB and flow on the tail verbs.</p><p>In <ref type="table">Tables 6 and 7</ref>, we show per-class accuracies on S1, on selected verbs and nouns, respectively. We arrange  00.00 40.00 00.00 20.00 sprinkle 00.00 00.00 11.11 00.00 sample 00.00 00.00 07.14 00.00 pat 25.00 33.33 00.00 16.67 <ref type="table">Table 6</ref>: Examples of predicted verbs on S1 for each modality individually, and for TBN (Single Model).</p><p>the chosen set of verbs and nouns in three main categories: top: TBN outperforms the best individual modality, mid: TBN performs comparably with the best modality, and bottom: TBN performs worse than the best individual modality. We shade the rows reflecting these three groups in the order mentioned above.  <ref type="table">Table 7</ref>: Examples of predicted nouns on S1 for each modality individually, and for TBN (Single Model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>A few conclusions could be made from these tables about the advantages of the proposed mid-level fusion:</p><p>1. Fusion can improve results when all modalities are individually performing well for both verb and noun classes (e.g. 'open', 'fridge'), as well as when all modalities are under-performing (e.g. 'scoop', 'salad').</p><p>2. Fusion can though be difficult at times, particularly when two of the three modalities are uninformative (e.g. 'divide', 'fish').</p><p>3. All nouns for which audio is outperforming other modalities have distinct sounds (e.g. 'switch', 'paper').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Similarly, audio is least distinctive when the noun does not have a sound per se or its sound depends on the action (e.g. 'chicken', 'salt').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Code and Models</head><p>Python code of our TBN model, and pre-trained model on EPIC-Kitchens is available at http://github.com/ekazakos/temporal-binding-network</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>As the width of the temporal binding window increases (left to right), modalities (appearance, motion and audio) are fused with varying temporal shifts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: our proposed Temporal Binding Network (TBN). Modalities are sampled within a TBW, and modalityspecific weights (same colour) are shared amongst different inputs. Modalities are fused with mid-level fusion and trained jointly. Predictions from multiple TBWs, possibly overlapping, are averaged. Right: TSN [42] with an additional audio stream performing late fusion. Modalities are trained independently. Note that while in TSN a prediction is made for each modality, TBN produces a single prediction per TBW after fusing all modality representations. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A single TBN block showing architectural details and feature sizes. Outputs from multiple TBN blocks are averaged as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Confusion matrix for the largest-15 verb classes, with audio (left), as well as the difference to the confusion matrix without audio (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Our submission on the action recognition challenge for Seen (top) and Unseen (bottom) Kitchens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Confusion matrices for largest-15 verb classes (top) and the largest-15 noun classes (bottom), without (left) and with (middle) audio, as well as their difference (right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION Comparison of our fusion method to single modality performance. For both splits, the fusion outperforms single modalities. For the seen split, the RGB and Flow modalities perform comparatively, whereas for the unseen split the Flow modality outperforms RGB by a large margin. Audio is comparable to RGB on top-1 verb accuracy for both splits.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Top-1 Accuracy</cell><cell></cell><cell cols="2">Top-5 Accuracy</cell><cell cols="3">Avg Class Precision</cell><cell cols="3">Avg Class Recall</cell></row><row><cell></cell><cell>RGB</cell><cell>45.68</cell><cell>36.80</cell><cell>19.86</cell><cell>85.56</cell><cell>64.19</cell><cell>41.89</cell><cell>61.64</cell><cell>34.32</cell><cell>09.96</cell><cell>23.81</cell><cell>31.62</cell><cell>08.81</cell></row><row><cell></cell><cell>Flow</cell><cell>55.65</cell><cell>31.17</cell><cell>20.10</cell><cell>85.99</cell><cell>56.00</cell><cell>39.30</cell><cell>48.83</cell><cell>26.84</cell><cell>09.02</cell><cell>27.58</cell><cell>24.15</cell><cell>07.89</cell></row><row><cell>S1</cell><cell>Audio</cell><cell>43.56</cell><cell>22.35</cell><cell>14.21</cell><cell>79.66</cell><cell>43.68</cell><cell>27.82</cell><cell>32.28</cell><cell>19.10</cell><cell>07.27</cell><cell>25.33</cell><cell>18.16</cell><cell>06.17</cell></row><row><cell></cell><cell cols="2">TBN (RGB+Flow) 60.87</cell><cell>42.93</cell><cell>30.31</cell><cell>89.68</cell><cell>68.63</cell><cell>51.81</cell><cell>61.93</cell><cell>39.68</cell><cell>18.11</cell><cell>39.99</cell><cell>38.37</cell><cell>16.90</cell></row><row><cell></cell><cell>TBN (All)</cell><cell>64.75</cell><cell>46.03</cell><cell>34.80</cell><cell>90.70</cell><cell>71.34</cell><cell>56.65</cell><cell>55.67</cell><cell>43.65</cell><cell>22.07</cell><cell>45.55</cell><cell>42.30</cell><cell>21.31</cell></row><row><cell></cell><cell>RGB</cell><cell>34.89</cell><cell>21.82</cell><cell>10.11</cell><cell>74.56</cell><cell>45.34</cell><cell>25.33</cell><cell>19.48</cell><cell>14.67</cell><cell>04.77</cell><cell>11.22</cell><cell>17.24</cell><cell>05.67</cell></row><row><cell></cell><cell>Flow</cell><cell>48.21</cell><cell>22.98</cell><cell>14.48</cell><cell>77.85</cell><cell>45.55</cell><cell>29.33</cell><cell>23.00</cell><cell>13.29</cell><cell>05.63</cell><cell>19.61</cell><cell>16.09</cell><cell>07.61</cell></row><row><cell>S2</cell><cell>Audio</cell><cell>35.43</cell><cell>11.98</cell><cell>06.45</cell><cell>69.20</cell><cell>29.49</cell><cell>16.18</cell><cell>22.46</cell><cell>09.41</cell><cell>04.59</cell><cell>18.02</cell><cell>09.79</cell><cell>04.19</cell></row><row><cell></cell><cell cols="2">TBN (RGB+Flow) 49.61</cell><cell>25.68</cell><cell>16.80</cell><cell>78.36</cell><cell>50.94</cell><cell>32.61</cell><cell>30.54</cell><cell>20.56</cell><cell>09.89</cell><cell>21.90</cell><cell>20.62</cell><cell>11.21</cell></row><row><cell></cell><cell>TBN (All)</cell><cell>52.69</cell><cell>27.86</cell><cell>19.06</cell><cell>79.93</cell><cell>53.78</cell><cell>36.54</cell><cell>31.44</cell><cell>21.48</cell><cell>12.00</cell><cell>28.21</cell><cell>23.53</cell><cell>12.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparing top-1 accuracy of All modalities (left) to RGB+Flow (right). Actions are split in segments with 'irrelevant' background sounds, and the 'rest' of the test set.method helps to significantly boost the performance of the tail classes (Fig. 6, right and table in Appendix C), where individual modality performance tends to suffer. Efficacy of audio: We train TBN only with the visual modalities (RGB+Flow) and the results can be seen in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Top-1 Accuracy</cell><cell></cell><cell cols="2">Top-5 Accuracy</cell><cell cols="3">Avg Class Precision</cell><cell cols="3">Avg Class Recall</cell></row><row><cell></cell><cell>Concatenation</cell><cell>64.75</cell><cell>46.03</cell><cell>34.80</cell><cell>90.70</cell><cell>71.34</cell><cell>56.65</cell><cell>55.67</cell><cell>43.65</cell><cell>22.07</cell><cell>45.55</cell><cell>42.30</cell><cell>21.31</cell></row><row><cell>S1</cell><cell cols="2">Context gating [22] 63.77</cell><cell>44.33</cell><cell>33.47</cell><cell>90.04</cell><cell>69.09</cell><cell>54.10</cell><cell>57.31</cell><cell>42.20</cell><cell>21.72</cell><cell>45.63</cell><cell>41.53</cell><cell>20.20</cell></row><row><cell></cell><cell>Gating fusion [4]</cell><cell>61.52</cell><cell>43.54</cell><cell>31.61</cell><cell>89.54</cell><cell>68.42</cell><cell>52.57</cell><cell>52.07</cell><cell>39.62</cell><cell>18.39</cell><cell>42.55</cell><cell>39.77</cell><cell>18.66</cell></row><row><cell></cell><cell>Concatenation</cell><cell>52.69</cell><cell>27.86</cell><cell>19.06</cell><cell>79.93</cell><cell>53.78</cell><cell>36.54</cell><cell>31.44</cell><cell>21.48</cell><cell>12.00</cell><cell>28.21</cell><cell>23.53</cell><cell>12.69</cell></row><row><cell>S2</cell><cell cols="2">Context gating [22] 52.65</cell><cell>27.35</cell><cell>19.16</cell><cell>79.25</cell><cell>52.00</cell><cell>36.40</cell><cell>30.82</cell><cell>23.16</cell><cell>11.72</cell><cell>23.39</cell><cell>25.03</cell><cell>12.58</cell></row><row><cell></cell><cell>Gating fusion [4]</cell><cell>50.16</cell><cell>27.25</cell><cell>18.41</cell><cell>78.80</cell><cell>50.84</cell><cell>34.04</cell><cell>28.42</cell><cell>22.42</cell><cell>12.34</cell><cell>23.92</cell><cell>24.15</cell><cell>13.14</cell></row></table><note>. The table shows that the model's accuracy increases consistently when audio is incorporated, even for the 'irrelevant' segments. Both mod- els (All and RGB+Flow) show a drop in performance for</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION VERB NOUN ACTION</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Top-1 Accuracy</cell><cell></cell><cell cols="2">Top-5 Accuracy</cell><cell cols="3">Avg Class Precision</cell><cell cols="3">Avg Class Recall</cell></row><row><cell></cell><cell>Attention Clusters [19]</cell><cell>40.39</cell><cell>19.37</cell><cell>11.09</cell><cell>78.13</cell><cell>41.73</cell><cell>24.36</cell><cell>21.17</cell><cell>09.65</cell><cell>02.50</cell><cell>14.89</cell><cell>11.50</cell><cell>03.41</cell></row><row><cell></cell><cell>[8] (from leaderboard)</cell><cell>48.23</cell><cell>36.71</cell><cell>20.54</cell><cell>84.09</cell><cell>62.32</cell><cell>39.79</cell><cell>47.26</cell><cell>35.42</cell><cell>11.57</cell><cell>22.33</cell><cell>30.53</cell><cell>09.78</cell></row><row><cell>S1</cell><cell>Ours (TSN [42] w. Audio)</cell><cell>55.49</cell><cell>36.27</cell><cell>23.95</cell><cell>87.04</cell><cell>64.17</cell><cell>44.26</cell><cell>53.85</cell><cell>30.94</cell><cell>13.55</cell><cell>30.60</cell><cell>29.82</cell><cell>11.11</cell></row><row><cell></cell><cell cols="2">Ours (TBN, Single Model) 64.75</cell><cell>46.03</cell><cell>34.80</cell><cell>90.70</cell><cell>71.34</cell><cell>56.65</cell><cell>55.67</cell><cell>43.65</cell><cell>22.07</cell><cell>45.55</cell><cell>42.30</cell><cell>21.31</cell></row><row><cell></cell><cell>Ours (TBN, Ensemble)</cell><cell>66.10</cell><cell>47.89</cell><cell>36.66</cell><cell>91.28</cell><cell>72.80</cell><cell>58.62</cell><cell>60.74</cell><cell>44.90</cell><cell>24.02</cell><cell>46.82</cell><cell>43.89</cell><cell>22.92</cell></row><row><cell></cell><cell>Attention Clusters [19]</cell><cell>32.37</cell><cell>11.95</cell><cell>05.60</cell><cell>69.89</cell><cell>31.82</cell><cell>15.74</cell><cell>17.21</cell><cell>03.86</cell><cell>01.84</cell><cell>11.59</cell><cell>07.94</cell><cell>02.64</cell></row><row><cell></cell><cell>[8] (from leaderboard)</cell><cell>39.40</cell><cell>22.70</cell><cell>10.89</cell><cell>74.29</cell><cell>45.72</cell><cell>25.26</cell><cell>22.54</cell><cell>15.33</cell><cell>06.21</cell><cell>13.06</cell><cell>17.52</cell><cell>06.49</cell></row><row><cell>S2</cell><cell>Ours (TSN [42] w. Audio)</cell><cell>46.61</cell><cell>22.50</cell><cell>13.05</cell><cell>78.19</cell><cell>48.59</cell><cell>29.13</cell><cell>28.92</cell><cell>15.48</cell><cell>06.47</cell><cell>21.58</cell><cell>16.61</cell><cell>07.55</cell></row><row><cell></cell><cell cols="2">Ours (TBN, Single Model) 52.69</cell><cell>27.86</cell><cell>19.06</cell><cell>79.93</cell><cell>53.78</cell><cell>36.54</cell><cell>31.44</cell><cell>21.48</cell><cell>12.00</cell><cell>28.21</cell><cell>23.53</cell><cell>12.69</cell></row><row><cell></cell><cell>Ours (TBN, Ensemble)</cell><cell>54.46</cell><cell>30.39</cell><cell>20.97</cell><cell>81.23</cell><cell>55.69</cell><cell>39.40</cell><cell>32.57</cell><cell>21.68</cell><cell>10.96</cell><cell>27.60</cell><cell>25.58</cell><cell>13.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>videos from the training set, for qualitative examples. Video can be watched at https: //www.youtube.com/watch?v=VzoaKsDvv1o. For each, we show the ground truth, and the predictions of individual modalities (RGB, Flow, Audio) compared with our TBN (Single Model).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>). RGB Flow Audio TBN open 63.32 66.81 51.05 79.08 walk 55.56 11.11 55.56 88.89 turn-on 13.79 13.79 33.79 53.10 scoop 02.27 04.55 02.27 18.18 look 14.29 14.29 00.00 28.57 scrape 25.00 00.00 16.67 25.00 hold 00.00 20.00 00.00 20.00 set 33.33 00.00 16.67 33.33 cook 28.57 00.00 14.29 28.57 finish 00.00 00.00 16.67 16.67 insert 01.79 00.00 07.14 03.57 divide</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Flow Audio TBN paella 25.00 00.00 00.00 50.00 fridge 83.25 80.10 60.73 87.96 hand 56.38 59.73 43.62 76.51 sponge 25.27 32.97 23.08 48.35 salt 40.98 27.87 16.39 62.30 switch 50.00 00.00 75.00 75.00 knife 36.29 52.12 27.80 52.12 salad 14.29 19.05 04.76 19.05 tortilla 42.86 00.00 14.29 42.86 leaf 00.00 10.00 10.00 10.00 pizza 100.00 09.09 36.36 72.73 fish 90.00 00.00 00.00 50.00 bowl 51.49 29.79 19.57 42.98 chicken 31.58 15.79 07.89 26.32 paper 03.70 00.00 14.81 07.41</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Audio visual scene-aware dialog (avsd) track for natural language generation in dstc7</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSTC7 at AAAI2019 Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gated multimodal units for information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y Gmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Gonzlez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLRW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Torralba</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Read</surname></persName>
		</author>
		<title level="m">Deep aligned representations. CoRR, abs/1706.00932</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You-do, i-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teesid</forename><surname>Leelasawassuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osian</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Calway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2.5D visual sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">and Andrew Zisserman. A better baseline for ava</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ActivityNet Workshop at CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7763" to="7774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action recognition with coarse-to-fine deep feature integration and asynchronous fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal keyless attention fusion for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper into first-person activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification. CoRR, abs/1706.06905</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a Text-Video Embedding from Imcomplete and Heterogeneous Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Trespassing the boundaries: Labeling temporal bounds for object interactions in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recalibration of the multisensory temporal window of integration results from changing task demands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Mgevand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Molholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashabari</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Foxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learnable PINs: Cross-modal embeddings for person identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seeing voices and hearing faces: Cross-modal biometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">When correlation implies causation in multisensory integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cesareav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Parise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">O</forename><surname>Spence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="49" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arda</forename><surname>Senocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Actor and observer: Joint modeling of first and third-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suriya</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multimodal multi-stream deep learning for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bappaditya</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giduthuri</forename><surname>Sateesh Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phyo</forename><surname>Phyo San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The effects of visual training on multisensory temporal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">R</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">T</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="page" from="479" to="489" />
		</imprint>
	</monogr>
	<note>Experimental Brain Research</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all we need: Nailing down object-centric attention for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The construct of the multisensory temporal binding window and its dysregulation in developmental disabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="123" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-stream multi-class fusion of deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognizing micro-actions and reactions from paired egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Yonetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal perception and prediction in ego-centric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
