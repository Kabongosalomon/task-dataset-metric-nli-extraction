<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging the Invariant Side of Generative Zero-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Indiana University-Purdue University Indianapolis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shandong Normal Unversity</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Queensland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging the Invariant Side of Generative Zero-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional zero-shot learning (ZSL) methods generally learn an embedding, e.g., visual-semantic mapping, to handle the unseen visual samples via an indirect manner. In this paper, we take the advantage of generative adversarial networks (GANs) and propose a novel method, named leveraging invariant side GAN (LisGAN), which can directly generate the unseen features from random noises which are conditioned by the semantic descriptions. Specifically, we train a conditional Wasserstein GANs in which the generator synthesizes fake unseen features from noises and the discriminator distinguishes the fake from real via a minimax game. Considering that one semantic description can correspond to various synthesized visual samples, and the semantic description, figuratively, is the soul of the generated features, we introduce soul samples as the invariant side of generative zero-shot learning in this paper. A soul sample is the meta-representation of one class. It visualizes the most semantically-meaningful aspects of each sample in the same category. We regularize that each generated sample (the varying side of generative ZSL) should be close to at least one soul sample (the invariant side) which has the same class label with it. At the zero-shot recognition stage, we propose to use two classifiers, which are deployed in a cascade way, to achieve a coarse-to-fine result. Experiments on five popular benchmarks verify that our proposed approach can outperform state-of-the-art methods with significant improvements 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In general, a computer vision algorithm can only handle the objects which appeared in the training dataset. In other words, an algorithm can only recognize the objects which are seen before. However, for some specific real-world applications, we either do not have the training sample of one object or the sample is too expensive to be labeled. For <ref type="bibr" target="#b0">1</ref> Codes and datasets are available at github.com/lijin118/LisGAN instance, we want the approach to trigger a message when it encounters a sample with a rare gene mutation from one species. Unfortunately, we did not have the visual features of the sample for training. The things we have are merely the images taken from normal instances and some semantic descriptions which describe the characteristics of the mutation and how it differs from normal ones. Conventional machine learning algorithms would fail in this task, but a human being would not. A human being is able to recognize an unseen object at the first glance by only reading some semantic descriptions. Inspired by this, zero-shot learning (ZSL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref> is proposed to handle unseen objects by the model which is trained on only seen objects and semantic descriptions about both seen and unseen categories.</p><p>Since the seen and unseen classes are connected by the semantic descriptions, a natural idea is to learn a visualsemantic mapping so that both seen and unseen samples can be compared in the semantic space. For instance, previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref> learn either shallow or deep embeddings for zero-shot learning. These methods handle the unseen samples via an indirect way. Considering that one semantic description can correspond to enormous number of visual samples, the performance of zero-shot learning is restricted with the limited semantic information.</p><p>Recently, thanks to the advances in generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref>, a few approaches are proposed to directly generate unseen samples from the random noises and semantic descriptions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b40">40]</ref>, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. With the generated unseen samples, zero-shot learning can be transformed to a general supervised machine learning problem. In such a learning paradigm, however, the challenges of zero-shot learning have been also passed on to the GANs. In the GANs based paradigms for zero-shot learning, we have to address the spurious and soulless generating problem. Specifically, we generally have only one semantic description, e.g., one attributes vector, one article or one paragraph of texts, for a specific category, but the semantic description is inherently related to a great mass of images in the visual space. For instance, "a tetrapod with a tail" can be mapped to many animals, e.g., cats, dogs and horses. At the same time, some objects from different categories have very similar attributes, such as "tigers" and "ligers". Thus, the generative adversarial networks for zero-shot learning must challenge two issues: 1) how to guarantee the generative diversity based on limited and even similar attributes? 2) how to make sure that each generated sample is highly related with the real samples and corresponding semantic descriptions? However, since deploying GANs to address the ZSL problem is a new topic, most of existing works did not explicitly address the two issues. In this paper, we propose a novel approach which takes the two aspects into consideration and carefully handles them in the formulation.</p><p>At first, to guarantee that the generated samples are meaningful, we propose to generate samples from random noises which are conditioned with the class semantic descriptions. At the same time, we also introduce the supervised classification loss in the GAN discriminator to preserve the inter-class discrimination during the adversarial training. Furthermore, to ensure that each synthesized sample (the varying side of generative zero-shot learning) is highly related with the real ones and corresponding semantic descriptions (the invariant side), we introduce soul samples in this paper, as shown in <ref type="figure">Fig. 3</ref>. For unseen classes, the visual characteristics of a generated sample only depend on the semantic descriptions. Thus, the semantic information is the soul of the generated samples. The soul sample must be not very specific so that it can plainly visualize the most semantically-meaningful aspects and relate to as many samples as possible. For the seen images, therefore, we define that a soul sample is an average representation of them. For the generated samples, we regularize them to be close to soul samples. Thus, we can guarantee that each generated sample is highly related with the real ones and corresponding semantic descriptions.</p><p>To summarize, the main contributions of this paper are: 1) We propose a novel ZSL method LisGAN which takes advantage of generative adversarial networks. Specifically, we deploy the conditional GANs to tackle the two issues: generative diversity and generative reliability. To improve the quality of generated features, we introduce soul samples which are defined as the representations of each category. By further considering the multi-view nature of different images, we propose to define multiple soul samples for each class. We regularize each generated sample to be close to at least one soul sample so that the varying side in generative zero-shot learning would not be divorced from the invariant side.</p><p>2) At the zero-shot recognition stage, we propose that if we have high confidence in recognizing an unseen sample, the sample (with its assigned pseudo label) will be leveraged as the reference to recognize other unseen samples. Specifically, we propose to use two classifiers, which are deployed in a cascade way, to achieve a coarse-to-fine result. We also report a simple yet efficient method to measure the classification confidence in this paper.</p><p>3) Extensive experiments on five widely used datasets verify that our proposed method can outperform state-ofthe-art methods with remarkable improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Zero-Shot Learning</head><p>Inspired by the human ability that one can recognize an object at the first glance by only knowing some semantic descriptions of it, zero-shot learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">35]</ref> aims to learn a model with good generalization ability which can recognize unseen objects by only giving some semantic attributes. A typical zero-shot learning model is trained on visual features which only contain the seen samples and semantic features which contain both seen and unseen samples. Since the seen objects and unseen ones are only connected in the semantic space and the unseen objects need to be recognized by the visual features, zero-shot learning methods generally learn a visual-semantic embedding with the seen samples. At the zero-shot classification stage, unseen samples are projected into the semantic space and labeled by semantic attributes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>. Instead of learning a visual-semantic embedding, some previous works also propose to learn a semantic-visual mapping so that the unseen samples can be represented by the seen ones <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. In addition, there are also some works to learn an intermediate space shared by the visual features and semantic features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39]</ref>. Besides, ZSL is also related with domain adaptation and cold-start recommendation <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>.</p><p>From the recent literatures, typical zero-shot learning tasks are zero-shot classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">36]</ref>, zero-shot retrieval <ref type="bibr" target="#b21">[22]</ref> and generalized zero-shot recognition <ref type="bibr" target="#b32">[32]</ref>. The main difference between zero-shot learning and generalized zero-shot recognition is that the former only classifies the unseen samples in the unseen category and the latter recognizes samples, which can be either seen ones and unseen ones, in both seen and unseen categories.</p><p>It is easy to observe that conventional zero-shot learning methods are indirect. They usually need to learn a space Random Noises</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN Discriminator</head><p>Fake visual samples Real/Fake?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN Generator</head><p>Real Seen Samples  mapping function. Recently, by taking advantage of generative adversarial networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>, several methods <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b40">40]</ref> are proposed to directly generate unseen samples from their corresponding attributes, which converts the conventional zero-shot learning to a classic supervised learning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative Adversarial Nets</head><p>A typical generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref> consists of two components: a generator and a discriminator. The two players are trained in an adversarial manner. Specifically, the generator G tries to generate fake images from input noises to fool the discriminator, while the discriminator D attempts to distinguish real images and fake ones. In general, the input of G is random noise and the output is the synthesized image. The inputs of D are both real images and fake images, the output is a probability distribution. In this paper, we deploy G to generate sample features instead of image pixels.</p><p>Although GANs has shown quite impressive results and profound impacts, the vanilla GAN is very hard to train. Wasserstein GANs (WGANs) <ref type="bibr" target="#b2">[3]</ref> presents an alternative to traditional GAN training. WGANs can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. In addition, conditional GANs <ref type="bibr" target="#b22">[23]</ref> are proposed to enhance the outputs of traditional GANs. With conditional GANs, one can incorporate the class labels and other information into the generator and discriminator to synthesize specified samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Definitions and Notations</head><p>Given n labeled seen samples with both visual features X ∈ R d×n and semantic descriptions A ∈ R m×n for training, zero-shot learning aims to recognize n u unknown visual samples X u ∈ R d×nu which only have semantic attributes A u ∈ R m×nu for training. Let Y and Y u be the label space of X and X u , respectively, in zero-shot learning we have Y ∩ Y u = ∅. Suppose that we have C and C u categories in total for seen data and unseen data, respectively, classical zero-shot learning recognizes X u by only searching in C u , while generalized zero-shot learning searches in C ∪ C s . The semantic descriptions A and A u are either provided as binary/numerical vectors or word embedding/RNN features. Each semantic description a corresponds to a category y. Formally, given {X, A, Y } and {A u , Y u } for training, the task of zero shot learning is to learn a function f : X u → Y u and generalized zero shot learning is to learn a function f :</p><formula xml:id="formula_0">{X , X u } → Y ∪ Y u .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Idea</head><p>In this paper, we take advantage of GANs to directly generate fake visual features for unseen samples from random noises and the semantic descriptions. Then, the synthesized visual features are used as references to classify real unseen samples. Since we only have A u and the GAN discriminator cannot access X u in the training stage, the real or fake game, therefore, cannot be played. Thus, we mainly train our GAN on the seen classes. At the same time, we deploy the conditional GANs so that the class embedding can be incorporated into both generator G and discriminator D. Since {A, Y } and {A u , Y u } are interconnected, i.e., A and A u have the same semantic space, the conditional GAN which generates high-quality samples for seen classes is also expected to generate high-quality samples for unseen categories. The main idea of this paper is illustrated in <ref type="figure" target="#fig_2">Fig 2.</ref> Compared with existing methods which also deploy GANs for zero-shot learning, our novelty comes from two aspects. The first one is that we introduce multiple soul samples per class to regularize the generator. The second is that we leverage the unseen samples which are classified with high confidence to facilitate the subsequent unseen samples. Experiments reported in section 5 show that we can achieve a significant improvement against state-ofthe-art methods on various datasets. <ref type="figure">Figure 3</ref>. Soul samples of the horse category. Considering the nature multi-view property of visual objects, e.g., real images of an object are usually captured from different views, we propose to learn multiple soul samples for each class. By such a formulation, the domain shift issue caused by different views can be alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Train the LisGAN</head><p>Given the seen samples {X, A, Y }, the attributes A u of the unseen sample and random noises z ∼ N (0, 1), the GAN generator G uses the input a and nosies z to synthesize fake features. At the same time, the GAN discriminator D takes the features of real image x and G(z, a) as inputs to discriminate whether an input feature is real or fake. Formally, the loss of G can be formulated as follows:</p><formula xml:id="formula_1">L G = −E[D(G(z, a))] − λE[logP (y|G(z, a))],<label>(1)</label></formula><p>where the first term is the Wasserstein loss <ref type="bibr" target="#b2">[3]</ref> and the second one is the supervised classification loss on the synthesized features, λ &gt; 0 is a balancing parameter. Similarly, the loss of the discriminator can be formulated as follows:</p><formula xml:id="formula_2">L D = E[D(G(z, a))] − E[D(x)] − λ(E[logP (y|G(z, a))] + E[logP (y|x)]) − βE[( ∇xD(x) 2 − 1) 2 ],<label>(2)</label></formula><p>where β &gt; 0 is a hyper-parameter. The fourth term, similar with the third one, is a supervised classification loss on real samples. The last term is used to enforce the Lipschitz constraint <ref type="bibr" target="#b8">[9]</ref>, in whichx = µx + (1 − µ)G(z, a) with µ ∼ U (0, 1). As suggested in <ref type="bibr" target="#b8">[9]</ref>, we fix β = 10.</p><p>In our model, we take the CNN features of samples as the visual input X. Both the generator and discriminator are implemented with fully connected layers and ReLU activations. Thus, the model is feasible to incorporate into different CNN architectures. At the same time, the output of the generator is directly visual features rather than image pixels. By optimizing the above two-player minimax game, the conditional GAN generator is able to synthesize fake features of the seen images with the class embedding A. Since the unseen objects share the same semantic space with the seen samples, the conditional GAN generator can also synthesize visual features for unseen categories via A u . With the optimization problem in Eq. (1) and Eq. (2), our model can guarantee the generative diversity with similar attributes. With the supervised classification loss, it can also ensure that the learned features are discriminative for further classification. However, the model does not explicitly address the quality of the generated features. In this paper, to make sure that each generated feature is highly related with the semantic descriptions and real samples, we introduce soul samples to regularize the generator. Since the soul samples of a category should reflect the most remarkable characteristics of the class as much as possible, we deploy the average representation of all samples from the category c to define the soul sample of c, which is similar with prototypical networks for few-shot learning <ref type="bibr" target="#b31">[31]</ref>. Furthermore, considering the nature multi-view property of real samples, as shown in <ref type="figure">Fig. 3</ref>, we further propose that a category c should have multiple soul samples to address the multi-view issue. To this end, we first group the real features of one seen class into k clusters. For simplicity, we fix k = 3 in this paper. Then, we calculate a soul sample for each cluster.</p><formula xml:id="formula_3">Let {X c 1 , X c 2 , · · · , X c k } be the k clusters of category c, the soul samples S c = {s c 1 , s c 2 , · · · , s c k } are defined as: s c k = 1 |X c k | xi∈X c k x i .<label>(3)</label></formula><p>Similarly, for the generated fake features, we can also define the soul samples c k as:</p><formula xml:id="formula_4">s c k = 1 |X c k | xi∈X c kx i ,<label>(4)</label></formula><p>wherex i = G(z, a) is a generated fake feature.</p><p>In this paper, we encourage that each generated samplẽ x for class c should be close to at least one soul sample s c . Formally, we introduce the following regularization:</p><formula xml:id="formula_5">L R1 = 1 n1 n1 i=1 min j∈[1,k] x i − s c j 2 2 ,<label>(5)</label></formula><p>where n 1 is the number of generated samples and k is the number of soul samples per class. At the same time, since the soul samples can also be seen as the centroid of one cluster, we encourage that the fake soul samples should be close to at least one real soul sample from the same class, which can be formulated as:</p><formula xml:id="formula_6">L R2 = 1 C C c=1 min j∈[1,k] s c j − s c j 2 2 ,<label>(6)</label></formula><p>where C is the number of total categories. With the two regularizations L R1 and L R2 , our model avoids to generate soulless features. Each of the generated features would be close to the real ones, which guarantees the quality of the fake features. From another perspective, L R1 is an individual regularization which addresses single samples and L R2 is a group regularization which takes care of a cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Predict Unseen Samples</head><p>Once the GAN is trained to be able to generate visual features for seen classes, it can also synthesize visual features for the unseen ones with random noises and semantic attributes A u . Then, the zero-shot learning is automatically converted to a supervised learning problem. Specifically, we can train a softmax classifier on the generated features and classify the real unseen features. The softmax is formulated as minimizing the following negative log likelihood:</p><formula xml:id="formula_7">min θ − 1 |X | (x,y)∈(X ,Y) logP (y|x; θ),<label>(7)</label></formula><p>where θ is the training parameter and</p><formula xml:id="formula_8">P (y|x; θ) = exp(θ y x) N i=1 exp(θ i x) .<label>(8)</label></formula><p>In this paper, we further propose that we can leverage an unseen sample if we have sufficient confidence in believing that the sample has been correctly classified. Since the output of the softmax layer is a vector which contains the probabilities of all possible categories, the entropy of the vector can be used to measure the certainty of the results. If a probability vector has lower entropy, we have more confidence of the results. Therefore, we leverage the samples which have low classification entropy and deploy them as references to classify the other unseen samples. Specifically, we calculate the sample entropy by:</p><formula xml:id="formula_9">E(y) = − C c=1 y c log y c .<label>(9)</label></formula><p>In our model, we deploy two classifiers via a cascade manner to predict the unseen samples. The first classifier is used to evaluate the classification confidence and the second is used to leverage the correctly classified samples. In our zero-shot recognition, the first classifier is a softmax trained on the generated fake features, while second classifier can be either a trained classifier, e.g., softmax classifier, SVM, or just a training-free classifier, e.g., NNC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>APascal-aYahoo (aPaY) contains 32 categories from both PASCAL VOC 2008 dataset and Yahoo image search engine. Specifically, 20 classes are from PASCAL and 12 classes are from Yahoo. The total number of aPaY is 15,339. Following previous work <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b40">40]</ref>, we deploy the PASCAL VOC 2008 as seen dataset and the Yahoo as unseen one. An additional 64-dimensional attribute vector is annotated for each category.</p><p>Animals with Attributes (AwA) <ref type="bibr" target="#b13">[14]</ref> consists of 30,475 images of 50 animals classes. The animals classes are aligned with Osherson's classical class/attribute matrix, thereby providing 85 numeric attribute values for each class.</p><p>Caltech-UCSD Birds-200-2011 (CUB) <ref type="bibr" target="#b33">[33]</ref> is an extended version of the CUB-200 dataset. CUB is a challenging dataset which contains 11,788 images of 200 bird species. Each species is associated with a Wikipedia article and organized by scientific classification (order, family, genus, species). A vocabulary of 28 attribute groupings and 312 binary attributes were associated with the dataset based on an online tool for bird species identification. Oxford Flowers (FLO) <ref type="bibr" target="#b24">[25]</ref> dataset consists of 8,189 images which comes from 102 flower categories. Each class consists of between 40 and 258 images. The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories. For this dataset, we use the same semantic descriptions provided by Reed et al. <ref type="bibr" target="#b27">[28]</ref>.</p><p>SUN attributes (SUN) <ref type="bibr" target="#b26">[27]</ref> is a large-scale scene attribute dataset, which spans 717 categories and 14,340 images in total. Each category includes 102 attribute labels.</p><p>For clarity, we report the dataset statistics and zero-shot split settings in <ref type="table" target="#tab_0">Table 1</ref>. The zero-shot splits of aPaY, AwA, CUB and SUN are same with previous work <ref type="bibr" target="#b35">[35]</ref> and the splits of FLO is same with <ref type="bibr" target="#b27">[28]</ref>. For the real CNN features, we follow previous work <ref type="bibr" target="#b34">[34]</ref> to extract 2048-dimensional features from ResNet-101 <ref type="bibr" target="#b9">[10]</ref> which is pre-trained on Im-ageNet. For the semantic descriptions, we use the default attributes included in the datasets. Specifically, since FLO did not provide attributes with the dataset, we use the 1024dimensional RNN descriptions via the model of <ref type="bibr" target="#b27">[28]</ref>. For fair comparisons, all of our experimental settings are same with the protocols reported in previous work <ref type="bibr" target="#b34">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation and Compared Methods</head><p>In our model, the GAN is implemented via multilayer perceptron with Rectified Linear Unit (ReLU) activation. Specifically, the generator G contains a fully connected layer with 4,096 hidden units. The noise z is conditioned by the semantic description a and then severed as the inputs of G. An additional ReLU layer is deployed as the output layer of G which outputs the synthesized fake features. The discriminator D takes the real features and the synthesized fake features from G and processes them via an FC layer, a Leaky ReLU layer, an FC layer and a ReLU layer. The discriminator has two branches for output. One is used to tell fake from real and the other is a standard n-ways classifier to predict the correct category of each sample. In this paper, we set λ = 0.01 and β = 10. The weight for two regularizations are all set to 0.01. The sample entropy threshold is set to be smaller than the median of all entropies. One can also tune the hyper-parameters by cross-validation.</p><p>The compared methods are representative ones pub-  lished in the fast few years and the state-of-the-art ones reported very recently. Specifically, we compare our approach with: DAP <ref type="bibr" target="#b14">[15]</ref>, CONSE <ref type="bibr" target="#b25">[26]</ref>, SSE <ref type="bibr" target="#b38">[38]</ref>, DeViSE <ref type="bibr" target="#b6">[7]</ref>, SJE <ref type="bibr" target="#b1">[2]</ref>, ESZSL <ref type="bibr" target="#b28">[29]</ref>, ALE <ref type="bibr" target="#b0">[1]</ref>, SYNC <ref type="bibr" target="#b3">[4]</ref>, SAE <ref type="bibr" target="#b12">[13]</ref>, DEM <ref type="bibr" target="#b37">[37]</ref>, GAZSL <ref type="bibr" target="#b40">[40]</ref> and f-CLSWGAN <ref type="bibr" target="#b34">[34]</ref>. Following previous work <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b40">40]</ref>, we report the average per-class top-1 accuracy for each of the evaluated method. Specifically, for classic zero-shot learning, we report the top-1 accuracy of unseen samples by only searching the unseen label space. However, for the generalized zero-shot learning, we report the accuracy on both seen classes and unseen classes with the same settings in <ref type="bibr" target="#b35">[35]</ref>. Some of the results reported in this paper are also cited from <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Zero-shot Learning</head><p>We report the zero-shot learning results on the five datasets in <ref type="table" target="#tab_1">Table 2</ref>. In these experiments, the possible categories of unseen samples are searched from only Y u . It can be seen that our method achieves the best on four of the five evaluations. We also achieved state-of-the-art result on the last dataset. Specifically, we achieved 2.6% improvement over the state-of-the-art method on aPaY dataset. We also achieved 2.4%, 1.5% and 2.4% on AWA, CUB and FLO. From the results, we can also observe that the GANbased methods, e.g., GAZSL, f-CLSWGAN and ours, generally perform better than embedding ones, e.g., SSE, ALE and SAE. The embedding methods handle the unseen samples via an indirect manner, while the GAN method directly handle it by converting it to a supervised learning task. The results suggest that GAN could be a promising way to address zero-shot learning problem in the future. Apart from generating visual features from noises, GANs can also be used for semantic augmentation in zero-shot learning. In our future work, we will incorporate semantic data augmentation in our model to cover more unseen samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generalized Zero-shot Learning</head><p>We further report the experiment results of generalized zero-shot learning in <ref type="table" target="#tab_2">Table 3 and Table 4</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows the results on aPaY dataset and <ref type="table" target="#tab_3">Table 4</ref> shows the results on the other 4 datasets. In generalized zero-shot learning, the seen classes are split into two parts: one for training and the other for test. At the test stage, both seen and unseen samples are recognized by searching the possible categories from Y ∪ Y u . The splits of seen classes can be seen in <ref type="table" target="#tab_0">Table 1</ref> and more details can be found in previous work <ref type="bibr" target="#b35">[35]</ref>. Since both seen and unseen classes are tested in generalized zeroshot learning, we also report the harmonic mean of seen accuracy and unseen accuracy in the tables.</p><p>From the results in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref>, we can draw the similar conclusions as from <ref type="table" target="#tab_1">Table 2</ref>. Our approach performs better than existing methods. Our results are significantly better on the unseen samples and harmonic mean, which means our proposed method has a much better generalized ability. It is able to classify the samples into the true category. Our approach is stably dependable on both seen and unseen classes. Although some previous methods, e.g., DAP, ESZSL and SAE, perform well on the conventional zero-shot learning setting with unseen samples, their performances degrade dramatically on the generalized zero-shot learning. They tend to mess up when the possible categories of unseen samples become large. Thus, the applicability of these methods is limited in real applications.</p><p>The harmonic mean is more stable regarding outliers than the arithmetic mean and geometric mean. Thus, from the results reported in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref>, we can also observe that our method is more stable than the compared methods. It avoids extreme results on different evaluations. In terms of the harmonic mean, we achieved up to 2.8%, 2.7%, 1.9%, 2.7% and 0.8% improvements on aPaY, AwA, CUB, FLO and SUN, respectively. The average is over the five is 2.2%. Although our method did not perform the best on some seen categories, it performs almost neck to neck with the previous state-of-the-arts. These results verified the outstanding generalization ability of our method.</p><p>Considering the fact that both GAZSL and f-CLSWGAN leverage GANs to synthesize unseen samples, the performance boost of our method can be attributed to two aspects. One is that we introduce soul samples to guarantee that each generated sample is highly related with the semantic description. The soul samples regularizations also address the multi-view characteristic. As a result, it can automatically take care of the domain-shift problem caused by different views in zero-shot learning. The other aspect is that our cascade classifier is able to leverage the results from the first classifier and strengthen the second one. Such a formulation provides the results via a coarse-to-fine manner. The results verify that it is beneficial to leverage the invariant side of generative ZSL. The invariant side regularizations guarantee that each synthesized sample is highly related with the real ones and corresponding semantic descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Model Analysis</head><p>In this section, we analyze our model under different settings. Since our GAN generates visual features rather than image pixels, it is inappropriate to show the synthesized results with images. We will analyze our model in terms of the generalization ability and stability. The sensitivity of hyper-parameters are also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Class-wise Accuracy</head><p>To show the experimental results of our method in a more fine-grained scale, we report the confusion matrix of f-CLSGAN and our method on the aPaY dataset in <ref type="figure" target="#fig_3">Fig. 4</ref>. Compared with <ref type="figure" target="#fig_3">Fig. 4(a)</ref> and <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, we can see that our method generally has better accuracy on most of the categories. Notably, we can see that the accuracy on category "tvmonitor", "donkey" and "jetski" are boosted around 10% against f-CLSWGAN. There is also a common phenomenon that the ZSL methods perform poorly on some unseen categories. We will investigate fine-grained / classwise zero-shot learning in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Parameter Sensitivity</head><p>In our model, we have several hype-parameters to tune. The parameter β controls the Lipschitz constraint. As suggested in <ref type="bibr" target="#b8">[9]</ref>, we fix β = 10 in this paper. The parameter λ balances the supervised classification loss, its influence is reported in <ref type="figure">Fig. 5(a)</ref>. In our formulation, we also introduced a weight coefficient to adjust the contribution of soul sample regularizations. Its sensitivity is reported in <ref type="figure">Fig. 5(b)</ref>. Similarly, <ref type="figure">Fig. 5</ref>(c) and <ref type="figure">Fig. 5(d)</ref> show the effects of sample entropy threshold and synthesized sample numbers per class, respectively. From the results, we can see that the weight parameters for classification loss and soul sample regularization should be relatively small. The sample entropy threshold is recommended to set to be smaller than the median of all samples. The more synthesized samples, the better results generally there will be. However, more samples also introduce more noises and need more training costs. In practice, we suggest to split the seen categories as training set and validation set for cross-validation. Specifically. we report the sensitivity of k in <ref type="figure" target="#fig_5">Fig. 7(a)</ref>. Since k is not sensitive, we fix k = 3 to reduce the computation cost.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Model Stability</head><p>Since our approach deploys an adversarial training manner, it needs several epochs to achieve the balance between the generator and the discriminator. In <ref type="figure">Fig. 6</ref>, we report the zero-shot learning and generalized zero-shot learning results of our method with different epochs in terms of testing error. The results reflect the training stability of our model. It can be seen that our model shows a stable training trend with the increasing of training epochs. Although there are small fluctuations, our model can achieve a stable results with 30 epochs. For different real-world applications, one can deploy cross-validation to choose the optimal epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Ablation Analysis</head><p>Conditional WGAN has been a cutting-edge but popular technique in computer vision tasks. It is more like an infrastructure in the community. Thus, we fix the conditional WGAN and focus on soul sample regularization and the cascade classifier in this section. We first report the results of plain conditional WGAN. Then, we introduce additional components into the model and observe the effects of them. The results of ablation analysis are reported in <ref type="figure" target="#fig_5">Fig. 7(b)</ref>. The five settings demonstrate that different components in our framework are all significant. The supervised loss guarantees that the generated features are discriminative. The soul samples regularizations constrain that each synthesized sample is close to the very semantic descriptions. Multiple soul samples per class provide a relaxed solution to handle domain shift problem caused by the multi-view issue. The cascade classifier leverages the result of sample entropy and presents a more fine accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel zero-shot learning method by taking advantage of generative adversarial networks. Specially, we deploy conditional WGAN to synthesize fake unseen samples from random noises. To guarantee that each generated sample is close to real ones and their corresponding semantic descriptions, we introduce soul samples regularizations in the GAN generator. At the zero-shot recognition stage, we further propose to use a cascade classifier to fine-tune the accuracy. Extensive experiments on five popular benchmarks verified that our method can outperform previous state-of-the-art ones with remarkable advances. In our future work, we will explore data augmentation with GAN which can be used to synthesize more semantic descriptions to cover more unseen samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Zero-shot learning with GANs, i.e., generative ZSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Idea illustration of our LisGAN (Leveraging invariant side GAN). We train a conditional WGAN to generate fake unseen images from random noises and semantic attributes. Multiple soul samples for each class are introduced to regularize the generator. Unseen samples classified with high confidence are leveraged to fine-tune final results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>0.05 0.02 0.03 0.04 0.20 0.03 0.01 0.38 0.18 0.0 0.06 0.01 0.04 0.04 0.03 0.04 0.22 0.01 0.01 0.31 0.13 0.09 0.08 0.0 0.0 0.79 0.0 0.02 0.0 0.01 0.02 0.0 0.0 0.14 0.02 0.0 0.0 0.06 0.18 0.15 0.01 0.01 0.46 0.0 0.0 0.11 0.02 0.0 0.0 0.05 0.04 0.20 0.0 0.02 0.64 0.0 0.0 0.04 0.0 0.02 0.0 0.0 0.04 0.07 0.04 0.01 0.0 0.18 0.43 0.0 0.20 0.0 0.0 0.02 0.0 0.01 0.0 0.89 0.06 0.0 0.0 0.02 0.0 0.0 0.0 0.01 0.0 0.02 0.01 0.01 0.95 0.01 0.0 0.01 0.0 0.0 0.12 0.0 0.0 0.01 0.02 0.0 0.0 0.45 0.33 0.06 0.01 0.02 0.04 0.0 0.01 0.0 0.04 0.0 0.0 0.63 0.23 0.0 0.03 0.0 0.0 0.01 0.0 0.02 0.01 0.06 0.02 0.0 0.0 0.84 0.04 0.0 0.0 0.0 0.29 0.03 0.01 0.04 0.20 0.0 0.0 0.13 0The confusion matrix on the evaluation of aPaY dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Parameter sensitivity. The horizontal axis of (c) indicates the sample entropy threshold is not larger than the entropy of x% samples where all sample entropies are sorted from small to large., e.g., 50 indicates the sample entropy threshold is set as the median of all sample entropies. The horizontal axis of (d) indicates synthesized sample numbers per class. The trends of training stability. For GZSL in (b), we report the harmonic mean on both seen and unseen samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The results of different k (number of clusters) and ablation analysis of ZSL with aPaY.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics. The (number) in # Seen Classes indicates the number of seen classes used for test in the GZSL.</figDesc><table><row><cell>Dataset</cell><cell>aPaY</cell><cell>AwA</cell><cell>CUB</cell><cell>FLO</cell><cell>SUN</cell></row><row><cell># Samples</cell><cell cols="3">15,339 30,475 11,788</cell><cell>8,189</cell><cell>14,340</cell></row><row><cell># Attributes</cell><cell>64</cell><cell>85</cell><cell>312</cell><cell>1,024</cell><cell>102</cell></row><row><cell># Seen Classes</cell><cell>20 (5)</cell><cell cols="4">40 (13) 150 (50) 82 (20) 645 (65)</cell></row><row><cell># Unseen Classes</cell><cell>12</cell><cell>10</cell><cell>50</cell><cell>20</cell><cell>72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The top-1 accuracy (%) of zero-shot learning on different datasets. The best results are highlighted with bold numbers.</figDesc><table><row><cell>Methods</cell><cell>aPaY</cell><cell>AwA</cell><cell>CUB</cell><cell cols="2">FLO SUN</cell></row><row><cell>DAP [15]</cell><cell>33.8</cell><cell>44.1</cell><cell>40.0</cell><cell>-</cell><cell>39.9</cell></row><row><cell>CONSE [26]</cell><cell>26.9</cell><cell>45.6</cell><cell>34.3</cell><cell>-</cell><cell>38.8</cell></row><row><cell>SSE [38]</cell><cell>34.0</cell><cell>60.1</cell><cell>43.9</cell><cell>-</cell><cell>51.5</cell></row><row><cell>DeViSE [7]</cell><cell>39.8</cell><cell>54.2</cell><cell>52.0</cell><cell>45.9</cell><cell>56.5</cell></row><row><cell>SJE [2]</cell><cell>32.9</cell><cell>65.6</cell><cell>53.9</cell><cell>53.4</cell><cell>53.7</cell></row><row><cell>ESZSL [29]</cell><cell>38.3</cell><cell>58.2</cell><cell>53.9</cell><cell>51.0</cell><cell>54.5</cell></row><row><cell>ALE [1]</cell><cell>39.7</cell><cell>59.9</cell><cell>54.9</cell><cell>48.5</cell><cell>58.1</cell></row><row><cell>SYNC [4]</cell><cell>23.9</cell><cell>54.0</cell><cell>55.6</cell><cell>-</cell><cell>56.3</cell></row><row><cell>SAE [13]</cell><cell>8.3</cell><cell>53.0</cell><cell>33.3</cell><cell>-</cell><cell>40.3</cell></row><row><cell>DEM [37]</cell><cell>35.0</cell><cell>68.4</cell><cell>51.7</cell><cell>-</cell><cell>61.9</cell></row><row><cell>GAZSL [40]</cell><cell>41.1</cell><cell>68.2</cell><cell>55.8</cell><cell>60.5</cell><cell>61.3</cell></row><row><cell>f-CLSWGAN [34]</cell><cell>40.5</cell><cell>68.2</cell><cell>57.3</cell><cell>67.2</cell><cell>60.8</cell></row><row><cell>LisGAN [Ours]</cell><cell>43.1</cell><cell>70.6</cell><cell>58.8</cell><cell>69.6</cell><cell>61.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">The results (top-1 accuracy %) of generalized zero-</cell></row><row><cell cols="2">shot learning on aPaY dataset.</cell><cell cols="2">The Mean in this table</cell></row><row><cell cols="4">is the harmonic mean of seen and unseen samples, i.e.,</cell></row><row><cell cols="3">Mean=(2*Unseen*Seen)/(Unseen+Seen).</cell><cell></cell></row><row><cell>Methods</cell><cell>Unseen</cell><cell>aPaY Seen</cell><cell>Mean</cell></row><row><cell>DAP [15]</cell><cell>4.8</cell><cell>78.3</cell><cell>9.0</cell></row><row><cell>CONSE [26]</cell><cell>0.0</cell><cell>91.2</cell><cell>0.0</cell></row><row><cell>SSE [38]</cell><cell>0.2</cell><cell>78.9</cell><cell>0.4</cell></row><row><cell>DeViSE [7]</cell><cell>4.9</cell><cell>76.9</cell><cell>9.2</cell></row><row><cell>SJE [2]</cell><cell>3.7</cell><cell>55.7</cell><cell>6.9</cell></row><row><cell>ESZSL [29]</cell><cell>2.4</cell><cell>70.1</cell><cell>4.6</cell></row><row><cell>ALE [1]</cell><cell>4.6</cell><cell>73.7</cell><cell>8.7</cell></row><row><cell>SYNC [4]</cell><cell>7.4</cell><cell>66.3</cell><cell>13.3</cell></row><row><cell>SAE [13]</cell><cell>0.4</cell><cell>80.9</cell><cell>0.9</cell></row><row><cell>DEM [37]</cell><cell>11.1</cell><cell>75.1</cell><cell>19.4</cell></row><row><cell>GAZSL [40]</cell><cell>14.2</cell><cell>78.6</cell><cell>24.0</cell></row><row><cell>f-CLSWGAN [34]</cell><cell>32.9</cell><cell>61.7</cell><cell>42.9</cell></row><row><cell>LisGAN [Ours]</cell><cell>34.3</cell><cell>68.2</cell><cell>45.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The results (top-1 accuracy %) of generalized zero-shot learning. The Mean in this table is the harmonic mean of seen and unseen samples, i.e., Mean=(2*Unseen*Seen)/(Unseen+Seen). The best results are highlighted with bold numbers.</figDesc><table><row><cell>Methods</cell><cell cols="12">AwA Unseen Seen Mean Unseen Seen Mean Unseen Seen Mean Unseen Seen Mean CUB FLO SUN</cell></row><row><cell>DAP [15]</cell><cell>0.0</cell><cell>88.7</cell><cell>0.0</cell><cell>1.7</cell><cell>67.9</cell><cell>3.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.2</cell><cell>25.2</cell><cell>7.2</cell></row><row><cell>CONSE [26]</cell><cell>0.4</cell><cell>88.6</cell><cell>0.8</cell><cell>1.6</cell><cell>72.2</cell><cell>3.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6.8</cell><cell>39.9</cell><cell>11.6</cell></row><row><cell>SSE [38]</cell><cell>7.0</cell><cell>80.5</cell><cell>12.9</cell><cell>8.5</cell><cell>46.9</cell><cell>14.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.1</cell><cell>36.4</cell><cell>4.0</cell></row><row><cell>DeViSE [7]</cell><cell>13.4</cell><cell>68.7</cell><cell>22.4</cell><cell>23.8</cell><cell>53.0</cell><cell>32.8</cell><cell>9.9</cell><cell>44.2</cell><cell>16.2</cell><cell>16.9</cell><cell>27.4</cell><cell>20.9</cell></row><row><cell>SJE [2]</cell><cell>11.3</cell><cell>74.6</cell><cell>19.6</cell><cell>23.5</cell><cell>59.2</cell><cell>33.6</cell><cell>13.9</cell><cell>47.6</cell><cell>21.5</cell><cell>14.7</cell><cell>30.5</cell><cell>19.8</cell></row><row><cell>ESZSL [29]</cell><cell>5.9</cell><cell>77.8</cell><cell>11.0</cell><cell>2.4</cell><cell>70.1</cell><cell>4.6</cell><cell>11.4</cell><cell>56.8</cell><cell>19.0</cell><cell>11.0</cell><cell>27.9</cell><cell>15.8</cell></row><row><cell>ALE [1]</cell><cell>14.0</cell><cell>81.8</cell><cell>23.9</cell><cell>4.6</cell><cell>73.7</cell><cell>8.7</cell><cell>13.3</cell><cell>61.6</cell><cell>21.9</cell><cell>21.8</cell><cell>33.1</cell><cell>26.3</cell></row><row><cell>SYNC [4]</cell><cell>10.0</cell><cell>90.5</cell><cell>18.0</cell><cell>7.4</cell><cell>66.3</cell><cell>13.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7.9</cell><cell>43.3</cell><cell>13.4</cell></row><row><cell>SAE [13]</cell><cell>1.1</cell><cell>82.2</cell><cell>2.2</cell><cell>0.4</cell><cell>80.9</cell><cell>0.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.8</cell><cell>18.0</cell><cell>11.8</cell></row><row><cell>DEM [37]</cell><cell>30.5</cell><cell>86.4</cell><cell>45.1</cell><cell>11.1</cell><cell>75.1</cell><cell>19.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20.5</cell><cell>34.3</cell><cell>25.6</cell></row><row><cell>GAZSL [40]</cell><cell>19.2</cell><cell>86.5</cell><cell>31.4</cell><cell>23.9</cell><cell>60.6</cell><cell>34.3</cell><cell>28.1</cell><cell>77.4</cell><cell>41.2</cell><cell>21.7</cell><cell>34.5</cell><cell>26.7</cell></row><row><cell>f-CLSWGAN [34]</cell><cell>57.9</cell><cell>61.4</cell><cell>59.6</cell><cell>43.7</cell><cell>57.7</cell><cell>49.7</cell><cell>59.0</cell><cell>73.8</cell><cell>65.6</cell><cell>42.6</cell><cell>36.6</cell><cell>39.4</cell></row><row><cell>LisGAN [Ours]</cell><cell>52.6</cell><cell>76.3</cell><cell>62.3</cell><cell>46.5</cell><cell>57.9</cell><cell>51.6</cell><cell>57.7</cell><cell>83.8</cell><cell>68.3</cell><cell>42.9</cell><cell>37.8</cell><cell>40.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the National Nat- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-rank embedded ensemble semantic dictionary for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative zeroshot learning via low-rank embedded semantic dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning discriminative latent attributes for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4223" to="4232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2452" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08345</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hannes Nickisch, and Stefan Harmeling</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="453" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4247" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From zero-shot learning to cold-start recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two birds one stone: on both cold-start and long-tail recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="898" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Heterogeneous domain adaptation through progressive alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer independently together: A generalized framework for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYB</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">I read, i saw, i tell: Texts assisted fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jidong</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards affordable semantic searching: Zero-shot. retrieval via dominant attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A generative model for zero shot learning using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2751" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ridge regression, hubness, and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikumi</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuo</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-KDD</title>
		<imprint>
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via synthesized examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gundeep</forename><surname>V Kumar Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Zeroshot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04394</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot classification with discriminative semantic representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4166" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6034" to="6042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
