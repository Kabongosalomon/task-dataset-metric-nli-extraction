<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
							<email>hanguo@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Mohit Bansal UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mohit Bansal UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multitask architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-ofthe-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model's learned saliency and entailment skills. rian, and Claire Cardie. 2013. A sentence compression based framework to query-focused multidocument summarization. In ACL.</p><p>Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818-833. Springer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstractive summarization is the challenging NLG task of compressing and rewriting a document into a short, relevant, salient, and coherent summary. It has numerous applications such as summarizing storylines, event understanding, etc. As compared to extractive or compressive summarization <ref type="bibr" target="#b26">(Jing and McKeown, 2000;</ref><ref type="bibr">Knight and * Equal contribution (published at ACL 2018)</ref>. <ref type="bibr" target="#b30">Marcu, 2002;</ref><ref type="bibr" target="#b10">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b17">Filippova et al., 2015;</ref><ref type="bibr" target="#b23">Henß et al., 2015)</ref>, abstractive summaries are based on rewriting as opposed to selecting. Recent end-to-end, neural sequence-tosequence models and larger datasets have allowed substantial progress on the abstractive task, with ideas ranging from copy-pointer mechanism and redundancy coverage, to metric reward based reinforcement learning <ref type="bibr" target="#b47">(Rush et al., 2015;</ref><ref type="bibr" target="#b9">Chopra et al., 2016;</ref><ref type="bibr" target="#b39">Nallapati et al., 2016;</ref><ref type="bibr" target="#b48">See et al., 2017)</ref>.</p><p>Despite these strong recent advancements, there is still a lot of scope for improving the summary quality generated by these models. A good rewritten summary is one that contains all the salient information from the document, is logically followed (entailed) by it, and avoids redundant information. The redundancy aspect was addressed by coverage models <ref type="bibr" target="#b49">(Suzuki and Nagata, 2016;</ref><ref type="bibr" target="#b6">Chen et al., 2016;</ref><ref type="bibr" target="#b39">Nallapati et al., 2016;</ref><ref type="bibr" target="#b48">See et al., 2017)</ref>, but we still need to teach these models about how to better detect salient information from the input document, as well as about better logicallydirected natural language inference skills.</p><p>In this work, we improve abstractive text summarization via soft, high-level (semantic) layerspecific multi-task learning with two relevant auxiliary tasks. The first is that of document-toquestion generation, which teaches the summarization model about what are the right questions to ask, which in turn is directly related to what the salient information in the input document is. The second auxiliary task is a premise-to-entailment generation task to teach it how to rewrite a summary which is a directed-logical subset of (i.e., logically follows from) the input document, and contains no contradictory or unrelated information. For the question generation task, we use the SQuAD dataset <ref type="bibr" target="#b45">(Rajpurkar et al., 2016)</ref>, where we learn to generate a question given a sentence containing the answer, similar to the recent work by <ref type="bibr" target="#b14">Du et al. (2017)</ref>. Our entailment generation task is based on the recent SNLI classification dataset and task <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref>, converted to a generation task .</p><p>Further, we also present novel multi-task learning architectures based on multi-layered encoder and decoder models, where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks, while keeping the lower-level (lexico-syntactic) layers unshared. We also explore different ways to optimize the shared parameters and show that 'soft' parameter sharing achieves higher performance than hard sharing.</p><p>Empirically, our soft, layer-specific sharing model with the question and entailment generation auxiliary tasks achieves statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets. It also performs significantly better on the DUC-2002 transfer setup, demonstrating its strong generalizability as well as the importance of auxiliary knowledge in low-resource scenarios. We also report improvements on our auxiliary question and entailment generation tasks over their respective previous state-of-the-art. Moreover, we significantly decrease the training time of the multitask models by initializing the individual tasks from their pretrained baseline models. Finally, we present human evaluation studies as well as detailed quantitative and qualitative analysis studies of the improved saliency detection and logical inference skills learned by our multi-task model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatic text summarization has been progressively improving over the time, initially more focused on extractive and compressive models <ref type="bibr" target="#b26">(Jing and McKeown, 2000;</ref><ref type="bibr" target="#b30">Knight and Marcu, 2002;</ref><ref type="bibr" target="#b10">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b17">Filippova et al., 2015;</ref><ref type="bibr" target="#b28">Kedzie et al., 2015)</ref>, and moving more towards compressive and abstractive summarization based on graphs and concept maps <ref type="bibr" target="#b20">(Giannakopoulos, 2009;</ref><ref type="bibr" target="#b18">Ganesan et al., 2010;</ref><ref type="bibr" target="#b16">Falke and Gurevych, 2017)</ref> and discourse trees <ref type="bibr" target="#b19">(Gerani et al., 2014)</ref>, syntactic parse trees <ref type="bibr" target="#b8">(Cheung and Penn, 2014;</ref><ref type="bibr">Wang et al., 2013)</ref>, and Abstract Meaning Representations (AMR) <ref type="bibr" target="#b34">(Liu et al., 2015;</ref><ref type="bibr" target="#b13">Dohare and Karnick, 2017)</ref>. Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances in hierarchical, distractive, saliency, and graphattention modeling <ref type="bibr" target="#b47">(Rush et al., 2015;</ref><ref type="bibr" target="#b9">Chopra et al., 2016;</ref><ref type="bibr" target="#b39">Nallapati et al., 2016;</ref><ref type="bibr" target="#b6">Chen et al., 2016;</ref><ref type="bibr" target="#b50">Tan et al., 2017)</ref>. <ref type="bibr" target="#b44">Paulus et al. (2018)</ref> and <ref type="bibr" target="#b23">Henß et al. (2015)</ref> incorporated recent advances from reinforcement learning. Also, <ref type="bibr" target="#b48">See et al. (2017)</ref> further improved results via pointercopy mechanism and addressed the redundancy with coverage mechanism.</p><p>Multi-task learning (MTL) is a useful paradigm to improve the generalization performance of a task with related tasks while sharing some common parameters/representations <ref type="bibr" target="#b4">(Caruana, 1998;</ref><ref type="bibr" target="#b0">Argyriou et al., 2007;</ref><ref type="bibr" target="#b31">Kumar and Daumé III, 2012)</ref>. Several recent works have adopted MTL in neural models <ref type="bibr" target="#b35">(Luong et al., 2016;</ref><ref type="bibr" target="#b38">Misra et al., 2016;</ref><ref type="bibr" target="#b22">Hashimoto et al., 2017;</ref><ref type="bibr" target="#b46">Ruder et al., 2017;</ref><ref type="bibr" target="#b27">Kaiser et al., 2017)</ref>. Moreover, some of the above works have investigated the use of shared vs unshared sets of parameters. On the other hand, we investigate the importance of soft parameter sharing and highlevel versus low-level layer-specific sharing.</p><p>Our previous workshop paper  presented some preliminary results for multi-task learning of textual summarization with entailment generation. This current paper has several major differences: (1) We present question generation as an additional effective auxiliary task to enhance the important complementary aspect of saliency detection; (2) Our new high-level layer-specific sharing approach is significantly better than alternative layer-sharing approaches (including the decoder-only sharing by ); (3) Our new soft sharing parameter approach gives stat. significant improvements over hard sharing; (4) We propose a useful idea of starting multi-task models from their pretrained baselines, which significantly speeds up our experiment cycle 1 ; (5) For evaluation, we show diverse improvements of our soft, layer-specific MTL model (over state-of-the-art pointer+coverage baselines) on the CNN/DailyMail, Gigaword, as well as DUC datasets; we also report human evaluation plus analysis examples of learned saliency and entailment skills; we also report improvements on the auxiliary question and entailment generation tasks over their respective previous state-of-the-art.</p><p>In our work, we use a question generation task to improve the saliency of abstractive summarization in a multi-task setting. Using the SQuAD dataset <ref type="bibr" target="#b45">(Rajpurkar et al., 2016)</ref>, we learn to generate a question given the sentence containing the answer span in the comprehension (similar to <ref type="bibr" target="#b14">Du et al. (2017)</ref>). For the second auxiliary task of entailment generation, we use the generation version of the RTE classification task <ref type="bibr" target="#b11">(Dagan et al., 2006;</ref><ref type="bibr" target="#b32">Lai and Hockenmaier, 2014;</ref><ref type="bibr" target="#b25">Jimenez et al., 2014;</ref><ref type="bibr" target="#b3">Bowman et al., 2015)</ref>. Some previous work has explored the use of RTE for redundancy detection in summarization by modeling graph-based relationships between sentences to select the most non-redundant sentences <ref type="bibr" target="#b37">(Mehdad et al., 2013;</ref><ref type="bibr" target="#b21">Gupta et al., 2014)</ref>, whereas our approach is based on multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>First, we introduce our pointer+coverage baseline model and then our two auxiliary tasks: question generation and entailment generation (and finally the multi-task learning models in Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Pointer+Coverage Model</head><p>We use a sequence-attention-sequence model with a 2-layer bidirectional LSTM-RNN encoder and a 2-layer uni-directional LSTM-RNN decoder, along with <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> style attention. Let x = {x 1 , x 2 , ..., x m } be the source document and y = {y 1 , y 2 , ..., y n } be the target summary. The output summary generation vocabulary distribution conditioned over the input source document is P v (y|x; θ) = n t=1 p v (y t |y 1:t−1 , x; θ). Let the decoder hidden state be s t at time step t and let c t be the context vector which is defined as a weighted combination of encoder hidden states. We concatenate the decoder's (last) RNN layer hidden state s t and context vector c t and apply a linear transformation, and then project to the vocabulary space by another linear transformation. Finally, the conditional vocabulary distribution at each time step t of the decoder is defined as:</p><formula xml:id="formula_0">p v (y t |y 1:t−1 , x; θ) = sfm(V p (W f [s t ; c t ]+b f )+b p ) (1) where, W f , V p , b f , b p are trainable parameters,</formula><p>and sfm(·) is the softmax function.</p><p>Pointer-Generator Networks Pointer mechanism <ref type="bibr" target="#b17">(Vinyals et al., 2015)</ref> helps in directly copying the words from the source sequence during target sequence generation, which is a good fit for a task like summarization. Our pointer mechanism approach is similar to <ref type="bibr" target="#b48">See et al. (2017)</ref>, who use a soft switch based on the generation probability p g = σ(W g c t +U g s t +V g e w t−1 +b g ), where σ(·) is a sigmoid function, W g , U g , V g and b g are parameters learned during training. e w t−1 is the previous time step output word embedding. The final word distribution is P f (y) = p g ·P v (y)+(1−p g )·P c (y), where P v vocabulary distribution is as shown in Eq. 1, and copy distribution P c is based on the attention distribution over source document words.</p><p>Coverage Mechanism Following previous work <ref type="bibr" target="#b48">(See et al., 2017)</ref>, coverage helps alleviate the issue of word repetition while generating long summaries. We maintain a coverage vector c t = t−1 t=0 α t that sums over all of the previous time steps attention distributions α t , and this is added as input to the attention mechanism. Coverage loss is L cov (θ) = t i min(α t,i ,ĉ t,i ). Finally, the total loss is a weighted combination of cross-entropy loss and coverage loss:</p><formula xml:id="formula_1">L(θ) = − log P f (y) + λL cov (θ)<label>(2)</label></formula><p>where λ is a tunable hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two Auxiliary Tasks</head><p>Despite the strengths of the baseline model described above with attention, pointer, and coverage, a good summary should also contain maximal salient information and be a directed logical entailment of the source document. We teach these skills to the abstractive summarization model via multi-task training with two related auxiliary tasks: question generation task and entailment generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Generation</head><p>The task of question generation is to generate a question from a given input sentence, which in turn is related to the skill of being able to find the important salient information to ask questions about. First the model has to identify the important information present in the given sentence, then it has to frame (generate) a question based on this salient information, such that, given the sentence and the question, one has to be able to predict the correct answer (salient information in this case). A good summary should also be able to find and extract all the salient information in the given source document, and hence we incorporate such capabilities into our abstractive text summarization model by multi-task learning it with a question generation task, sharing some common parameters/representations (see more details in Sec. 4). For setting up the question generation task, we follow <ref type="bibr" target="#b14">Du et al. (2017)</ref> and use the SQuAD dataset to extract sentencequestion pairs. Next, we use the same sequenceto-sequence model architecture as our summarization model. Note that even though our question generation task is generating one question at a time 2 , our multi-task framework (see Sec. 4) is set up in such a way that the sentence-level knowledge from this auxiliary task can help the documentlevel primary (summarization) task to generate multiple salient facts -by sharing high-level semantic layer representations. See Sec. 7 and Table 10 for a quantitative evaluation showing that the multi-task model can find multiple (and more) salient phrases in the source document. Also see Sec. 7 (and supp) for challenging qualitative examples where baseline and SotA models only recover a small subset of salient information but our multi-task model with question generation is able to detect more of the important information.</p><p>Entailment Generation The task of entailment generation is to generate a hypothesis which is entailed by (or logically follows from) the given premise as input. In summarization, the generation decoder also needs to generate a summary that is entailed by the source document, i.e., does not contain any contradictory or unrelated/extraneous information as compared to the input document. We again incorporate such inference capabilities into the summarization model via multi-task learning, sharing some common representations/parameters between our summarization and entailment generation model (more details in Sec. 4). For this task, we use the entailmentlabeled pairs from the SNLI dataset <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref> and set it up as a generation task (using the same strong model architecture as our abstractive summarization model). See Sec. 7 and <ref type="table">Table 9</ref> for a quantitative evaluation showing that the multi-task model is better entailed by the source document and has fewer extraneous facts. Also see Sec. 7 and supplementary for qualitative examples of how our multi-task model with the entailment auxiliary task is able to generate more logically-entailed summaries than the baseline and <ref type="bibr">2</ref> We also tried to generate all the questions at once from the full document, but we obtained low accuracy because of this task's challenging nature and overall less training data.  <ref type="figure">Figure 1</ref>: Overview of our multi-task model with parallel training of three tasks: abstractive summary generation (SG), question generation (QG), and entailment generation (EG). We share the 'blue' color representations across all the three tasks, i.e., second layer of encoder, attention parameters, and first layer of decoder.</p><p>SotA models, which instead produce extraneous, unrelated words not present (in any paraphrased form) in the source document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-Task Learning</head><p>We employ multi-task learning for parallel training of our three tasks: abstractive summarization, question generation, and entailment generation. In this section, we describe our novel layerspecific, soft-sharing approaches and other multitask learning details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Layer-Specific Sharing Mechanism</head><p>Simply sharing all parameters across the related tasks is not optimal, because models for different tasks have different input and output distributions, esp. for low-level vs. high-level parameters. Therefore, related tasks should share some common representations (e.g., high-level information), as well as need their own individual task-specific representations (esp. low-level information). To this end, we allow different components of model parameters of related tasks to be shared vs. unshared, as described next. Encoder Layer Sharing: <ref type="bibr" target="#b2">Belinkov et al. (2017)</ref> observed that lower layers (i.e., the layers closer to the input words) of RNN cells in a seq2seq machine translation model learn to represent word structure, while higher layers (farther from input) are more focused on high-level semantic meanings (similar to findings in the computer vision community for image features (Zeiler and Fergus, 2014)). We believe that while textual summarization, question generation, and entailment generation have different training data distributions and low-level representations, they can still benefit from sharing their models' high-level components (e.g., those that capture the skills of saliency and inference). Thus, we keep the lower-level layer (i.e., first layer closer to input words) of the 2layer encoder of all three tasks unshared, while we share the higher layer (second layer in our model as shown in <ref type="figure">Fig. 1</ref>) across the three tasks. Decoder Layer Sharing: Similarly for the decoder, lower layers (i.e., the layers closer to the output words) learn to represent word structure for generation, while higher layers (farther from output) are more focused on high-level semantic meaning. Hence, we again share the higher level components (first layer in the decoder far from output as shown in <ref type="figure">Fig. 1</ref>), while keeping the lower layer (i.e., second layer) of decoders of all three tasks unshared. Attention Sharing: As described in Sec. 3.1, the attention mechanism defines an attention distribution over high-level layer encoder hidden states and since we share the second, high-level (semantic) layer of all the encoders, it is intuitive to share the attention parameters as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Soft vs. Hard Parameter Sharing</head><p>Hard-sharing: In the most common multi-task learning hard-sharing approach, the parameters to be shared are forced to be the same. As a result, gradient information from multiple tasks will directly pass through shared parameters, hence forcing a common space representation for all the related tasks. Soft-sharing: In our soft-sharing approach, we encourage shared parameters to be close in representation space by penalizing their l 2 distances. Unlike hard sharing, this approach gives more flexibility for the tasks by only loosely coupling the shared space representations. We minimize the following loss function for the primary task in soft-sharing approach:</p><formula xml:id="formula_2">L(θ) = − log P f (y)+λL cov (θ)+γ θ s −ψ s (3)</formula><p>where γ is a hyperparameter, θ represents the primary summarization task's full parameters, while θ s and ψ s represent the shared parameter subset between the primary and auxiliary tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fast Multi-Task Training</head><p>During multi-task learning, we alternate the minibatch optimization of the three tasks, based on a tunable 'mixing ratio' α s : α q : α e ; i.e., optimizing the summarization task for α s mini-batches followed by optimizing the question generation task for α q mini-batches, followed by entailment generation task for α e mini-batches (and for 2way versions of this, we only add one auxiliary task at a time). We continue this process until all the models converge. Also, importantly, instead of training from scratch, we start the primary task (summarization) from a 90%-converged model of its baseline to make the training process faster. We observe that starting from a fully-converged baseline makes the model stuck in a local minimum.</p><p>In addition, we also start all auxiliary models from their 90%-converged baselines, as we found that starting the auxiliary models from scratch has a chance to pull the primary model's shared parameters towards randomly-initialized auxiliary model's shared parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Datasets: We use CNN/DailyMail dataset <ref type="bibr" target="#b24">(Hermann et al., 2015;</ref><ref type="bibr" target="#b39">Nallapati et al., 2016)</ref> and <ref type="bibr">Gigaword (Rush et al., 2015)</ref> datasets for summarization, and the Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref> and the Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b45">(Rajpurkar et al., 2016)</ref> datasets for our entailment and question generation tasks, resp. We also show generalizability/transfer results on DUC-2002 with our CNN/DM trained models. Supplementary contains dataset details. Evaluation Metrics: We use the standard ROUGE evaluation package <ref type="bibr" target="#b33">(Lin, 2004)</ref> for reporting the results on all of our summarization models. Following previous work <ref type="bibr" target="#b9">(Chopra et al., 2016;</ref><ref type="bibr" target="#b39">Nallapati et al., 2016)</ref>, we use ROUGE full-length F1 variant for all our results. Following <ref type="bibr" target="#b48">See et al. (2017)</ref>, we also report ME-TEOR (Denkowski and Lavie, 2014) using the MS-COCO evaluation script <ref type="bibr" target="#b7">(Chen et al., 2015)</ref>. Human Evaluation Criteria: We used Amazon MTurk to perform human evaluation of summary relevance and readability. We selected human annotators that were located in the US, had an ap-  <ref type="table">Table 2</ref>: Summarization results on Gigaword. ROUGE scores are full length F-1. All the multitask improvements are statistically significant over the state-of-the-art baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task with Entailment Generation</head><p>We first perform multi-task learning between abstractive summarization and entailment generation with soft-sharing of parameters as discussed in Sec. 4. <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> shows that this multi-task setting is better than our strong baseline models and the improvements are statistically significant on all metrics 5 on both CNN/DailyMail (p &lt; 0.01 in ROUGE-1/ROUGE-L/METEOR and p &lt; 0.05 in ROUGE-2) and Gigaword (p &lt; 0.01 on all metrics) datasets, showing that entailment generation task is inducing useful inference skills to the summarization task (also see analysis examples in Sec. 7).   Multi-Task with Entailment and Question Generation Finally, we perform multi-task learning with all three tasks together, achieving the best of both worlds (inference skills and saliency). Table 1 and <ref type="table">Table 2</ref> show that our full multi-task model achieves the best scores on CNN/DailyMail and Gigaword datasets, and the improvements are statistically significant on all metrics on both CNN/DailyMail (p &lt; 0.01 in ROUGE-1/ROUGE-L/METEOR and p &lt; 0.02 in ROUGE-2) and Gigaword (p &lt; 0.01 on all metrics). Finally, our 3-way multi-task model (with both entailment and question generation) outperforms the publicly-available pretrained result ( †) of the previous SotA <ref type="bibr" target="#b48">(See et al., 2017)</ref> with stat. significance (p &lt; 0.01), as well the higher-reported results ( ) on ROUGE-1/ROUGE-2 (p &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task with Question Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Human Evaluation</head><p>We also conducted a blind human evaluation on Amazon MTurk for relevance and readability, based on 100 samples, for both CNN/DailyMail and Gigaword (see instructions in Sec. 5). <ref type="table">Table.</ref> 3 shows the CNN/DM results where we do pairwise comparison between our 3-way multi-task model's output summaries w.r.t. our baseline summaries and w.r.t. <ref type="bibr" target="#b48">See et al. (2017)</ref> summaries. As shown, our 3-way multi-task model achieves both higher due to adding more data, we separately trained word embeddings on each auxiliary dataset (i.e., SNLI and SQuAD) and incorporated them into the summarization model. We found that both our 2-way multi-task models perform significantly better than these models using the auxiliary wordembeddings, suggesting that merely adding more data is not enough.   <ref type="bibr" target="#b48">See et al. (2017)</ref>, our MTL model is higher in relevance scores but a bit lower in readability scores (and is higher in terms of total aggregate scores). One potential reason for this lower readability score is that our entailment generation auxiliary task encourages our summarization model to rewrite more and to be more abstractive than <ref type="bibr" target="#b48">See et al. (2017)</ref> -see abstractiveness results in <ref type="table">Table 11</ref>.</p><p>We also show human evaluation results on the Gigaword dataset in <ref type="table" target="#tab_4">Table 4</ref> (again based on pairwise comparisons for 100 samples), where we see that our MTL model is better than our state-of-theart baseline on both relevance and readability. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generalizability Results (DUC-2002)</head><p>Next, we also tested our model's generalizability/transfer skills, where we take the models trained on CNN/DailyMail and directly test them on DUC-2002. We take our baseline and 3way multi-task models, plus the pointer-coverage model from <ref type="bibr" target="#b48">See et al. (2017)</ref>. <ref type="bibr">8</ref> We only retune the beam-size for each of these three models separately (based on DUC-2003 as the validation set). 9 As shown in <ref type="table" target="#tab_6">Table 5</ref>, our multitask model achieves statistically significant improvements over the strong baseline (p &lt; 0.01 in ROUGE-1 and ROUGE-L) and the pointercoverage model from <ref type="bibr" target="#b48">See et al. (2017)</ref> (p &lt; 0.01 in all metrics). This demonstrates that our model is able to generalize well and that the auxiliary knowledge helps more in low-resource scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Auxiliary Task Results</head><p>In this section, we discuss the individual/separated performance of our auxiliary tasks. 7 Note that we did not have output files of any previous work's model on Gigaword; however, our baseline is already a strong state-of-the-art model as shown in <ref type="table">Table 2</ref>. <ref type="bibr">8</ref> We use the publicly-available pretrained model from <ref type="bibr" target="#b48">See et al. (2017)</ref>'s github for these DUC transfer results, which produces the † results in <ref type="table">Table 1</ref>. All other comparisons and analysis in our paper are based on their higher results. <ref type="bibr">9</ref> We follow previous work which has shown that larger beam values are better and feasible for DUC corpora. However, our MTL model still achieves stat. significant improvements (p &lt; 0.01 in all metrics) over <ref type="bibr" target="#b48">See et al. (2017)</ref> without beam retuning (i.e., with beam = 4).   Entailment Generation We use the same architecture as described in Sec. 3.1 with pointer mechanism, and <ref type="table" target="#tab_8">Table 6</ref> compares our model's performance to . Our pointer mechanism gives a performance boost, since the entailment generation task involves copying from the given premise sentence, whereas the 2-layer model seems comparable to the 1-layer model. Also, the supplementary shows some output examples from our entailment generation model.</p><p>Question Generation Again, we use same architecture as described in Sec. 3.1 along with pointer mechanism for the task of question generation. <ref type="table" target="#tab_9">Table 7</ref> compares the performance of our model w.r.t. the state-of-the-art <ref type="bibr" target="#b14">Du et al. (2017)</ref>. Also, the supplementary shows some output examples from our question generation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ablation and Analysis Studies</head><p>Soft-sharing vs. Hard-sharing As described in Sec. 4.2, we choose soft-sharing over hard-sharing because of the more expressive parameter sharing it provides to the model. Empirical results in <ref type="table">Table.</ref> 8 prove that soft-sharing method is statistically significantly better than hard-sharing with p &lt; 0.001 in all metrics. 10</p><p>Comparison of Different Layer-Sharing Methods We also conducted ablation studies among various layer-sharing approaches. <ref type="table">Table 8</ref> shows results for soft-sharing models with decoder-only sharing (D1+D2; similar to ) as well as lower-layer sharing (encoder layer 1 + decoder layer 2, with and without attention  <ref type="table">Table 8</ref>: Ablation studies comparing our final multi-task model with hard-sharing and different alternative layer-sharing methods. Here E1, E2, D1, D2, Attn refer to parameters of the first/second layer of encoder/decoder, and attention parameters. Improvements of final model upon ablation experiments are all stat. signif. with p &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Average Entailment Probability Baseline 0.907 Multi-Task (EG) 0.912 <ref type="table">Table 9</ref>: Entailment classification results of our baseline vs. EG-multi-task model (p &lt; 0.001).</p><p>shared). As shown, our final model (high-level semantic layer sharing E2+Attn+D1) outperforms these alternate sharing methods in all metrics with statistical significance (p &lt; 0.05). 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Improvements in Entailment</head><p>We employ a state-of-the-art entailment classifier <ref type="bibr" target="#b5">(Chen et al., 2017)</ref>, and calculate the average of the entailment probability of each of the output summary's sentences being entailed by the input source document. We do this for output summaries of our baseline and 2-way-EG multi-task model (with entailment generation).</p><p>As can be seen in <ref type="table">Table 9</ref>, our multi-task model improves upon the baseline in the aspect of being entailed by the source document (with statistical significance p &lt; 0.001). Further, we use the Named Entity Recognition (NER) module from CoreNLP <ref type="bibr" target="#b36">(Manning et al., 2014)</ref> to compute the number of times the output summary contains extraneous facts (i.e., named entities as detected by the NER system) that are not present in the source documents, based on the intuition that a well-entailed summary should not contain unrelated information not followed from the input premise. We found that our 2-way MTL model with entailment generation reduces this extraneous count by 17.2% w.r.t. the baseline.</p><p>The qualitative examples below further discuss this issue of generating unrelated information.   <ref type="table">Table 11</ref>: Abstractiveness: novel n-gram percent.</p><p>Quantitative Improvements in Saliency Detection For our saliency evaluation, we used the answer-span prediction classifier from Pasunuru and Bansal (2018) trained on SQuAD <ref type="bibr" target="#b45">(Rajpurkar et al., 2016)</ref> as the keyword detection classifier. We then annotate the ground-truth and model summaries with this keyword classifier and compute the % match, i.e., how many salient words from the ground-truth summary were also generated in the model summary. The results are shown in Table 10, where the 2-way-QG MTL model (with question generation) versus baseline improvement is stat. significant (p &lt; 0.01). Moreover, we found 93 more cases where our 2-way-QG MTL model detects 2 or more additional salient keywords than the pointer baseline model (as opposed to vice versa), showing that sentence-level question generation task is helping the document-level summarization task in finding more salient terms.</p><p>Qualitative Examples on Entailment and Saliency Improvements <ref type="figure" target="#fig_0">Fig. 2</ref> presents an example of output summaries generated by <ref type="bibr" target="#b48">See et al. (2017)</ref>, our baseline, and our 3-way multitask model. <ref type="bibr" target="#b48">See et al. (2017)</ref> and our baseline models generate phrases like "john hartson" and "hampden injustice" that don't appear in the input document, hence they are not entailed by the input. 12 Moreover, both models missed salient information like "josh meekings", "leigh griffiths", and "hoops", that our multi-task model recovers. <ref type="bibr">13</ref> Hence, our 3-way multi-task model generates summaries that are both better at logical entailment and contain more salient information. We refer to supplementary <ref type="figure" target="#fig_3">Fig. 5</ref> for more details and similar examples for separated 2-way multi-task models (supplementary <ref type="figure" target="#fig_1">Fig. 3, Fig. 4</ref>). <ref type="bibr">12</ref> These extra, non-entailed unrelated/contradictory information are not present at all in any paraphrase form in the input document. <ref type="bibr">13</ref> We consider the fill-in-the-blank highlights annotated by human on CNN/DailyMail dataset as salient information.</p><p>Input Document: celtic have written to the scottish football association in order to gain an ' understandingóf the refereeing decisions during their scottish cup semi-final defeat by inverness on sunday . the hoops were left outraged by referee steven mcleanś failure to award a penalty or red card for a clear handball in the box by josh meekings to deny leigh griffithś goal-bound shot during the first-half . caley thistle went on to win the game 3-2 after extra-time and denied rory deliaś men the chance to secure a domestic treble this season . celtic striker leigh griffiths has a goal-bound shot blocked by the outstretched arm of josh meekings . celticś adam matthews -lrb-right -rrb-slides in with a strong challenge on nick ross in the scottish cup semi-final . ' given the level of reaction from our supporters and across football , we are duty bound to seek an understanding of what actually happened ,ćeltic said in a statement . they added , ' we have not been given any other specific explanation so far and this is simply to understand the circumstances of what went on and why such an obvious error was made .however , the parkhead outfit made a point of congratulating their opponents , who have reached the first-ever scottish cup final in their history , describing caley as a ' fantastic club and saying ' reaching the final is a great achievement .ćeltic had taken the lead in the semi-final through defender virgil van dijkś curling free-kick on 18 minutes , but were unable to double that lead thanks to the meekings controversy . it allowed inverness a route back into the game and celtic had goalkeeper craig gordon sent off after the restart for scything down marley watkins in the area . greg tansey duly converted the resulting penalty . edward ofere then put caley thistle ahead , only for john guidetti to draw level for the bhoys . with the game seemingly heading for penalties , david raven scored the winner on 117 minutes , breaking thousands of celtic hearts . celtic captain scott brown -lrb-left -rrb-protests to referee steven mclean but the handball goes unpunished . griffiths shows off his acrobatic skills during celticś eventual surprise defeat by inverness . celtic pair aleksandar tonev -lrb-left -rrb-and john guidetti look dejected as their hopes of a domestic treble end . Ground-truth: celtic were defeated 3-2 after extra-time in the scottish cup semi-final . leigh griffiths had a goal-bound shot blocked by a clear handball. however, no action was taken against offender josh meekings . the hoops have written the sfa for an 'understanding' of the decision . <ref type="bibr" target="#b48">See et al. (2017)</ref>: john hartson was once on the end of a major hampden injustice while playing for celtic . but he can not see any point in his old club writing to the scottish football association over the latest controversy at the national stadium . hartson had a goal wrongly disallowed for offside while celtic were leading 1-0 at the time but went on to lose 3-2 . Our Baseline: john hartson scored the late winner in 3-2 win against celtic . celtic were leading 1-0 at the time but went on to lose 3-2 . some fans have questioned how referee steven mclean and additional assistant alan muir could have missed the infringement . Multi-task: celtic have written to the scottish football association in order to gain an ' understanding ' of the refereeing decisions . the hoops were left outraged by referee steven mclean 's failure to award a penalty or red card for a clear handball in the box by josh meekings . celtic striker leigh griffiths has a goal-bound shot blocked by the outstretched arm of josh meekings . <ref type="bibr" target="#b48">See et al. (2017)</ref>, our baseline, and 3-way multi-task model with summarization and both entailment generation and question generation. The boxed-red highlighted words/phrases are not present in the input source document in any paraphrasing form. All the unboxedgreen highlighted words/phrases correspond to the salient information. See detailed discussion in <ref type="figure" target="#fig_0">Fig.  1 and Fig. 2</ref> above. As shown, the outputs from <ref type="bibr" target="#b48">See et al. (2017)</ref> and the baseline both include nonentailed words/phrases (e.g. "john hartson"), as well as they missed salient information ("hoops", "josh meekings", "leigh griffiths") in their output summaries. Our multi-task model, however, manages to accomplish both, i.e., cover more salient information and also avoid unrelated information. The boxed-red highlights are extraneously-generated words not present/paraphrased in the input document. The unboxed-green highlights show salient phrases. <ref type="bibr" target="#b48">See et al. (2017)</ref>, we also compute the abstractiveness score as the number of novel n-grams between the model output summary and source document. As shown in <ref type="table">Table 11</ref>, our multi-task model (EG + QG) is more abstractive than <ref type="bibr" target="#b48">See et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Example of summaries generated by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstractiveness Analysis As suggested in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a multi-task learning approach to improve abstractive summarization by incorporating the ability to detect salient information and to be logically entailed by the document, via question generation and entailment generation auxiliary tasks. We propose effective soft and highlevel (semantic) layer-specific parameter sharing and achieve significant improvements over the state-of-the-art on two popular datasets, as well as a generalizability/transfer DUC-2002 setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN/DailyMail</head><p>Dataset CNN/DailyMail dataset <ref type="bibr" target="#b24">(Hermann et al., 2015;</ref><ref type="bibr" target="#b39">Nallapati et al., 2016)</ref> is a large collection of online news articles and their multi-sentence summaries. We use the original, non-anonymized version of the dataset provided by <ref type="bibr" target="#b48">See et al. (2017)</ref>. Overall, the dataset has 287, 226 training pairs, 13, 368 validation pairs and, 11, 490 test pairs. On an average, a source document has 781 tokens and a target summary has 56 tokens.</p><p>Gigaword Corpus Gigaword is based on a large collection of news articles, where the article's first sentence is considered as the input document and the headline of the article as output summary. We use the annotated corpus provided by <ref type="bibr" target="#b47">Rush et al. (2015)</ref>. It has around 3.8 million training samples. For validation, we use 2, 000 samples and for test evaluation we use the standard test set provided by <ref type="bibr" target="#b47">Rush et al. (2015)</ref>. Following previous work, we keep our vocabulary size to 50, 000 frequent words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUC Corpus</head><p>We use the DUC-2002 14 document summarization dataset for checking our model's generalizability capabilities. DUC-2002 corpus consists of 567 documents with one or two human annotated reference summaries. We also tried beam retuning using DUC-2003 15 as a validation set, which consists of 624 documents with single human annotated reference summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI corpus</head><p>We use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) for our entailment generation task. Following , we use the same re-splits provided by them to ensure a zero train-test overlap and multi-reference setup. This dataset has a total of 145, 822 unique premise pairs out of 190, 113 pairs, which are used for training, and the rest of them are divided equally into validation and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD Dataset</head><p>We use Stanford Question Answering Dataset (SQuAD) for our question generation task <ref type="bibr" target="#b45">(Rajpurkar et al., 2016)</ref>. In SQuAD dataset, given the comprehension and question, the task is to predict the answer span in the comprehension. However, in our question generation task, we extract the sentence from the comprehension containing the answer span and create a sentence-question pair similar to <ref type="bibr" target="#b14">Du et al. (2017)</ref>. The dataset has around 100K sentence-question pairs from 536 articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>The following training details are common across all models and datasets. We use LSTM-RNN in our sequence models with hidden state size of 256 dimension. We use 128 dimension word embedding representations. We do not use dropout or any other regularization techniques, but we clip the gradient to allow a maximum gradient norm value of 2.0. We use Adam optimizer <ref type="bibr" target="#b29">(Kingma and Ba, 2015)</ref> with a learning rate of 0.001. Also, we share the word embeddings representation of both encoder and decoder in our models. All our tuning decisions (including soft/hard and layerspecific sharing decisions) were made on the appropriate validation/development set. CNN/DailyMail: For all the models involving CNN/DailyMail dataset, we use a maximum encoder RNN step size of 400 and a maximum decoder RNN step size of 100. We use a minibatch size of 16. We initialize the LSTM-RNNs with uniform random initialization in the range [−0.02, 0.02]. We set λ to 1.0 in the joint crossentropy and coverage loss. Also, we only add coverage to the converged model with attention and pointer mechanism, and make the learning rate from 0.001 to 0.0001. During multi-task learning, we use coverage mechanism for primary (CNN/DailyMail summarization) task but not for auxiliary tasks (because they do not have traditional redundancy issues). The penalty coefficient γ for soft-sharing is set to 5 × 10 −5 and 1 × 10 −5 for 2-way and 3-way multi-task models respectively (the range of the penalty value is intuitively chosen such that we balance the crossentropy and regularization losses). In inference time, we use a beam search size of 4, following previous work <ref type="bibr" target="#b48">(See et al., 2017)</ref>. Gigaword: For all the models involving Gigaword dataset, we use a maximum encoder RNN step size of 50 and a maximum decoder RNN step size of 20. We use a mini-batch size of 256. We initialize the LSTM-RNNs with uniform random initialization in the range [−0.01, 0.01]. We do not use cov-Input Document: john hughes has revealed how he came within a heartbeat of stepping down from his job at inverness as the josh meekings controversy went into overdrive this week . the caley thistle boss says he felt so repulsed by the gut-wrenching predicament being endured by his young defender -before he was dramatically cleared -that he was ready to walk away from his post and the games he loves , just weeks before an historic scottish cup final date . keen cyclist hughes set off on a lonely bike ride after hearing meekings had been cited for the handball missed by officials in the semi-final against celtic , and admits his head was in a spin over an affair that has dominated the back-page headlines since last sunday . inverness defender josh meekings will be allowed to appear in scottish cup final after his ban was dismissed . only messages of support awaiting him on his return from footballing friends brought him back from the brink of quitting . hughes , who lives in the black isle just north of inverness , said : ' i came in here this morning after a day off . i turned my phone off and was away myself , away out on the bike with plenty of thinking time : a great freedom of mind . ' i was that sick of what has been going on in scottish football i was seriously contemplating my own future . i 'm serious when i say that . ' i had just had it up to here and was ready to just give it up . if it was n't for what happened when i turned my phone back on , with the phone calls and texts i received from people i really value in football , that my spirits picked up again . ' the calls and texts came in from all over the place , from some of the highest levels across the game . i 've had phone calls that have really got me back on my feet . ' i would n't like to name them all , but there were a lot of good people and a good few close friends in the football fraternity . meekings was not sent off and no penalty was given as inverness went on to beat celtic 3-2 after extra-time . ' they were saying : " you need to lead from the front , you need to fight it . " that restored and galvanised that focus and drive in me . and , if that was how i was feeling , how was the boy josh meekings feeling ? it should never have come to this . ' meekings was cleared to play in the final by the judicial panel yesterday , but hughes insists this ' unprecedented ' sfa wrangle must be the catalyst for change in scottish football 's governance . although those who sit on the panel are drawn from many walks of life , ranging from former players and coaches to ex-refs and members of the legal profession , hughes said he wants ' real football people ' drafted in instead of the ' suits ' he claims lack understanding of the nuances and spirit of the professional game . and he seemed to point a thinly-veiled finger of accusation at sfa chief executive stewart regan by alleging that compliance officer tony mcglennan was a mere ' patsy ' in the process . (...) Ground-truth: Inverness defender josh meekings has won appeal against one-match ban . the 22year-old was offered one-game suspension following incident . however , an independent judicial panel tribunal overturned decision . inverness reached the scottish cup final with 3-2 win over celtic . <ref type="bibr" target="#b48">See et al. (2017)</ref>: Josh meekings has been cleared to play in the scottish cup final .The englishman admitted he was fortunate not to have conceded a penalty and been sent off by referee steven mclean for stopping leigh griffiths net-bound effort on his goal-line . Meekings was not sent off and no penalty was given as inverness went on to beat celtic 3-2 . Our Baseline: Josh meekings cleared to play in the scottish cup final on may 30 . Inverness defender josh meekings will be allowed to appear in scottish cup final . Meekings was not sent off and no penalty was given as inverness went on to beat celtic 3-2 . Multi-task: Josh meekings has been cleared to play in the scottish cup final . Inverness defender josh meekings will be allowed to appear in scottish cup final after his ban was dismissed . Inverness went on to beat celtic 3-2 after extra-time .  <ref type="bibr" target="#b48">See et al. (2017)</ref>, our baseline, and 2-way multitask model with summarization and entailment generation. Boxed-red highlighted words/phrases are not present in the input source document in any paraphrasing form. As shown, both <ref type="bibr" target="#b48">See et al. (2017)</ref> and the baseline generate extraneous information that is not entailed by the source documents ("referee steven mclean" and "may 30"), but our multi-task model avoids such unrelated information to generate summaries that logically follow from the source document.</p><p>Input Document: bending and rising in spectacular fashion , these stunning pictures capture the paddy fields of south east asia and the arduous life of the farmers who cultivate them . in a photo album that spans over china , thailand , vietnam , laos and cambodia , extraordinary images portray the crop 's full cycle from the primitive sowing of seeds to the distribution of millions of tonnes for consumption . the pictures were taken by professional photographer scott gable , 39 , who spent four months travelling across the region documenting the labour and threadbare equipment used to harvest the carbohydrate-rich food . scroll down for video . majestic : a farmer wades through the mud with a stick as late morning rain falls on top of dragonsbone terraces in longsheng county , china . rice is a staple food for more than one-half the world 's population , but for many consumers , its origin remains somewhat of a mystery . the crop accounts for one fifth of all calories consumed by humans and 87 per cent of it is produced in asia . it is also the thirstiest crop there is -according to the un , farmers need at least 2,000 litres of water to make one kilogram of rice . mr gable said he was determined to capture every stage of production with his rice project -from the planting to the harvesting all the way down to the shipping of the food . after acquiring some contacts from experts at cornell university in new york and conducting his own research , he left for china last may and spent the next four months traveling . he said : ' the images were taken over a four month period from april to july last year across asia . i visited china , thailand , vietnam , laos and cambodia as part of my rice project . video courtesy of www.scottgable.com . breathtaking : a paddy field worker toils on the beautiful landscape of dragonsbone terraces in longsheng county , china . farmers ' procession : a rice planting festival parade takes place near the village of pingan in guangxi province , china . ' the project is one part of a larger three part project on global food staples -rice , corn and wheat . i am currently in the process of shooting the corn segment . ' the industrialisation of our food and mono-culture food staples have interested me for some time so that 's probably what inspired me to do this project . ' i shot the whole project using a canon slr and gopros . the actual shooting took four months and then post production took another four more months . ' the reaction to my work has been incredibly positive -i was able to secure a solo gallery show and create quite a bit of interest online which has been great . ' family crop : a hani woman in traditional clothing sits on top of her family 's rice store in yunnan province , china . arduous labour : employees of taiwan 's state-run rice experimental station are pictured beating rice husks by hand as the sun shines on them . mr gable spent months learning mandarin chinese in preparation for his trip , but the language barrier was still his greatest challenge . (...) Ground-truth: the spectacular photos were taken at paddy fields in china , thailand , vietnam , laos and cambodia . photographer scott gable spent four months travelling region to document the process of harvesting the crop . rice accounts for one fifth of all calories consumed by humans but crop is often still cultivated in primitive way . <ref type="bibr" target="#b48">See et al. (2017)</ref>: the pictures were taken by professional photographer scott gable , 39 , who spent four months travelling across the region documenting the labour and the arduous life of the farmers who cultivate them . the images were taken over a four month period from april to july last year across asia . mr gable said he was determined to capture every stage of production with his rice project . Our Baseline: rice is a staple food for more than one-half the world 's population . crop accounts for one fifth of all calories consumed by humans and 87 per cent of it is produced in asia . Multi-task: in a photo album that spans over china , thailand , vietnam , laos and cambodia , extraordinary images portray the crop 's full cycle from the primitive sowing of seeds to the distribution of millions of tonnes for consumption . the crop accounts for one fifth of all calories consumed by humans and 87 per cent of it is produced in asia .  <ref type="bibr" target="#b48">See et al. (2017)</ref>, our baseline, and 2-way multi-task model with summarization and question generation. All the unboxed-green highlighted words/phrases correspond to the salient information (based on the cloze-blanks of the original CNN/DailyMail Q&amp;A task/dataset <ref type="bibr" target="#b24">(Hermann et al., 2015)</ref>). As shown, our multi-task model is able to generate most of this saliency information, while the outputs from <ref type="bibr" target="#b48">See et al. (2017)</ref> and baseline missed most of them, especially the country names.</p><p>Input Document: celtic have written to the scottish football association in order to gain an ' understandingóf the refereeing decisions during their scottish cup semi-final defeat by inverness on sunday . the hoops were left outraged by referee steven mcleanś failure to award a penalty or red card for a clear handball in the box by josh meekings to deny leigh griffithś goal-bound shot during the first-half . caley thistle went on to win the game 3-2 after extra-time and denied rory deliaś men the chance to secure a domestic treble this season . celtic striker leigh griffiths has a goal-bound shot blocked by the outstretched arm of josh meekings . celticś adam matthews -lrb-right -rrb-slides in with a strong challenge on nick ross in the scottish cup semi-final . ' given the level of reaction from our supporters and across football , we are duty bound to seek an understanding of what actually happened ,ćeltic said in a statement . they added , ' we have not been given any other specific explanation so far and this is simply to understand the circumstances of what went on and why such an obvious error was made .however , the parkhead outfit made a point of congratulating their opponents , who have reached the first-ever scottish cup final in their history , describing caley as a ' fantastic club and saying ' reaching the final is a great achievement .ćeltic had taken the lead in the semi-final through defender virgil van dijkś curling free-kick on 18 minutes , but were unable to double that lead thanks to the meekings controversy . it allowed inverness a route back into the game and celtic had goalkeeper craig gordon sent off after the restart for scything down marley watkins in the area . greg tansey duly converted the resulting penalty . edward ofere then put caley thistle ahead , only for john guidetti to draw level for the bhoys . with the game seemingly heading for penalties , david raven scored the winner on 117 minutes , breaking thousands of celtic hearts . celtic captain scott brown -lrb-left -rrb-protests to referee steven mclean but the handball goes unpunished . griffiths shows off his acrobatic skills during celticś eventual surprise defeat by inverness . celtic pair aleksandar tonev -lrb-left -rrb-and john guidetti look dejected as their hopes of a domestic treble end . Ground-truth: celtic were defeated 3-2 after extra-time in the scottish cup semi-final . leigh griffiths had a goal-bound shot blocked by a clear handball. however, no action was taken against offender josh meekings . the hoops have written the sfa for an 'understanding' of the decision . <ref type="bibr" target="#b48">See et al. (2017)</ref>: john hartson was once on the end of a major hampden injustice while playing for celtic . but he can not see any point in his old club writing to the scottish football association over the latest controversy at the national stadium . hartson had a goal wrongly disallowed for offside while celtic were leading 1-0 at the time but went on to lose 3-2 . Our Baseline: john hartson scored the late winner in 3-2 win against celtic . celtic were leading 1-0 at the time but went on to lose 3-2 . some fans have questioned how referee steven mclean and additional assistant alan muir could have missed the infringement . Multi-task: celtic have written to the scottish football association in order to gain an ' understanding ' of the refereeing decisions . the hoops were left outraged by referee steven mclean 's failure to award a penalty or red card for a clear handball in the box by josh meekings . celtic striker leigh griffiths has a goal-bound shot blocked by the outstretched arm of josh meekings .  <ref type="bibr" target="#b48">See et al. (2017)</ref>, our baseline, and 3-way multi-task model with summarization and both entailment generation and question generation. The boxed-red highlighted words/phrases are not present in the input source document in any paraphrasing form. All the unboxedgreen highlighted words/phrases correspond to the salient information. See detailed discussion in <ref type="figure" target="#fig_1">Fig. 3</ref> and <ref type="figure" target="#fig_2">Fig. 4</ref> above. As shown, the outputs from <ref type="bibr" target="#b48">See et al. (2017)</ref> and the baseline both include nonentailed words/phrases (e.g. "john hartson"), as well as they missed salient information ("hoops", "josh meekings", "leigh griffiths") in their output summaries. Our multi-task model, however, manages to accomplish both, i.e., cover more salient information and also avoid unrelated information.</p><p>Premise: People walk down a paved street that has red lanterns hung from the buildings. Entailment: People walk down the street. Premise: A young woman on a boat in a light colored bikini kicks a man wearing a straw cowboy hat. Entailment: A young woman strikes a man with her feet. <ref type="figure">Figure 6</ref>: Output examples from our entailment generation model.</p><p>Input: The college of science was established at the university in 1865 by president father patrick dillon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>In what year was the college of science established ? Input: Notable athletes include swimmer sharron davies , diver tom daley , dancer wayne sleep , and footballer trevor francis . Question: What is the occupation of trevor francis ? <ref type="figure">Figure 7</ref>: Output examples from our question generation model. erage mechanism to our Gigaword models. Also, we set our beam search size to 5, following previous work <ref type="bibr" target="#b39">(Nallapati et al., 2016)</ref>. DUC: For the CNN/DM to DUC domain-transfer experiments where we allow the beam sizes of all models to be individually re-tuned on DUC-2003, the chosen tuned beam values are 10, 4, 3 for the multi-task model, baseline, and <ref type="bibr" target="#b48">See et al. (2017)</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Multi-Task Learning Details</head><p>Multi-Task Learning with Question Generation Two important hyperparameters tuned are the mixing ratio between summarization and entailment generation, as well as the soft-sharing coefficient. Here, we choose the mixing ratios 3:2 between CNN/DailyMail and SQuAD, 100:1 between Gigaword and SQuAD. Intuitively, these mixing ratios are close to the ratio of their dataset sizes. We set the soft-sharing coefficient γ to 5 × 10 −5 and 1 × 10 −5 for CNN/DailyMail and Gigaword, resp.</p><p>Multi-Task Learning with Entailment Generation Here, we choose the mixing ratios 3:2 between CNN/DailyMail and SNLI, 20:1 between Gigaword and SNLI. We again set the soft-sharing coefficient γ to 5 × 10 −5 and 1 × 10 −5 for CNN/DailyMail and Gigaword, resp.</p><p>Multi-Task Learning with Question and Entailment Generation Here, we choose the mixing ratios and soft-sharing coefficients to be 4:3:3 and 5 × 10 −5 for CNN/DailyMail, and 100:1:5 and 1.5 × 10 −6 for Gigaword respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Auxiliary Output Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Entailment Generation Examples</head><p>See <ref type="figure">Fig. 6</ref> for interesting output examples by our entailment generation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Question Generation Examples</head><p>See <ref type="figure">Fig. 7</ref> for interesting output examples by our question generation model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example summary from our 3way MTL model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Example showing summaries generated by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example showing summaries generated by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Example of summaries generated by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>DailyMail summarization results. ROUGE scores are full length F-1 (as previous work). All the multi-task improvements are statistically significant over the state-of-the-art baseline.Training Details All our soft/hard and layerspecific sharing decisions were made on the validation/development set. Details of RNN hidden state sizes, Adam optimizer, mixing ratios, etc. are provided in the supplementary for reproducibility.</figDesc><table><row><cell>Models Seq2Seq(50k vocab) (See et al., 2017) Pointer (See et al., 2017) Pointer+Coverage (See et al., 2017) Pointer+Coverage (See et al., 2017)  † Two-Layer Baseline (Pointer+Coverage) ⊗ ⊗ + Entailment Generation ⊗ + Question Generation ⊗ + Entailment Gen. + Question Gen.</cell><cell cols="2">ROUGE-1 PREVIOUS WORK 31.33 36.44 39.53 38.82 OUR MODELS 39.56 39.84 39.73 39.81</cell><cell>ROUGE-2 11.81 15.66 17.28 16.81 17.52 17.63 17.59 17.64</cell><cell>ROUGE-L 28.83 33.42 36.38 35.71 36.36 36.54 36.48 36.54</cell><cell>METEOR 12.03 15.35 18.72 18.14 18.17 18.61 18.33 18.54</cell></row><row><cell cols="2">Table 1: CNN/proval rate greater than 95%, and had at least 10,000 approved HITs. For the pairwise model comparisons discussed in Sec. 6.2, we showed the annotators the input article, the ground truth sum-mary, and the two model summaries (randomly shuffled to anonymize model identities) -we then asked them to choose the better among the two model summaries or choose 'Not-Distinguishable' if both summaries are equally good/bad. In-</cell><cell cols="4">Models ABS+ (Rush et al., 2015) PREVIOUS WORK R-1 29.76 11.88 26.96 R-2 R-L RAS-El (Chopra et al., 2016) 33.78 15.97 31.15 lvt2k (Nallapati et al., 2016) 32.67 15.59 30.64 Pasunuru et al. (2017) 32.75 15.35 30.82 OUR MODELS 2-Layer Pointer Baseline ⊗ 34.26 16.40 32.03 ⊗ + Entailment Generation 35.45 17.16 33.19 ⊗ + Question Generation 35.48 17.31 32.97 ⊗ + Entailment + Question 35.98 17.76 33.63</cell></row><row><cell cols="2">structions for relevance were defined based on</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">the summary containing salient/important infor-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">mation from the given article, being correct</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(i.e., avoiding contradictory/unrelated informa-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">tion), and avoiding redundancy. Instructions for</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">readability were based on the summary's fluency,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>grammaticality, and coherence.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6 Results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">6.1 Summarization (Primary Task) Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pointer+Coverage Baseline We start from the strong model of See et al. (2017). 3 Table 1 shows</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">that our baseline model performs better than or comparable to See et al. (2017). 4 On Gigaword</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dataset, our baseline model (with pointer only,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">since coverage not needed for this single-sentence</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">summarization task) performs better than all pre-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>vious works, as shown in Table 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>CNN/DM Human Evaluation: pairwise comparison between our 3-way multi-task (MTL) model w.r.t. our baseline and See et al. (2017).</figDesc><table><row><cell>Models MTL wins Baseline wins Non-distinguish.</cell><cell>Relevance Readability Total 33 32 65 22 22 44 45 46 91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Gigaword Human Evaluation: pairwise</cell></row><row><cell>comparison between our 3-way multi-task (MTL)</cell></row><row><cell>model w.r.t. our baseline.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc><ref type="bibr" target="#b48">See et al. (2017)</ref> 34.30 14.25 30.82 Baseline 35.96 15.91 32.92 Multi-Task (EG + QG) 36.73 16.15 33.58</figDesc><table><row><cell>Models</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>ROUGE F1 scores on DUC-2002. relevance and higher readability scores w.r.t. the baseline. W.r.t.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance of our pointer-based entailment generation (EG) models compared with previous SotA work. M, C, R, B are short for Meteor, CIDEr-D, ROUGE-L, and BLEU-4, resp.</figDesc><table><row><cell>Models Du et al. (2017) Our 1-layer pointer QG 15.4 75.3 36.2 M C R 15.2 -38.0 10.8 B 9.2 Our 2-layer pointer QG 17.5 95.3 40.1 13.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Performance of our pointer-based question generation (QG) model w.r.t. previous work.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Final Model 39.81 17.64 36.54 18.54 SOFT-VS.-HARD SHARING Hard-sharing 39.51 17.44 36.33 18.21 LAYER SHARING METHODS D1+D2 39.62 17.49 36.44 18.34 E1+D2 39.51 17.51 36.37 18.15 E1+Attn+D2 39.32 17.36 36.11 17.88</figDesc><table><row><cell>Models</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Saliency classification results of our baseline vs. QG-multi-task model (p &lt; 0.01).</figDesc><table><row><cell>Models See et al. (2017) MTL (3-way)</cell><cell>2-gram 3-gram 4-gram 2.24 6.03 9.72 2.84 6.83 10.66</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">About 4-5 days for approach vs. only 10 hours for us. This will allow the community to try many more multi-task training and tuning ideas faster.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use two layers so as to allow our high-versus lowlevel layer sharing intuition. Note that this does not increase the parameter size much (23M versus 22M for<ref type="bibr" target="#b48">See et al. (2017)</ref>).4  As mentioned in the github for<ref type="bibr" target="#b48">See et al. (2017)</ref>, their publicly released pretrained model produces the lower scores that we represent by † inTable 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Stat. significance is computed via bootstrap test<ref type="bibr" target="#b40">(Noreen, 1989;</ref><ref type="bibr" target="#b15">Efron and Tibshirani, 1994)</ref> with 100K samples.6  In order to verify that our improvements were from the auxiliary tasks' specific character/capabilities and not just</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">In the interest of space, most of the analyses are shown for CNN/DailyMail experiments, but we observed similar trends for the Gigaword experiments as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Note that all our soft and layer sharing decisions were strictly made on the dev/validation set (see Sec. 5).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://www-nlpir.nist.gov/projects/duc/ guidelines/2002.html 15 https://www-nlpir.nist.gov/projects/duc/ guidelines/2003.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments. This work was supported by DARPA (YFA17-D17AP00022), Google Faculty Research Award, Bloomberg Data Science Research Grant, and NVidia GPU awards. The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What do neural machine translation models learn about morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distraction-based neural networks for modeling documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft COCO captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised sentence enhancement for automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><forename type="middle">Chi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="775" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="399" to="429" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Text summarization using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibhansh</forename><surname>Dohare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Karnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01678</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to ask: Neural question generation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bringing structure into summaries: Crowdsourcing a benchmark corpus of concept maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Falke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on computational linguistics</title>
		<meeting>the 23rd international conference on computational linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Abstractive summarization of product reviews using discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bita</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nejat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1602" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic summarization from multiple documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Ph. D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Text summarization through entailment-based minimum vertex cover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manpreet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Mirkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lexical and Computational Semantics (* SEM 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A joint many-task model: Growing a neural network for multiple nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A reinforcement learning approach for adaptive single-and multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Henß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Mieskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GSCL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Duenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Baquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="732" to="742" />
		</imprint>
	</monogr>
	<note>Av Juan Dios Bátiz, and Av Mendizábal</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cut and paste based text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000</title>
		<meeting>the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">One model to learn them all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1706.05137</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting salient updates for disaster summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Kedzie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1608" to="1617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: A probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="107" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning task grouping and overlap in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Illinois-lh: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL: HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Abstractive meeting summarization with entailment and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond T</forename><surname>Tompa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th European Workshop on Natural Language Generation</title>
		<meeting>of the 14th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="136" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-tosequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric W Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multitask video captioning with video and entailment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multireward reinforced summarization with saliency and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Towards improving abstractive summarization via entailment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In NFiS@EMNLP</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Sluice networks: Learning what to share between loosely related tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Sogaard</surname></persName>
		</author>
		<idno>abs/1705.08142</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rnn-based encoder-decoder approach with word frequency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
