<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
							<email>kylemin@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
							<email>jjcorso@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>A2CL-PT</term>
					<term>temporal activity localization</term>
					<term>adversarial learn- ing</term>
					<term>weakly-supervised learning</term>
					<term>center loss with a pair of triplets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporally localizing activities within untrimmed videos has been extensively studied in recent years. Despite recent advances, existing methods for weakly-supervised temporal activity localization struggle to recognize when an activity is not occurring. To address this issue, we propose a novel method named A2CL-PT. Two triplets of the feature space are considered in our approach: one triplet is used to learn discriminative features for each activity class, and the other one is used to distinguish the features where no activity occurs (i.e. background features) from activity-related features for each video. To further improve the performance, we build our network using two parallel branches which operate in an adversarial way: the first branch localizes the most salient activities of a video and the second one finds other supplementary activities from non-localized parts of the video. Extensive experiments performed on THUMOS14 and ActivityNet datasets demonstrate that our proposed method is effective. Specifically, the average mAP of IoU thresholds from 0.1 to 0.9 on the THUMOS14 dataset is significantly improved from 27.9% to 30.0%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main goal of temporal activity localization is to find the start and end times of activities from untrimmed videos. Many of the previous approaches are fully supervised: they expect that ground-truth annotations for temporal boundaries of each activity are accessible during training <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. However, collecting these frame-level activity annotations is time-consuming and difficult, leading to annotation noise. Hence, a weakly-supervised version has taken foot in the community: here, one assumes that only video-level groundtruth activity labels are available. These video-level activity annotations are much easier to collect and already exist across many datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31]</ref>, thus weakly-supervised methods can be applied to a broader range of situations.</p><p>Current work in weakly-supervised temporal activity localization shares a common framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9]</ref>. First, rather than using a raw video, they use c is their corresponding center and cn is the negative center. A triplet of (F, c, cn) is used to learn discriminative features. We propose to exploit another triplet of (c, F, f ) which distinguishes background features from the activity-related features. We call this method of two triplets ACL-PT. In addition, we design our network with two parallel branches so that the two separate sets of centers can be learned in an adversarial way. We call our final proposed method A2CL-PT. (b): Sample frames of a video containing Diving activity class from THUMOS14 dataset <ref type="bibr" target="#b4">[5]</ref> and the corresponding results of activity localization. It is shown that our final method A2CL-PT performs the best.</p><p>a sequence of features extracted by deep networks where the features are much smaller than the raw video in size. Second, they apply a fully-connected layer to embed the pre-extracted features to the task-specific feature space. Third, they project the embedded features to the label space by applying a 1-D convolutional layer to those features. The label space has the same dimension as the number of activities, so the final output becomes a sequence of vectors that represents the classification scores for each activity over time. Each sequence of vectors is typically referred to as CAS (Class Activation Sequence) <ref type="bibr" target="#b20">[21]</ref> or T-CAM (Temporal Class Activation Map) <ref type="bibr" target="#b16">[17]</ref>. Finally, activities are localized by thresholding this T-CAM. T-CAM is sometimes applied with the softmax function to generate class-wise attention. This top-down attention represents the probability mass function for each activity over time.</p><p>An important component in weakly-supervised temporal activity localization is the ability to automatically determine background portions of the video where no activity is occurring. For example, BaS-Net <ref type="bibr" target="#b8">[9]</ref> suggests using an additional suppression objective to suppress the network activations on the background portions. Nguyen et al. <ref type="bibr" target="#b17">[18]</ref> proposes a similar objective to model the background contents. However, we argue that existing methods are not able to sufficiently distinguish background information from activities of interest for each video even though such an ability is critical to strong temporal activity localization.</p><p>To this end, we propose a novel method for the task of weakly-supervised temporal activity localization, which we call Adversarial and Angular Center Loss with a Pair of Triplets (A2CL-PT). It is illustrated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Our key innovation is that we explicitly enable our model to capture the background re-gion of the video while using an adversarial approach to focus on completeness of the activity learning. Our method is built on two triplets of vectors of the feature space, and one of them is designed to distinguish background portions from the activity-related parts of a video. Our method is inspired by the angular triplet-center loss (ATCL) <ref type="bibr" target="#b9">[10]</ref> originally designed for multi-view 3D shape retrieval. Let us first describe what ATCL is and then how we develop our novel method of A2CL-PT.</p><p>In ATCL <ref type="bibr" target="#b9">[10]</ref>, a center is defined as a parameter vector representing the center of a cluster of feature vectors for each class. During training, the centers are updated by reducing the angular distance between the embedded features and their corresponding class centers. This groups together features that correspond to the same class and distances features from the centers of other class clusters (i.e. negative centers), making the learned feature space more useful for discriminating between classes. It follows that each training sample is a triplet of a feature vector, its center, and a negative center where the feature serves as an anchor.</p><p>Inspired by ATCL, we first formulate a loss function to learn discriminative features. ATCL cannot be directly applied to our problem because it assumes that all the features are of the same size, whereas an untrimmed video can have any number of frames. Therefore, we use a different feature representation at the video-level. We aggregate the embedded features by multiplying the top-down attention described above at each time step. The resulting video-level feature representation has the same dimension as the embedded features, so we can build a triplet whose anchor is the video-level feature vector (it is (F, c, c n ) in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). This triplet ensures that the embedded features of the same activity are grouped together and that they have high attention values at time steps when the activity occurs.</p><p>More importantly, we argue that it is possible to exploit another triplet. Let us call the features at time steps when some activity occurs activity features, and the ones where no activity occurs background features. The main idea is that the background features should be distinguished from the activity features for each video. First, we generate a new class-wise attention from T-CAM. It has higher attention values for the background features when compared to the original top-down attention. If we aggregate the embedded features with this new attention, the resulting video-level feature will be more attended to the background features than the original video-level feature is. In a discriminative feature space, the original video-level feature vector should be closer to its center than the new video-level feature vector is. This property can be achieved by using the triplet of the two different video-level feature vectors and their corresponding center where the center behaves as an anchor (it is (c, F, f ) in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). The proposed triplet is novel and will be shown to be effective. Since we make use of a pair of triplets on the same feature space, we call it Angular Center Loss with a Pair of Triplets (ACL-PT).</p><p>To further improve the localization performance, we design our network to have two parallel branches which find activities in an adversarial way, also illus-trated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Using a network with a single branch may be dominated by salient activity features that are too short to localize all the activities in time. We zero out the most salient activity features localized by the first branch for each activity so that the second (adversarial) branch can find other supplementary activities from the remaining parts of the video. Here, each branch has its own set of centers which group together the features for each activity and one 1-D convolutional layer that produces T-CAM. The two adversary T-CAMs are weighted to produce the final T-CAM that is used to localize activities. We want to note that our network produces the final T-CAM with a single forward pass so it is trained in an end-to-end manner. We call our final proposed method Adversarial and Angular Center Loss with a Pair of Triplets (A2CL-PT). It is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(b) that our final method performs the best.</p><p>There are three main contributions in this paper: • We propose a novel method using a pair of triplets. One facilitates learning discriminative features. The other one ensures that the background features are distinguishable from the activity-related features for each video. • We build an end-to-end two-branch network by adopting an adversarial approach to localize more complete activities. Each branch comes with its own set of centers so that embedded features of the same activity can be grouped together in an adversarial way by the two branches. • We perform extensive experiments on THUMOS14 and ActivityNet datasets and demonstrate that our method outperforms all the previous state-of-theart approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Center loss (CL) <ref type="bibr" target="#b24">[25]</ref> is recently proposed to reduce the intra-class variations of feature representations. CL learns a center for each class and penalizes the Euclidean distance between the features and their corresponding centers. Tripletcenter loss (TCL) <ref type="bibr" target="#b3">[4]</ref> shows that using a triplet of each feature vector, its corresponding center, and a nearest negative center is effective in increasing the inter-class separability. TCL enforces that each feature vector is closer to its corresponding center than to the nearest negative center by a pre-defined margin. Angular triplet-center loss (ATCL) <ref type="bibr" target="#b9">[10]</ref> further improves TCL by using the angular distance. In ATCL, it is much easier to design a better margin because it has a clear geometric interpretation and is limited from 0 to π. BaS-Net <ref type="bibr" target="#b8">[9]</ref> and Nguyen et al. <ref type="bibr" target="#b17">[18]</ref> are the leading state-of-the-art methods for weakly-supervised temporal activity localization. They take similar approaches to utilize the background portions of a video. There are other recent works without explicit usage of background information. Liu et al. <ref type="bibr" target="#b11">[12]</ref> utilizes multi-branch network where T-CAMs of these branches differ from each other. This property is enforced by the diversity loss: the sum of the simple cosine distances between every pair of the T-CAMs. 3C-Net applies an idea of CL, but the performance is limited because CL does not consider the inter-class separability. Sequences of features are extracted from two input streams using pre-trained I3D networks <ref type="bibr" target="#b0">[1]</ref>. We use two fully-connected layers with ReLU activation (FC) to compute the embedded features X r i , X o i . Next, T-CAMs C r i , C o i are computed by applying 1-D convolutional layers (Conv). The most salient activity features localized by the first branch are zeroed out for each activity class, and the resulting features are applied with Using an end-to-end two-branch network that operates in an adversarial way is proposed in Adversarial Complementary Learning (ACoL) <ref type="bibr" target="#b29">[30]</ref> for the task of weakly-supervised object localization. In ACoL, object localization maps from the first branch are used to erase the salient regions of the input feature maps for the second branch. The second branch then tries to find other complementary object areas from the remaining regions. To the best of our knowledge, we are the first to merge the idea of ACoL with center loss and to apply it to weaklysupervised temporal activity localization.</p><formula xml:id="formula_0">different 1-D convolutional layers (Conv) to produce C ra i , C oa i . Using the embedded features X r i , X o i and T-CAMs C r i , C o i , C ra i , C oa i ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The overview of our proposed method is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The total loss function is represented as follows:</p><formula xml:id="formula_1">L = αL A2CL-PT + L CLS<label>(1)</label></formula><p>where L A2CL-PT and L CLS denote our proposed loss term and the classification loss, respectively. α is a hyperparameter to control the weight of A2CL-PT term.</p><p>In this section, we describe each component of our method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Embedding</head><p>Let us say that we have N training videos {v i } N i=1 . Each video v i has its groundtruth annotation for video-level label y i ∈ R Nc where N c is the number of activity classes. y i (j) = 1 if the activity class j is present in the video and y i (j) = 0 otherwise. We follow previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref> to extract the features for both RGB and optical flow streams. First, we divide v i into non-overlapping 16-frame segments. We then apply I3D <ref type="bibr" target="#b0">[1]</ref> pretrained on Kinetics dataset <ref type="bibr" target="#b5">[6]</ref> to the segments. The intermediate D-dimensional (D = 1024) outputs after the global pooling layer are the pre-extracted features. For the task-specific feature embedding, we use two fully-connected layers with ReLU activation. As a result, sequences of the embedded features X r i , X o i ∈ R D×li are computed for RGB and optical flow stream where l i denotes the temporal length of the features of the video v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Angular Center Loss with a Pair of Triplets (ACL-PT)</head><p>For simplicity, we first look at the RGB stream. The embedded features X r i are applied with a 1-D convolutional layer. The output is T-CAM C r i ∈ R Nc×li which represents the classification scores of each activity class over time. We compute class-wise attention A r i ∈ R Nc×li by applying the softmax function to T-CAM:</p><formula xml:id="formula_2">A r i (j, t) = exp C r i (j, t) li t =1 exp C r i (j, t )<label>(2)</label></formula><p>where j ∈ {1, ..., N c } denotes each activity class and t is for each time step. Since this top-down attention represents the probability mass function of each activity over time, we can use it to aggregate the embedded features X r i :</p><formula xml:id="formula_3">F r i (j) = li t=1 A r i (j, t)X r i (t)<label>(3)</label></formula><p>where F r i (j) ∈ R D denotes a video-level feature representation for the activity class j. Now, we can formulate a loss function that is inspired by ATCL [10] on the video-level feature representations as follows:</p><formula xml:id="formula_4">L r ATCL = 1 N N i=1 j:yi(j)=1 max 0, D F r i (j), c r j − D F r i (j), c r n r i,j + m 1<label>(4)</label></formula><p>where c r j ∈ R D is the center of activity class j, n r i,j = argmin k =j D F r i (j), c r k is an index for the nearest negative center, and m 1 ∈ [0, π] is an angular margin. It is based on the triplet of (F r i (j), c r j , c r n r i,j ) that is illustrated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Here, D(·) represents the angular distance:</p><formula xml:id="formula_5">D F r i (j), c r j = arccos F r i (j) · c r j F r i (j) 2 c r j 2<label>(5)</label></formula><p>Optimizing the loss function of Eq. 4 ensures that the video-level features of the same activity class are grouped together and that the inter-class variations of those features are maximized at the same time. As a result, the embedded features are learned to be discriminative and T-CAM will have higher values for the activity-related features.</p><p>For the next step, we exploit another triplet. We first compute a new classwise attention a r i ∈ R Nc×li from T-CAM:</p><formula xml:id="formula_6">a r i (j, t) = exp βC r i (j, t) li t =1 exp βC r i (j, t )<label>(6)</label></formula><p>where β is a scalar between 0 and 1. This new attention still represents the probability mass function of each activity over time, but it is supposed to have lower values for the activity features and higher values for the background features when compared to the original attention A r i . Therefore, if we aggregate the embedded features X r i using a r i , the resulting new video-level feature f r i should attend more strongly to the background features than F r i is. This property can be enforced by introducing a different loss function based on the new triplet of (c r j , F r i (j), f r i (j)) that is also illustrated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>:</p><formula xml:id="formula_7">L r NT = 1 N N i=1 j:yi(j)=1 max 0, D F r i (j), c r j − D f r i (j), c r j + m 2<label>(7)</label></formula><p>where the subscript NT refers to the new triplet and m 2 ∈ [0, π] is an angular margin. Optimizing this loss function makes the background features more distinguishable from the activity features. Merging the two loss functions of Eq. 4 and Eq. 7 gives us a new loss based on a pair of triplets, which we call Angular Center Loss with a Pair of Triplets (ACL-PT):</p><formula xml:id="formula_8">L r ACL-PT = L r ATCL + γL r NT<label>(8)</label></formula><p>where γ is a hyperparameter denoting the relative importance of the two losses. Previous works on center loss <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref> suggest using an averaged gradient (typically denoted as ∆c r j ) to update the centers for better stability. Following this convention, the derivatives of each term of Eq. 8 with respect to the centers are averaged. For simplicity, we assume that the centers have unit length. Refer to the supplementary material for general case without such assumption. Let L r ATCLi,j andL r NTi,j be the loss terms inside the max operation of the i-th sample and of the j-th activity class as follows:</p><formula xml:id="formula_9">L r ATCLi,j = D F r i (j), c r j − D F r i (j), c r n r i,j + m 1 (9) L r NTi,j = D F r i (j), c r j − D f r i (j), c r j + m 2<label>(10)</label></formula><p>Next, let g r 1i,j and g r 2i,j be the derivatives of Eq. 9 with respect to c r j and c r n r i,j , respectively; and let h r i,j be the derivative of Eq. 10 with respect to c r j . For example, g r 1i,j is given by:</p><formula xml:id="formula_10">g r 1i,j = − F r i (j) sin D F r i (j), c r j F r i (j) 2<label>(11)</label></formula><p>Then, we can represent the averaged gradient considering the three terms:</p><formula xml:id="formula_11">∆c r j = ∆ g r 1 i,j + ∆ g r 2 i,j + ∆ h r i,j<label>(12)</label></formula><p>For example, ∆ g r</p><formula xml:id="formula_12">1 i,j</formula><p>is computed as follows:</p><formula xml:id="formula_13">∆ g r 1 i,j = 1 N i:yi(j)=1 g r 1i,j δ(L r ATCLi,j &gt; 0) 1 + i:yi(j)=1 δ(L r ATCLi,j &gt; 0)<label>(13)</label></formula><p>Here, δ(condition) = 1 if the condition is true and δ(condition) = 0 otherwise. Finally, the centers are updated using ∆c r j for every iteration of the training process by a gradient descent algorithm. More details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adopting an adversarial approach (A2CL-PT)</head><p>We further improve the performance of the proposed ACL-PT by applying an adversarial approach inspired by ACoL <ref type="bibr" target="#b29">[30]</ref>. For each stream, there are two parallel branches that operate in an adversarial way. The motivation is that a network with a single branch might be dominated by salient activity features that are not enough to localize all the activities in time. We zero out the most salient activity features localized by the first branch for activity class j of v i as follows:</p><formula xml:id="formula_14">X ra i,j (t) = 0, if C r i (j, t) ∈ top-k a elements of C r i (j) X r i (t), otherwise<label>(14)</label></formula><p>where X ra i,j ∈ R D×li denotes the input features of activity class j for the second (adversarial) branch and k a is set to li sa for a hyperparameter s a that controls the ratio of zeroed-out features. For each activity class j, a separate 1-D convolutional layer of the adversarial branch transforms X ra i,j to the classification scores of the activity class j over time. By iterating over all the activity classes, new T-CAM C ra i ∈ R Nc×li is computed. We argue that C ra i can be used to find other supplementary activities that are not localized by the first branch. By using the original features X r i , new T-CAM C ra i , and a separate set of centers {c ra j } Nc j=1 , we can compute the loss of ACL-PT for this adversarial branch L ra ACL-PT in a similar manner (Eq. 1-7). We call the sum of the losses of the two branches Adversarial and Angular Center Loss with a Pair of Triplets (A2CL-PT):</p><formula xml:id="formula_15">L r A2CL-PT = L r ACL-PT + L ra ACL-PT<label>(15)</label></formula><p>In addition, the losses for the optical flow stream L o ACL-PT and L oa ACL-PT are also computed in the same manner. As a result, the total A2CL-PT term is given by:</p><formula xml:id="formula_16">L A2CL-PT = L r A2CL-PT + L o A2CL-PT<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification Loss</head><p>Following the previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>, we use the cross-entropy between the predicted pmf (probability mass function) and the ground-truth pmf of activities for classifying different activity classes in a video. We will first look at the RGB stream. For each video v i , we compute the class-wise classification scores s r i ∈ R Nc by averaging top-k elements of C r i per activity class where k is set to li s for a hyperparameter s. Then, the softmax function is applied to compute the predicted pmf of activities p r i ∈ R Nc . The ground-truth pmf q i is obtained by l 1 -normalizing y i . Then, the classification loss for the RGB stream is:</p><formula xml:id="formula_17">L r CLS = 1 N N i=1 Nc j=1 −q i (j) log p r i (j)<label>(17)</label></formula><p>The classification loss for the optical flow stream L o CLS is computed in a similar manner. L ra CLS and L oa CLS of adversarial branches are also computed in the same way.</p><p>Finally, we compute the final T-CAM C F i from the four T-CAMs (two from the RGB stream: C r i , C ra i , two from the optical flow stream: C o i , C oa i ) as follows:</p><formula xml:id="formula_18">C F i = w r · (C r i + ωC ra i ) + w o · (C o i + ωC oa i )<label>(18)</label></formula><p>where w r , w o ∈ R Nc are class-specific weighting parameters that are learned during training and ω is a hyperparameter for the relative importance of T-CAMs from the adversarial branch. We can then compute the classification loss for the final T-CAM L F CLS in the same manner. The total classification loss is given by:</p><formula xml:id="formula_19">L CLS = L r CLS + L ra CLS + L o CLS + L oa CLS + L F CLS<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Classification and Localization</head><p>During the test time, we use the final T-CAM C F i for the classification and localization of activities following the previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref>. First, we compute the class-wise classification scores s F i ∈ R Nc and the predicted pmf of activities p F i ∈ R Nc as described in Section 3.4. We use p F i for activity classification. For activity localization, we first find a set of possible activities that has positive classification scores, which is {j : s F i (j) &gt; 0}. For each activity in this set, we localize all the temporal segments that has positive T-CAM values for two or more successive time steps. Formally, a set of localized temporal segments for v i is:</p><formula xml:id="formula_20">{[s, e] : ∀t ∈ [s, e], C F i (t) &gt; 0 and C F i (s − 1) &lt; 0 and C F i (e + 1) &lt; 0} (20)</formula><p>where C F i (0) and C F i (l i + 1) are defined to be any negative values and e ≥ s + 2. The localized segments for each activity are non-overlapping. We assign a confidence score for each localized segment, which is the sum of the maximum T-CAM value of the segment and the classification score of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation</head><p>We evaluate our method on two datasets: THUMOS14 <ref type="bibr" target="#b4">[5]</ref> and ActivityNet1.3 <ref type="bibr" target="#b2">[3]</ref>. For the THUMOS14 dataset, the validation videos are used for training without temporal boundary annotations and the test videos are used for evaluation following the convention in the literature. This dataset is known to be challenging because each video has a number of activity instances and the duration of the videos varies widely. For the ActivityNet1.3 dataset, we use the training set for training and the validation set for evaluation. Following the standard evaluation protocol, we report mean average precision (mAP) at different intersection over union (IoU) thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>First, we extract RGB frames from each video at 25 fps and generate optical flow frames by using the TV-L1 algorithm <ref type="bibr" target="#b28">[29]</ref>. Each video is then divided into non-overlapping 16-frame segments. We apply I3D networks <ref type="bibr" target="#b0">[1]</ref> pre-trained on Kinetics dataset <ref type="bibr" target="#b5">[6]</ref> to the segments to obtain the intermediate 1024-dimensional features after the global pooling layer. We train our network in an end-to-end manner using a single GPU (TITAN Xp).</p><p>For the THUMOS14 dataset <ref type="bibr" target="#b4">[5]</ref>, we train our network using a batch size of 32. We use the Adam optimizer <ref type="bibr" target="#b6">[7]</ref> with learning rate 10 −4 and weight decay 0.0005. The centers are updated using the SGD algorithm with learning rate 0.1 for the RGB stream and 0.2 for the optical flow stream. The kernel size of the 1-D convolutional layers for the T-CAMs is set to 1. We set α in Eq. 1 to 1 and γ in Eq. 8 to 0.6. For β in Eq. 6, we randomly generate a number between 0.001 and 0.1 for each training sample. We set angular margins m 1 to 2 and m 2 to 1. s a of Eq. 14 and s for the classification loss are set to 40 and 8, respectively. Finally, ω in Eq. 18 is set to 0.6. The whole training process of 40.5k iterations takes less than 14 hours.</p><p>For the ActivityNet1.3 dataset <ref type="bibr" target="#b2">[3]</ref>, it is shown from the previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16</ref>] that post-processing of the final T-CAM is required. We use an additional 1-D convolutional layer (kernel size=13, dilation=2) to post-process the final T-CAM. The kernel size of the 1-D convolutional layers for T-CAMs is set to 3. In addition, we change the batch size to 24. The learning rate for centers are 0.05 and 0.1 for the RGB and optical flow streams, respectively. We set α to 2, γ to 0.2, and ω to 0.4. The remaining hyperparameters of β, m 1 , m 2 , s a , and s are the same as above. We train the network for 175k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with the State-of-the-art</head><p>We compare our final method A2CL-PT with other state-of-the-art approaches on the THUMOS14 dataset <ref type="bibr" target="#b4">[5]</ref> in <ref type="table">Table 1</ref>. Full supervision refers to training from frame-level activity annotations, whereas weak supervision indicates training <ref type="table">Table 1</ref>. Performance comparison of A2CL-PT with state-of-the-art methods on the THUMOS14 dataset <ref type="bibr" target="#b4">[5]</ref>. A2CL-PT significantly outperforms all the other weaklysupervised methods. † indicates an additional usage of other ground-truth annotations or independently collected data. A2CL-PT also outperforms all weakly †-supervised methods that use additional data at higher IoUs (from 0.4 to 0.9). The column AVG is for the average mAP of IoU threshold from 0.1 to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervision</head><p>Method mAP(%)@ IoU 0. only from video-level activity labels. For fair comparison, we use the symbol † to separate methods utilizing additional ground-truth annotations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref> or independently collected data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. The column AVG is for the average mAP of IoU thresholds from 0.1 to 0.9 with a step size of 0.1. Our method significantly outperforms other weakly-supervised methods across all metrics. Specifically, an absolute gain of 2.1% is achieved in terms of the average mAP when compared to the best previous method (BaS-Net <ref type="bibr" target="#b8">[9]</ref>). We want to note that our method performs even better than the methods of weak † supervision at higher IoUs. We also evaluate A2CL-PT on the ActivityNet1.3 dataset <ref type="bibr" target="#b2">[3]</ref>. Following the standard evaluation protocol of the dataset, we report mAP at different IoU thresholds, which are from 0.05 to 0.95. As shown in <ref type="table">Table 2</ref>, our method again achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study and Analysis</head><p>We perform an ablation study on the THUMOS14 dataset <ref type="bibr" target="#b4">[5]</ref>. In <ref type="table">Table 3</ref>, we analyze the two main contributions of this work, which are the usage of the newlysuggested triplet (Eq. 7) and the adoption of adversarial approach (Eq. 15). ATCL refers to the baseline that uses only the loss term of Eq. 4. We use the superscript + to indicate the addition of adversarial branch. As described in Section 3.2, ACL-PT additionally uses the new triplet on top of the baseline. <ref type="table">Table 2</ref>. Performance comparison on the ActivityNet1.3 dataset <ref type="bibr" target="#b2">[3]</ref>. A2CL-PT again achieves the best performance. † indicates an additional usage of other ground-truth annotations or independently collected data. The column AVG is for the average mAP of IoU threshold from 0.5 to 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervision</head><p>Method mAP(%)@ IoU 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 AVG Weak † Liu et al. <ref type="bibr" target="#b11">[12]</ref> 34.0 ----20  <ref type="table">Table 3</ref>. Performance comparison of different ablative settings on the THUMOS14 dataset <ref type="bibr" target="#b4">[5]</ref>. The superscript + indicates that we add an adversarial branch to the baseline method. It demonstrates that both components are effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>New triplet Adversarial mAP(%)@ IoU 0.3 0.4 0.5 0.6 0.7 AVG(0. We can observe that our final proposed method, A2CL-PT, performs the best. It implies that both components are necessary to achieve the best performance and each of them is effective. Interestingly, adding an adversarial branch does not bring any performance gain without our new triplet. We think that although using ACL-PT increases the localization performance by learning discriminative features, it also makes the network sensitive to salient activity-related features. We analyze the impact of two main hyperparameters in <ref type="figure" target="#fig_3">Fig. 3</ref>. The first one is α that controls the weight of A2CL-PT term (Eq. 1), and the other one is ω that is for the relative importance of T-CAMs from adversarial branches (Eq. 18). We can observe from <ref type="figure" target="#fig_3">Fig. 3</ref>(a) that positive α always brings the performance gain. It indicates that A2CL-PT is effective. As seen in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>, the performance is increased by using an adversarial approach when ω is less or equal to 1. If ω is greater than 1, T-CAMs of adversarial branches will play a dominant role in activity localization. Therefore, the results tell us that the adversarial branches provide mostly supplementary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Analysis</head><p>We perform a qualitative analysis to better understand our method. In <ref type="figure" target="#fig_4">Fig. 4</ref>, qualitative results of our A2CL-PT on four videos from the test set of the THUMOS14 dataset <ref type="bibr" target="#b4">[5]</ref> are presented. If ω is too large, the performance is decreased substantially. It implies that T-CAMs of adversarial branches provide mostly supplementary information.</p><p>also show the results of BaS-Net <ref type="bibr" target="#b8">[9]</ref>, which is the leading state-of-the-art method.</p><p>We use three different colors on the contours of sampled frames: blue, green, and red which denote true positive, false positive, and false negative, respectively. In (a), there are multiple instances of false positive. These false positives are challenging because the person in the video swings the javelin, which can be mistaken for a throw. Similar cases are observed in (b). One of the false positives includes the person drawing the line on the field, which looks similar to a HammerThrow activity. In (c), some false negative segments are observed. Interestingly, this is because the ground-truth annotations are wrong; that is, the ThrowDiscus activity is annotated but it does not actually occur in these cases. In (d), all the instances of the HighJump activity are successfully localized. Other than the unusual situations, our method performs well in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented A2CL-PT as a novel method for weakly-supervised temporal activity localization. We suggest using two triplets of vectors of the feature space to learn discriminative features and to distinguish background portions from activity-related parts of a video. We also propose to adopt an adversarial approach to localize activities more thoroughly. We perform extensive experiments to show that our method is effective. A2CL-PT outperforms all the existing state-of-the-art methods on major datasets. Ablation study demonstrates that both contributions are significant. Finally, we qualitatively analyze the effectiveness of our method in detail. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a): An illustration of the proposed A2CL-PT. F and f are aggregated videolevel features where f is designed to be more attended to the background features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An illustration of our overall architecture. It consists of two streams (RGB and optical flow), and each stream consists of two (first and adversarial) branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>we compute the term of A2CL-PT (Eq. 16). The final T-CAM C F i is computed from the four T-CAMs and these T-CAMs are used to compute the loss function for classification (Eq. 19).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(a), (b), (c), and (d) are examples of JavelinThrow, HammerThrow, ThrowDiscus, and HighJump, respectively. Detection denotes the localized activity segments. For additional comparison, we We analyze the impact of two main hyperparameters α and ω. (a): Positive α always provides the performance gain, so it indicates that our method is effective. (b):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results on the THUMOS14 dataset<ref type="bibr" target="#b4">[5]</ref>. Detection denotes the localized activity segments. The results of BaS-Net<ref type="bibr" target="#b8">[9]</ref> are also included for additional comparison. Contours of the sampled frames have three different colors. We use blue, green, and red to indicate true positives, false positives, and false negatives, respectively.(a): An example of JavelinThrow activity class. The observed false positives are challenging. The person in the video swings the javelin on the frames of these false positives, which can be mistaken for a throw. (b): An example of HammerThrow. One of the false positives include the person who draws the line on the field. It is hard to distinguish the two activities. (c): An example of ThrowDiscus. Multiple false negatives are observed, which illustrates the situations where the ground-truth activity instances are wrongly annotated. (d): An example of HighJump without such unusual cases. It can be observed that our method performs well in general.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Ours) 36.8 33.6 30.8 27.8 24.9 22.0 18.1 14.9 10.2 5.2 22.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">9 -</cell><cell>-</cell><cell>-</cell><cell cols="2">5.7 21.2</cell></row><row><cell></cell><cell cols="2">Nguyen et al. [18] 36.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>19.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.9</cell><cell>-</cell></row><row><cell></cell><cell>STAR [27]</cell><cell>31.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>18.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.7</cell><cell>-</cell></row><row><cell></cell><cell>STPN [17]</cell><cell>29.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>16.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.6</cell><cell>-</cell></row><row><cell>Weak</cell><cell>MAAN [28] BaS-Net [9]</cell><cell>33.7 34.5</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell cols="3">--22.5 -21.9 -</cell><cell>--</cell><cell>--</cell><cell>5.5 4.9</cell><cell>-22.2</cell></row><row><cell></cell><cell>A2CL-PT (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We thank Stephan Lemmer, Victoria Florence, Nathan Louis, and Christina Jung for their valuable feedback and comments. This research was, in part, supported by NIST grant 60NANB17D191.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A largescale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Triplet-center loss for multi-view 3d object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Background suppression network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Angular triplet-center loss for multi-view 3d shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8682" to="8689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised temporal action localization through contrast based evaluation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2901464</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2901464" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8679" to="8687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6752" to="6761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization with background modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5502" to="5511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cdc: Convolutionalde-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoloc: Weaklysupervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Segregated temporal assembly recurrent networks for weakly supervised multiple action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9070" to="9078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Marginalized average attentional network for weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8668" to="8678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
