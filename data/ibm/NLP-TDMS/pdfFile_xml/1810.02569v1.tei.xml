<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Object Detection in Artworks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Gonthier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Telecom ParisTech</orgName>
								<orgName type="institution" key="instit2">Universite Paris-Saclay</orgName>
								<address>
									<postCode>75013</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Telecom ParisTech</orgName>
								<orgName type="institution" key="instit2">Universite Paris-Saclay</orgName>
								<address>
									<postCode>75013</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bonfait</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Telecom ParisTech</orgName>
								<orgName type="institution" key="instit2">Universite Paris-Saclay</orgName>
								<address>
									<postCode>75013</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Universite de Bourgogne</orgName>
								<orgName type="laboratory">UMR CNRS UB 5605</orgName>
								<address>
									<postCode>21000</postCode>
									<settlement>Dijon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Object Detection in Artworks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>weakly supervised detection · transfer learning · art analysis · multiple instance learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method for the weakly supervised detection of objects in paintings. At training time, only image-level annotations are needed. This, combined with the efficiency of our multiple-instance learning method, enables one to learn new classes on-the-fly from globally annotated databases, avoiding the tedious task of manually marking objects. We show on several databases that dropping the instance-level annotations only yields mild performance losses. We also introduce a new database, IconArt, on which we perform detection experiments on classes that could not be learned on photographs, such as Jesus Child or Saint Sebastian. To the best of our knowledge, these are the first experiments dealing with the automatic (and in our case weakly supervised) detection of iconographic elements in paintings. We believe that such a method is of great benefit for helping art historians to explore large digital databases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Several recent works show that recycling analysis tools that have been developed for natural images (photographs) can yield surprisingly good results for analysing paintings or drawings. In particular, impressive classification results are obtained on painting databases by using convolutional neural networks (CNNs) designed for the classification of photographs <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b53">55]</ref>. These results occur in a general context were methods of transfer learning <ref type="bibr" target="#b15">[15]</ref> (changing the task a model was trained for) and domain adaptation (changing the nature of the data a model was trained on) are increasingly applied. Classifying and analysing paintings is of course of great interest to art historians, and can help them to take full advantage of the massive artworks databases that are built worldwide.</p><p>More difficult than classification, and at the core of many recent computer vision works, the object detection task (classifying and localising an object) has been less studied in the case of paintings, although exciting results have been obtained, again using transfer techniques <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b29">29]</ref>.</p><p>Methods that detect objects in photographs have been developed thanks to massive image databases on which several classes (such as cats, people, cars) have been manually localised with bounding boxes. The PASCAL VOC <ref type="bibr" target="#b18">[18]</ref> and MS COCO <ref type="bibr" target="#b35">[35]</ref> datasets have been crucial in the development of detection methods and the recently introduced Google Open Image Dataset (2M images, 15M boxes for 600 classes) is expected to push further the limits of detection. Now, there is no such database (with localised objects) in the field of Art History, even though large databases are being build by many institutions or academic research teams, e.g. <ref type="bibr">[44,</ref><ref type="bibr">43,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b51">53]</ref>. Some of these databases include image-level annotations, but none includes location annotations. Besides, manually annotating such large databases is tedious and must be performed each time a new category is searched for. Therefore, it is of great interest to develop weakly supervised detection methods, that can learn to detect objects using image-level annotations only. While this aspect has been thoroughly studied for natural images, only a few studies have been dedicated to the case of painting or drawings.</p><p>Moreover, these studies are mostly dedicated to the cross depiction problem: they learn to detect the same objects in photographs and in paintings, in particular man-made objects (cars, bottles ...) or animals. While these may be useful to art historians, it is obviously needed to detect more specific objects or attributes such as ruins or nudity, and characters of iconographic interest such as Mary, Jesus as a child or the crucifixion of Jesus, for instance. These last categories can hardly be directly inherited from photographic databases.</p><p>For these two reasons, the lack of location annotations and the specificity of the categories of interest, a general method allowing the weakly supervised detection on specific domains such as paintings would be of great interest to art historians and more generally to anyone needing some automatic tools to explore artistic databases. We propose some contributions in this direction:</p><p>-We introduce a new multiple-instance learning (MIL) technique that is simple and quick enough to deal with large databases, -We demonstrate the utility of the proposed technique for object detection on weakly annotated databases, including photographs, drawings and paintings. These experiments are performed using image-level annotations only. -We propose the first experiments dealing with the recognition and detection of iconographic elements that are specific to Art History, exhibiting both successful detections and some classes that are particularly challenging, especially in a weakly supervised context.</p><p>We believe that such a system, enabling one to detect new and unseen category with minimal supervision, is of great benefit for dealing efficiently with digital artwork databases. More precisely, iconographic detection results are useful for different and particularly active domains of humanities: Art History (to gather data relative to the iconography of recurrent characters, such as the Virgin Mary or San Sebastian, as well as to study the formal evolution of their representations), Semiology (to infer mutual configurations or relative dimensions of the iconographic elements), History of Ideas and Cultures (with category such as nudity, ruins), Material Culture Studies, etc.</p><p>In particular, being able to detect iconographic elements is of great importance for the study of spatial configurations, which are central to the reading of images and particularly timely given the increasing importance of Semiology. To fix ideas, we can give two examples of potential use. First, the order in which iconographic elements are encountered (e.g. Gabriel and Mary), when reading an image from left to right, has received much attention from art historians <ref type="bibr" target="#b21">[21]</ref>. In the same spirit, recent studies <ref type="bibr" target="#b5">[6]</ref> on the meaning of mirror images in early modern Italy could benefit from the detection of iconographic elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object recognition and detection in artworks Early works on cross-domain (or cross-depiction) image comparisons were mostly concerned with sketch retrieval, see e.g. <ref type="bibr" target="#b13">[13]</ref>. Various local descriptors were then used for comparing and classifying images, such as part-based models <ref type="bibr" target="#b44">[46]</ref> or mid-level discriminative patches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">10]</ref>. In order to enhance the generalisation capacity of these approaches, it was proposed in <ref type="bibr" target="#b52">[54]</ref> to model object through graphs of labels. More generally, it was shown in <ref type="bibr" target="#b26">[26]</ref> that structured models are more prone to succeed in cross-domain recognition than appearance-based models.</p><p>Next, several works have tried to transfer the tremendous classification capacity of convolutional neural networks to perform cross-domain object recognition, in particular for paintings. In <ref type="bibr" target="#b11">[11]</ref>, it is shown that recycling CNNs directly for the task of recognising objects in paintings, without fine-tuning, yields surprisingly good results. Similar conclusions were also given in <ref type="bibr" target="#b53">[55]</ref> for artistic drawings. In <ref type="bibr" target="#b33">[33]</ref>, a robust low rank parametrized CNN model is proposed to recognise common categories in an unseen domain (photo, painting, cartoon or sketch). In <ref type="bibr" target="#b51">[53]</ref>, a new annotated database is introduced, on which it is shown that fine-tuning improves recognition performances. Several works have also successfully adapted CNNs architectures to the problem of style recognition in artworks <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">37]</ref>. More generally, the use of CNNs opens the way to other artwork analysis tasks, such as visual links retrieval <ref type="bibr" target="#b43">[45]</ref>, scene classification <ref type="bibr" target="#b20">[20]</ref>, author classification <ref type="bibr" target="#b49">[51]</ref> or possibly to generic artwork content representation <ref type="bibr" target="#b46">[48]</ref>.</p><p>The problem of object detection in paintings, that is, being able to both localise and recognise objects, has been less studied. In <ref type="bibr" target="#b12">[12]</ref>, it is shown that applying a pre-trained object detector (Faster R-CNN <ref type="bibr" target="#b42">[42]</ref>) and then selecting the localisation with highest confidence can yield correct detections of PASCAL VOC classes. Other works attacked this difficult problem by restricting it to a single class. In <ref type="bibr" target="#b23">[23]</ref>, it is shown that deformable part model outperforms other approaches, including some CNNs, for the detection of people in cubist artworks. In <ref type="bibr" target="#b41">[41]</ref>, it is shown that the YOLO network trained on natural images can, to some extend, be used for people detection in cubism. In <ref type="bibr" target="#b50">[52]</ref>, it is proposed to perform people detection in a wide variety of artworks (through a newly introduced database) by fine-tuning a network in a supervised way. People can be detected with high accuracy even though the database has very large stylistic variations and includes paintings that strongly differs from photographs in the way they represent people.</p><p>Weakly supervised detection refers to the task of learning an object detector using limited annotations, usually image-level annotations only. Often, a set of detections (e.g. bounding boxes) is considered at image level, assuming we only know if at least one of the detection corresponds the category of interest. The corresponding statistical problem is referred to as multiple instance learning (MIL) <ref type="bibr" target="#b14">[14]</ref>. A well-known solution to this problem through a generalisation of Support Vector Machine (SVM) has been proposed in <ref type="bibr" target="#b1">[2]</ref>. Several approximations of the involved non-convex problem have been proposed, see e.g. <ref type="bibr" target="#b22">[22]</ref> or the recent survey <ref type="bibr" target="#b6">[7]</ref>.</p><p>Recently, this problem has been attacked using classification and detection neural networks. In <ref type="bibr" target="#b45">[47]</ref>, it is proposed to learn a smooth version of an SVM on the features from R-CNN <ref type="bibr" target="#b24">[24]</ref> and to focus on the initialisation phase which is crucial due to the non-convexity of the problem. In <ref type="bibr" target="#b0">[1]</ref>, it is proposed to learn to detect new specific classes by taking advantage of the knowledge of wider classes. In <ref type="bibr" target="#b4">[5]</ref> a weakly supervised deep detection network is proposed based on Fast R-CNN <ref type="bibr" target="#b25">[25]</ref>. Those works have been improved in <ref type="bibr" target="#b48">[50]</ref> by adding a multistage classifier refinement. In <ref type="bibr" target="#b8">[9]</ref> a multi-fold split of the training data is proposed to escape local optima. In <ref type="bibr" target="#b34">[34]</ref>, a two step strategy is proposed, first collecting good regions by a mask-out classification, then selecting the best positive region in each image by a MIL formulation and then fine-tuning a detector with those propositions as "ground truth" bounding boxes. In <ref type="bibr" target="#b16">[16]</ref> a new pooling strategy is proposed to efficiently learn localisation of objects without doing bounding boxes regression.</p><p>Weakly supervised strategies for the cross domain problem have been much less studied. In <ref type="bibr" target="#b12">[12]</ref>, a relatively basic methodology is proposed, in which for each image the bounding box with highest (class agnostic) "objectness" score is classified. In <ref type="bibr" target="#b29">[29]</ref>, it is proposed to do mixed supervised object detection with cross-domain learning based on the SSD network <ref type="bibr" target="#b36">[36]</ref>. Object detectors are learnt by using instance-level annotations on photographs and image-level annotations on a target domain (watercolor, cartoon, etc.). We will perform comparisons of our approach with these two methods in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Weakly supervised detection by transfer learning</head><p>In this section, we propose our approach to the weakly supervised detection of visual category in paintings. In order to perform transfer learning, we first apply Faster R-CNN <ref type="bibr" target="#b42">[42]</ref> (a detection network trained on photographs) which is used as a feature extractor, in the same way as in <ref type="bibr" target="#b12">[12]</ref>. This results in a set of candidate bounding boxes. For a given visual category, the goal is then, using image-level annotations only, to decide which boxes correspond to this category. For this, we propose a new multiple-instance learning method, that will be detailed in Section 3.1. In contrast with classical approaches to the MIL problem such as <ref type="bibr" target="#b1">[2]</ref> the proposed heuristic is very fast. This, combined with the fact that we do not need fine-tuning, permits a flexible on-the-fly learning of new category in a few minutes. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the situation we face at training time. For each image, we are given a set of bounding boxes which receive a label +1 (the visual category of interest is present at least once) or −1 (the category is not present in this image). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multiple Instance Learning</head><p>The usual way to perform MIL is through the resolution of a non-convex energy minimisation <ref type="bibr" target="#b1">[2]</ref>, although efficient convex relaxations have been proposed <ref type="bibr" target="#b30">[30]</ref>. One disadvantage of these approaches is their heavy computational cost. In what follows, we propose a simple and fast heuristic to this problem.</p><p>For simplicity of the presentation, we assume only one visual category. Assume we have N images at hand, each of which contains K bounding boxes. Each image receives a label y = +1 when it is a positive example (the category is present) and y = −1 otherwise. We denote by n 1 the number of positive examples in the training set, and by n −1 the number of negative examples.</p><p>Images are indexed by i, the K regions provided by the object detector are indexed by k, the label of the i-th image is denoted by y i and the high level semantic feature vector of size M associated to the k-th box in the i-th image is denoted X i,k . We also assume that the detector provides a (class agnostic) "objectness" score for this box, denoted s i,k .</p><p>We make the (strong) hypothesis that if y i = +1, then there is at least one of the K regions in image i that contains an occurrence of the category. In a sense, we assume that the region proposal part is robust enough to transfer detections from photography to the target domain.</p><p>Following this assumption, our problem boils down to the classic multipleinstance classification problem <ref type="bibr" target="#b14">[14]</ref>: if for image i we have y i = +1, then at least one of the boxes contains the category, whereas if y i = −1 no box does. The goal is then to decide which boxes correspond to the category. Instead of the classical SVM generalisation proposed in <ref type="bibr" target="#b1">[2]</ref> and based on an iterative procedure, we look for an hyperplan minimising the functional defined below. We look for</p><formula xml:id="formula_0">w ∈ R M , b ∈ R achieving min (w,b) L(w, b) (1) with φ(w, b) = N i=1 −y i n yi T anh max k∈{1..K} w T X i,k + b<label>(2)</label></formula><p>and</p><formula xml:id="formula_1">L(w, b) = φ(w, b) + C * ||w|| 2 ,<label>(3)</label></formula><p>where C is a constant balancing the regularisation term. The intuition behind this formulation is that minimising L(w, b) amounts to seek a hyperplan separating the most positive element of each positive image from the least negative element of the negative image, sharing similar ideas as in MI-SVM <ref type="bibr" target="#b1">[2]</ref> or Latent-SVM <ref type="bibr" target="#b19">[19]</ref>. The T anh is here to mimic the SVM formulation in which only the worst margins count. We divide by n yi to account for unbalanced data. Indeed most example images are negative ones (n −1 &gt;&gt; n 1 )).</p><p>The main advantage of this formulation is that it can be realised by a simple gradient descent, therefore avoiding costly multiple SVM optimisation. If the dataset is too big to fit in the memory, we switch to a stochastic gradient descent by considering random batches in the training set.</p><p>As this problem is non-convex, we try several random initialisation and we select the couple w, b minimising the classification function φ(w, b). Although we did not explore this possibility it may be interesting to keep more than one vector to describe a class, since one iconographic element could have more that one specific feature, each stemming from a distinctive part.</p><p>In practice, we observed consistently better results when modifying slightly the above formulation by considering the (class-agnostic) "objectness" score associated to each box (as returned by Faster R-CNN). Therefore we modify function φ to</p><formula xml:id="formula_2">φ s (w, b) = N i=1 −y i n yi T anh max k∈{1..K} (s i,k + ) w T X i,k + b<label>(4)</label></formula><p>with ≥ 0. The motivation behind this formulation is that the score s i,k , roughly a probability that there is an object (of any category) in box k, provides a prioritisation between boxes. Once the best couple (w , b ) has been found, we compute the following score, reflecting the meaningfulness of category association :</p><formula xml:id="formula_3">S(x) = T anh{(s(x) + ) w T x + b }<label>(5)</label></formula><p>At test time, each box with a positive score (5) (where s(x) is the objectness score associated to x) is affected to the category. The approach is then straightforwardly extended to an arbitrary number of categories, by computing a couple (w , b ) per category. Observe, however, that this leads to non-comparable scores between categories. Among all boxes affected to each class, a non-maximal suppression (NMS) algorithm is then applied in order to avoid redundant detections. The resulting multiple instance learning method is called MI-max.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation details</head><p>Faster R-CNN We use the detection network Faster R-CNN <ref type="bibr" target="#b42">[42]</ref>. We only keep its region proposal part (RPN) and the features corresponding to each proposed region. In order to yield and efficient and flexible learning of new classes, we choose to avoid retraining or even fine-tuning. Faster R-CNN is a meta-network in which a pre-trained network is enclosed. The quality of features depends on the enclosed network and we compare several possibility in the experimental part.</p><p>Images are resized to 600 by 1000 before applying Faster R-CNN. We only keep the 300 boxes having best "objectness" scores (after a NMS phase), along with their high-level features <ref type="bibr" target="#b2">3</ref> . An example of extracted boxes is shown in <ref type="figure" target="#fig_1">figure  2</ref>. About 5 images per second can be obtained on a standard GPU. This part can be performed offline since we don't fine-tune the network.</p><p>As mentioned in <ref type="bibr" target="#b31">[31]</ref>, residual network (ResNet) appears to be the best architecture for transfer learning by feature extractions among the different ImageNet models, and we therefore choose these networks for our Faster R-CNN versions. One of them (denoted RES-101-VOC07) is a 101 layers ResNet trained for the detection task on PASCAL VOC2007. The other one (denoted RES-152-COCO) is a 152 layers ResNet trained on MS COCO <ref type="bibr" target="#b35">[35]</ref>. We will also compare our approach to the plain application of these networks for the detection tasks when possible, that is when they were trained on classes we want to detect. We refer to these approaches as FSD (fully supervised detection) in our experiments.</p><p>For implementation, we build on the Tensorflow 4 implementation of Faster R-CNN of Chen and al. <ref type="bibr" target="#b7">[8]</ref> 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MI-max</head><p>When a new class is to be learned, the user provides a set of weakly annotated images. The MI-max framework described above is then run to find a linear separator specific to the class. Note that both the database and the library of classifiers can be enriched very easily. Indeed, adding an image to the database only requires running it through the Faster R-CNN network and adding a new class only requires a MIL training.</p><p>For training the MI-max, we use a batch size of 1000 examples (for smaller sets, all features are loaded into the GPU), 300 iterations of gradient descent with a learning rate of 0.01 and = 0.01 (4). The whole process takes 750s for 20 classes on PASCAL VOC07 trainval (5011 images) with 12 random start points per class, on a consumer GPU (GTX 1080Ti). Actually the random restarts are performed in parallel to take advantage of the presence of the features in the GPU memory since the transfer of data from central RAM to the GPU memory is a bottleneck for our method. The 20 classes can be learned in parallel.</p><p>For the experiments of Section 4.3, we also perform a grid search on the hyperparameter C (3) by splitting the training set into training and validation sets. We learn several couples (w,b) for each possible value of C (different initialisation) and the one that minimises the loss (4) for each class is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we perform weakly supervised detection experiments on different databases, in order to illustrate different assets of our approach.</p><p>In all cases, and besides other comparisons, we compare our approach (MImax) to the following baseline, which is actually the approach chosen for the detection experiments in <ref type="bibr" target="#b12">[12]</ref> (except that we do not perform box expansion): the idea is to consider that the region with the best "objectness" score is the region corresponding to the label associated to the image (positive or negative). This baseline will be denoted as MAX. Linear-SVM classifier are learnt using those features per class in a one-vs-the-rest manner. The weight parameter that produces the highest AP (Average Precision) score is selected for each class by a cross validation method 6 and then a classifier is retrained with the best hyper-parameter on all the training data per class. This baseline requires to train several SVMs and is therefore costly.</p><p>At test time, the labels and the bounding boxes are used to evaluate the performance of the methods in term of AP par class. The generated boxes are filtered by a NMS with an Intersection over Union (IoU) <ref type="bibr" target="#b18">[18]</ref> threshold of 0.3 and a confidence threshold of 0.05 for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on PASCAL VOC</head><p>Before proceeding with the transfer learning and testing our method on paintings, we start with a sanity check experiment on PASCAL VOC2007 <ref type="bibr" target="#b18">[18]</ref>. We compare our weakly supervised approach, MI-max, to the plain application of the fully supervised Faster R-CNN <ref type="bibr" target="#b42">[42]</ref> and to the weakly supervised MAX procedure recalled above. We perform the comparison using two different architectures (for the three methods), RES-101-VOC07 and RES-512-COCO, as explained in the previous section.  <ref type="table" target="#tab_0">Table 1</ref> our weakly supervised approach (only considering annotations at the image level 8 ) yields performances that are only slightly below the ones of the fully supervised approach (using instance-level annotations). On the average, the loss is only 1.1% of mAP when using RES-512-COCO (for both methods). The baseline MAX procedure (used for transfer learning on paintings in <ref type="bibr" target="#b11">[11]</ref>) yields notably inferior performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detection evaluation on Watercolor2k and People-Art databases</head><p>We compare our approach with two recent methods performing object detection in artworks, one in a fully supervised way <ref type="bibr" target="#b50">[52]</ref> for detecting people, the other using a (partly) weakly supervised method to detect several VOC classes on watercolor images <ref type="bibr" target="#b29">[29]</ref>. For the learning stage, the first approach uses instance-level annotations on paintings, while the second one uses instance-level annotations on photographs and image-level annotations on paintings. In both cases, it is shown that using image-level annotations only (our approach, MI-max) only yields a light loss of performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1 : Watercolor2k</head><p>This database, introduced in <ref type="bibr" target="#b29">[29]</ref>, and available online <ref type="bibr" target="#b8">9</ref> , is a subset of watercolor artworks from the BAM! database <ref type="bibr" target="#b51">[53]</ref> with instance-level annotations for 6 classes (bike, bird, dog, cat, car, person) that are included in the PASCAL VOC, in order to study cross-domain transfer learning. On this database, we compare our approach to the methods from <ref type="bibr" target="#b29">[29]</ref> and from <ref type="bibr" target="#b4">[5]</ref>, to the baseline MAX discussed above, as well as to the classical MIL approach MI-SVM <ref type="bibr" target="#b1">[2]</ref> (using a maximum of 50 iterations and no restarts).</p><p>In <ref type="bibr" target="#b29">[29]</ref>, a style transfer transformation (Cycle-GAN <ref type="bibr" target="#b54">[56]</ref>) is applied to natural images with instance-level annotation. The images are transferred to the new modality (i.e. watercolor) in order to fine-tune a detector pre-trained on natural images. This detector is used to predict localisation of objects on watercolor images annotated at the image level. The detector is then fine-tuned on those images in a fully supervised manner. Bilen and Vedaldi <ref type="bibr" target="#b4">[5]</ref> proposed a Weakly Supervised Deep Detection Network (WSDDN), which consists in transforming a pre-trained network by replacing its classification part by a two streams network (a region proposal stream and a classification one) combined with a weighted MIL pooling strategy. From <ref type="table" target="#tab_1">Table 2</ref>, one can see that our approach performs clearly better than the other ones using image-level annotations only ( <ref type="bibr" target="#b4">[5]</ref>, MAX, MI-SVM). We also observe only a minor degradation of average performances (54.3 % versus 48.9 %) with respect to the method <ref type="bibr" target="#b29">[29]</ref>, which is retrained using style transfer and instance-level annotations on photographs.</p><p>Experiment 2 : People-Art This database, introduced in <ref type="bibr" target="#b50">[52]</ref>, is made of artistic images and bounding boxes for the single class person. This database is particularly challenging because of its high variability in styles and depiction techniques. The method introduced in <ref type="bibr" target="#b50">[52]</ref> yields excellent detection performances on this database, but necessitates instance-level annotations for training. The authors rely on Fast R-CNN <ref type="bibr" target="#b25">[25]</ref>, of which they only keep the three first layers, before re-training the remaining of the network using manual location annotations on their database.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, one can see that our approach MI-max yields detection results that are very close to the fully supervised results from <ref type="bibr" target="#b50">[52]</ref>, despite a much lighter training procedure. In particular, as already explained, our procedure can be trained directly on large, globally annotated database, for which manually entering instance-level annotations is tedious and time-costly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Detection on IconArt database</head><p>In this last experimental section, we investigate the ability of our approach to learn and detect new classes that are specific to the analysis of artworks, some of which cannot be learnt on photographs. Typical such examples include iconic characters in certain situations, such as Child Jesus, the crucifixion of Jesus, Saint Sebastian, etc. Although there has been a recent effort to increase open-access databases of artworks by academia and/or museums workforce <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b39">39]</ref>, they usually don't include systematic and reliable keywords. One exception is the database from the Rijkmuseum, with labels based on the IconClass classification system <ref type="bibr" target="#b28">[28]</ref>, but this database is mostly composed of prints, photographs and drawings. Moreover, these databases don't include the localisation of objects or characters.</p><p>In order to study the ability of our (and other) systems to detect iconographic elements, we gathered 5955 painting images from Wikicommons 13 , ranging from the 11th to the 20th century, which are partially annotated by the Wikidata 14 contributors. We manually checked and completed image-level annotations for 7 classes. The dataset is split in training and test sets, as shown in <ref type="table" target="#tab_4">Table 4</ref>. For a subset of the test set, and only for the purpose of performance evaluation, instance-level annotations have been added. The resulting database is called IconArt <ref type="bibr" target="#b15">15</ref> . Example images are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. To the best of our knowledge, the presented experiments are the first investigating the ability of modern detection tools to classify and detect such iconographic elements in paintings. Moreover, we investigate this aspect in a weakly supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Angel   To fix ideas on the difficulty of dealing with iconographic elements, we start with a classification experiment. For this, we use the same classification approach as in <ref type="bibr" target="#b11">[11]</ref>, using InceptionResNetv2 <ref type="bibr" target="#b47">[49]</ref> as a feature extractor <ref type="bibr" target="#b16">16</ref> . We also perform classification-by-detection experiments, using the previously described MAX approach (as in <ref type="bibr" target="#b12">[12]</ref>) and our approach, MI-max. In both cases, for each class, the score at the image level is the highest confidence detection score for this class on all the regions of the image. Results are displayed in <ref type="table" target="#tab_5">Table 5</ref>. First, we observe that classification results are very variable depending on the class. Classes such as Jesus Child, Mary or crucifixion have relatively high classification scores. Others, such as Saint Sebastian, are only scarcely classified, probably due to a limited quantity of examples and a high variability of poses, scales and depiction styles. We can also observe that, as mentioned in <ref type="bibr" target="#b12">[12]</ref>, the classification by detection can provide better scores than global classification, possibly because of small objects, such as angels in our case. Observe that these classification scores can probably be increased using multi-scale learning (as in <ref type="bibr" target="#b49">[51]</ref>), augmentation schemes and an ensemble of networks <ref type="bibr" target="#b12">[12]</ref>. Next, we evaluate the detection performance of our method, first with a restrictive metric : AP per class with an IoU 0.5 (as in all previous detection experiments in this paper), then with a less restrictive metric with IoU 0.1. Results are displayed in <ref type="table" target="#tab_6">Table 6</ref>. Results on this very demanding experiment are a mixed-bag. Some classes, such as crucifixion, and to a less extend nudity or Jesus Child are correctly detected. Others, such as angel, ruins or Saint Sebastian, hardly get it up to 15% detection scores, even when using the relaxed criterion IoU 0.1. Beyond a relatively small number of examples and very strong scale and pose variations, there are further reasons for this :</p><p>-The high in-class depiction variability (for Saint Sebastian for instance) -The many occlusions between several instances of a same class (angel) -The fact that some parts of an object can be more discriminative than the whole object (nudity) Illustrations of successes and failures are displayed, respectively on Figures 4 and 5. On the negative examples, one can see that often a larger region than the element of interest is selected or that a whole group of instances is selected instead of a single one. Future work could focus on the use of several couples (w,b) instead of one to prevent those problems. <ref type="figure">Fig. 4</ref>. Successful examples using our MI-max-C detection scheme. We only show boxes whose scores are over 0.75. <ref type="figure">Fig. 5</ref>. Failure examples using our MI-max-C detection scheme. We only show boxes whose scores are over 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Results from this paper confirm that transfer learning is of great interest to analyse artworks databases. This was previously shown for classification and fully supervised detection schemes, and was here investigated in the case of weakly supervised detection. We believe that this framework is particularly suited to develop tools helping art historians, because it avoids tedious annotations and opens the way to learning on large datasets. We also show, in this context, experiments dealing with iconographic elements that are specific to Art History and cannot be learnt on natural images.</p><p>In future works, we plan to use localisation refinement methods, to further study how to avoid poor local optima in the optimisation procedure, to add contextual information for little objects, and possibly to fine-tune the network (as in <ref type="bibr" target="#b16">[16]</ref>) to learn better features on artworks. Another exciting direction is to investigate the potential of weakly supervised learning on large databases with image-level annotations, such as the ones from the Rijkmuseum [44] or the French Museum consortium <ref type="bibr">[43]</ref>. Acknowledgements. This work is supported by the "IDI 2017" project funded by the IDEX Paris-Saclay, ANR-11-IDEX-0003-02.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of positive and negative sets of detections (bounding boxes) for the angel category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Some of the regions of interest generated by the region proposal part (RPN) of Faster R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Example images from the IconArt database. Angel on the first line, Saint Sebastian on the second. We can see some of the challenges posed by this database: tiny objects, occlusions and large pose variability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>VOC 2007 test Average precision (%). Comparison of the Faster R-CNN detector (trained in a fully supervised manner : FSD) and our MI-max algorithm (trained in a weakly supervised manner) for two networks RES-101-VOC07 and RES-152-COCO. Method aero bicy bird boa bot bus car cat cha cow dtab dog hors mbik pers plnt she sofa trai tv mean RES-FSD [27] 73.6 82.3 75.4 64.0 57.4 80.2 86.5 86.2 52.7 85.2 66.9 87.0 87.1 82.9 81.2 45.7 76.8 71.2 82.6 75.5 75.0 101-MAX 20.8 47.0 26.1 20.2 8.3 41.1 44.9 60.1 31.7 54.8 46.4 42.9 62.2 58.7 20.9 21.6 37.6 16.7 42.0 19.8 36.2 VOC07 MI-max 7 63.5 78.4 68.5 54.0 50.7 71.8 85.6 77.1 52.7 80.0 60.1 78.3 80.5 73.5 74.7 37.4 71.2 65.2 75.7 67.7 68.3 ± 0.2 RES-FSD [27] 91.0 90.4 88.3 61.2 77.7 92.2 82.2 93.2 67.0 89.4 65.8 88.0 92.0 89.5 88.5 56.9 85.1 81.0 89.8 85.2 82.7 152-MAX [12] 58.8 64.7 52.4 8.6 20.8 55.2 66.8 76.1 19.4 66.3 6.7 59.7 56.4 43.3 15.5 18.3 80.3 7.6 71.8 32.6 44.1 COCO MI-max 7 88.0 90.2 84.3 66.0 78.7 93.8 92.7 90.7 63.7 78.8 61.5 88.4 90.9 88.8 87.9 56.8 75.5 81.3 88.4 86.1 81.6 ± 0.3</figDesc><table><row><cell>Net</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Watercolor2k (test set) Average precision (%). Comparison of the proposed MI-max method to alternative approaches. Our] 11 85.2 48.2 49.2 31.0 30.0 57.0 50.1 ± 1.1</figDesc><table><row><cell>Net</cell><cell>Method</cell><cell cols="2">bike bird car cat dog person mean</cell></row><row><cell>VGG</cell><cell cols="2">WSDDN [5] 10 1.5 26.0 14.6 0.4 0.5 33.3</cell><cell>12.7</cell></row><row><cell>SSD</cell><cell cols="2">DT+PL [29] 10 76.5 54.9 46.0 37.4 38.5 72.3</cell><cell>54.3</cell></row><row><cell>RES-152-COCO</cell><cell cols="2">MAX [12] MI-SVM [2] 66.8 23.5 6.7 13.0 8.4 14.1 74.0 34.5 26.8 17.8 21.5 21.0</cell><cell>32.6 22.1</cell></row><row><cell></cell><cell>MI-max [</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>People-Art (test set) Average precision (%). Comparison of the proposed MI-max method to alternative approaches.</figDesc><table><row><cell>Net</cell><cell>Method</cell><cell>person</cell></row><row><cell cols="2">Fast R-CNN (VGG16) Fine tuned [52] 12</cell><cell>59</cell></row><row><cell>RES-152-COCO</cell><cell>MAX [12] MI-SVM [2]</cell><cell>25.9 13.3</cell></row><row><cell>RES-152-COCO</cell><cell cols="2">MI-max [Our] 55.4 ± 0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Statistics of the IconArt database</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>IconArt classification test set classification average precision (%).</figDesc><table><row><cell>Net</cell><cell>Method</cell><cell cols="4">angel JCchild crucifixion Mary nudity ruins StSeb mean</cell></row><row><cell>InceptionResNetv2 [49]</cell><cell></cell><cell>44.1 77.2</cell><cell>57.8</cell><cell>81.1 77.4 74.6 26.8</cell><cell>62.7</cell></row><row><cell></cell><cell>MAX [12]</cell><cell>49.3 74.7</cell><cell>30.3</cell><cell>67.5 57.4 43.2 7.0</cell><cell>47.1</cell></row><row><cell>RES-152-COCO</cell><cell cols="2">MI-max [Our] 57.4 60.7</cell><cell>79.9</cell><cell cols="2">70.4 65.3 45.9 17.0 56.7 ± 1.0</cell></row><row><cell></cell><cell cols="2">MI-max-C [Our] 61.0 68.9</cell><cell>80.2</cell><cell cols="2">71.4 66.3 51.7 14.8 59.2 ± 1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>IconArt detection test set detection average precision (%). All methods based on RES-152-COCO.</figDesc><table><row><cell>Method</cell><cell>Metric</cell><cell cols="4">angel JCchild crucifixion Mary nudity ruins StSeb mean</cell></row><row><cell>MAX [12]</cell><cell cols="3">AP IoU 0.5 1.4 AP IoU 0.1 10.1 36.2 3.9</cell><cell>7.4 28.2</cell><cell>2.8 18.4 14.0 1.6 2.8 3.9 0.3 0.9</cell><cell>2.9 15.9</cell></row><row><cell>MI-max [Our]</cell><cell cols="2">AP IoU 0.5 0.3 AP IoU 0.1 6.4</cell><cell>0.9 25.3</cell><cell>37.3 74.4</cell><cell>3.8 21.2 0.5 10.9 10.7 ± 1.7 44.6 30.9 6.8 17.2 29.4 ± 1.7</cell></row><row><cell>MI-max-C [Our]</cell><cell cols="3">AP IoU 0.5 3.0 AP IoU 0.1 12.3 41.2 17.7</cell><cell>32.6 74.4</cell><cell>4.8 23.5 1.1 9.6 13.2 ± 3.1 46.3 31.2 13.6 16.1 33.6 ± 2.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The layer f c7 of size M = 2048 in the ResNet case, often called 2048-D. 4 https://www.tensorflow.org/ 5 Code can be found on GitHub https://github.com/endernewton/tf-faster-rcnn.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use a 3-fold cross validation while<ref type="bibr" target="#b12">[12]</ref> use constant training and validation set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">It is the average performance on 100 runs of our algorithm.<ref type="bibr" target="#b7">8</ref> However, observe that since we are relying on Faster R-CNN, our system uses a subpart trained using class agnostic bounding boxes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/naoto0804/cross-domain-detection<ref type="bibr" target="#b10">10</ref> The performance come from the original paper<ref type="bibr" target="#b29">[29]</ref>.<ref type="bibr" target="#b11">11</ref> Standard deviation computed on 100 runs of the algorithm.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">The performance come from the original paper. 13 https://commons.wikimedia.org/wiki/Main Page 14 https://www.wikidata.org/wiki/Wikidata:Main Page 15 The database is available online https://wsoda.telecom-paristech.fr/downloads/ dataset/IconArt v1.zip.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">Only the center of the image is provided to the network and extracted features are 1536-D.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>YOLO9000: Better, Faster, Stronger</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Painting-to-3d model alignment via discriminative visual elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep multibranch neural network for painting categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="414" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Master and judge: The mirror as dialogical device in italian renaissance art theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Bosio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dialogical Imaginations: Debating Aisthesis as Social Perception</title>
		<editor>Zimmermann, M.</editor>
		<meeting><address><addrLine>Diaphanes</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gagnon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.10.009</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2017.10.009" />
		<title level="m">Multiple Instance Learning: A Survey of Problem Characteristics and Applications. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="329" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An Implementation of Faster RCNN with Study for Region Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02138</idno>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2016.2535231</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2535231" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The state of the art: Object retrieval in paintings using discriminative regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Workshop at the European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
	<note>search of art</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Art of Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="721" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual image retrieval by elastic matching of user sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/donahue14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Xing, E.P., Jebara, T.</editor>
		<meeting>the 31st International Conference on Machine Learning. Machine Learning Research<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<ptr target="https://www.europeana.eu/portal/en" />
	</analytic>
	<monogr>
		<title level="j">Europeana: Collections Europeana</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain transfer for delving into deep networks capacity to de-abstract art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Badea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vertan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="337" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dal lato dell&apos;immagine: destra e sinistra nelle descrizioni di Bellori e altri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gasparro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Belvedere</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deterministic annealing for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting people in cubist art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at the European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="101" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.81</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.81" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-depiction problem: Recognition and synthesis of photographs and artwork</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Corradi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="103" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<ptr target="http://www.iconclass.nl/home" />
		<title level="m">Iconclass: Home -Iconclass</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A convex relaxation for weakly supervised classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6413</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08974</idno>
		<title level="m">Do Better ImageNet Models Transfer Better</title>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recognizing Art Style Automatically in painting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lecoutre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Negrevergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.591</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.591" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5543" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DeepArt: Learning Joint Representations of Visual Arts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>She</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123405</idno>
		<ptr target="https://doi.org/10.1145/3123266.3123405" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The rijksmuseum challenge: Museum-centered visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia Retrieval</title>
		<meeting>International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">451</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MET: Image and Data Resources -The Metropolitan Museum of Art</title>
		<ptr target="https://www.metmuseum.org/about-the-met/policies-and-documents/image-resources" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pharos consortium: PHAROS: The International Consortium of Photo Archives</title>
		<ptr target="http://pharosartresearch.org/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<ptr target="https://art.rmngp.fr/en" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">44</biblScope>
		</imprint>
	</monogr>
	<note>Réunion des Musées Nationaux-Grand Palais: Images d&apos;Art. Rijksmuseum: Online Collection Catalogue -Research</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual Link Retrieval in a Database of Paintings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabella</forename><surname>Striolo Carlotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaplan</forename><surname>Dilenardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frederic</surname></persName>
		</author>
		<ptr target="https://doi.org/978−3−319−46604−052" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Data-driven visual similarity for cross-domain image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/songb14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>Xing, E.P., Jebara, T.</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Strezoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00684</idno>
		<title level="m">OmniArt: Multi-task Deep Learning for Artistic Data Analysis</title>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3059" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning scale-variant and scale-invariant features for deep image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Postma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.06.005</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2016.06.005" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="583" to="592" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detecting people in artwork with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Westlake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning graphs to model visual objects across different depictive styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="313" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object recognition in art drawings: Transfer of a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Monson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Honig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2299" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
