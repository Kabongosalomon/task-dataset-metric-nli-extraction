<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizing Monocular 3D Human Pose Estimation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chen</surname></persName>
							<email>yan-chen16@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyuan</forename><surname>Qian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizing Monocular 3D Human Pose Estimation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The availability of the large-scale labeled 3D poses in the Human3.6M dataset plays an important role in advancing the algorithms for 3D human pose estimation from a still image. We observe that recent innovation in this area mainly focuses on new techniques that explicitly address the generalization issue when using this dataset, because this database is constructed in a highly controlled environment with limited human subjects and background variations. Despite such efforts, we can show that the results of the current methods are still error-prone especially when tested against the images taken in-the-wild. In this paper, we aim to tackle this problem from a different perspective. We propose a principled approach to generate high quality 3D pose ground truth given any in-the-wild image with a person inside. We achieve this by first devising a novel stereo inspired neural network to directly map any 2D pose to high quality 3D counterpart. We then perform a carefully designed geometric searching scheme to further refine the joints. Based on this scheme, we build a large-scale dataset with 400,000 in-the-wild images and their corresponding 3D pose ground truth. This enables the training of a high quality neural network model, without specialized training scheme and auxiliary loss function, which performs favorably against the state-of-the-art 3D pose estimation methods. We also evaluate the generalization ability of our model both quantitatively and qualitatively. Results show that our approach convincingly outperforms the previous methods. We make our dataset and code publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ity, human-computer interaction, and video surveillance.</p><p>Recently, significant advances have been achieved in 2D human pose estimation due to the powerful deep Convolutional Neural Networks (CNNs) and the availability of large-scale in-the-wild 2D human pose datasets with manual annotations. However, advances in 3D human pose estimation remain limited. This problem is widely studied in the literature and is mainly tackled with the following types of technical methodologies namely 2D-to-3D pose estimation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, monocular image based 3D pose estimation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44]</ref>, and multi-view images based 3D pose estimation <ref type="bibr" target="#b30">[31]</ref>. Hu-man3.6M dataset <ref type="bibr" target="#b11">[12]</ref> plays an important role in the passive 3D human pose estimation methods. It is collected in a highly constrained environment with limited subjects, and background variations. The innovation of these methods mainly focuses on new techniques that explicitly address the generalization issues when using this dataset. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the current methods are still problematic when tested against the in-the-wild images.</p><p>To solve the problem, we can improve the generalization ability with well-annotated in-the-wild 3D pose data. <ref type="bibr">Rogez et al. [32]</ref> propose a method to solve the limitations of the laboratory 3D datasets by artificially composing different images to generate a synthetic one based on the 3D Motion Capture (MoCap) data. However, the details and variety level of these synthetic images are limited compared with the in-the-wild images.</p><p>In this paper, we introduce a principled method to generate high quality 3D labels of the in-the-wild images. Inspired by <ref type="bibr" target="#b30">[31]</ref>, to solve the depth ambiguity problem in 3D human pose estimation, we devise a stereo inspired 3D label generator utilizing the 2D poses from multi-view to generate a high quality 3D human pose. We also propose a geometric search scheme to further refine the predicted 3D human pose. Given any image with the 2D ground truth, the proposed 3D label generator can produce its high quality counterpart.</p><p>To this end, based on the 3D label generator, we collect more than 400,000 in-the-wild images with high quality 3D labels from the wildly used 2D pose datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43]</ref>. With the proposed in-the-wild 3D pose dataset, we train a high performance baseline network which achieves favorable results against the state-of-the-art methods , both quantitatively and qualitatively. Furthermore, we introduce a method that utilizes the predicted 3D human pose on the task of action classification to evaluate the generalization ability quantitatively.</p><p>Our contributions can be summarized as follows:</p><p>• We propose a novel stereo inspired neural network to generate high quality 3D pose labels for in-the-wild images. We also devise a geometric searching scheme to further refine the 3D joints. • We build a large-scale dataset with 400,000 in-the-wild images and the corresponding high quality 3D pose labels. • We train a baseline network with the proposed dataset that performs favorably against the state-of-the-art approaches, both quantitatively and qualitatively. Experimental results demonstrate that the proposed dataset can significantly boost the generalization performance on the realistic scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Synthetic Images and Additional Annotations. Most of the existing 3D pose datasets such as Human3.6M <ref type="bibr" target="#b11">[12]</ref>, and HumanEva <ref type="bibr" target="#b33">[34]</ref> are collected in indoor scenes and cannot cover various activities. To solve it, several methods attempt to use the graphics methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref> to enrich the training samples. Rogez and Schmid <ref type="bibr" target="#b31">[32]</ref> intro-duce a computer graphics engine that artificially composes different images to generate synthetic poses based on the 3D Motion Capture (MoCap) data. Recently, to alleviate the need for the accurate 3D labels of in-the-wild images, Pavlakos et al. <ref type="bibr" target="#b26">[27]</ref> and Shi et al. <ref type="bibr" target="#b32">[33]</ref> provide in-the-wild images with additional annotations which contain the forward or backward information of each bone. However, the aforementioned methods either have limited details and variety level of the synthetic images or require a large number of manual annotations. Different from them, we propose a network to automatically generate a large-scale in-the-wild 3D human pose dataset.</p><p>2D-to-3D Pose Estimation. Several methods tackle 3D pose understanding from 2D pose <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>. Martinez et al. <ref type="bibr" target="#b18">[19]</ref> propose a simple multilayer perceptron to regress the locations of the 3D joints. Fang et al. <ref type="bibr" target="#b5">[6]</ref> introduce a model to encode the mapping function of human pose from 2D to 3D by explicitly encoding human body configuration with pose grammar. Despite the consideration of the domain knowledge of the human body, models trained with 2D/3D key-points from Hu-man3.6M containing only fifteen activities cannot perform well to various actions. Training with 2D/3D pairs including more than 2,500 activities generated by the unity toolbox 2 , we devise a network can map an in-the-wild 2D pose to its high quality 3D counterpart.</p><p>Monocular Image Based 3D Pose Estimation. Recently, several methods have been proposed to estimate the 3D pose on the monocular image <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>To improve the generalization on realistic scenes, some attempt to estimate 3D human pose in a semi-supervised way. Zhou et al. <ref type="bibr" target="#b46">[47]</ref> employ a weakly-supervised transfer learning method with a 3D geometric loss.Yang et al. <ref type="bibr" target="#b43">[44]</ref> propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. However, their mechanisms are to transfer the domain knowledge of the constrained dataset to the in-thewild images without adding new subjects or more activities.</p><p>Trained with our proposed in-the-wild 3D dataset, the network performs better on the in-the-wild images.</p><p>Multi-View Images Based 3D Pose Estimation. Some methods attempt to estimate the 3D human pose from multiple views of different cameras <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref>. Amin et al. <ref type="bibr" target="#b1">[2]</ref> propose the evidence across multiple viewpoints to allow for robust 3D pose estimation. Rhodin et al. <ref type="bibr" target="#b30">[31]</ref> propose to predict the same 3D pose in all views with only a small number of labeled images. However, compared with generating 2D joints from different cameras, obtaining the images from multi-views are more difficult. Based on these methods, we devise a network with the simple 2D joints from the multi-view to generate the high quality 3D human poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first introduce the principles of the network design. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we present a novel 3D label generator, which consists of stereoscopic view synthesis subnetwork, 3D pose reconstruction subnetwork, and a geometric search scheme. In addition, we propose a largescale in-the-wild 3D pose dataset, and its 3D pose labels are provided by the 3D pose generator. Furthermore, we adopt a baseline network to evaluate the proposed in-the-wild 3D pose dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Principles of Network Design</head><p>2D-to-3D human pose estimation inherently accompanies with the depth ambiguity since the mapping function from 2D to 3D is not unique. Amin et al. <ref type="bibr" target="#b1">[2]</ref> propose the evidence across multiple viewpoints to allow for robust 3D pose estimation. Recently, Luo et al. <ref type="bibr" target="#b17">[18]</ref> confirm that utilizing images from two different cameras can achieve excellent performance in the stereo matching area. Inspired by <ref type="bibr" target="#b17">[18]</ref>, we devise the stereo inspired neural network utilizing the 2D key-points from two different viewpoints to alleviate the depth ambiguity of predicting 3D poses. Different from the previous methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2]</ref> using multi-view images as inputs to estimate the 3D pose, our method is relatively easier to obtain the training data, since the unity toolbox can generate a large number of 2D/3D pairs automatically.</p><p>In addition, most existing 2D-to-3D methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref> mainly focus on the domain-knowledge of the human body or the architecture of the network while ignoring that a reasonable predicted 3D human pose can be re-projected to its 2D input with zero-pixel error. Based on this principle, we devise a geometric search scheme to further refine the predicted coarse 3D human pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stereoscopic View Synthesis Subnetwork</head><p>Stereoscopic view synthesis subnetwork is proposed to synthesize the 2D pose from the right viewpoint . Given an image with 2D key-points from the left viewpoint, we generate the 2D key-points from the right viewpoint. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we input the left-view 2D pose (u L , v L ) to regress the location of the right-view 2D pose (u R , v R ). However, the challenge is how to obtain the ground truth of the 2D pose from the right viewpoint .</p><p>We employ a large bunch of 3D key-points and their corresponding camera intrinsic matrix from the realistic 3D pose dataset (i.e., Human3.6M <ref type="bibr" target="#b11">[12]</ref>) and the synthetic data generated by the unity toolbox to train the subnetwork. To obtain the ground truth of the right-view 2D pose, we move 3D joints to the right direction slightly along the X axis in the camera coordinate system while keep Y and Z unchanged. We then re-map them into the 2D key-points based on the camera calibration by the following equation:</p><formula xml:id="formula_0">s   u R v R 1   =   α x 0 u 0 0 α y v 0 0 0 1     x c + ∆x y c z c   = M c P c ,<label>(1)</label></formula><p>where s denotes the scale factor, (u R , v R ) represents the right-view 2D pose, α x and α y are the scale factors, (u 0 , v 0 ) denotes the origin coordinate of the RGB image. P c = (x c , y c , z c ) is the location of 3D key-points in the camera coordinate system. M c represents the camera intrinsic matrix. ∆ x = 500mm denotes the shift distance.</p><p>The subnetwork contains the linear-ReLU layers, residual connections, batch normalization layers, and dropout layers with max-norm constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Pose Reconstruction Subnetwork</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, 3D pose reconstruction subnetwork directly regresses the location of 3D key-points based on the input left-view 2D pose and synthesized right-view 2D pose. It shares the same architecture as the stereoscopic view synthesis subnetwork and takes the left-view and synthetic right-view 2D poses as the inputs. More precisely, when inputting the multi-view 2D poses, we combine them by the concatenation operation. After the operation of a fully connected network structure, we obtain a coarse 3D human pose. The 3D pose reconstruction can be represented by the following equation:</p><formula xml:id="formula_1">Q r = f r ((u L , v L ), (u R , v R )),<label>(2)</label></formula><p>where Q r = (x r , y r , z r ) ∈ R 3×N denotes the predicted 3D human pose by the 3D pose reconstruction subnetwork, , N denotes the joint number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Geometric Search Scheme</head><p>The geometric search scheme aims to further refine the coarse 3D human pose. It ensures the refined 3D human pose can be projected to the input 2D joints via the camera intrinsic matrix with zero-pixel error. Mathematically, our geometric search scheme can be represented by the following equation:</p><formula xml:id="formula_2">Q g = f geo (P gt , Q r ),<label>(3)</label></formula><p>where P gt = (u gt , v gt ) ∈ R 2×N represents ground truth of the real 2D pose, Q g = (x g , y g , z g ) ∈ R 3×N is the final output of the 3D label generator. Actually, the 3D pose reconstruction subnetwork outputs the 3D human pose aligned to the root joint (pelvis). What our model predicts in our case is 3D key-points with relative depth (relative to hip). Therefore, the projection is not possible because it requires absolution depth. According to the camera calibration principle, we propose the heuristic projection to constrain the consistence between the input 3D pose and projected 2D pose. <ref type="figure" target="#fig_2">Figure 3</ref> shows the procedure of the geometric search scheme. Based on the z r and P gt , we initialize ∆z = 0 (∆z represents the hypothetical depth to the camera). Then We can infer the x r and y r according to the camera intrinsic matrix and ∆z, and the process of searching the optimal 3D joint can be described as follows:</p><formula xml:id="formula_3">x r = (u gt − c x )(z r + ∆z)/f x y r = (v gt − c y )(z r + ∆z)/f y<label>(4)</label></formula><p>, where f x and f y denote focal length of the camera and c x and c y represent the optical axis points of the camera. By increasing ∆z with step = 1mm, until we obtain the optimal value that can satisfy the following loss function,</p><formula xml:id="formula_4">L geo = arg min ∆z (( x r − x r ) 2 + ( y r − y r ) 2 ) 2 2<label>(5)</label></formula><p>In this way, we can obtain a reasonable ∆z, the location of the 3D pose Q g in the 3D space, and the final output of the 3D label generator Q g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">A Large-Scale in-the-Wild 3D Pose Dataset</head><p>The proposed 3D label generator can map a 2D human pose to its high quality 3D counterpart. The existing datasets for 2D human pose estimation such as Leeds Sports Pose dataset (LSP) <ref type="bibr" target="#b12">[13]</ref>, MPII human pose dataset (MPII) <ref type="bibr" target="#b2">[3]</ref> and Ai Challenger dataset for 2D human pose estimation (Ai-Challenger) <ref type="bibr" target="#b42">[43]</ref> can be used to extract the high quality 3D labels by the 3D label generator. We crop the single person from the 2D datasets and set each human body to be in the center of the image. Then we resize these images into 256 × 256. Given the well-annotated 2D key-points, the 3D label generator can output the high quality 3D labels of the in-the-wild images. Finally, we collect a large-scale in-the-wild dataset containing more than 400,000 images (320,000 training images and the rest for testing) with high quality 3D labels. Experimental results demonstrate that the proposed in-the-wild 3D pose dataset can improve the performance of 3D human pose estimation quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Baseline Network</head><p>We adopt the backbone of Zhou et al. <ref type="bibr" target="#b46">[47]</ref> as the baseline network to evaluate the in-the-wild 3D pose dataset quantitatively and qualitatively. This network can be viewed as a two-stage pose estimator. The first stage is to use the stacked hourglass network <ref type="bibr" target="#b22">[23]</ref> for 2D human pose estimation. Each stack is in an encoder-decoder structure. The <ref type="table">Table 1</ref>. Quantitative evaluations on the Human3.6M <ref type="bibr" target="#b11">[12]</ref> under Protocol#1 (no rigid alignment or similarity transform is applied in postprocessing). GT indicates that the network was trained on ground truth 2D pose. GS denotes the geometric search scheme. Unity denotes the model trained with the additional 2D/3D key-points generated by the unity toolbox. The bold-faced numbers represent the best results.  <ref type="bibr" target="#b43">[44]</ref>. We train the network only using the first two stages of the method with Human3.6M and in-the-wild 3D pose dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present the experiments and results of the 3D label generator. Trained with 2D ground truth, our 3D label generator achieves state-of-the-art results on the Human3.6M dataset <ref type="bibr" target="#b11">[12]</ref>. Experimental results denote that the generator can provide high quality labels for the in-the-wild images. Meanwhile, we investigate the efficacy of the stereoscopic view synthesis subnetwork, 3D pose reconstruction subnetwork, and geometric search scheme respectively. In addition, we compare with the methods of <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44]</ref> to verify the effectiveness of in-the-wild 3D pose dataset. To further verify the generalization ability of the model, we attempt to use our predicted 3D poses to the task of classification on the Penn Action dataset <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We numerically evaluate the publicly available 3D human pose estimation dataset: Human3.6M <ref type="bibr" target="#b11">[12]</ref>. We also conduct qualitative experiments on in-the-wild images.</p><p>3D Pose Datasets. Human3.6M is a large-scale dataset with 2D joint locations and 3D ground truth collected by the MoCap system in the laboratory environment. It consists of 3.6 million RGB images of 11 different professional actors performing 15 everyday activities. Following our baseline method <ref type="bibr" target="#b46">[47]</ref>, we employ data from subjects S1, S5, S6, S7, S8 for training and evaluate on the data from subjects S9 and S11. We refer the MPJPE that evaluated on the predicted 3D pose after alignment of the root without any rigid alignment transformation as protocol#1.</p><p>MPI-INF-3DPH <ref type="bibr" target="#b19">[20]</ref> is a recent dataset that includes both indoor and outdoor scenes, which contains 2929 frames from six subjects performing seven actions, to evaluate the generalization ability quantitatively. We only use the test split of this dataset to demonstrate the generalization ability of the trained model. 2D Pose Datasets. MPII and LSP are the most widely used dataset for 2D human pose estimation. Ai-Challenger is proposed recently for multi-person 2D pose estimation, which consists of 210,000 images for training, 30,000 images for validation and 60,000 images for testing. We qualitatively compare the generalization ability on these 2D datasets. Penn Action Dataset. Penn Action Dataset <ref type="bibr" target="#b45">[46]</ref> contains 2,326 video sequences of 15 different actions, e.g., pull-up, squat and push-up, with 1,258 clips for training and 1,068 clips for testing. The performance is measured by the mean classification accuracy across the splits <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We introduce the implementation details of the 3D label generator and the baseline network based on the RGB images for 3D human pose estimation.</p><p>3D Label Generator. We train the proposed 3D label generator using Pytorch <ref type="bibr" target="#b25">[26]</ref> toolbox and Adam <ref type="bibr" target="#b13">[14]</ref> solver to optimize the parameters. We set momentum, momen-tum2, and weight decay as 0.9, 0.99, and 10 −4 , respectively. Kaiming initialization <ref type="bibr" target="#b6">[7]</ref> is used to initialize the weights of our linear layers. The network is trained for a total of 200 epoch. The learning rate is set to be 10 −3 and exponential decay. We train the stereoscopic view synthesis subnetwork with 4.8 million 2D/3D key-points pairs from the Unity toolbox and Human3.6M. We set the batch size as 64 and normalize the dataset to [−1, 1]. Training on a Nvidia TI-TAN X GPU, the network converges within one day. When training the 3D pose reconstruction subnetwork, we fix the parameters of the stereoscopic view synthesis subnetwork. We adopt the same training scheme when training the 3D pose reconstruction subnetwork.</p><formula xml:id="formula_5">(a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d)</formula><p>Baseline Network. Stochastic Gradient Descent (SGD) optimization is used for training. Each training batch contains both the Human3.6M and in-the-wild images in the ratio of 1:1. We fine-tune the 2D module based on the checkpoint of Zhou et al. <ref type="bibr" target="#b46">[47]</ref> with the 2D annotated Hu-man3.6M dataset and the in-the-wild dataset containing MPII, LSP, and Ai-Challenger. Both data from Human3.6M and the proposed in-the-wild 3D pose dataset are employed for training the two stage baseline network. We train the network with the loss following Zhou et al. <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluations on 3D Label Generator</head><p>Quantitative Results. <ref type="table">Table 1</ref> denotes the comparisons with previous methods on the Human3.6M <ref type="bibr" target="#b11">[12]</ref>. The 3D label generator achieves state-of-the-art performance. For protocol#1, the generator trained with 2D/3D ground truth from Human3.6M has 17% (37.6mm vs. 45.5mm) improvements compared with the method of Martinez et al. <ref type="bibr" target="#b18">[19]</ref>. To improve the generalization ability, we also train the network with synthetic 2D/3D pairs generated by the unity toolbox. There is 10% improvement compared with the method of Martinez et al. <ref type="bibr" target="#b18">[19]</ref>. Since the domain gap between the synthetic data and the real data, the model trained with both dataset performs slight worse than the model trained with only the Human3.6M dataset. However, the qualitative performance on the in-the-wild images is significantly improved, as we will show in the following.</p><p>Qualitative Results. Compared with the method of Martinez et al. <ref type="bibr" target="#b18">[19]</ref>, we demonstrate the generalization ability qualitatively on the images from MPII and LSP. Both of the networks to estimate the 3D human pose are based on the 2D ground truth of these datasets, As shown in <ref type="figure" target="#fig_3">Figure 4</ref> (b), (d), the generalization ability of our algorithm outperform the generalization results of Martinez et al. <ref type="bibr" target="#b18">[19]</ref>. Because of the 2D/3D key-points pairs generated by the unity toolbox, the generalization ability of the network is highly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation of Stereoscopic View Synthesis Subnetwork.</head><p>To evaluate the quality of synthetic right-view 2D pose, we apply the PCKh metrics following 2D human pose estimation method <ref type="bibr" target="#b22">[23]</ref>. The subnetwork trained with merely Hu-man3.6M achieves 98.2%, while that trained with the Hu-man3.6M and synthesized dataset by unity toolbox obtains 95.3% in PCKh-0.5 scores. It demonstrates that the stereoscopic view synthesis is able to generate high quality rightview 2D pose based on the left-view 2D pose.</p><p>Validation of 3D Pose Reconstruction Subnetwork. In Section 3.1, we note that the stereoscopic architecture can alleviate the depth ambiguity in 3D human pose estimation. As shown in <ref type="table">Table 1</ref>, compared with the monocular structure of Martinez et al. <ref type="bibr" target="#b18">[19]</ref>, our network without the geometric search scheme has 7.7% (42.0mm vs. 45.5mm) improvements. Both of the networks have the same architecture and parameters, the only difference is that we take the 2D poses from multi-view as inputs. Results illustrate that the rationality of the designed stereoscopic network structure which can boost the performance in 3D human pose estimation.</p><p>Validation of the Geometric Search Scheme. We indicate that a high quality 3D human pose can be projected to its 2D counterpart with zero-pixel error. Based on this premise, we devise a geometric search scheme to further refine the coarse 3D human pose. As shown in <ref type="table">Table 1</ref>, we further analyze the effectiveness of the geometric search Ablation Study We discuss the truth of ∆x in <ref type="bibr" target="#b0">(1)</ref>. As shown in <ref type="table" target="#tab_1">Table 2</ref>, we conduct experiments on 3D label generator with different ∆x. The results show that our generator is not sensitive to ∆x. The value of ∆x around 500mm can meet our requirement for generating precise 3D labels. However, ∆x cannot be set with too large value(e.g., 5000mm) that will lead to failures in generating 3D poses. </p><formula xml:id="formula_6">(a) (b) (c) (d) (a) (b) (c) (d) (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluations on Baseline Network</head><p>In this section, we mainly compare the two different methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44]</ref> with the baseline network trained with the proposed in-the-wild 3D pose dataset. The baseline network is an upgraded version of the Stacked Hourglass Network <ref type="bibr" target="#b22">[23]</ref> with an additional depth regression module. To focus on the analysis of the proposed 3D pose dataset, we set all the backbone with the same components consisting of 2 stacked hourglass modules, 2 residual blocks, and 2 depth regression modules. <ref type="table">Table 3</ref>. Quantitative evaluations on the Human3.6M <ref type="bibr" target="#b11">[12]</ref> under Protocol#1 (no rigid alignment or similarity transform applied in post-processing). The bold-faced numbers represent the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protocol#1</head><p>Direct Quantitative Results. As shown in <ref type="table">Table 3</ref>, trained with in-the-wild 3D pose dataset without the additional geometric loss <ref type="bibr" target="#b46">[47]</ref> or the adversarial learning method <ref type="bibr" target="#b43">[44]</ref>, our baseline network can outperform them. Compared with Zhou et al. <ref type="bibr" target="#b46">[47]</ref>, there is about 10.7% improvement (58.0mm vs. 64.9mm) on the Human3.6M dataset. It proves that our dataset promotes the accuracy of 3D human estimation on the images taken laboratory scenarios.  <ref type="figure">Figure 6</ref>. Evaluation results of action classification on Penn action dataset <ref type="bibr" target="#b45">[46]</ref>. The detected 3D human pose by our method is more conducive to the action classification task.</p><p>Qualitative Results. By visualizing the 3D skeleton of the predicted human body, we show that the model trained with the in-the-wild 3D pose data is robust in the realistic scenes. <ref type="figure" target="#fig_4">Figure 5</ref> shows the visualization results by different methods. As one can see in the figure, our baseline network trained with the proposed in-the-wild 3D pose dataset can estimate more reasonable results, which in term proves the quality of our proposed dataset. In addition, we discover that our model can handle challenging samples such as leaning over, sitting cross-legged, and jumping.</p><p>Cross-Domain Generalization. We further verify the generalization introduced by our proposed 3D pose dataset on the MPI-INF-3DHP <ref type="bibr" target="#b20">[21]</ref>. Without any retraining the model on this dataset, we compare the results by Zhou et al. <ref type="bibr" target="#b46">[47]</ref>, Yang et al. <ref type="bibr" target="#b43">[44]</ref> and our baseline network. As reported in <ref type="table" target="#tab_3">Table 4</ref>, one can observe that the method trained with in-the-wild 3D data significantly improves the generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Evaluations by Action Classification.</head><p>To further evaluate the generalization ability of different methods, we propose an approach to utilize detected 3D joints for action classification on the Penn Action dataset <ref type="bibr" target="#b45">[46]</ref>. Using only the coordinate location of predicted 3D joints, we devise a simple network consist of multiple fully connected layers, the detailed network structure can be found in our supplementary file. When training this network, we use the location of predicted 3D joints of 25 consecutive frames in Penn Action dataset <ref type="bibr" target="#b45">[46]</ref> as the inputs. As shown in <ref type="figure">Figure 6</ref>, we can find that the model trained with our predicted 3D joints are more precise than the model of Zhou et al. <ref type="bibr" target="#b46">[47]</ref> (93% vs. 80%). In addition, we analyze the difference between the detected 3D joints of the two models. <ref type="figure" target="#fig_5">Figure 7</ref> shows an example that the method of Zhou et al. <ref type="bibr" target="#b46">[47]</ref> predicts 3D pose at almost the same depth level, while the depth of the predicted 3D poses by our baseline network varies with the baseball player movements. The results demonstrate that the inaccurate predicted depth leads to a lower accuracy of action classification. The superior performance in action recognition task can further prove the generalization of our baseline network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we solve the generalization problem of 3D human pose estimation from a novel perspective. We propose a principled approach to generate high quality 3D labels given an in-the-wild image automatically. Based on the stereo inspired structure, the proposed network with a carefully designed geometric search scheme significantly outperforms other methods quantitatively and qualitatively. We proposed an in-the-wild 3D pose dataset containing more than 400,000 images by employing this network as a 3D label generator. Experimental results show that the baseline model trained with the proposed dataset can significantly improve the performance on the public Human3.6M and boost the generalization ability on the in-the-wild images. In the future work, we plan to investigate to generate high quality 3D labels on the in-the-wild videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>3D human pose estimation is one of the fundamental problems in computer vision. It is widely used in a large number of areas such as action recognition, virtual real- * Indicates equal contribution. 1 https://github.com/llcshappy/ Monocular-3D-Human-PoseOriginalZhou et al.<ref type="bibr" target="#b46">[47]</ref> Yang et al.<ref type="bibr" target="#b43">[44]</ref> Ours 3D pose estimation on challenging images. The proposed method performs favorably against the state-of-the-art 3D pose estimation algorithms due to our generated in-the-wild 3D pose dataset. The red lines denote the skeletons of the left and torso part of the human body while blue lines represent the right part. All the predicted poses are demonstrated at the same viewpoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the 3D label generator. The generator consists of stereoscopic view synthesis subnetwork, 3D pose reconstruction subnetwork, and a geometric search scheme. Given the 2D pose from the left viewpoint, stereoscopic view synthetic subnetwork aims to generate the 2D pose from the right viewpoint. 3D pose reconstruction subnetwork utilizes the multi-view 2D poses to estimate a coarse 3D human pose. Geometric search scheme is applied to further refine the predicted 3D human pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Geometric search scheme. Qr = ( xr, yr, zr) denotes the predicted 3D human pose by the 3D pose reconstruction subnetwork with hypothetical depth to the camera. Qg = ( xg, yg, zg) represents the 3D human pose with the absolute depth to the camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative evaluations on the in-the-wild images. (a) Original in-the-wild images, (b) Results of Martinez et al. [19], (c) Our results w/o geometric search scheme, (d) Our results w/ geometric search scheme. The proposed 3D label generator outperforms the method of Martinez et al. [19]. The proposed geometric search scheme can refine the coarse 3D human pose. (All the predicted 3D poses are demonstrated from the front viewpoint.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visual results on the in-the-wild images. The proposed dataset helps to generate more reasonable results in terms of 3D human skeletons. (a) Original in-the-wild images, (b) Results with geometric loss [47], (c) Results with GANs [44], (d) Results with the proposed dataset. 3D human pose presents in a novel viewpoint. scheme by comparing the performance between the method w/o or w/ using it. The experimental results validate the effectiveness of the geometric search scheme. As one can see in the figure 4(c) and (d), after the geometric search scheme, the predicted 3D human poses become more reasonable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of the 3D skeletons of the video sequence extracted in the Penn<ref type="bibr" target="#b45">[46]</ref> dataset. (a) Video sequences, (b) 3D human skeletons from the front viewpoint predicted by Zhou et al.<ref type="bibr" target="#b46">[47]</ref>, (c) 3D human skeletons from the top viewpoint predicted by Zhou et al.<ref type="bibr" target="#b46">[47]</ref>, (d) 3D human skeletons from the front viewpoint predicted by our method, (e) 3D human skeletons from the top viewpoint predicted by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Protocol#1 (↓) Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD Walk WalkT. Average</figDesc><table><row><cell>LinKDE [12]</cell><cell>132.7</cell><cell>183.6</cell><cell cols="6">132.3 164.4 162.1 205.9 150.6 171.3</cell><cell>151.6</cell><cell>243.0</cell><cell cols="3">162.1 170.7 177.1</cell><cell>96.6</cell><cell>127.9</cell><cell>162.1</cell></row><row><cell>Tekin et al. [39]</cell><cell>102.4</cell><cell>147.2</cell><cell>88.8</cell><cell cols="5">125.3 118.0 182.7 112.4 129.2</cell><cell>138.9</cell><cell>224.9</cell><cell cols="3">118.4 138.8 126.3</cell><cell>55.1</cell><cell>65.8</cell><cell>125.0</cell></row><row><cell>Zhou et al. [50]</cell><cell>87.4</cell><cell>109.3</cell><cell>87.1</cell><cell cols="4">103.2 116.2 143.3 106.9</cell><cell>99.8</cell><cell>124.5</cell><cell>199.2</cell><cell cols="3">107.4 118.1 114.2</cell><cell>79.4</cell><cell>97.7</cell><cell>113.0</cell></row><row><cell>Park et al. [25]</cell><cell>100.3</cell><cell>116.2</cell><cell>90.0</cell><cell cols="5">116.5 115.3 149.5 117.6 106.9</cell><cell>137.2</cell><cell>190.8</cell><cell cols="3">105.8 125.1 131.9</cell><cell>62.6</cell><cell>96.2</cell><cell>117.3</cell></row><row><cell>Nie et al. [24]</cell><cell>90.1</cell><cell>88.2</cell><cell>85.7</cell><cell>95.6</cell><cell cols="3">103.9 103.0 92.4</cell><cell>90.4</cell><cell>117.9</cell><cell>136.4</cell><cell>98.5</cell><cell>94.4</cell><cell>90.6</cell><cell>86.0</cell><cell>89.5</cell><cell>97.5</cell></row><row><cell>Metha et al. [21]</cell><cell>57.5</cell><cell>68.6</cell><cell>59.6</cell><cell>67.3</cell><cell>78.1</cell><cell>82.4</cell><cell>56.9</cell><cell>69.1</cell><cell>100.0</cell><cell>117.5</cell><cell>69.4</cell><cell>68.0</cell><cell>76.5</cell><cell>55.2</cell><cell>61.4</cell><cell>72.9</cell></row><row><cell>Metha et al. [22]</cell><cell>62.6</cell><cell>78.1</cell><cell>63.4</cell><cell>72.5</cell><cell>88.3</cell><cell>93.8</cell><cell>63.1</cell><cell>74.8</cell><cell>106.6</cell><cell>138.7</cell><cell>78.8</cell><cell>73.9</cell><cell>82.0</cell><cell>55.8</cell><cell>59.6</cell><cell>80.5</cell></row><row><cell>Lin et al. [17]</cell><cell>58.0</cell><cell>68.2</cell><cell>63.3</cell><cell>65.8</cell><cell>75.3</cell><cell>93.1</cell><cell>61.2</cell><cell>65.7</cell><cell>98.7</cell><cell>127.7</cell><cell>70.4</cell><cell>68.2</cell><cell>72.9</cell><cell>50.6</cell><cell>57.7</cell><cell>73.1</cell></row><row><cell>Tome et al. [40]</cell><cell>65.0</cell><cell>73.5</cell><cell>76.8</cell><cell>86.4</cell><cell>86.3</cell><cell cols="2">110.7 68.9</cell><cell>74.8</cell><cell>110.2</cell><cell>173.9</cell><cell>84.9</cell><cell>85.8</cell><cell>86.3</cell><cell>71.4</cell><cell>73.1</cell><cell>88.4</cell></row><row><cell>Tekin et al. [38]</cell><cell>54.2</cell><cell>61.4</cell><cell>60.2</cell><cell>61.2</cell><cell>79.4</cell><cell>78.3</cell><cell>63.1</cell><cell>81.6</cell><cell>70.1</cell><cell>107.3</cell><cell>69.3</cell><cell>70.3</cell><cell>74.3</cell><cell>51.8</cell><cell>63.2</cell><cell>69.7</cell></row><row><cell>Pavlakos et al. [28]</cell><cell>67.4</cell><cell>71.9</cell><cell>66.7</cell><cell>69.1</cell><cell>72.0</cell><cell>77.0</cell><cell>65.0</cell><cell>68.3</cell><cell>83.7</cell><cell>96.5</cell><cell>71.7</cell><cell>65.8</cell><cell>74.9</cell><cell>59.1</cell><cell>63.2</cell><cell>71.9</cell></row><row><cell>Martinez et al. [19]</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Fang et al. [6]</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>73.3</cell><cell>53.4</cell><cell>55.7</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Sun et al. [36]</cell><cell>52.8</cell><cell>54.8</cell><cell>54.2</cell><cell>54.3</cell><cell>61.8</cell><cell>67.2</cell><cell>53.1</cell><cell>53.6</cell><cell>71.7</cell><cell>86.7</cell><cell>61.5</cell><cell>53.4</cell><cell>61.6</cell><cell>47.1</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell>Yang et al. [44]</cell><cell>51.5</cell><cell>58.9</cell><cell>50.4</cell><cell>57.0</cell><cell>62.1</cell><cell>65.4</cell><cell>49.8</cell><cell>52.7</cell><cell>69.2</cell><cell>85.2</cell><cell>57.4</cell><cell>58.4</cell><cell>43.6</cell><cell>60.1</cell><cell>47.7</cell><cell>58.6</cell></row><row><cell>Martinez et al. [19] (GT) w/o GS</cell><cell>37.7</cell><cell>44.4</cell><cell>40.3</cell><cell>42.1</cell><cell>48.2</cell><cell>54.9</cell><cell>44.4</cell><cell>42.1</cell><cell>54.6</cell><cell>58.0</cell><cell>45.1</cell><cell>46.4</cell><cell>47.6</cell><cell>36.4</cell><cell>40.4</cell><cell>45.5</cell></row><row><cell>Martinez et al. [19] (GT) w/ GS</cell><cell>33.1</cell><cell>39.8</cell><cell>34.5</cell><cell>37.5</cell><cell>39.5</cell><cell>45.7</cell><cell>40.4</cell><cell>31.7</cell><cell>44.9</cell><cell>49.2</cell><cell>37.8</cell><cell>39.2</cell><cell>39.8</cell><cell>30.3</cell><cell>33.8</cell><cell>38.5</cell></row><row><cell>Ours (GT) w/o GS</cell><cell>35.6</cell><cell>41.3</cell><cell>39.4</cell><cell>40.0</cell><cell>44.2</cell><cell>51.7</cell><cell>39.8</cell><cell>40.2</cell><cell>50.9</cell><cell>55.4</cell><cell>43.1</cell><cell>42.9</cell><cell>45.1</cell><cell>33.1</cell><cell>37.8</cell><cell>42.0</cell></row><row><cell>Ours (GT) w/ GS</cell><cell>32.1</cell><cell>39.2</cell><cell>33.4</cell><cell>36.4</cell><cell>38.9</cell><cell>45.9</cell><cell>38.4</cell><cell>31.7</cell><cell>42.5</cell><cell>48.1</cell><cell>37.8</cell><cell>37.9</cell><cell>38.7</cell><cell>30.6</cell><cell>32.6</cell><cell>37.6</cell></row><row><cell>Ours (GT) w/ GS + unity</cell><cell>36.5</cell><cell>42.7</cell><cell>38.2</cell><cell>39.6</cell><cell>45.3</cell><cell>50.8</cell><cell>40.2</cell><cell>34.8</cell><cell>45.0</cell><cell>50.3</cell><cell>39.4</cell><cell>39.9</cell><cell>42.5</cell><cell>32.2</cell><cell>33.8</cell><cell>40.8</cell></row></table><note>second stage is a depth regression module. Given the 2D body joints heat-maps and intermediate features generated from stacked hourglass network, it can predict the depth of each joint. Since we have a large-scale in-the-wild 3D pose dataset, we discard the weakly-supervised designs em- ployed by Zhou et al. [47] and Yang et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluations on the Human3.6M<ref type="bibr" target="#b11">[12]</ref> under Protocol#1 without using the geometric search scheme and the dataset from the unity toolbox.</figDesc><table><row><cell>∆x/mm</cell><cell>Martinez et al. [19]</cell><cell>250</cell><cell>500</cell><cell>750</cell></row><row><cell>Ave./mm</cell><cell>45.5</cell><cell cols="2">42.2 42.0</cell><cell>42.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative evaluations on the MPI-INF-3DPH<ref type="bibr" target="#b19">[20]</ref>. No training data from this dataset have been used for training by any method.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="7">Zhou et al. [47] Yang et al. [44] Ours</cell></row><row><cell></cell><cell></cell><cell>PCK</cell><cell></cell><cell>69.2</cell><cell></cell><cell></cell><cell>69.0</cell><cell></cell><cell>71.2</cell></row><row><cell></cell><cell></cell><cell>AUC</cell><cell></cell><cell>32.5</cell><cell></cell><cell></cell><cell>32.0</cell><cell></cell><cell>33.8</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>40 50 60 70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Zhou et al. [47] Ours</cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epochs</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://unity3d.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiview pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pose locality constrained representation for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning knowledge-guided pose grammar machine for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view 3d human pose estimation in complex environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human pose estimation and activity recognition from multiview videos: Comparative explorations of recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of selected topics in signal processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Representation Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Determination of 3d human-body postures from a single view. Computer Vision Graphics and Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent 3d pose sequence machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation using transfer learning and improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>arxiv preprint. arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Fbipose: Towards bridging the gap between 2d images and 3d human poses using forward-or-backward information</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ai challenger: A large-scale dataset for going deeper in image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
