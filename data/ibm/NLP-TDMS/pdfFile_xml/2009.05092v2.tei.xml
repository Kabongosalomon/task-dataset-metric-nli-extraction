<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
							<email>huichen@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dialogue relation extraction (DRE) aims to detect the relation between two entities mentioned in a multi-party dialogue. It plays an important role in constructing knowledge graphs from conversational data increasingly abundant on the internet and facilitating intelligent dialogue system development. The prior methods of DRE do not meaningfully leverage speaker information-they just prepend the utterances with the respective speaker names. Thus, they fail to model the crucial inter-speaker relations that may give additional context to relevant argument entities through pronouns and triggers. We, however, present a graph attention network-based method for DRE where a graph, that contains meaningfully connected speaker, entity, entity-type, and utterance nodes, is constructed. This graph is fed to a graph attention network for context propagation among relevant nodes, which effectively captures the dialogue context. We empirically show that this graph-based approach quite effectively captures the relations between different entity pairs in a dialogue as it outperforms the state-of-the-art approaches by a significant margin on the benchmark dataset DialogRE. Our code is released at: https://github.com/declare-lab/dialog-HGAT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) task aims to recognize relations between two entities present in a document. It plays a pivotal role in understanding unstructured text and constructing knowledge bases <ref type="bibr">(Peng et al. 2017;</ref><ref type="bibr" target="#b20">Quirk and Poon 2017)</ref>. Although the task of document-level relation extraction has been studied extensively in the past, the task of relation extraction from dialogues has yet to receive extensive study.</p><p>Conversational text exhibits intra-and inter-utterance relations <ref type="bibr" target="#b19">(Poria et al. 2020)</ref>, which makes it different from the text in previous document-level relation extraction. Most previous works focus on professional and formal literature like biomedical documents <ref type="bibr" target="#b11">(Li et al. 2016;</ref><ref type="bibr" target="#b28">Wu et al. 2019)</ref> and Wikipedia articles <ref type="bibr" target="#b6">(Elsahar et al. 2018;</ref><ref type="bibr" target="#b31">Yao et al. 2019;</ref><ref type="bibr" target="#b12">Mesquita et al. 2019)</ref>. These kinds of datasets are wellformatted and logically coherent with clear referential semantics. Hence for most NLP tasks analyzing a few continuous sentences are enough to grasp pivotal information. However, for dialogue relation extraction, conversational text is sampled from daily chat, which is more casual in nature. Hence its logic is simpler but entangled and referential ambiguity always occurs to an external reader. Com-  pared with formal literature, it has lower information density <ref type="bibr" target="#b27">(Wang and Liu 2011)</ref> but is more difficult for model to understand. Moreover, compared with other document-level RE dataset such as DocRED, dialogue text has much more cross-sentence relations <ref type="bibr" target="#b32">(Yu et al. 2020</ref>). <ref type="figure" target="#fig_0">Fig. 1</ref> presents an example of dialogue relation extraction, taken from Dialo-gRE <ref type="bibr" target="#b32">(Yu et al. 2020)</ref> dataset. In order to infer the relation between Speaker1 and Emma, we may need to find some triggers to recognize the characteristics of Emma. Triggers are evidences that can support the inference. As we can see, the following utterances are talking about Emma, and the key word baby daughter mentioned by Speaker1 is a trigger, which provides an evidence that Emma is Speaker1's daughter.</p><p>Prior works show that triggers of arguments facilitate the document-level relation inference. Thus, DocRED dataset <ref type="bibr" target="#b31">(Yao et al. 2019)</ref> provides several supporting evidences for each argument pair. Some efforts utilize the dependency paths of arguments to find possible triggers. For example, LSR model <ref type="bibr" target="#b13">(Nan et al. 2020</ref>) constructs meta dependency paths of each argument pair and aggregates all the word representations located in these paths to their model, in order to enhance model's reasoning ability. <ref type="bibr" target="#b21">Sahu et al. (2019)</ref> uses syntactic parsing and coreference resolution to find intra-and inter-related words of each ar-gument. <ref type="bibr" target="#b5">Christopoulou, Miwa, and Ananiadou (2019)</ref> proposes an edge-oriented graph to synthesize argument-related information. These models are graph-based and have proven powerful in encoding long-distance information. However, for dialogue relation extraction, interlocutors exist in every utterance of the dialogue, and they are often considered as an argument. Although these previous approaches have utilized entity features of arguments, most of them employ meta dependency paths to find the related words, which results in the missing of necessary information related to speakers, since the speaker references have very little dependency features in each utterance. We think the structure of our graph allows it to model the intra-and inter-speaker relations through paths that involve conversational discourse and word-level semantics. This phenomenon enables the model to outshine the state-of-the-art frameworks int the task of dialogue level relation extraction.</p><p>In this work, we propose a simple yet effective attentionbased heterogeneous graph neural network to tackle the dialogue relation extraction task by using multi-type features to create the graph and employing graph attention mechanism to propagate contextual information. Different from most of the previous works, our proposed model is customized for the relation extraction task in dialogue background, as we have specially modeled speaker information and designed a mechanism to propagate massages among different sentences for better inter-sentence representation learning. The remainder of this paper is organized as follows: Section 3 elaborates on our proposed framework; Section 4 introduces the used dataset and baseline models; Section 5 lays out the experiment results and analysis; Section 6 briefly discusses relevant works of heterogeneous graph neural networks; and Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background of Graph Attention Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Attention Network (GAT)</head><p>The graph attention network is composed of graph attentional layers. Given a graph G = (V, E) and its node embeddings h = { h 1 , h 2 , ..., h N }, h i ∈ R F , GAT layer updates all the embeddings to h = { h 1 , h 2 , ..., h n } using a self-attention mechanism. In the attention computation part, each node only with its neighbour nodes serves as inputs to generate a set of attention weights, which is called masked attention:</p><formula xml:id="formula_0">α ij = exp LeakyReLU(ã T [W h i W h j ]) k∈Ni exp LeakyReLU(ã T [W h i W h k ])<label>(1)</label></formula><p>Where W ∈ R F ×F is a weight matrix to linearly transform embeddings to another feature space,ã ∈ R 2F weight matrix in a feed-forward neural network to generate the attention weights. Then these weights will be applied to original node features to generate new features:</p><formula xml:id="formula_1">h i = K k=1 σ   j∈Ni α k ij W k h j   (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Heterogeneous Graph Neural Network (HGNN)</head><p>Massive work on graph neural network treats the graph as homogeneous ones, where nodes and edges are of the same type. However, considering the complexity of the real world, the attributes of things and their relations vary greatly. As a result, it is difficult to use a homogeneous graph to describe them. Heterogeneous graphs, which assume multitype nodes and edges, make mathematical modeling of the real world more approachable. A heterogeneous graph can be defined by a graph topology G = (V, E) with a node type mapping: φ: V − → A and an edge type mapping ψ: E − → R. Particularly, a graph is a heterogeneous graph when the types of nodes |E| &gt; 1 or the types of edges |E| &gt; 1.</p><p>To construct neural networks on heterogeneous graphs, effective information extraction and message passing scheme should be formulated. Meta-path <ref type="bibr" target="#b22">(Sun et al. 2011)</ref>, which has been used as a general structure to capture different semantics in heterogeneous graphs, is utilized in HGNNs. A meta-path is a path defined on the edge and node type set {R, E}, in the form of A 1</p><formula xml:id="formula_2">R1 − − → A 2 R2 − − → ... R l −→ A l+1 .</formula><p>It specifies a composition relation R = R 1 • ... • R l between objects A 1 and A l+1 . In our work, we firstly build a heterogeneous graph composed of hierarchical and functional language components from the dataset, then applies heterogeneous graph attention operations on task-specified useful meta-paths to enhance the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Given a dialogue containing N utterances D = {u 1 , u 2 , ..., u N } and argument pairs A = {(x 1 , y 1 ), (x 2 , y 2 ), . . . }, where subject x i and object y i are entities mentioned in the dialogue, the goal is to identify the relation between argument pairs (x i , y i ). For document-level relation extraction task, subject entity and object entity are often distributed in various sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><p>In this work, a conversation is represented as a heterogeneous graph for both intra-and inter-sentence relation inferences. We first utilize Utterance Encoder to encode sentential level utterance information. These utterance encodings along with word embeddings, speaker embeddings, argument embeddings, and entity-type embeddings are logically connected to form a heterogeneous graph-discussed in detail later in this section. Further, this graph is fed through a graph attention layer <ref type="bibr" target="#b25">(Veličković et al. 2018</ref>) that aggregates information from the neighboring nodes.</p><p>Lastly, we concatenate the learned argument embeddings and feed them to a classifier. An overview of the proposed framework is shown in <ref type="figure">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Preprocessing</head><p>We use spaCy 1 to tokenize utterances and at the same time, we obtain part-of-speech (POS) tags and named-entity types</p><formula xml:id="formula_3">1 https://spacy.io Nodes Speaker Utterance Word Entity Type Argument S 1 S 2 U 1 U 2 U 3 U 4 w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 PER ORG GPE OTHER ARG1 ARG2 GAT c 1 c 2 LSTM c 3 c 4 LSTM LSTM LSTM U 1 U 2 U 3 U 4 L S T M L S T M L S T M w1 w5 w4 Utterance 1 L S T M L S T M L S T M w7 w8 w4</formula><p>Utterance 4 Classify Relation Labels ARG2 ARG1 w 8 w 1 <ref type="figure">Figure 2</ref>: An overview of the proposed framework. of each token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Utterance Encoding 2. Message Passing 3. Relation Classification</head><formula xml:id="formula_4">Concatenation S 1 S 2 U 1 U 2 U 3 U 4 w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 PER ORG GPE OTHER ARG1 ARG2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Utterance Encoder</head><p>Given a dialogue D, each GloVe <ref type="bibr" target="#b18">(Pennington, Socher, and Manning 2014)</ref> initialized utterance u i is first fed to a contextual encoder to obtain the contextualized representations of each constituent word. Bidirectional long short-term memory network (BiLSTM) is used as our contextual encoder. The operation of BiLSTM in our model can be defined as:</p><formula xml:id="formula_5">← − h i j = LST M l ( ←−− h i j+1 , e i j ) (3) − → h i j = LST M r ( −−→ h i j−1 , e i j )<label>(4)</label></formula><formula xml:id="formula_6">h i j = [ ← − h i j ; − → h i j ]<label>(5)</label></formula><p>where ← − h i j and − → h i j denote the hidden representations in the j-th layer of utterance u i from two directions, h i j is the contextual representation which is the concatenation of ← − h i j and − → h i j , and e i j stands for the embedding of the j-th token in utterance u i . Unlike the previous approaches  that only rely on semanticcontextual features for utterance encoding, we also employ syntactic features. Thereby, we concatenate semantic wordembedding e w initialized by GloVe <ref type="bibr" target="#b18">(Pennington, Socher, and Manning 2014)</ref>, syntactic Part of Speech embedding e p , and entity-type embedding e t to form token embedding e: e = [e w ; e p ; e t ].</p><p>To encode non-local contextual information between each utterances, we max pool the hidden states of each utterancelevel BiLSTM (local LSTM), and then feed the sequence c = {c 1 , c 2 , ..., c N } to a conversational-level BiLSTM (global LSTM). The operation of global LSTM is the same as Eqs.</p><p>(3) to (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Graph Construction</head><p>Node Construction There are five types of nodes in our proposed heterogeneous graph: utterance nodes, entity-type nodes, word nodes, speaker nodes, and argument nodes. Each type of node is used to encode a type of information in the dialogue.</p><p>Utterance and Entity-Type Nodes. Utterance nodes are used to represent the utterance-level information in a conversation. We use the outputs of utterance encoder which contains the utterance level encoding to initialize our utterance nodes.</p><p>Entity-type nodes represent the types of words that include a variety of named and numeric entities, such as PER-SON or LOCATION. Naturally, each constituent word of a named entity is connected to its corresponding type node. Since the different mentions of the same entity may have different types in one conversation, we aim to capture all the type information of each entity via this entity-type node. For example, 'Frank' can be a string if it represents an alternative name, and at the same time, it can be a person if it refers to a speaker in the conversation. We believe that entity-type information has a positive influence on the relation-inference process. Each Entity-Type node is initialized with our created Entity-Type embedding according to its entity type.</p><p>Word, Speaker, and Argument Nodes. Word nodes represent the vocabulary of the conversation. Each word node is connected with the utterance which contains the word and it is also connected with all the possible entity types that the word could have in the conversation. We initialize the states of word nodes with GloVe <ref type="bibr" target="#b18">(Pennington, Socher, and Manning 2014)</ref>.</p><p>Speaker node represents each unique speaker in the conversation. Each speaker node is connected with the utterances uttered by the speaker himself/herself. This type of node is initialized with its specific embedding and it is used to gather global information about the utterances made by the speaker.</p><p>Argument nodes are two special nodes that used to encode argument's relative positional information about the argument pair. One stands for the subject argument and the other represents the object argument. Similar to speaker nodes, argument nodes are also encoded by a specific embedding.</p><p>Edge Construction The proposed graph is undirected but the propagation has directions. There are five types of edges: utterance-word, utterance-argument, utterancespeaker, type-word, and type-argument. 'A-B' means there are edges between node A and B. Each edge has its own type. These edges are randomly initialized except utteranceword edge.</p><p>For the edge between utterance and word nodes, we adopt POS tags to initialize the edge features, since POS tags can reflect the local information of each word. This kind of edge aggregates not only global semantic features of the conversation but also local syntactic features to the word nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Attention Layer</head><p>We use graph attention mechanism <ref type="bibr" target="#b25">(Veličković et al. 2018)</ref> to aggregate discourse information and entity-type information to basic nodes. Let's take node i as an example. The graph attention mechanism described in the following shows how i's neighborhood j aggregates its information to i:</p><formula xml:id="formula_8">F(h i , h j ) = LeakyReLU(a T (W i h i ; W j h j ; E ij ) (7) α ij = softmax(F(h i , h j )) = exp(F(h i , h j )) k exp(F(h i , h k ))<label>(8)</label></formula><formula xml:id="formula_9">h i = || K k=1 σ( j α k ij W k q h j )<label>(9)</label></formula><p>where h i and h j are representations of node i and j, W i , W j , W q and a T are trainable weight matrices, E ij is the edge weight matrix that is mapped to the multi-dimensional embedding space, α ij is the attention weight between i and j, σ is an activation function, and || is concatenation operation.</p><p>Message Propagation Although graph attention operation can effectively aggregate neighbor features, only one message passing will make the node structure relatively shallow. To make node features more informative, we update all basic nodes multiple times. Our meta path is as follows: First, we use utterance nodes to update word nodes, speaker nodes and argument nodes; secondly, the updated word nodes propagate messages to entity-type nodes; then entity-type nodes update word nodes, argument nodes, and entity-type nodes; next we use word nodes, speaker nodes, and argument nodes to update utterance nodes; and lastly the updated utterance nodes update word nodes, speaker nodes and argument nodes. The path can be denoted as</p><formula xml:id="formula_10">V u − &gt; V b − &gt; V t − &gt; V b − &gt; V u − &gt; V b , where V u , V b</formula><p>, and V t refer to utterance nodes, basic nodes, and entity-type nodes.</p><p>Following <ref type="bibr" target="#b26">Wang et al. (2020)</ref>, we add a residual connection <ref type="bibr" target="#b8">(He et al. 2016)</ref> to avoid gradient vanishing during updating:ĥ</p><formula xml:id="formula_11">i =h i + h i<label>(10)</label></formula><p>whereh i is the output learned in the graph attention layer, and h i is the original input of the graph attention layer.</p><p>In our model, we first aggregate utterance nodes to basic nodes, so that semantic and syntactic features obtained in the utterance graph encoder can be intactly passed to word nodes, speaker nodes, and argument nodes. In message passing, except for graph attention operation, there is also a twolayer feed-forward network which can be denoted as:</p><formula xml:id="formula_12">h new i = FFN(ĥ i )</formula><p>(11) Suppose we have the initial embeddings of utterance nodes, basic nodes and entity-type nodes, denoted as embedding matrices H u = {H u , H b , H t }, the message propagating process can be written as:</p><formula xml:id="formula_13">H 1 b = GAT(H 0 b , H 0 u )<label>(12)</label></formula><formula xml:id="formula_14">H 1 t = GAT(H 0 t , H 1 b )<label>(13)</label></formula><formula xml:id="formula_15">H 2 b = GAT(H 1 b , H 1 t )<label>(14)</label></formula><formula xml:id="formula_16">H 1 u = GAT(H 0 u , H 2 b )<label>(15)</label></formula><formula xml:id="formula_17">H 3 b = GAT(H 2 b , H 1 u )<label>(16)</label></formula><p>where the GAT operation is the same as Eqs. <ref type="formula">(7)</ref> to <ref type="formula" target="#formula_0">(11)</ref>. The superscripts represent it is the n th new value of that matrix and 0 marks the initial value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Relation Classifier</head><p>After the message propagation in the heterogeneous graph, we obtain new representations of all entities. We select the argument nodes τ x and τ y , as well as the corresponding word nodes e x and e y from basic nodes, and concatenate them. Finally, they are fed to a linear transformation and a sigmoid function to get the predictions: e x = [maxpool(τ x ); maxpool(e x )] (17) e y = [maxpool(τ y ); maxpool(e y )]</p><formula xml:id="formula_18">e = [e x ; e y ]<label>(18)</label></formula><p>P (r|e x , e y ) = σ(W e e + b e ) r</p><p>where P (r|e x , e y ) is the probability of relation type r given argument pair (e x , e y ), W e and b e are linear transformation weight and bias vector, maxpool is max pooling operation, and σ is sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Used</head><p>We evaluate the proposed framework on the DialogRE dataset <ref type="bibr" target="#b32">(Yu et al. 2020)</ref>, which contains totally 1,788 dialogues and 10,168 relational triples. DialogRE is adapted from the complete transcripts of Friends, which is a widely used corpus in dialogue research these years <ref type="bibr" target="#b3">(Chen, Zhou, and Choi 2017;</ref><ref type="bibr" target="#b35">Zhou and Choi 2018;</ref><ref type="bibr" target="#b29">Yang and Choi 2019)</ref>, and there are 36 possible relation types, most of which focus on biographical attributes of person entities. Each dialogue contains several relational triples (x, y, r), and the task is to predict the relation r between each entity pair (x, y). In the experiments, the dataset is partitioned into train, dev, and test set with roughly 60/20/20 ratio. Following the evaluation metrics of DialogRE, we report macro F1 scores of the proposed model and all the baselines in both the standard and conversational settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings and Hyperparameters</head><p>In our experiments, we tune the parameters of batch size, learning rate, and BiLSTM hidden size by testing the performance on the validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline models</head><p>Sequence-based Models We select convolutional neural networks(CNN) <ref type="bibr" target="#b33">(Zeng et al. 2014)</ref>, LSTM, and BiL-STM <ref type="bibr" target="#b2">(Cai, Zhang, and Wang 2016)</ref> as the sequence-based baselines. These models take word embeddings, mention embeddings, and type embeddings as features. Concretely, they use GloVe and spaCy to get word embeddings and label named-entity types, and then take an average of all the embeddings of mention names for each entity to get mention embeddings.</p><p>Graph-based Models As our proposed model is graphbased, we also select two graph-based models AG-GCN <ref type="bibr" target="#b7">(Guo, Zhang, and Lu 2019)</ref> and LSR <ref type="bibr" target="#b13">(Nan et al. 2020)</ref> as the baselines. AGGCN directly feeds the full dependency tree of each sentence to a graph convolutional network which takes self-attention weights as soft edges. It achieves state-of-the-art results in various relation extraction tasks. LSR adopts an adaptation of Kirchhoffs Matrix-Tree Theorem <ref type="bibr" target="#b23">(Tutte 1984;</ref><ref type="bibr" target="#b10">Koo et al. 2007</ref>) to induce the latent dependency structure of each document and then feeds the latent structure to a densely connected graph convolutional network to inference the relations. These graph-based models both utilize dependency information to construct the inference graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparision with Baselines</head><p>We present our main results on DialogRE dataset in <ref type="table" target="#tab_3">Table 2</ref>. As shown in <ref type="table" target="#tab_3">Table 2</ref>, our model surpasses the state-of-theart method by 9.6%/7.5% F1 scores, and 8.4%/5.7% F1 c scores in both validation and test sets, which demonstrates the effectiveness of the information propagation along taskspecific functional meta-paths in the heterogeneous graph. Whatever purely sequential models or graph-based models that are built from local transformers focus on modeling the sequence within a sentence scope. As a result, intersentence communication usually passes through a long distance, which causes information loss or disruption. However, this kind of information exchange is critically important for dialog-style text, because logical connections are not locally compact within adjacent sentences, instead they are spread over the whole conversations. Our proposed model, on the opposite, constructs a heterogeneous graph with shorter distances between logically closed but syntactically faraway word pairs. Hence the long-distance issue is mitigated.</p><p>We also compare the model sizes as an efficiency indicator. Although creating numerous nodes and edges inevitably brings overhead, the total number of parameters is still moderate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To understand the impact of the components in our model, we perform ablation study using our proposed model on Di-alogRE dataset. The ablation results are shown in <ref type="table" target="#tab_5">Table 3</ref>. First, we remove local LSTM and global LSTM. The results showing drops in all the evaluation metrics prove that the contextual encoder plays an important role in semantic feature extraction. Second, we remove the specific argument nodes and have observed that F1 and F1 c scores decrease to 55.0% and 50.2% on test set. This proves our design on argument nodes effectively synthesize argument features to the model. Further, we test the performance of the syntactic features we inject by removing POS embedding, NER embedding, and POS edge features. All the scores decrease, and specifically, the removal of POS embedding leads to about 2% drops in all the evaluation metrics.  Rand init vs. GloVe Additionally, we have compared different initialization strategies on word nodes. When we transfer GloVe initialization method to a random but trainable initialization method, we can observe a 3% to 4% decrease in all the metrics. This demonstrates our GloVe initialization strategy retains word features which have a positive influence on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is our design of meta path optimal?</head><p>We test the performance of our message propagation strategy via changing the update strategies. In our proposed model, those basic nodes composed of word, speaker, and argument nodes are updated totally thrice, i.e., they are first updated by utterance nodes, second updated by entity type nodes, and ultimately updated by utterance again. In our ablation study, we try to update basic nodes once, which means basic nodes are only updated by utterance nodes. Results present a dramatically drop of the evaluation scores, especially the standard F1 scores. However, if we add two more updates, that is to say, after our default updates, entity type nodes update basic nodes again and then utterance nodes update basic again, the results don't have an increase on the evaluation scores but decrease a bit. This proves that our message propagation strategy is the optimum now. If we only update the basic nodes once, node features are not informative enough. But if we update the basic nodes too many times, the features may be overfitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Studies</head><p>In the dataset, 95% of relation pairs have argument pairs that span two sentences. Therefore, it is crucial to model long range inter-sentential relationships. Our model can propagate relational information more effectively. Comparing to the LSTM model, speaker nodes, utterance nodes and unique word nodes shorten the information propagation path between two argument nodes. Considering the following example in <ref type="figure" target="#fig_1">Fig. 3</ref>  the subject 'Mindy'. However, for bi-LSTM model, it will need to overcome long range of irrelevant information that will affect the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Error Analysis</head><p>Entity-type information involves in the information propagation process and thus affect the contents of output embeddings. The model is prone to make incorrectly biased predictions which highly relies on the entity types of two arguments if it fails to acquire enough certainty from other information sources. For example, given an entity pair of two human names, both with the named entity type 'PERSON'. Sometimes the model inclines to deem the relationship between the pair to be 'per:alternate name' instead of correct 'per:alumni' or 'per:roommate'. This is because for all of these classes, 'PERSON-PERSON' is a preferable entitytype pair. However, the class 'per:alternate name' (22.01%) presents more frequently than 'per:alumni' (1.83%) and 'per:roommate' (1.29%) in the dataset. When information aggregated from all sources other than entity pair is not evident for judgment, entity bias misguides the model to wrong classification results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Graph-based models have raised popular attention from NLP researchers, as it is demonstrated as a powerful mathematical tool to represent complicated syntactic and semantic relations among structured language data. Early work applies classic graph processing algorithms onto language graphs. <ref type="bibr" target="#b14">Pang and Lee (2004)</ref> construct a text graph and adopt the minimum-cut method to cluster the nodes for sentiment analysis. <ref type="bibr" target="#b0">Agirre and Soroa (2009)</ref> leverage PageRank algorithm on personalized subgraphs of a wordnet to disambiguate polysemous words according to connected context words. Recently, with the achievement of graph neural networks <ref type="bibr" target="#b9">(Kipf and Welling 2017)</ref>, to incorporate syntactic features, which are easy to be expressed by graphs, into end-to-end learning models becomes a growing trend. <ref type="bibr">Peng et al. (2017)</ref> firstly try to build a computation graph from syntactic parsing trees and employing graph LSTM to obtain better word embeddings for multi-ary relation extraction. <ref type="bibr" target="#b34">Zhang, Qi, and Manning (2018)</ref> design a pruning algorithm for syntactic graphs and add a graph convolution layer on top of the sequential LSTM encoder in the learning process. The combination with typical attention-based language models such as transformer <ref type="bibr" target="#b24">(Vaswani et al. 2017</ref>) is also studied. The work in <ref type="bibr" target="#b1">(Cai and Lam 2020;</ref><ref type="bibr" target="#b30">Yao, Wang, and Wan 2020)</ref> use transformer-based graph convolutional networks to explicitly encode relations among distant syntactic nodes, to address the long-distance propagation issue.</p><p>Other works introduce heterogeneous graph neural networks into NLP tasks, like text classification <ref type="bibr" target="#b11">(Linmei et al. 2019</ref>), text summarization , user profiling <ref type="bibr" target="#b4">(Chen et al. 2019)</ref>, and event categorization <ref type="bibr" target="#b15">(Peng et al. 2019)</ref>. These works prove that heterogeneous graph neural network is a powerful tool in NLP. For the relation extraction task, Christopoulou, Miwa, and Ananiadou (2019) construct an edge-oriented heterogeneous graph that contains sentence, mention, and entity information. However, syntactic information is neglected in their model. Different from them, homogeneous nodes in our graph is all independent, and we take syntactic features to initialize sentence information as well as edges features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we present an attention-based heterogeneous graph to deal with the dialogue relation extraction task. This heterogeneous graph attention network has modeled multi-type features of the conversation, like utterance, word, speaker, argument, and entity type information. On the benchmark DialogRE dataset, our proposed framework outperforms the strong baselines and the state-of-the-art approaches by a significant margin, which proves the proposed framework can effectively capture relations between different entities in the conversation. Future works will focus on applying the relation knowledge to assist dialogue generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example adapted from DialogRE dataset. Words with red and blue background represent subject and object entities. Words with yellow background represent triggers that facilitate the relation inference. Solid and dash lines stand for intra-and inter-utterance relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Case study: an example to show the effective message propagation between argument pairs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Yeah, and it was uhm... it was like a real little person laugh too. It was... it was like uhm... Only... only not creepy. Well... well, what did you do to make her laugh? I uhm... Well, I sang... well actually I rapped... Baby Got Back... You WHAT? You sang... to our baby daughter...</figDesc><table><row><cell>Speaker 1</cell><cell>Speaker 2</cell></row><row><cell>I just finished getting Phoebe all dressed to meet Mike's parents. She's</cell><cell>pe r: ch ild re n</cell></row><row><cell>so nervous, it's so sweet!</cell><cell></cell></row><row><cell>per:girl/boyfriend</cell><cell>Guess what? I made Emma laugh</cell></row><row><cell></cell><cell>today.</cell></row><row><cell>You WHAT? And I missed it? Because I</cell><cell></cell></row><row><cell>was giving a makeover to that stupid</cell><cell></cell></row><row><cell>hippie?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>lists the major pa-</cell></row><row><cell>rameters used in our experiments.</cell><cell></cell></row><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Word Embedding Dimension</cell><cell>300</cell></row><row><cell>NER Embedding Dimension</cell><cell>30</cell></row><row><cell>POS Embedding Dimension</cell><cell>30</cell></row><row><cell>Local BiLSTM Hidden Size</cell><cell>200</cell></row><row><cell>Local BiLSTM Layers</cell><cell>2</cell></row><row><cell>Global BiLSTM Hidden Size</cell><cell>128</cell></row><row><cell>Global BiLSTM Layers</cell><cell>2</cell></row><row><cell>Multihead Attention Number</cell><cell>10</cell></row><row><cell>Learning Rate</cell><cell>0.0005</cell></row><row><cell>Batch Size</cell><cell>16</cell></row><row><cell>Edge Embedding Dimension</cell><cell>50</cell></row><row><cell cols="2">Table 1: Parameter settings.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Main results on DialogRE dataset. Values in the #params column refer to parameter sizes of the models. F 1 and F 1 c are macro F1 scores under standard setting and conversational setting, respectively. The unit of all the scores is %.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation results on DialogRE dataset. 't' means the number of updates for basic nodes. The unit of all the scores is %.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personalizing pagerank for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter</title>
		<meeting>the 12th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph Transformer for Graphto-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7464" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
	<note>: Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised User Profiling with Heterogeneous Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2116" to="2122" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4927" to="4938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention Guided Graph Convolutional Networks for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured prediction models via the matrix-tree theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention networks for semi-supervised short text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Linmei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4823" to="4832" />
		</imprint>
	</monogr>
	<note>BioCreative V CDR task corpus: a resource for chemical disease relation extraction</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledgenet: A benchmark dataset for knowledge base population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannaviccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="749" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reasoning with Latent Structure Refinement for Document-Level Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-grained event categorization with heterogeneous graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3238" to="3245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00357</idno>
		<title level="m">Beneath the Tip of the Iceberg: Current Challenges and New Directions in Sentiment Analysis Research</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction beyond the Sentence Boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inter-sentence Relation Extraction with Documentlevel Graph Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4309" to="4316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Tutte</surname></persName>
		</author>
		<title level="m">Graph Theory. In Claren-don Press</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Neural Networks for Extractive Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6209" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A pilot study of opinion summarization in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the Association for Computational Linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the Association for Computational Linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Renet: A deep learning approach for extracting genedisease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research in Computational Molecular Biology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FriendsQA: Open-domain question answering on TV show transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Transformer for Graph-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7145" to="7154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DocRED: A Large-Scale Document-Level Relation Extraction Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dialogue-Based Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4927" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">They exist! introducing plural mentions to coreference resolution and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
