<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Börje</forename><forename type="middle">F</forename><surname>Karlsson</surname></persName>
						</author>
						<title level="a" type="main">CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition (NER) in Chinese is essential but difficult because of the lack of natural delimiters. Therefore, Chinese Word Segmentation (CWS) is usually considered as the first step for Chinese NER. However, models based on word-level embeddings and lexicon features often suffer from segmentation errors and out-of-vocabulary (OOV) words. In this paper, we investigate a Convolutionall Attention Network called CAN for Chinese NER, which consists of a character-based convolutional neural network (CNN) with local-attention layer and a gated recurrent unit (GRU) with global self-attention layer to capture the information from adjacent characters and sentence contexts. Also, compared to other models, not depending on any external resources like lexicons and employing small size of char embeddings make our model more practical. Extensive experimental results show that our approach outperforms state-of-the-art methods without word embedding and external lexicon resources on different domain datasets including Weibo, MSRA and Chinese Resume NER dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Named Entity Recognition (NER) aims at identifying text spans which are associated with a specific semantic entity type such as person (PER), organization (ORG), location (LOC), and geopolitical entity (GPE). NER receives constant research attention as it is the first step in a wide range of downstream Natural Language Processing (NLP) tasks, e.g., entity linking <ref type="bibr" target="#b0">[1]</ref>, relation extraction <ref type="bibr" target="#b1">[2]</ref>, event extraction <ref type="bibr" target="#b2">[3]</ref>, and co-reference resolution <ref type="bibr" target="#b3">[4]</ref>.</p><p>The standard method of existing state-of-the-art models for English NER treats it as a word-by-word sequence labeling task and makes full use of the Recurrent Neural Network (RNN) and Conditional Random Field (CRF) to capture context information at the word level <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. These models for English NER predict a tag for each word assuming that words can be separated clearly by explicit word separators, e.g., blank space. Therefore, for Chinese language without natural delimiters, it is intuitive to apply Chinese Word Segmentation (CWS) first to get word boundaries and then use a word-level sequence labeling model similar to the English NER models. However, in Chinese language, word boundaries can be ambiguous, which leads the possibility that entity boundaries does not match word boundaries. For example, in a sentence, "西藏自治区 (Tibet Autonomous Region)" is a GPE type in NER task while can be segmented as one single word or as two words "西藏 (Tibet)" and "自治 区 (autonomous region)" separately, depending on different granularity of segmentation tools. But most of the time, it is hard to determine or unify the granularity of word segmentation. Also, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, different segmentation sometimes leads different sentence meaning in Chinese, which can even result in different named entities. Obviously, it is impossible to make the right extraction with word-based NER model if the boundaries are mistakenly detected at the first time. Most recent neural network based Chinese NER models rely heavily on the word-level embeddings and external lexicon sets <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The quality of those models would potentially be affected by different word embedding representations and lexicon features. Moreover, word-based models will suffer from OOV issues for Chinese words can be enormous and named entities are important source of OOV words. We also list other potential problems as follows: (1) Word embeddings dependency increases the model size and makes the fine-tuning process harder in the training step;</p><p>(2) It is hard to learn word representation correctly without enough labeled utterances for named entities are usually the proper nouns. (3) Large lexicons are much expensive for real NER system because it will spend large memories and long matching time to obtain the features, that makes the model inefficient. (4) It is barely impossible to clean noise words in the large lexicon. Both of word embeddings and lexicon are hard to be updated after trained.</p><p>Moreover, character embedding can only carry limited information for losing word and word sequence information. For instance, the character "拍" in word "球拍" (bat) and "拍 卖" (auction) has entirely different meanings. How to better integrate the segmentation information and exploit local context information is the key in the character-based model. <ref type="bibr" target="#b9">[10]</ref> leverage lexicons to add all the embeddings of candidate word segmentation to their last character embeddings as soft features and construct a convolutional neural network (CNN) to encode characters as word-level information. <ref type="bibr" target="#b10">[11]</ref> propose a multi-task architecture to learn NER tagging and Chinese word segmentation together with each part using a characterbased Bi-LSTM. We propose a convolutional attention layer to extract the implicit local context features from character sequence. With the segmentation vector softly concatenating into character embedding, the convolutional attention layer is able to group implicitly meaning-related characters and reduce the impact of segmentation errors. Results show that our model outperforms other Chinese NER models without external resources.</p><p>The main contributions of this paper can be summarized as follows:</p><p>• We first combine the CNN with the local-attention mechanism to enhance the ability of the model to capture implicitly local context relations among character sequence. Compared with experimental results of baseline with normal CNN layer, our Convolutional Attention layer leads a remarkable improvement of F1 performance. • We introduce a character-based Chinese NER model that consists of CNN with local attention and Bi-GRU with global self-attention layers. Our model achieve state-of-the-art F1-scores without using any external word embeddings and lexicon resources, which is more practical for real NER system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>We utilize BiGRU-CRF as our basic structure. Our model considers multi-level context features using convolutional attention layer, GRU layer and global attention layer. The whole architecture of our proposed model is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formulation</head><p>In the Chinese NER task, we denote an input sentence as X i = {x i,1 , x i,2 , x i,3 , ..., x i,τ }, where x i,τ ∈ R de represents the τ -th character in sentence X i and d e is the dimension of the input embeddings. Correspondingly, we denote the sentence label sequence as Y i = {y i,1 , y i,2 , y i,3 , ..., y i,τ }, where y i,τ ∈ Y belongs to the set of all possible labels. The objective is learning a function f θ : X → Y to obtain the entity types including the 'O' type for all the characters in the input text. In the following text, we take one instance as the example and therefore omitting subindex i in the formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Attention Layer</head><p>The convolutional attention layer aims to encode the sequence of input characters and implicitly group meaningrelated characters in the local context.</p><p>The input representation for each character is constructed as x = [x ch ; x seg ], where x ch ∈ R d ch and x seg ∈ R dseg are character embedding and segmentation mask, respectively. The segmentation information is encoded by BMES scheme <ref type="bibr" target="#b11">[12]</ref>.  For every window in CNN, whose window size is k, we first concatenate a position embedding to each character embedding, helping to keep the sequential relations in the local window context. The dimension of the position embedding equals to the window size k with the initial values of 1 at the position where the character lies in the window and 0 at other positions. So, the dimension of the concatenated embedding is d e = d ch + d pos + d seg . Then we apply a local attention inside the window to capture the relations between the center character and each context token, followed by a CNN with sum pooling layer. We set the hidden dimension as d h . For the j-th character, the local attention takes all the concatenated embeddings x j− k−1 2 , ...x j , ..., x j+ k−1 2 in the window as the input and outputs k hidden vectors</p><formula xml:id="formula_0">h j− k−1 2 , ..., h j , ..., h j+ k−1 2</formula><p>. The hidden vectors are calculated as follows:</p><formula xml:id="formula_1">h m = α m x m ,<label>(1)</label></formula><p>where m ∈ {j − k−1 2 , ..., j + k−1 2 } and α m is the attention weight, which is calculated as:</p><formula xml:id="formula_2">α m = exp s(x j , x m ) n∈{j− k−1 2 ,...,j+ k−1 2 } exp s(x j , x n )</formula><p>.</p><p>(2)</p><p>The score function s is defined as follows:</p><formula xml:id="formula_3">s(x j , x k ) = v tanh(W 1 x j + W 2 x k ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">v ∈ R d h and W 1 , W 2 ∈ R d h ,de .</formula><p>The CNN layer contains d h kernels on a context window of k tokens as:</p><formula xml:id="formula_5">h c j = k [W c * h j− k−1 2 :j+ k−1 2 + b c ],<label>(4)</label></formula><p>where W c ∈ R k×d h ×de and b c ∈ R k×d h . The operation * denotes element-wise product and h j− k−1</p><formula xml:id="formula_6">2 :j+ k−1 2 means a concatenation of the hidden states h j− k−1 2 , ..., h j+ k−1 2</formula><p>, both of which are calculated at the first dimension. Finally a sumpooling is also conducted on the first dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BiGRU-CRF with Global Attention</head><p>After extracting the local context features by convolutional attention layer, we feed them into a BiGRU-CRF based model to predict final label for each character.</p><p>The BiGRU layer is to model the sequential sentence information and calculated as follows:</p><formula xml:id="formula_7">h r j = BiGRU(h r j−1 , h c j ; W r , U r ),<label>(5)</label></formula><p>where h c j is the output of convolutional attention layer, h r j−1 is the previous hidden state for the BiGRU layer, and W r , U r ∈ R d h ×d h are the parameters. Then we apply a global self-attention layer to better handle the sentence-level information as:</p><formula xml:id="formula_8">h g j = n s=1 α g j,s h r s<label>(6)</label></formula><p>where j = 1, ..., τ denotes all the characters in a sentence instance and α g j,s is calculated as:</p><formula xml:id="formula_9">α g j,s = exp s(h r j , h r s ) n∈{1,...,τ } exp s(h r j , h r n ) .<label>(7)</label></formula><p>The score function s is similar to Equation 3 with different</p><formula xml:id="formula_10">parameters v g ∈ R d h and W g 1 , W g 2 ∈ R d h ,d h instead.</formula><p>Finally, a standard CRF layer is used on the top of the concatenation of the output of BiGRU layer and global attention layer, which is denoted as</p><formula xml:id="formula_11">H τ = [h r τ ; h g τ ].</formula><p>Given the predicted tag sequence Y = {y 1 , y 2 , y 3 , ..., y τ }, the probability of the ground-truth label sequence is computed by:</p><formula xml:id="formula_12">P (Y|X) = exp( i (W yi CRF H i + b (yi−1,yi) CRF )) y exp( i (W y i CRF H i + b (y i−1 ,y i ) CRF )) ,<label>(8)</label></formula><p>where y denotes an arbitary label sequence, W yi CRF and b (yi−1,yi) CRF are trainable parameters. In decoding, we use Viterbi algorithm to get the predicted tag sequence.</p><p>Till now we get the whole architecture which contains character-based, word-based, and sentence-based information altogether using multi-feature embeddings, CNN featurizers with local attention, and global self-attention mechanism.  </p><formula xml:id="formula_13">X i , Y i )}| K i=1</formula><p>, the loss function L can be defined as follows:</p><formula xml:id="formula_14">L = K i=1 logP (Y i |X i )<label>(9)</label></formula><p>In the training phase, at each iteration, we first shuffle all the training instances, and then feed them to the model with batch updates. We use AdaDelta <ref type="bibr" target="#b12">[13]</ref> algorithm to optimize the final objective with all the parameters as described in Section III-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>To demonstrate the effectiveness of our proposed model, we did some experiments on Chinese NER datasets of different domains. We will describe the details of datasets, setting and results in our experiments. Standard precision (P), recall (R) and F1-score (F1) are used as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Data We use four datasets in our experiments. For news domain, we experiment on OntoNotes 4 <ref type="bibr" target="#b13">[14]</ref> and MSRA NER dataset of SIGHAN Bakeoff 2006 <ref type="bibr" target="#b14">[15]</ref>. For the social media domain, we adopt the same annotated Weibo corpus as <ref type="bibr" target="#b15">[16]</ref> which is extracted from Sina Weibo 1 . For more variety in test domains, we also use Chinese Resume dataset <ref type="bibr" target="#b9">[10]</ref> collected from Sina Finance 2 .</p><p>Weibo dataset is annotated with four entity types: PER (Person), ORG (Organization), LOC (Location) and GPE (Geo-Political Entity), including named and nominal mentions. Weibo corpus is already divided into training, development and test sets. Chinese Resume dataset is annotated with eight types of named entities: CONT (Country), EDU (Educational Institution), LOC, PER, ORG, PRO (Profession), RACE (Ethnicity Background) and TITLE (Job Title). OntoNotes 4 dataset is annotated with four named entity categories: PER, ORG, LOC and GPE. We follow the same data split method of <ref type="bibr" target="#b16">[17]</ref>    <ref type="table" target="#tab_2">Table I</ref>. Gold segmentation is unavailable for Weibo dataset, Chinese Resume datasets and MSRA test sections. We followed <ref type="bibr" target="#b9">[10]</ref> to automatically segment Weibo dataset, Chinese Resume dataset and MSRA test sections using the model of <ref type="bibr" target="#b20">[21]</ref>. We treat NER as a sequential labeling problem and adopt BIOES tagging style in this paper since it has been shown that models using BIOES are remarkably better than BIO <ref type="bibr" target="#b21">[22]</ref>.</p><p>Hyper-parameter settings For hyper-parameter configuration, we adjust them according to the performance on the development set of Chinese NER task. We set the character embedding size, hidden sizes of CNN and Bi-GRU to 300 dims. After comparing to experimental results with different window sizes of CNN, we set the window size as 5. Adadelta is used for optimization, with an initial learning rate of 0.005. The character embeddings used in our experiments are from <ref type="bibr" target="#b22">[23]</ref>, which is trained by Skip-Gram with Negative Sampling (SGNS) on Baidu Encyclopedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>In this section, we will give the experimental results of our proposed model and previous state-of-the-art methods on Weibo dataset, Chinese Resume dataset, OntoNotes 4 dataset and MSRA dataset, respectively. We propose two baselines and a CAN model. In the experiment results table, we use Baseline to represent the BiGRU + CRF model and Baseline + CNN to indicate CNN + BiGRU + CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Weibo Dataset:</head><p>We compare our proposed model with the latest models on Weibo dataset. <ref type="table" target="#tab_2">Table II</ref> shows the F1scores for named entities (NE), nominal entities (NM, excluding named entities) and both (Overall). We observe that our proposed model achieves state-of-the-art performance.</p><p>Existing state-of-the-art systems include <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b9">[10]</ref>, who leverage rich external data like cross-domain data, semi-supervised data and lexicon or joint train NER task with Chinese Word Segmentation (CWS) task. <ref type="bibr" target="#b2">3</ref> In the first block of <ref type="table" target="#tab_2">Table II</ref>, we report the performance of the latest models. The model that jointly train embeddings with NER task proposed by <ref type="bibr" target="#b15">[16]</ref> achieves F1-score of 56.05% on overall performance. The model <ref type="bibr" target="#b17">[18]</ref> that jointly train CWS task improves the F1-score to 58.99%. <ref type="bibr" target="#b19">[20]</ref> propose a unified model to exploit cross-domain and semi-supervised data, which improves the F1-score from 54.82% to 58.23% compared with the model proposed by <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b10">[11]</ref> use an adversarial transfer learning framework to incorporate taskshared word boundary information from CWS task and achieves F1-score of 58.70%. <ref type="bibr" target="#b9">[10]</ref> leverage a lattice structure to integrate lexicon information into their model and achieve F1-score of 58.79%.</p><p>In the second block of <ref type="table" target="#tab_2">Table II</ref>, we give the results of our baselines and proposed models. Our baseline Bi-GRU + CRF achieves a F1-score of 53.80% and adding a normal CNN layer as featurizer improve F1-score to 55.91%. Replacing normal CNN with our convolutional attention layer will significantly improve the F1-score to 59.31%, which is the highest result among existing models. The improvement demonstrates the effectiveness of our proposed model.   <ref type="table" target="#tab_2">Table III</ref>. <ref type="bibr" target="#b9">[10]</ref> release Chinese Resume dataset and achieve F1-score of 94.46% with lattice structure incorporating additional lexicon information. It can be seen that our proposed baseline (CNN + Bi-GRU + CRF) outperforms <ref type="bibr" target="#b9">[10]</ref> with F1-score of 94.60%. Adding our convolutional attention leads a further improvement and achieves state-of-the-art F1-score of 94.94%, which demonstrates the effectiveness of our proposed model.</p><p>3) OntoNotes Dataset: <ref type="table" target="#tab_2">Table IV</ref> shows the comparisons on OntoNotes 4 dataset. <ref type="bibr" target="#b3">4</ref> In the first block, we list the performance of previous methods for Chinese NER task on OntoNotes 4 dataset. <ref type="bibr" target="#b24">[25]</ref> propose a model combining neural and discrete feature, e.g., POS tagging features, CWS features and orthographic features, improving the F1-score from 68.57% to 76.40%. Leveraging bilingual data, <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b25">[26]</ref> achieves F1-score of 74.32% and 73.88% respectively.</p><p>[10] ‡ is the character-based model with bichar and softword.</p><p>In the second block of    <ref type="table" target="#tab_9">Table V</ref> shows comparisons on MSRA dataset. In the first block, we give the performance of previous methods for Chinese NER task on MSRA dataset. <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b28">[29]</ref> leverage rich hand-crafted features and <ref type="bibr" target="#b29">[30]</ref> exploit multi-prototype embeddings features for Chinese NER task. <ref type="bibr" target="#b30">[31]</ref> introduce radical features into LSTM-CRF. <ref type="bibr" target="#b10">[11]</ref> make use of Adversarial Transfer Learning and global selfattention to improve performance. <ref type="bibr" target="#b31">[32]</ref> propose a characterbased CNN-BiLSTM-CRF model to incorporate stroke embeddings and generate n-gram features. <ref type="bibr" target="#b9">[10]</ref> introduce a lattice structure to incorporate lexicon information into the neural network, which actually includes word embedding information. Although the model achieves state-of-the-art F1-score of 93.18%, it leverages external lexicon data and the result is dependent on the quality of the lexicon.</p><p>In the second block, we list baselines and proposed model. It can seen that our Baseline + CNN is already outperform most previous methods. Compared with the state-of-the-art model proposed by <ref type="bibr" target="#b9">[10]</ref>, our char-based method achieves competitive F1-score of 92.97% without additional lexicon data and word embedding information. Our CAN model achieves state-of-the-art result among the character-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effectiveness of Convolutional Attention and Global</head><p>Self-Attention: Shown by <ref type="table" target="#tab_2">Table II</ref>, III and V, the performance of our proposed model demonstrates the effectiveness of the Convolutional Attention Network. To better evaluate the effect of the Attention Mechanism, we visualize the normalized attention weights α l m for each window from Eq. 2, as in <ref type="figure" target="#fig_2">Figure 3a</ref>. Each row of the matrix represents location attention weights in each window. For example, the third row indicates that the relationship between center character "总" and contexts "美 国 总 统 克". We can see from the <ref type="figure" target="#fig_2">Figure  3a</ref> that the word-level features can be extracted through the local attention. In the context, the center character "美" tends to have a stronger connection with its related character "国", which means they have a higher possibility of consisting of a Chinese word "美国 (American)". Also for characters "克","林" and "顿" tend to have a strong connection because "克林顿" means "Clinton". Character "欧" and "洲" have strong connection seen from the <ref type="figure" target="#fig_2">Figure 3a</ref> because "欧 洲" represents "Europe" in Chinese. Therefore, both experiments results and visualization verifies that the Convolutional Attention is effective for obtaining the phrase information between adjacent characters.</p><p>In <ref type="figure" target="#fig_2">Figure 3b</ref>, we visualize the global self-attention matrix. From the picture, we can find that global self-attention can capture the sentence context information from the longdistance relationship of words to overcome the limitation of Recurrent Neural Network. For the word "克林顿 (Clinton)", the global self-attention learns the dependencies with "前往 (leave for)" and "1号 (on the 1st)". Distinguished by the color, "克林顿 (Clinton)" has a stronger connection with "前 往 (leave for)" than with "1号 (on the 1st)", which accords with the expectation that the predicate in the sentence provides more information to the subject than the adverbial of time.</p><p>2) Results Analysis: Our proposed model outperforms previous work on Weibo and Chinese Resume dataset and gains competitive results on MSRA and OntoNotes 4 datasets without using any external resources. The experiments results demonstrate the effectiveness of our proposed model, especially among the char-based model. The performance improvement after adding Convolutional Attention Layer and Global Attention Layer verifies that our model can capture the relationship between character and its local context, as well as the relationship between word and global context. However, although we can obtain comparable results compared with other models without external resources, we find that our model performs relatively unsatisfying in the OntoNotes 4 dataset. It may be explained by the reason that discrete features and external resources like other labeled data or lexicons have a more positive influence on this dataset when the model cannot learn enough information from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neural Network Models</head><p>It has been shown that neural networks, such as LSTM and CNN, can outperform conventional machine learning methods without requiring handcrafted features. <ref type="bibr">[</ref>  extract word-level context information and <ref type="bibr" target="#b4">[5]</ref> futher introduced hierarchy structure by incorporating BiLSTM-based character embeddings. Many works integrating word-level information and character-level information have been found to achieve good performance <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b35">[36]</ref>. External knowledge has also been exploited for NER tasks. To utilize character-level knowledge, character-level pre-trained <ref type="bibr" target="#b36">[37]</ref> and co-trained <ref type="bibr" target="#b7">[8]</ref> neural language models were introduced. Recently, many works exploit learning pre-trained language representations with deep language models to improve the performance of downstream NLP tasks, such as ELMo <ref type="bibr" target="#b37">[38]</ref> and BERT <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism related Models</head><p>Recently, Attention Mechanism has shown a very good performance on a variety of tasks including machine translation, machine comprehension and related NLP tasks <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. In language understanding task, <ref type="bibr" target="#b42">[43]</ref> exploit selfattention to learn long range dependencies. <ref type="bibr" target="#b43">[44]</ref> proposed the model employing an attention mechanism to combine the character-based representation with the word embedding instead of simply concatenating them. This method allows the model to dynamically decide which source of information to use for each word, and therefore outperforming the concatenation method used in previous work. <ref type="bibr" target="#b44">[45]</ref> use pictures in Tweets as external information through an adaptive coattention network to decide whether and how to integrate the images into the model. The method can only apply to websites like Tweets which has text-related images, but the resources like that are insufficient. <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b10">[11]</ref> employ selfattention to directly capture the global dependencies of the inputs for NER task and demonstrate the effectiveness of self-attention in Chinese NER task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Chinese NER</head><p>Multiple previous works tried to address the problems that the Chinese language doesn't have explicit word boundaries. Traditional models depended on hand-crafted features and CRFs-based models <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Character-based LSTM-CRF model was applied to Chinese NER to utilize both character-level and radical-level representations <ref type="bibr" target="#b30">[31]</ref>. <ref type="bibr" target="#b15">[16]</ref> applied character positional embeddings and proposed a jointly trained model for embeddings and NER. To better integrate word boundary information into Chinese NER model, <ref type="bibr" target="#b17">[18]</ref> co-trained NER and word segmentation to improve both tasks. <ref type="bibr" target="#b19">[20]</ref> unified cross-domain learning and semi-supervised learning to obtain information from out-ofdomain corpora and in-domain unannotated texts. Instead of performing word segmentation first, <ref type="bibr" target="#b9">[10]</ref> constructed a wordcharacter lattice by matching words in texts with a lexicon to avoid segmentation errors. <ref type="bibr" target="#b10">[11]</ref> used the adversarial network to jointly train Chinese NER task and Chinese Word Segmentation task to extract task-shared word boundary information. <ref type="bibr" target="#b48">[49]</ref> leveraged character-level BiLSTM to extract higherlevel features from crowd-annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a Convolutional Attention Network model to improve Chinese NER model performance and preclude word embedding and additional lexicon dependencies, that makes the model more efficient and robust. In our model, we implement local-attention CNN and Bi-GRU with the global self-attention structure to capture word-level features and context information with char-level features. Experiments show that our model outperforms the state-ofart systems on the different domain datasets.</p><p>For future works, we would like to study how to joint learning word segmentation and NER tasks to further reduce constraints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Entity Ambiguity with Word Segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The Whole Architecture. A convolutional attention layer is constructed to encode both character-and word-level information. The BiGRU-CRF layer is extended by a global self-attention layer to capture long sequential sentence-level relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Attention visualization. The left part shows the normalized Convolutional Attention weights in each window in a sentence. The right part indicates the global self-attention weights among the whole sentence. For both pictures, the x-axis represents the context while the y-axis on behalf of the query in the attention mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Statistics of datasets D. Training For training, we exploit log-likelihood objective as the loss function. Given a set of training examples {(</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>on OntoNotes. For MSRA dataset, it only contains three annotated named entities: ORG, PER and LOC. The development subset is not available in MSRA dataset. The detail statistic information of our datasets is</figDesc><table><row><cell>Models</cell><cell>NE</cell><cell>NM</cell><cell>Overall</cell></row><row><cell>[16, Peng and Dredze 2015]</cell><cell>51.96</cell><cell>61.05</cell><cell>56.05</cell></row><row><cell>[18, Peng and Dredze 2016]</cell><cell>55.28</cell><cell>62.97</cell><cell>58.99</cell></row><row><cell>[19, He and Sun 2017]</cell><cell>50.60</cell><cell>59.32</cell><cell>54.82</cell></row><row><cell>[20, He and Sun 2017]</cell><cell>54.50</cell><cell>62.17</cell><cell>58.23</cell></row><row><cell>[11, Cao et al. 2018]</cell><cell>54.34</cell><cell>57.35</cell><cell>58.70</cell></row><row><cell>[10, Zhang and Yang 2018]</cell><cell>53.04</cell><cell>62.25</cell><cell>58.79</cell></row><row><cell>Baseline</cell><cell>49.02</cell><cell>58.80</cell><cell>53.80</cell></row><row><cell>Baseline + CNN</cell><cell>53.86</cell><cell>58.05</cell><cell>55.91</cell></row><row><cell>CAN Model</cell><cell>55.38</cell><cell>62.98</cell><cell>59.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc></figDesc><table /><note>Weibo NER results shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Results on Chinese Resume Dataset. For models proposed by [10], 1 represents the char-based LSTM model, 2 indicates the word-based LSTM model and 3 is the Lattice model.</figDesc><table /><note>2) Chinese Resume Dataset: The Chinese Resume test results are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table</head><label></label><figDesc>IV, we give the results of our baselines and proposed models. Consistent with observations</figDesc><table><row><cell>Models</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>[25, Yang et al. 2016]</cell><cell>65.59</cell><cell>71.84</cell><cell>68.57</cell></row><row><cell>[25, Yang et al. 2016]  *</cell><cell>72.98</cell><cell>80.15</cell><cell>76.40</cell></row><row><cell>[17, Che et al. 2013]  *</cell><cell>77.71</cell><cell>72.51</cell><cell>75.02</cell></row><row><cell>[26, Wang et al. 2013]  *</cell><cell>76.43</cell><cell>72.32</cell><cell>74.32</cell></row><row><cell>[10, Zhang and Yang 2018]  †</cell><cell>76.35</cell><cell>71.56</cell><cell>73.88</cell></row><row><cell>[10, Zhang and Yang 2018]  ‡</cell><cell>74.36</cell><cell>69.43</cell><cell>71.81</cell></row><row><cell>Baseline</cell><cell>70.67</cell><cell>71.64</cell><cell>71.15</cell></row><row><cell>Baseline + CNN</cell><cell>72.69</cell><cell>71.51</cell><cell>72.10</cell></row><row><cell>CAN Model</cell><cell>75.05</cell><cell>72.29</cell><cell>73.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Results on OntoNotes</figDesc><table><row><cell>Models</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>[27, Chen et al. 2006]</cell><cell>91.22</cell><cell>81.71</cell><cell>86.20</cell></row><row><cell>[28, Zhang et al.2006]  *</cell><cell>92.20</cell><cell>90.18</cell><cell>91.18</cell></row><row><cell>[29, Zhou et al.2013]</cell><cell>91.86</cell><cell>88.75</cell><cell>90.28</cell></row><row><cell>[30, Lu et al. 2016]</cell><cell>-</cell><cell>-</cell><cell>87.94</cell></row><row><cell>[31, Dong et al. 2016]</cell><cell>91.28</cell><cell>90.62</cell><cell>90.95</cell></row><row><cell>[11, Cao et al. 2018]</cell><cell>91.30</cell><cell>89.58</cell><cell>90.64</cell></row><row><cell>[32, Zhou et al. 2018 ]</cell><cell>92.04</cell><cell>91.31</cell><cell>91.67</cell></row><row><cell>[10, Zhang and Yang 2018]</cell><cell>93.57</cell><cell>92.79</cell><cell>93.18</cell></row><row><cell>Baseline</cell><cell>92.54</cell><cell>88.20</cell><cell>90.32</cell></row><row><cell>Baseline + CNN</cell><cell>92.57</cell><cell>92.11</cell><cell>92.34</cell></row><row><cell>CAN Model</cell><cell>93.53</cell><cell>92.42</cell><cell>92.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Results on MSRA dataset on the Weibo and Resume datasets, our Convolutional Attention layer leads an increment of F1-score and our proposed model achieves a competitive F1-score of 73.64% among character-based model without using external data.</figDesc><table /><note>4) MSRA Dataset:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc><ref type="bibr" target="#b32">33</ref>] applied a CNN-CRF model and gained competitive results to the best statistical models. More recently, the LSTM-CRF architecture has been used on NER tasks.<ref type="bibr" target="#b33">[34]</ref> employed BiLSTM to</figDesc><table><row><cell>American president</cell><cell>Clinton</cell><cell>on the 1st leave for Europe</cell><cell>American president Clinton on the 1st leave for Europe</cell></row><row><cell></cell><cell cols="2">(a) Local attention.</cell><cell>(b) Global self-attention.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.weibo.com/ 2 http://finance.sina.com.cn/stock/index.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The results of<ref type="bibr" target="#b15">[16]</ref>,<ref type="bibr" target="#b17">[18]</ref> are taken from<ref type="bibr" target="#b23">[24]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">InTable IVand V, we use * to denote a model with external labeled data for semi-supervised learning. † denotes that the model use external lexicon data.<ref type="bibr" target="#b9">[10]</ref> with ‡ is the char-based model in the paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank our colleagues Haoyan Liu, Zijia Lin, as well as the anonymous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entity linking via joint encoding of types, descriptions, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2681" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Applying named entity recognition and co-reference resolution for segmenting english texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fragkou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="346" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Addressing domain adaptation for chinese word segmentation with global recurrent structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Chinese ner using lattice lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02023</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial transfer learning for chinese named entity recognition with self-attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional neural network with word embeddings for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04411</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Belvin</surname></persName>
		</author>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia, Penn</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Ontonotes release 4.0,&quot; LDC2011T03</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The third international chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving named entity recognition for chinese social media with word segmentation representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00786</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">F-score driven max margin neural network for named entity recognition in chinese social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="713" to="718" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A unified model for cross-domain and semi-supervised named entity recognition in chinese social media</title>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural word segmentation with rich pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3879" to="3889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analogical reasoning on chinese morphological and semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P18-2023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Supplementary results for named entity recognition on chinese social media with an updated dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Combining discrete and neural features for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<editor>CICLing</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective bilingual constraints for semi-supervised learning of named entity recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition with conditional probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Word segmentation and named entity recognition for sighan bakeoff3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="158" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition via joint identification and categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese journal of electronics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="230" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-prototype chinese character embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Characterbased lstm-crf with radical-level features for chinese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Understanding and Intelligent Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Five-stroke based cnn-birnn-crf network for chinese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boosting named entity recognition with neural character embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Niterói</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Janeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NEWS 2015 The Fifth Named Entities Workshop</title>
		<meeting>NEWS 2015 The Fifth Named Entities Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GRN: Gated relation network to enhance convolutional neural network for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2019</title>
		<meeting>AAAI 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semisupervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01586</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attending to characters in neural sequence labeling models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaptive co-attention network for named entity recognition in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition and word segmentation based on character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Sixth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chinese word segmentation and named entity recognition based on conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Sixth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adversarial learning for chinese ner from crowd annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
