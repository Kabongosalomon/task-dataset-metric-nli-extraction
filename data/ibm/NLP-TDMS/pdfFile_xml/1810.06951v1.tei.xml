<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Metric Learning with Hierarchical Triplet Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<email>whuang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
							<email>dongdk@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
							<email>mscott@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Metric Learning with Hierarchical Triplet Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep Metric Learning · Image Retrieval · Triplet Loss · Anchor-Neighbor Sampling</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel hierarchical triplet loss (HTL) capable of automatically collecting informative training samples (triplets) via a defined hierarchical tree that encodes global context information. This allows us to cope with the main limitation of random sampling in training a conventional triplet loss, which is a central issue for deep metric learning. Our main contributions are two-fold. (i) we construct a hierarchical class-level tree where neighboring classes are merged recursively. The hierarchical structure naturally captures the intrinsic data distribution over the whole dataset. (ii) we formulate the problem of triplet collection by introducing a new violate margin, which is computed dynamically based on the designed hierarchical tree. This allows it to automatically select meaningful hard samples with the guide of global context. It encourages the model to learn more discriminative features from visual similar classes, leading to faster convergence and better performance. Our method is evaluated on the tasks of image retrieval and face recognition, where it outperforms the standard triplet loss substantially by 1%18%. It achieves new state-of-the-art performance on a number of benchmarks, with much fewer learning iterations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distance metric learning or similarity learning is the task of learning a distance function over images in visual understanding tasks. It has been an active research topic in computer vision community. Given a similarity function, images with similar content are projected onto neighboring locations on a manifold, and images with different semantic context are mapped apart from each other. With the boom of deep neural networks (DNN), metric learning has been turned from learning distance functions to learning deep feature embeddings that better fits a simple distance function, such as Euclidean distance or cosine distance. Metric learning with DNNs is referred as deep metric learning, which has recently achieved great success in numerous visual understanding tasks, including images or object retrieval <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>, single-shot object classification <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32]</ref>, keypoint descriptor learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, face verification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref>, person re-identification <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref>, object tracking <ref type="bibr" target="#b28">[29]</ref> and etc.</p><p>Recently, there is a number of widely-used loss functions developed for deep metric learning, such as contrastive loss <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6]</ref>, triplet loss <ref type="bibr" target="#b21">[22]</ref> and quadruplet loss <ref type="bibr" target="#b4">[5]</ref>. These loss functions are calculated on correlated samples, with a common goal of encouraging samples from the same class to be closer, and pushing samples of different classes apart from each other, in a projected feature space. The correlated samples are grouped into contrastive pairs, triplets or quadruplets, which form the training samples for these loss functions on deep metric learning. Unlike softmax loss used for image classification, where the gradient is computed on each individual sample, the gradient of a deep metric learning loss often depends heavily on multiple correlated samples. Furthermore, the number of training samples will be increased exponentially when the training pairs, triplets or quadruplets are grouped. This generates a vast number of training samples which are highly redundant and less informative. Training that uses random sampling from them can be overwhelmed by redundant samples, leading to slow convergence and inferior performance.</p><p>Deep neural networks are commonly trained using online stochastic gradient descent algorithms <ref type="bibr" target="#b18">[19]</ref>, where the gradients for optimizing network parameters are computed locally with mini-batches, due to the limitation of computational power and memory storage. It is difficult or impossible to put all training samples into a single mini-batch, and the networks can only focus on local data distribution within a mini-batch, making it difficult to consider global data distribution over the whole training set. This often leads to local optima and slow convergence. This common challenge will be amplified substantially in deep metric learning, due to the enlarged sample spaces where the redundancy could become more significant. Therefore, collecting and creating meaningful training samples (e.g., in pairs, triplets or quadruplets) has been a central issue for deep metric learning, and an efficient sampling strategy is of critical importance to this task. This is also indicated in recent literature <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Our goal of this paper is to address the sampling issue of conventional triplet loss <ref type="bibr" target="#b21">[22]</ref>. In this work, we propose a novel hierarchical triplet loss (HTL) able to automatically collect informative training triplets via an adaptively-learned hierarchical class structure that encodes global context in an elegant manner. Specifically, we explore the underline data distribution on a manifold sphere, and then use this manifold structure to guide triplet sample generation. Our intuition of generating meaningful samples is to encourage the training samples within a mini-batch to have similar visual appearance but with different semantic content (e.g., from different categories). This allows our model to learn more discriminative features by identifying subtle distinction between the close visual concepts. Our main contribution are described as follows.</p><p>-We propose a novel hierarchical triplet loss that allows the model to collect informative training samples with the guide of a global class-level hierarchical tree. This alleviates main limitation of random sampling in training of deep metric learning, and encourages the model to learn more discriminative features from visual similar classes.</p><p>-We formulate the problem of triplet collection by introducing a new violate margin, which is computed dynamically over the constructed hierarchical tree. The new violate margin allows us to search informative samples, which are hard to distinguish between visual similar classes, and will be merged into a new class in next level of the hierarchy. The violate margin is automatically updated, with the goal of identifying a margin that generates gradients for violate triplets, naturally making the collected samples more informative.</p><p>-The proposed HTL is easily implemented, and can be readily integrated into the standard triplet loss or other deep metric learning approaches, such as contrastive loss, quadruplet loss, recent HDC <ref type="bibr" target="#b37">[38]</ref> and BIER <ref type="bibr" target="#b16">[17]</ref>. It significantly outperforms the standard triplet loss on the tasks of image retrieval and face recognition, and obtains new state-of-art results on a number of benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Deep Metric Learning. Deep metric learning maps an image into a feature vector in a manifold space via deep neural networks. In this manifold space, the Euclidean distance (or the cosine distance) can be directly used as the distance metric between two points. The contribution of many deep metric learning algorithms, such as <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, is the design of a loss function that can learn more discriminant features. Since neural networks are usually trained using the stochastic gradient descent (SGD) in mini-batches, these loss functions are difficult to approximate the target of metric learning -pull samples with the same label into nearby points and push samples with different labels apart.</p><p>Informative Sample Selection. Given N training images, there are about O(N 2 ) pairs, O(N 3 ) triplets, and 0(N 4 ) quadruplets. It is infeasible to traverses all these training tuples during training. Schroff at. el. <ref type="bibr" target="#b21">[22]</ref> constructed a minibatch of with 45 identities and each of which has 40 images. There are totally 1800 images in a mini-batch, and the approach obtained the state-of-art results on LFW face recognition challenge <ref type="bibr" target="#b7">[8]</ref>. While it is rather inconvenient to take thousands of images in a mini-batch with a large-scale network, due to the limitation of GPU memory. For deep metric learning, it is of great importance to selecting informative training tuples. Hard negative mining <ref type="bibr" target="#b3">[4]</ref> is widely used to select hard training tuples. Our work is closely related to that of <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b6">7]</ref> which inspired the current work. Distance distribution was applied to guide tuple sampling for deep metric learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b6">7]</ref>. In this work, we strive to a further step by constructing a hierarchical tree that aggregates class-level global context, and formulating tuple selection elegantly by introducing a new violate margin.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation: Challenges in Triplet Loss</head><p>We start by revisiting the main challenges in standard triplet loss <ref type="bibr" target="#b21">[22]</ref>, which we believe have a significant impact to the performance of deep triplet embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Let</p><formula xml:id="formula_0">(x i , y i ) be the i-th sample in the training set D = {(x i , y i )} N i=1 . The fea- ture embedding of x i is represented as φ (x i , θ) ∈ R d ,</formula><p>where θ is the learnable parameters of a differentiable deep networks, d is the dimension of embedding and y i is the label of x i . φ (·, θ) is usually normalized into unit length for the training stability and comparison simplicity as in <ref type="bibr" target="#b21">[22]</ref>. During the neural network training, training samples are selected and formed into triplets, each of which T z = (x a , x p , x n ) are consisted of an anchor sample x a , a positive sample x p and a negative sample x n . The labels of the triplet T z = x z a , x z p , x z n satisfy y a = y p = y n . Triplet loss aims to pull samples belonging to the same class into nearby points on a manifold surface, and push samples with different labels apart from each other. The optimization target of the triplet T z is,</p><formula xml:id="formula_1">l tri (T z ) = 1 2 x z a − x z p 2 − x z a − x z n 2 + α + .</formula><p>[·] + = max(0, ·) denotes the hinge loss function, and α is the violate margin that requires the distance x z a − x z n 2 of negative pairs to be larger than the distance x z a − x z p 2 of positive pairs. For all the triplets T in the training set</p><formula xml:id="formula_2">D = {(x i , y i )} N i=1</formula><p>, the final objective function to optimize is,</p><formula xml:id="formula_3">L = 1 Z T z ∈T l tri (T z ) ,</formula><p>where Z is the normalization term. For training a triplet loss in deep metric learning, the violate margin plays a key role to sample selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Challenges</head><p>Challenge 1: triplet loss with random sampling. For many deep metric learning loss functions, such as contrastive loss <ref type="bibr" target="#b5">[6]</ref>, triplet loss <ref type="bibr" target="#b21">[22]</ref> and quadruplet loss <ref type="bibr" target="#b4">[5]</ref>, all training samples are treated equally with a constant violate margin, which only allows training samples that violate this margin to produce gradients. For a training set</p><formula xml:id="formula_4">D = {(x i , y i )} N k=1</formula><p>with N samples, training a triplet loss will generate O N 3 triplets, which is infeasible to put all triplets into a single mini-batch. When we sample the triplets over the whole training set randomly, it has a risk of slow convergence and pool local optima. We identify the problem that most of training samples obey the violate margin when the model starts to converge. These samples can not contribute gradients to the learning process, and thus are less informative, but can dominate the training process, which significantly degrades the model capability, with a slow convergence. This inspired current work that formulates the problem of sample selection via setting a dynamic violate margin, which allows the model to focus on a small set of informative samples.</p><p>However, identifying informative samples from a vast number of the generated triplets is still challenging. This inspires us to strive to a further step, by sampling meaningful triplets from a structural class tree, which defines classlevel relations over all categories. This transforms the problem of pushing hard samples apart from each other into encouraging a larger distance between two confusing classes. This not only reduces the search space, but also avoid overfitting the model over individual samples, leading to a more discriminative model that generalizes better.</p><p>Challenge 2: risk of local optima. Most of the popular metric learning algorithms, such as the contrastive loss, the triplet loss, and the quadruplet loss, describe similarity relationship between individual samples locally in a minibatch, without considering global data distribution. In triplet loss, all triplet is treated equally. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, when the training goes after several epoches, most of training triplets dose not contribute to the gradients of learnable parameters in deep neural networks. There has been recent work that aims to solve this problem by re-weighting the training samples, as in <ref type="bibr" target="#b35">[36]</ref>. However, even with hard negative mining or re-weighting, the triplets can only see a few samples within a mini-batch, but not the whole data distribution. It is difficult for the triplet loss to incorporate the global data distribution on the target manifold space. Although the data structure in the deep feature space are changed dynamically during the training process, the relative position of data points can be roughly preserved. This allows us to explore the data distribution obtained in the previous iterations to guide sample selection in the current stage. With this prior knowledge of data structure, a triplet, which does not violate the original margin α, is possible to generate gradients that contribute to the network training, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. Discriminative capability can be enhanced by learning from these hard but informative triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hierarchical Triplet Loss</head><p>We describe details of the proposed hierarchical triplet loss, which contains two main components, constructing a hierarchical class tree and formulating the hierarchical triplet loss with a new violate margin. The hierarchical class tree is designed to capture global data context, which is encoded into triplet sampling via the new violate margin, by formulating the hierarchical triplet loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Manifold Structure in Hierarchy</head><p>We construct a global hierarchy at the class level. Given a neural network φ t (·, θ) (∈ R d ) pre-trained using the traditional triplet loss, we get the hierarchical data structure based on sample rules. Denote the deep feature of a sample x i as r i = φ t (x i , θ). We first calculate a distance matrix of C classes in the whole training set D. The distance between the p-th class and the q-th class is computed as,</p><formula xml:id="formula_5">d (p, q) = 1 n p n q i∈p,j∈q r i − r j 2 ,</formula><p>where n p and n q are the numbers of training samples in the p-th and the q-th classes respectively. Since the deep feature r i is normalized into unit length, the value of the interclass distance d (p, q) varies from 0 to 4. We build hierarchical manifold structure by creating a hierarchical tree, according to the computed interclass distances. The leaves of the hierarchical tree are the original image classes, where each class represents a leave node at the 0-th level. Then hierarchy is created by recursively merging the leave notes at different levels, based on the computed distance matrix. The hierarchical tree is set into L levels, and the average inner distance d 0 is used as the threshold for merging the nodes at the 0-th level.</p><formula xml:id="formula_6">d 0 = 1 C C c=1   1 n 2 c − n c i∈c,j∈c r i − r j 2   .</formula><p>where n c is the number of samples in the c-th class. Then the nodes are merged with different thresholds. At the l-th level of the hierarchical tree, the merging threshold is set to d l = l(4−d0) L + d 0 . Two classes with a distance less than d l are merged into a node at the l-th level. The node number at the l-th level is N l . The nodes are merged from the 0-th level to the L-th level. Finally, we generate a hierarchical tree H which starts from the leave nodes of original image classes to a final top node, as shown in <ref type="figure" target="#fig_2">Fig. 2 (a)</ref>. The constructed hierarchical tree captures class relationships over the whole dataset, and it is updated interactively at the certain iterations over the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical Triplet Loss</head><p>We formulate the problem of triplet collection into a hierarchical triplet loss. We introduce a dynamical violate margin, which is the main difference from the conventional triplet loss using a constant violate margin.</p><p>Anchor neighbor sampling. We randomly select l nodes at the 0-th level of the constructed hierarchical tree H. Each node represents an original class, and collecting classes at the 0-th level aims to preserve the diversity of training samples in a mini-batch, which is important for training deep networks with batch normalization <ref type="bibr" target="#b8">[9]</ref>. Then m − 1 nearest classes at the 0-th level are selected for each of the l nodes, based on the distance between classes computed in the feature space. The goal of collecting nearest classes is to encourage model to learn discriminative features from the visual similar classes. Finally, t images for each class are randomly collected, resulting in n (n = l mt) images in a mini-batch M. Training triplets within each mini-batch are generated from the collected n images based on class relationships. We write the anchor-neighbor sampling into A-N sampling for convenience.</p><p>Triplet generation and dynamic violate margin. Hierarchical triplet loss (computed on a mini-batch of M ) can be formulated as,</p><formula xml:id="formula_7">L M = 1 2Z M T z ∈T M x z a − x z p − x z a − x z n + α z + .</formula><p>where T M is all the triplets in the mini-batch M, and Z M = A 2 l m A 2 t C 1 t is the number of triplets. Each triplet is constructed as T z = (x a , x p , x n ), and the training triplets are generated as follows. A 2 l m indicates randomly selecting two classes -a positive class and a negative class, from all l m classes in the  mini-batch. A 2 t means selecting two samples -a anchor sample (x z a ) and a positive sample (x z p ), from the positive class, and C 1 t means randomly selecting a negative sample (x z n ) from the negative class. A 2 l m , A 2 t and C 1 t are notations in combinatorial mathematics. See reference <ref type="bibr" target="#b12">[13]</ref> for details. α z is a dynamic violate margin, which is different from the constant margin of traditional triplet loss. It is computed according to the class relationship between the anchor class y a and the negative class y n over the constructed hieratical class tree. Specifically, for a triplet T z , the violate margin α z is computed as,</p><formula xml:id="formula_8">α z = β + d H(ya,yn) − s ya ,</formula><p>where β (= 0.1) is a constant parameter that encourages the image classes to reside further apart from each other than the previous iterations. H (y a , y n ) is the hierarchical level on the class tree, where the class y a and the class y n are merged into a single node in the next level. d H(ya,yn) is the threshold for merging the two classes on H, and s ya = 1 n 2 ya −ny a i,j∈ya r i − r j 2 is the average distance between samples in the class y a . In our hierarchical triplet loss, a sample x a is encouraged to push the nearby points with different semantic meanings apart from itself. Furthermore, it also contributes to the gradients of data points which are very far from it, by computing a dynamic violate margin which encodes global class structure via H. For every individual triplet, we search on H to encode the context information of the data distribution for the optimization objective. Details of training process with the proposed hierarchical triplet loss are described in Algorithm 1.</p><p>Implementation Details. All our experiments are implemented using Caffe <ref type="bibr" target="#b9">[10]</ref> and run on an NVIDIA TITAN X(Maxwell) GPU with 12GB memory. The network architecture is a GoogLeNet <ref type="bibr" target="#b27">[28]</ref> with batch normalization <ref type="bibr" target="#b8">[9]</ref> which is pre-trained on the ImageNet dataset <ref type="bibr" target="#b20">[21]</ref>. The 1000-way fully connected layer is removed, and replace by a d dimensional fully connected layer. The new added layer is initialized with random noise using the "Xaiver" filler. We modify the memory management of Caffe <ref type="bibr" target="#b9">[10]</ref> to ensure it can take 650 images in a minibatch for GoogLeNet with batch normalization. The input images are resized and cropped into 224 × 224, and then subtract the mean value. The optimization method used is the standard SGD with a learning rate 1e −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Training with hierarchical triplet loss</head><p>Input: Backpropagate the gradients produced at the loss layer and update the learnable parameters ; <ref type="bibr" target="#b6">7</ref> At each epoch, update the hierarchical tree H with current model.</p><formula xml:id="formula_9">Training data D = {(x i , y i )} N k=1 . Network φ (·, θ) is initialized with a pretrained</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results and Comparisons</head><p>We evaluate the proposed hierarchical triplet loss on the tasks of image retrieval and face recognition. Extensive experiments are conducted on a number of benchmarks, including In-Shop Clothes Retrieval <ref type="bibr" target="#b14">[15]</ref> and Caltech-UCSD Birds 200 <ref type="bibr" target="#b30">[31]</ref> for image retrieval, and LFW <ref type="bibr" target="#b7">[8]</ref> for face verification. Descriptions of dataset and implementation details are presented as follows. <ref type="figure">Fig. 4</ref>. Anchor-Neighbor visualization on In-Shop Clothes Retrieval training set <ref type="bibr" target="#b14">[15]</ref>. Each row stands for a kind of fashion style. The row below each odd row is one of neighborhoods of the fashion style in the odd row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">In-Shop Clothes Retrieval</head><p>Datasets and performance measures. The In-Shop Clothes Retrieval dataset <ref type="bibr" target="#b14">[15]</ref> is very popular in image retrieval. It has 11735 classes of clothing items and 54642 training images. Following the protocol in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38]</ref>, 3997 classes are used for training (25882 images) and 3985 classes are for testing (28760 images). The test set are partitioned into the query set and the gallery set, both of which has 3985 classes. The query set has 14218 images and the gallery set has 12612 images. As in <ref type="figure">Fig. 4</ref>, there are a lot image classes that have very similar contents. For the evaluation, we use the most common Recall@K metric. We extract the features of each query image and search the K most similar images in the gallery set. If one of the K retrieved images have the same label with the query image, the recall will increase by 1, otherwise will be 0. We evaluate the recall metrics with K ∈ {1, 2, 4, 8, 16, 32}. Implementation details. Our network is based on GoogLeNet V2 <ref type="bibr" target="#b8">[9]</ref>. The dimension d of the feature embedding is 128. The triplet violate margin is set to 0.2. The hierarchical tree has 16 levels including the leaves level which contains the images classes. At the first epoch, the neural network is trained with the standard triplet loss which samples image classes for mini-batches randomly. Then during the training going on, the hierarchical tree is updated and used in the following steps. Since there are 3997 image classes for training and there many similar classes, the whole training needs 30 epoch and the batch size is set to 480. For every 10 epoch, we decrease the learning rate by multiplying 0.1. The testing codes are gotten from HDC <ref type="bibr" target="#b37">[38]</ref>. Result comparison. We compare our method with existing state-of-the-art algorithms and our baseline -triplet loss. <ref type="table" target="#tab_1">Table 1</ref> lists the results of image retrieval on In-Shop Clothes Retrieval. The proposed method achieves 80.9% Recall@1, and outperforms the baseline algorithm -triplet loss by 18.6%. It indicates that our algorithm can improve the discriminative power of the original triplet loss by a large margin. State-of-the-art algorithms, including HDC <ref type="bibr" target="#b37">[38]</ref>, and BIER <ref type="bibr" target="#b17">[18]</ref>, used boosting and ensemble method to take the advantage of different features and get excellent results. Our method demonstrates that by incorporate the global data distribution into deep metric learning, the performance will be highly improved. The proposed hierarchical loss get 80.9% Recall@1, which is 4.0% higher than BIER <ref type="bibr" target="#b17">[18]</ref> and 18.8% higher than HDC <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Caltech-UCSD Birds 200-2011</head><p>Datasets and performance measures. The triplet violate margin is set to 0.2. As in the previous section, the hierarchical tree is still set to 16 levels. All the training details are almost the same with the In-Shop Clothes Retrieval dataset. But since there are only 100 image classes for training, the dataset is very easy to get overfitting. When we train 10 epoches, the training stopped. The batch size is set to 50. For every 3 epoch, we decrease the learning rate by multiplying 0.1. Result comparison. <ref type="table" target="#tab_2">Table 2</ref> lists the results of image retrieval on Caltech-UCSD Birds 200-2011. The baseline -triplet loss already get the state-of-art results with 55.9% Recall@1 compared with the previous state-of-art HDC 54.6% and BIER 55.3%. If we use the anchor-neighbor sampling and the hierarchical loss, we get 57.1% Recall@1. Since there are only 100 classes and 6000 images for training, the network is very easy to get overfitting. The performance gain gotten by the hierarchical loss is only 1.2% Recall@1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>3 Cars-196 <ref type="bibr" target="#b10">[11]</ref> and Stanford Online Products <ref type="bibr" target="#b25">[26]</ref> Details of the Cars-196 and Stanford Online Products <ref type="bibr" target="#b25">[26]</ref> are described in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. The dimension of the feature embedding is set to 512. The triplet violate margin is set to 0.2, with a hierarchical tree of depth = 16. The whole training needs 30 epoch and the batch size is set to 50. For every 10 epoch, we decrease the learning rate by multiplying 0.1. Results are presented in <ref type="table" target="#tab_3">Table 3</ref>, where the proposed HTL outperforms our baseline, BIER and HDC, with clear margins on both datasets. Specifically, on the Cars-196, HTL achieves 81.4% Recall@1, which outperforms orginal triplet loss by 2.2%, and previous state-of-art by 3.4%. On the Stanford Online Products, HTL achieves 74.8% Recall@1, outperforming triplet loss by 2.2%, and previous state-of-art by 2.1%. These results demonstrate that the proposed HTL can improve original triplet loss efficiently, and further proved the generalization ability of HTL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">LFW Face Verification</head><p>Datasets and performance measures. The CASIA-WebFace dataset <ref type="bibr" target="#b36">[37]</ref> is one of the publicly accessible datasets for face recognition. It has been the most popular dataset for the training of face recognition algorithms, such as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref>. CASIA-WebFace has 10575 identities and 494414 images. We following the testing protocol in <ref type="bibr" target="#b36">[37]</ref> to test the performance of our algorithms. The face verification results on LFW dataset <ref type="bibr" target="#b7">[8]</ref> is reported. Implementation details. Since the triplet loss is very sensitive to the noise, we clear the CASIA-WebFace using the pre-trained model of VGG-Face <ref type="bibr" target="#b19">[20]</ref> and manually remove some noises. About 10% images are removed. Then the remained faces are used to train a SoftMax classifier. The network parameters are initialized by a pre-trained ImageNet model. We fine-tune the pre-trained classification network for face recognition using the hierarchical loss. Result comparison. The triplet loss gets 98.3% accuracy on the LFW face verification task, which is 1.12% lower than the SpereFace <ref type="bibr" target="#b13">[14]</ref> -99.42% which uses the same dataset for training. When we substitute the triplet loss with the hierarchical triplet loss, the results comes to 99.2. It's comparable with state-of-art results. This indicates that the hierarchical triplet loss has stronger discriminative power than triplet loss. While, since the triplet based method are very sensitive to noise, the hierarchical triplet loss get inferior performance compared with SphereFace <ref type="bibr" target="#b13">[14]</ref> 99.42% and FaceNet <ref type="bibr" target="#b21">[22]</ref> 99.65% . Sampling Matter. We investigate the influence of batch size on the test set of In-Shop Clothes Retrieval. <ref type="figure" target="#fig_6">Fig. 5 (a)</ref> shows that when the batch size grows from 60 to 480, the accuracy increases in the same iterations. When the training continues, the performance will fluctuates heavily and get overfitting. Besides, when come to the same results at 60% Recall@1, both the anchor-neighbor sampling with triplet loss and the hierarchical loss converge at about 2 times faster than random sampling (Batch Size = 480). <ref type="figure" target="#fig_6">Fig. 5 (b)</ref> shows the compares the convergence speed of the triplet loss (our baseline), the hierarchical triplet loss and the HDC <ref type="bibr" target="#b37">[38]</ref> on the test set of Caltech-UCSD Birds 200. Compared to the 60000 iterations (see in <ref type="bibr" target="#b37">[38]</ref>), the hierarchical triplet loss converges in 1000 iterations. The hierarchical triplet loss with anchor-Neighborhood sampling converge faster traditional and get better performance than HDC <ref type="bibr" target="#b37">[38]</ref>. Pool Local Optima. In <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>, we can find that the triplet loss get inferior performance than the hierarchical triplet loss on both the In-Shop Clothes Retrieval and Caltech-UCSD Birds 200. In the <ref type="figure" target="#fig_6">Fig. 5</ref>, the accuracy of the triplet loss start to fluctuate when the training continues going after the loss drops to very low. In fact, there are always very few or zeros triplets in mini-batch even when the network isn't gotten the best results. Then they don't produce gradients and will decay the learnable parameters in networks by SGD <ref type="bibr" target="#b18">[19]</ref>. So we incorporate the hierarchical structure to make points in the mini-batch know the position of point that are already far away, and then attempt to push them further from itself and its neighborhood classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sampling Matter and Local Optima</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>We perform ablation studies on In-Shop Clothes and CUB-200-2011, as reported in <ref type="table" target="#tab_4">Table 4</ref>. First, directly applying hard negative sampling (HNS) to the whole training set is difficult to obtain a performance gain. Actually, our baseline model applies a semi-HNS, which outperforms HNS. We design a strong class-level constrain -Anchor-Neighbor Sampling of HTL, which encourages the model to learn discriminative features from visual similar classes. This is the key to performance boost. Second, we integrated the proposed anchor-neighbor sampling and dynamic violate margin into HDC where a contrastive loss is used. As shown in <ref type="table" target="#tab_4">Table 4</ref> (bottom), HDC+ got an improvement of 7.3% R@1 on the In-Shop Clothes Retrieval, suggesting that our methods work practically well with a contrastive loss and HDC. Third, HTL with a depth of 16 achieves best performance at R@1 of 80.9%. This is used as default setting in all our experiments. We also include results of "flat" tree with depth=1. Results suggest that the "flat" tree with the proposed dynamic violate margin improves the R@1 from 75.3% to 78.9%, and hierarchy tree improves it further to 80.9%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a new hierarchical triplet loss (HTL) which is able to select informative training samples (triplets) via an adaptively-updated hierarchical tree that encodes global context. HTL effectively handles the main limitation of random sampling, which is a critical issue for deep metric learning. First, we construct a hierarchical tree at the class level which encodes global context information over the whole dataset. Visual similar classes are merged recursively to form the hierarchy. Second, the problem of triplet collection is formulated by proposing a new violate margin, which is computed dynamically based on the designed hierarchical tree. This allows it to learn from more meaningful hard samples with the guide of global context. The proposed HTL is evaluated on the tasks of image retrieval and face recognition, where it achieves new state-of-theart performance on a number of standard benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Caltech-UCSD Bird Species Dataset (b) Data Distribution and Triplets in a Mini-Batch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Caltech-UCSD Bird Species Dataset<ref type="bibr" target="#b30">[31]</ref>. Images in each row are from the same class. There are four classes in different colors -red, green, blue and yellow. (b) Data distribution and triplets in a mini-batch. Triplets in the top row violate the triplet constrain in the traditional triplet loss. Triplets in the bottom row are ignored in the triplet loss, but are revisited in the hierarchical triplet loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Hierarchical Tree (b) Data Distribution Visualization by t-SNE (a) A toy example of the hierarchical tree H. Different colors represent different image classes in CUB-200-2011<ref type="bibr" target="#b30">[31]</ref>. The leaves are the image classes in the training set. Then they are merged recursively until to the root node. (b) The training data distribution of 100 classes visualized by using t-SNE<ref type="bibr" target="#b15">[16]</ref> to reduce the dimension of triplet embedding from 512 to 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(a)Sampling strategy of each mini-batch. The images in red stand for anchors and the images in blue stand for the nearest neighbors. (b) Train CNNs with the hierarchical triplet loss. (c) Online update of the hierarchical tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 t ← t + 1 ; 3 5</head><label>2135</label><figDesc>ImageNet model. The hierarchical class tree H is built according to the features of the initialized model. The margin α z for any pair of classes is set to 0.2 at the beginning. Output: The learnable parameters θ of the neural network φ (·, θ). 1 while not converge do Sample anchors randomly and their neighborhoods according to H ; 4 Compute the violate margin for different pairs of image classes by searching through the hierarchical tree H ; Compute the hierarchical triplet loss in a mini-batch L M ; 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The Caltech-UCSD Birds 200 dataset (CUB-200-2011) [31] contains photos of 200 bird species with 11788 images. CUB-200-2011 serves as a benchmark in most existing work on deep metric learning and image retrieval. The first 100 classes (5864 images) are used for training, and the rest (5924 images) of classes are used for testing. The rest images are treated as both the query set and the gallery set. For the evaluation, we use the same Recall@K metric as in Section In-Shop Clothes Retrieval. Here, K ∈ {1, 2, 4, 8, 16, 32}. Implementation details. The dimension d of the feature embedding is 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Image Retrieval Results on In-Shop Clothes (b) Image Retrieval Results on CUB 200-2011 (a) Image retrieval results on In-Shop Clothes [15] with various batch sizes. (b) Image retrieval results on CUB-200-2011 [31].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons on the In-Shop Clothes Retrieval dataset<ref type="bibr" target="#b14">[15]</ref>.</figDesc><table><row><cell>R@</cell><cell>1</cell><cell>10 20 30 40 50</cell></row><row><cell cols="3">FashionNet+Joints[15] 41.0 64.0 68.0 71.0 73.0 73.5</cell></row><row><cell cols="3">FashionNet+Poselets[15] 42.0 65.0 70.0 72.0 72.0 75.0</cell></row><row><cell>FashionNet[15]</cell><cell cols="2">53.0 73.0 76.0 77.0 79.0 80.0</cell></row><row><cell>HDC[38]</cell><cell cols="2">62.1 84.9 89.0 91.2 92.3 93.1</cell></row><row><cell>BIER[18]</cell><cell cols="2">76.9 92.8 95.2 96.2 96.7 97.1</cell></row><row><cell>Ours Baseline</cell><cell cols="2">62.3 85.1 89.0 91.1 92.4 93.4</cell></row><row><cell>A-N Sampling</cell><cell cols="2">75.3 91.8 94.3 96.2 96.7 97.5</cell></row><row><cell>HTL</cell><cell cols="2">80.9 94.3 95.8 97.2 97.4 97.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the state-of-art on the CUB-200-2011 dataset<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table><row><cell>R@</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell cols="2">16 32</cell></row><row><cell>LiftedStruct[26]</cell><cell cols="6">47.2 58.9 70.2 80.2 89.3 93.2</cell></row><row><cell cols="7">Binomial Deviance[30] 52.8 64.4 74.7 83.9 90.4 94.3</cell></row><row><cell>Histogram Loss[30]</cell><cell cols="6">50.3 61.9 72.6 82.4 88.8 93.7</cell></row><row><cell>N-Pair-Loss[25]</cell><cell cols="5">51.0 63.3 74.3 83.2 -</cell><cell>-</cell></row><row><cell>HDC[38]</cell><cell cols="6">53.6 65.7 77.0 85.6 91.5 95.5</cell></row><row><cell>BIER[18]</cell><cell cols="6">55.3 67.2 76.9 85.1 91.7 95.5</cell></row><row><cell>Ours Baseline</cell><cell cols="6">55.9 68.4 78.2 86.0 92.2 95.5</cell></row><row><cell>HTL</cell><cell cols="6">57.1 68.8 78.7 86.5 92.5 95.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the state-of-art on the cars-196 and Stanford products.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cars-196</cell><cell></cell><cell cols="3">Stanford Online Products</cell></row><row><cell>R@</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16 32</cell><cell>1</cell><cell>10 100</cell><cell>100</cell></row><row><cell>HDC</cell><cell cols="7">73.7 83.2 89.5 93.8 96.7 98.4 69.5 84.4 92.8</cell><cell>97.7</cell></row><row><cell>BIER</cell><cell cols="7">78.0 85.8 91.1 95.1 97.3 98.7 72.7 86.5 94.0</cell><cell>98.0</cell></row><row><cell>Baseline</cell><cell cols="7">79.2 87.2 92.1 95.2 97.3 98.6 72.6 86.2 93.8</cell><cell>98.0</cell></row><row><cell cols="9">HTL(depth=16) 81.4 88.0 92.7 95.7 97.4 99.0 74.8 88.3 94.8 98.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation Studies on In-Shop Clothes Retrieval and CUB-200-2011. 83.5 87.9 90.5 91.3 93.0 51.4 63.9 74.8 83.4 90.0 94.3 Hard Negative Mining 60.1 84.3 88.2 90.2 91.5 92.6 51.6 63.9 74.2 84.4 89.9 94.6 Semi-Hard Negative Mining 62.3 85.1 89.0 91.1 92.4 93.4 55.9 68.4 78.2 86.0 92.2 95.5 Anchor-Neighbor Sampling (HTL) 75.3 91.8 94.3 96.2 96.7 97.5 56.4 68.5 78.5 86.2 92.4 95.5 HTL with A-N Sampling + Dynamic Violate Margin(αz) Class Proxy(flat/depth=1) 78.9 93.4 94.8 96.0 96.5 97.5 56.0 68.1 78.2 86.2 92.3 95.5 HTL(depth=8) 78.7 93.3 94.6 96.2 96.9 97.4 56.2 68.5 78.3 86.1 92.3 95.5 HTL(depth=16) 80.9 94.3 95.8 97.2 97.4 97.8 57.1 68.8 78.7 86.5 92.5 95.5 HTL(depth=32) 79.3 93.8 95.0 96.9 97.1 97.5 56.4 68.5 78.5 86.2 92.3 95.5 HDC+: Contrastive Loss with A-N Sampling + Dynamic Violate Margin(αz) HDC 62.1 84.9 89.0 91.2 92.3 93.1 53.6 65.7 77.0 85.6 91.5 95.5 HDC+ 69.4 88.6 93.4 94.1 95.3 96.5 54.1 66.3 77.2 85.6 91.7 95.5</figDesc><table><row><cell></cell><cell></cell><cell>In-Shop Clothes</cell><cell></cell><cell></cell><cell cols="3">CUB-200-2011</cell></row><row><cell>R@</cell><cell>1</cell><cell>10 20 30 40 50</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16 32</cell></row><row><cell></cell><cell cols="2">On Triplets with Sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random Sampling</cell><cell>59.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Openface: A general-purpose face recognition library with mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ludwiczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>CMU School of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Regularized diffusion process for visual retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="3967" to="3973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ensemble diffusion for retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="774" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hard negative mining for metric learning based zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: A deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar B G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 07-49</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th IEEE Workshop on 3D Representation and Recognition, ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Van Lint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
		<title level="m">A course in combinatorics</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bier -boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bier-boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5189" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Neural networks: tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC. vol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="732" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Going deeper with convolutions. Cvpr</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4170" to="4178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bacon: Building a classifier from only n samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. CVWW</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3109" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07567</idno>
		<title level="m">Sampling matters in deep embedding learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
