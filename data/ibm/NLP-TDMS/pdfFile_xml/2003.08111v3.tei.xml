<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer Networks for Trajectory Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Giuliari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Verona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irtiza</forename><surname>Hasan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Verona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer Networks for Trajectory Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most recent successes on forecasting the people motion are based on LSTM models and all most recent progress has been achieved by modelling the social interaction among people and the people interaction with the scene. We question the use of the LSTM models and propose the novel use of Transformer Networks for trajectory forecasting. This is a fundamental switch from the sequential step-by-step processing of LSTMs to the only-attention-based memory mechanisms of Transformers. In particular, we consider both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-ofthe-art on all natural language processing tasks. Our proposed Transformers predict the trajectories of the individual people in the scene. These are "simple" models because each person is modelled separately without any complex human-human nor scene interaction terms. In particular, the TF model without bells and whistles yields the best score on the largest and most challenging trajectory forecasting benchmark of TrajNet [1]. Additionally, its extension which predicts multiple plausible future trajectories performs on par with more engineered techniques on the 5 datasets of ETH [2]+UCY <ref type="bibr" target="#b1">[3]</ref>. Finally, we show that Transformers may deal with missing observations, as it may be the case with real sensor data. Code is available at github.com/FGiuliari/Trajectory-Transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Pedestrian forecasting, the goal of predicting future people motion given their past trajectories, has been steadily growing in attention by the research community. Further to being a crucial compound of trackers, especially for the cases of large motion and/or missing observations, the topic serves early action recognition, surveillance and automotive systems.</p><p>Starting from <ref type="bibr" target="#b2">[4]</ref>, Long Short-Term Memory (LSTM) networks have been the workhorse for forecasting and progress has been achieved by devising social pooling mechanisms to model the people social interaction <ref type="bibr" target="#b2">[4]</ref>, <ref type="bibr" target="#b3">[5]</ref>. The LSTM is based on sequentially processing sequences and storing hidden states to represent knowledge about the people, e.g. its speed, direction and motion pattern. Most modern approaches have challenged each other on the social interaction of pedestrians, each modelled with a separate LSTM and exchanging information by means of social pooling mechanisms <ref type="bibr" target="#b2">[4]</ref>, <ref type="bibr" target="#b3">[5]</ref>. In fact best performing approaches additionally include the semantics of the scene into the LSTMs <ref type="bibr" target="#b4">[6]</ref>, <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b6">[8]</ref>, <ref type="bibr" target="#b7">[9]</ref>. However LSTMs have also been target of criticism: their memory mechanism has been criticised <ref type="bibr" target="#b8">[10]</ref>, <ref type="bibr" target="#b9">[11]</ref> and, most recently, also their capability of modelling social interaction <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b12">[14]</ref>. An in-depth understanding of such mechanisms has not been supported by the adopted datasets, such as the 5 datasets of ETH <ref type="bibr" target="#b0">[2]</ref> and UCY <ref type="bibr" target="#b1">[3]</ref>, where performance measures are close to saturation, since leading techniques only report average forecasting errors of âˆ¼20cm across 200m-long pavements.</p><p>In this work we side-step social and map mechanisms, and propose to model the trajectories of individual people by Transformer Networks <ref type="bibr" target="#b13">[15]</ref>, for the first time. Transformer networks have been proposed for Natural Language Processing to model word sequences. These use attention instead of sequential processing. In other words, these estimate which part of the input sentence to focus on, when needing to translate, answer a question or complete the sentence <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>. Here we consider for trajectory forecasting the original Transformer Network (TF) and the Bidirectional Transformer (BERT) models, on which state-of-the-art NLP algorithms are based. <ref type="figure">Fig. 1</ref> illustrates the fundamental difference between TF and LSTM: LSTM sequentially processes the observations before starting to predict auto-regressively, while TF "looks" at all available observations, weighting them according to an attention mechanism.</p><p>We assess the performance of TF and BERT on the Tra-jNet benchmark <ref type="bibr">[1]</ref>, in order to have a clean evaluation (TrajNet uses a unified evaluation system with a dedicated server) against 42 forecasting approaches, on a large selection of datasets. Our TF outperforms all other techniques, also those including social mechanisms. TF compares favorably also on the ETH+UCY datasets, in particular beating all of the approaches that consider the individual trajectories only. Finally we conduct an ablation to highlight the potential of the Transformers, quantitatively and qualitatively. Of particular interest is the ability of TF to still predict from inputs with missing observation data, thanks to its attention mechanism, which the LSTM cannot do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Forecasting people trajectories has been studied for over two decades and relevant literature has been surveyed by the work of <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b16">[18]</ref>. For the purpose of this paper, we distinguish two main trends of related work: a first which has focused on progressing sequence modelling and a second which has modelled the interactions between the people and between the people and the scene. Sequence modelling: Trajectory forecasting has experienced a steady progress from hand-crafted energy-based optimization approaches to data-driven ones. Early work on human path prediction have adopted linear <ref type="bibr" target="#b17">[19]</ref> or Gaussian regression models <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref>, time-series analysis <ref type="bibr" target="#b20">[22]</ref> and autoregressive models <ref type="bibr" target="#b21">[23]</ref>, optimizing for hand-crafted energy functions. By contrast, later models have been most successful by the adoption of LSTM <ref type="bibr" target="#b22">[24]</ref> and RNN models, trained with copious amounts of data. In particular, LSTM can be <ref type="figure">Fig. 1</ref>. People trajectory forecasting stands for predicting the future motion of people (green ground-truth dots), given an observation interval (blue dots). LSTM (left) sequentially processes the observations before starting to predict, while TF analyses in one shot all available observations. employed to regress directly the predicted values <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b5">[7]</ref>, or to produce mean and (diagonal) covariance over the x, y coordinates in order to express the uncertainty associated to the prediction <ref type="bibr" target="#b2">[4]</ref>. In the latter case, we refer to this model as Gaussian LSTM. Here we argue that Transformer Networks are most suitable to sequence modelling and to forecast trajectories, thanks to their better capability to learn non-linear patterns, especially emerging when large amounts of data is available. Social models and context: Enabled by the flexibility of the LSTM machinery, best performance has been recently achieved by modelling the social interaction <ref type="bibr" target="#b2">[4]</ref>, <ref type="bibr" target="#b3">[5]</ref>, <ref type="bibr" target="#b23">[25]</ref> among people and the scene context <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b5">[7]</ref>, <ref type="bibr" target="#b4">[6]</ref>, aided by tracking dynamics <ref type="bibr" target="#b24">[26]</ref> and the spatio-temporal relations among neighboring people <ref type="bibr" target="#b25">[27]</ref>, <ref type="bibr" target="#b26">[28]</ref>. Much literature has recently criticised the capability of LSTM to model the humanhuman interaction <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b11">[13]</ref>, <ref type="bibr" target="#b12">[14]</ref>, maintaining that this limits the model generalization capability <ref type="bibr" target="#b10">[12]</ref>. Our work side-steps social and environmental interactions and focuses on the prediction of the motion of each person individually. Somehow surprisingly, our "simple" approach achieves best performance on the most challenging benchmark of TrajNet.</p><p>In this work, we leverage findings and state-of-the-art techniques developed within the NLP field to model word sequences. In particular, we consider here for trajectory forecasting the original Transformer Networks <ref type="bibr" target="#b13">[15]</ref>, first to model sequences merely by attention mechanisms. Aside TF, we consider the Bidirectional Transformers BERT <ref type="bibr" target="#b14">[16]</ref>, which forms the basis for the current performer on most NLP tasks <ref type="bibr" target="#b27">[29]</ref>. To the best of our knowledge, this is the first work adopting NLP technique for trajectory forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE TRANSFORMER MODEL</head><p>We propose a multi-agent framework where each person is modelled by an instance of our transformer network. Each Transformer Network predicts the future motion of the person as a result of its previous motion.</p><p>We describe in this section the model input and output (Sec. III-A), the encoder-decoder Transformer Network (TF) (Sec. III-B) and the just-encoder BERT model (Sec. III-C) and the implementation details (Sec. III-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model input and output</head><p>For each person, the transformer network outputs the predicted future positions by processing their current and prior positions (observations or motion history). We detail here each of the input and output information and parallel those with the established LSTM, with reference to <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>a) Observed and predicted trajectories: In formal terms, for person i, we are provided a set T obs = {x</p><formula xml:id="formula_0">(i) t } 0 t=âˆ’(T obs âˆ’1)</formula><p>of T obs observed current and prior positions in Cartesian coordinates x âˆˆ R 2 , and we are required to predict a set</p><formula xml:id="formula_1">T pred = {x (i) t } T pred t=1</formula><p>of T pred predicted positions. In order to let the transformer deal with the input, this is embedded onto a higher D-dimensional space by means of a linear projection with a matrix of weights W x , i.e., e (i,t)</p><formula xml:id="formula_2">obs = x (i) t W x .</formula><p>In the same way, the output of our transformer model for person i at time t is the D-dimensional vector e (i,t) pred , which is back-projected to the Cartesian person coordinates x (i) t . LSTM and TF share this aspect.</p><p>b) Positional encoding: The transformer encodes time for each past and future time instant t with a "positional encoding". In other words, each input embedding e (i,t) obs is timestamped with its time t. The same encoding is used to prompt the model to predict into future instants, as we detail in the next section.</p><p>More formally, the input embedding e (i,t) obs is time-stamped at time t by adding a positional encoding vector p t , of the same dimensionality D: Î¾</p><formula xml:id="formula_3">(i,t) obs = p t + e (i,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t) obs</head><p>We use sine/cosine functions to define p t as in <ref type="bibr" target="#b13">[15]</ref>:</p><formula xml:id="formula_4">p t = {p t,d } D d=1</formula><p>(1) where p t,d = sin sequences of up to 10000 elements and extends unseen lengths of sequences. In this aspect, TF differs greatly from LSTM, cf. <ref type="figure" target="#fig_0">Fig. 2</ref>. LSTM processes the input sequentially and the order of input positions determine the flow of time. It does not therefore need a positional encoding. However, LSTM needs to "unroll" at training time, i.e. back-propagate the signal sequentially across the LSTM blocks processing the observations. By contrast, the training of TF is parallelizable.</p><p>Notably, thanks to the positional encoding which timestamps the input, TF may deal with missing observations. Missing data is just neglected, but the model is aware of the relative time-stamps of the presented observations. In Sec. IV, we experiment on this unique feature, important when dealing with real sensor data. c) Regression Vs. classification: Regression Vs. classification is a recurrent question in trajectory forecasting. Regression techniques, predicting the (x,y) coordinates directly, generally outperform classification-based approaches, where the inputs are quantized into classes and the input data represented as one-hot-vectors. We test both approaches and confirm the better performance of regression. However, a classification approach, which we dub TF q , provides a probabilistic output across the quantized motions.</p><p>We leverage TF q to sample multiple future predictions, which we assess both quantitatively and qualitative. The predictions of TF q are multi-modal, as we illustrate in Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder-decoder Transformer (TF)</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, TF is a modular architecture, where both the encoder and the decoder are composed of 6 layers, each containing three building blocks: i. an attention module, ii. a feed-forward fully-connected module, and iii. two residual connections after each of the previous blocks.</p><p>The capability of the network to capture sequence nonlinearities lies mainly in the attention modules. Within each attention module, an entry of a sequence, named "query" (Q), is compared to all other sequence entries, named "keys" (K) by a scaled dot product, scaled by the equal query and key d k embedding dimensionality. The output is then used to weight the same sequence entries, named now "values" (V). Attention is therefore given by the equation:</p><formula xml:id="formula_5">Attention(Q, K, V ) = sof tmax QK T âˆš d k<label>(3)</label></formula><p>The goal of the encoding stage is to create a representation for the observation sequence, which makes the model memory.</p><p>To this goal, after the encoding of the T obs input embeddings Î¾ (i,t) s , the network outputs two vectors of keys K enc and values V enc which would be passed on to the decoder stage.</p><p>The decoder predicts auto-regressively the future track positions. At each new prediction step, a new decoder query Q dec is compared against the encoder keys K enc and values V enc according to Eq. (3) (encoder-decoder attention) and against the previous decoder prediction (self-attention).</p><p>Note the important difference w.r.t. LSTM: TF maintains the encoding output (memory) separate from the decoded sequence, while LSTM accumulates both into its hidden state, steering what to memorize or forget at each time. We believe this may contribute to explain how TF outperforms LSTM in long-term horizon predictions, cf. Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BERT</head><p>We consider for trajectory forecasting a second Transformer model, BERT <ref type="bibr" target="#b14">[16]</ref>. Differently from TF, BERT is only composed of an encoder and it trains and infers thanks to a masking mechanism. In other words, the model hides (masks) from the self-attention the output positions which it targets for prediction as the TF decoder also does. During training the model learns to predict masked positions. At inference, the model output predictions for the masked outputs.</p><p>BERT is the de-facto reference model for state-of-the-art NLP methods, but larger than TF (âˆ¼2.2 times larger). As we would illustrate in Sec. IV, training BERT on the current largest trajectory forecasting benchmarks does not keep up to the expectations. We draw inspiration from transfer learning and test therefore also how a BERT pre-trained on an NLP task performs on the target task. In particular, we take the lowercased English text using Whole-Word-Masking; we substitute for the word embedding from dictionary keys with similar linear modules encoding (x,y) positions; and then we similarly convert also the output into (x,y) positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation details</head><p>Our TF implementations adopts the parameters of the original Transformer Networks <ref type="bibr" target="#b13">[15]</ref>, namely d model = 512, 6 layers and 8 attention heads. We adopt an L2-loss between the predicted and annotated pedestrian positions and train the network via backpropagation with the Adam optimizer, linear warm-up phase for the first 5 epoch and a decaying learning rate afterward; dropout value of 0.1. The normalization of the network input influences its performance, as also maintained in <ref type="bibr" target="#b28">[30]</ref>, <ref type="bibr" target="#b29">[31]</ref>. So we normalize the people speeds by subtracting the mean and dividing by the standard deviation of the train set. For the TF q , we quantize the people motion by clustering speeds into 1000 joint (x,y) bins, then encode the position by 1000-way one-hot vectors. In order to get a good cluster granularity, we augment the training data by random scaling uniformly with scale s âˆˆ [0.5, 2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>We show the capabilities of the proposed Transformer networks for trajectory forecasting on two recent and large datasets: the TrajNet Challenge [1] dataset and the ETH+UCY dataset <ref type="bibr" target="#b0">[2]</ref>, <ref type="bibr" target="#b1">[3]</ref>. Additionally, we perform an ablation study to quantify the model robustness, also in comparison with the widely-adopted LSTM. This includes varying the observation horizon and testing the model on missing data, the latter occurring when some observation samples are missing due to frame-rate drops or excessive uncertainty in the tracking data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Trajnet Challenge</head><p>The TrajNet Dataset: At the moment of writing, the TrajNet Challenge 1 [1] does represent the largest multi-scenario forecasting benchmark <ref type="bibr" target="#b30">[32]</ref>; the challenge requests to predict 3161 human trajectories, observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) i.e., t âˆ’ 7, t âˆ’ 6, . . . , t, in world plane coordinates (the so-called world plane Human-Human protocol) and forecasting the following 12 (4.8 seconds), i.e., t + 1, . . . , t + 12. In fact, TrajNet is a superset of diverse datasets that requires to train on four families of trajectories, namely 1) BIWI Hotel <ref type="bibr" target="#b0">[2]</ref> (orthogonal bird's eye flight view, moving people), 2) Crowds UCY <ref type="bibr" target="#b1">[3]</ref> (3 datasets, tilted bird's eye view, camera mounted on building or utility poles, moving people), 3) MOT PETS <ref type="bibr" target="#b31">[33]</ref> (multisensor, different human activities) and 4) Stanford Drone Dataset <ref type="bibr" target="#b32">[34]</ref> (8 scenes, high orthogonal bird's eye flight view, different agents as people, cars etc.), for a total of 11448 trajectories. Testing is requested on diverse partitions of BIWI Hotel, Crowds UCY, Stanford Drone Dataset, and is evaluated by a specific server (ground-truth testing data is unavailable for applicants). As a proof of its toughness, it is worth noting that many recent studies restrict on subsets of TrajNet <ref type="bibr" target="#b33">[35]</ref>, <ref type="bibr" target="#b34">[36]</ref>, <ref type="bibr" target="#b35">[37]</ref>, adopting their train/test splits <ref type="bibr" target="#b36">[38]</ref>. We instead consider the whole TrajNet dataset for our experiments. TrajNet allows 1 http://trajnet.stanford.edu/ to consider concurrent trajectories, so that it is compliant with "social" approaches, that can apply. Conversely, it does not allow use raw images, so that approaches which infer on maps as <ref type="bibr" target="#b7">[9]</ref>, <ref type="bibr" target="#b6">[8]</ref>, <ref type="bibr" target="#b5">[7]</ref> cannot apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics:</head><p>In agreement with most literature on people trajectory forecasting, the TrajNet performance is measured in terms of: Mean Average Displacement (MAD, equivalently Average Displacement Error ADE <ref type="bibr" target="#b6">[8]</ref>), measuring the general fit of the prediction w.r.t. the ground truth, averaging the discrepancy at each time step; Final Average Displacement (FAD, equivalently Final Displacement Error FDE <ref type="bibr" target="#b6">[8]</ref>), to check the goodness of the prediction at the last time step. The average of MAD and FAD is used to rank the approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on TrajNet:</head><p>We report in <ref type="table" target="#tab_0">Table I</ref> the complete list of 22 published comparative approaches, for a total of 39 approaches at the moment of writing; we omit the 18 unpublished results, all of which, apart from one, nonetheless had lower performance than the previous published top-scoring approach REDv3 <ref type="bibr" target="#b12">[14]</ref>. In the table, "Rank" indicates the absolute ranking over all the approaches, including the unpublished ones; "Year" the year of publication of the method; "Context" indicates whether the additional social context (the trajectories of the other co-occurring people) is taken into account ("s") or not ("/").</p><p>The scores in blue italic refer to the methods proposed in this work (TF, TF q , BERT, BERT NLP pretrained). Surprisingly, the TF model is the new best, with an advantage in terms of both MAD and FAD w.r.t. the second REDv3 <ref type="bibr" target="#b12">[14]</ref> and reducing the total error across the 3161 test tracks by âˆ¼145 meters.</p><p>It is of interest that the top four approaches (including ours) are individual ones, so no social context is taken into account. These results undoubtedly suggest that in âˆ¼3 seconds of individual observation of an individual, much information about his future can be extracted, and TF is the most successful in doing it. In fact, social approaches appear at lower ranks: the first among them is the SR-LSTM <ref type="bibr" target="#b28">[30]</ref>, then the highly-cited Social Forces <ref type="bibr" target="#b37">[39]</ref> (rank 9 and 27), the MX-LSTM <ref type="bibr" target="#b38">[40]</ref> and Social GAN <ref type="bibr" target="#b3">[5]</ref>. The quantized TF q ranks 16th, very probably due to quantization errors. For the trajnet challenge the TF q was used in its deterministic mode, i.e. the class with highest confidence was selected for the 12 predictions. This is done so because TrajNet is not set up to evaluate best-of-N metric and only a single prediction can be evaluated by the server.</p><p>BERT trained from scratch on trajectories ranks 25th; its NLP-pretrained version, fine-tuned on TrajNet, follows immediately. The BERT performance may indicate that the model does require a way larger amount of training data, which at the present moment is absolutely not comparable to the size of an NLP dataset. For this reason, in the rest of the experiments we will concentrate on the TF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The ETH+UCY Benchmark</head><p>Prior to TrajNet, most literature have benchmarked forecasting performance on a set of 5 datasets, namely the ETH- univ and ETH-hotel <ref type="bibr" target="#b0">[2]</ref> video sequences and the UCY-zara01, UCY-zara02 and UCY-univ <ref type="bibr" target="#b1">[3]</ref> videos. Datasets and metrics: The ETH+UCY datasets consist overall of 5 videos taken from 4 different scenes (Zara1 and Zara2 are taken from the same camera but at a different time).</p><p>Following the evaluation protocol of <ref type="bibr" target="#b2">[4]</ref> we sample from the data each 0.4 seconds to get the trajectories. We observe each pedestrian for 3.2 seconds (8 frames) and get ground-truth data for the next 4.8 seconds (12 frames) to evaluate the predictions. The pedestrian positions are converted to world coordinates in meters from the original pixel locations using homography matrices released by the authors. The evaluation is done with a LOO approach training for 4 dataset and testing on the remaining one. Recent works brought up some issues with the ETH+UCY dataset, <ref type="bibr" target="#b10">[12]</ref> showed that Hotel contains trajectory that go in a different direction than most of the ones in the other 4 dataset, so learning an environmental prior can be difficult without data augmentation like rotation; <ref type="bibr" target="#b28">[30]</ref> bring up the issue that ETH is an accelerated video and so by using a sampling rate of 0.4 seconds the trajectory behave in a different way than the ones in the other 4 datasets, they showed how by reducing the sampling rate they were able to improve their results. We do not take any measure to fix these issues, in order to have a fair comparison against all the other methods that use these dataset using the standard protocol, but during our internal testing we noticed similar improvement when using their sampling rate for ETH. Performance is evaluated using MAD and FAD, in meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>In <ref type="table" target="#tab_0">Table II</ref>, we compare on the ETH+UCY against the most recent and best performing approaches: S-GAN <ref type="bibr" target="#b3">[5]</ref>, Social-BIGAT <ref type="bibr" target="#b4">[6]</ref> and Trajectron++ <ref type="bibr" target="#b5">[7]</ref>. Additionally we include the "individual" version of S-GAN <ref type="bibr" target="#b3">[5]</ref>, which does not leverage the social information. Note in the <ref type="table">Table the trend</ref>  to include and model as much information as possible. The three leading techniques of S-GAN and Trajectron are in fact "social", and one of the best performing ones, Social-BIGAT, additionally ingests the semantic map of the environment ("+map"). Additionally, note that best results are obtained by sampling 20 multiple plausible futures and selecting the best one according to best test performance. We dub this here the best-of-20 protocol, which any technique in <ref type="table" target="#tab_0">Table II</ref> adopts. The rightmost column in <ref type="table" target="#tab_0">Table II</ref> shows our proposed TF q model, the only which allow to sample distributions of trajectories. TF q achieves the second best performance, only 0.10 behind in terms of MAD and 0.10 in terms of FAD.</p><p>Consistently with the TrajNet challenge, an individual forecasting TF q technique yields a performance surprisingly ahead or comparable with the best social techniques, even if enclosing additional map information. And trend is also reflected by S-GAN <ref type="bibr" target="#b3">[5]</ref>, slightly under-performing its individual counterpart.</p><p>Note that the best-of-20 protocol is a sort of upper-bound reachable by sampling-based approaches; therefore, we analyze the behavior of our Transformer-based predictors TF in the single-trajectory deterministic regime as in <ref type="bibr" target="#b5">[7]</ref>, where each method gives a single prediction. Results are reported in <ref type="table" target="#tab_0">Table III</ref>.</p><p>The message is clear: when it comes to individual approaches, the transformer predictor is better than any individual LSTM-based approach. Notably, TF is better than the Social-LSTM <ref type="bibr" target="#b3">[5]</ref>, and it outperforms the Social Attention <ref type="bibr" target="#b4">[6]</ref> in terms of FAD too, by a large margin. Notably, the only case in which LSTM compares favorably with TF is on Zara1, which is the less structured of the datasets of the benchmark, mostly containing straight lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study and qualitative results</head><p>Here we conduct an ablation study on the proposed TF model for forecasting, compare it with the LSTM, and finally illustrate qualitative results. 1) Changing the Prediction Lengths : As a first study case, we compare the stability of the TF and LSTM models when predicting longer temporal horizons. Unfortunately, TrajNet does not allow to set the prediction horizon. We set therefore to pursue a test-time experiment of models trained on the large and complex TrajNet on longer video dataset. We collect these from the 5 datasets of ETH+UCY, by selecting those datasets which are not part of the TrajNet training set, namely ETH and Zara01. In <ref type="table" target="#tab_0">Table IV</ref>-C1, we vary the observation sequence, from 12 frames (4.8s) to 32 frames (12.8) at a step of 1.8s. Both TF and LSTM have been trained one-dataset-out with training sequences of 8 samples and 12 for the prediction. 2) Missing and noisy data : To the best of our knowledge, the problem of having missing coordinates in coordinate-based long-term forecasting 2 has been never taken into account. On the contrary, the problem of missing data is common in short term-forecasting (i.e. tracking <ref type="bibr" target="#b42">[44]</ref>), or forecasting of heterogeneous data <ref type="bibr" target="#b43">[45]</ref>, <ref type="bibr" target="#b44">[46]</ref>, <ref type="bibr" target="#b45">[47]</ref>, <ref type="bibr" target="#b46">[48]</ref>, <ref type="bibr" target="#b47">[49]</ref>, where in general is treated by designing ad-hoc extensions for filling properly the missing entries (the so called hindsighting <ref type="bibr" target="#b43">[45]</ref>). Compared to these techniques, our transformer architecture represents a novel view, since it does not need to fill missing data; instead, it exploits the remaining samples knowing when they have been observed thanks to the positional encoding. For example, supposing the t âˆ’ kth sample being missed in the observation sequence, the transformer will use the remaining tâˆ’7, . . . , tâˆ’k âˆ’1, tâˆ’k +1, . . . , t, with 1 â‰¤ k â‰¤ 8 to perform the prediction of t + 1, . . . , t + 12. This structural ability is absent in LSTM and RNN in general (they cannot work with missing data), and in this sense the Transformer is superior. If replacements of missing values can be computed, we found that simple linear interpolation gives slight improvements to the results.</p><p>Having witnessed the superiority of the transformer over LSTM in absolute sense (on TrajNet, and see Tab. I) and varying the forecasting horizons (Sec. IV-C1), we continue this analysis focusing on our proposed model on the same TrajNet dataset. The idea is to systematically drop one element at observation time, at a fixed position, from the most recent (time t, indicated also as the current frame, after that it starts the prediction) to the furthest (t âˆ’ 7). Results are reported in Tab. V.</p><p>The results show that, in a complex scenario such as TrajNet, dropping input frames impact the prediction performance, matching the intuition: the more dropped frames, the larger the performance decrease. Interestingly, the current frame plays a key importance, as it is the most recent observed input, from which future predictions start. In fact, dropping the current frame together with the most recent 6 nearly doubles 3) Qualitative results : Qualitative results can further motivate the numerical results presented so far. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we report two predictions assessed on TrajNet, built by using the official visualizer of the benchmark. In particular, we artificially superpose the predicted trajectories of LSTM and TF to highlight their different behavior. In <ref type="figure" target="#fig_1">Fig. 3 a)</ref>, the subject is going south, with a minimal acceleration (not immediately visible by the figure, but numerically present); LSTM takes this gentle acceleration, predicting a uniform acceleration toward south. TF captures better the dynamics, despite at the very end the final direction is not correct.</p><p>In <ref type="figure" target="#fig_1">Fig. 3 b)</ref> a similar behavior caused LSTM to predict a faster straight trajectory, while TF followed in this case the bending of the GT more precisely.</p><p>In general, we observed that LSTM generates trajectories way more regular than those predicted by TF, and this is certainly motivated by its unrolling, opposed to the en-coder+decoder architecture of TF. This is also the reason why LSTM is so effective on Zara1, consisting essentially in straight trajectories, and so scarce on Hotel (and in general on TrajNet) if compared to TF.</p><p>To further motivate this, in <ref type="figure" target="#fig_1">Fig. 3 c)</ref> and d), we show 100 sampled trajectories by TF q on Zara1, for two different cases. <ref type="figure" target="#fig_1">Fig. 3 c)</ref> presents essentially a monomodal distribution, with the samples concentrated around the GT, enriched by few articulated trajectories, that have low probability (they are few), but are still plausible. <ref type="figure" target="#fig_1">Fig. 3 d)</ref> shows that TF has learnt a multimodal distribution, which has at least three modes, one turning north, the other going diagonal, the third (with larger number of trajectories) going east.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We have proposed the use of Transformers Networks, based on attention mechanisms, to predict people future trajectories. The Transformers, state-of-the-art on all NLP tasks, also perform best on trajectory forecasting. We believe that this questions the widespread use of LSTMs for modelling people motion and that this questions the current formulation of complex social and environmental interactions, which our model does not need for best performance.</p><p>In addition to achieving the best performance on people forecasting datasets, the proposed Transfomers have shown better long-term prediction behavior, the capability to predict sensible multiple future trajectories and the unique feature of coping with missing input observations, as it may happen when dealing with real sensor data. Equipped with the better temporal models, we envisage potential to address even larger datasets of long-term sequences, where the importance of social terms may play more crucial roles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Model illustration of LSTM (left) and TF (right). At each time step, LSTM leverages the current-frame information and its hidden state. By contrast, TF leverages the encoder representation of the observed input positions and the previously predicted outputs. In purple and grey are the self-attention and encoder-decoder attention modules, that allow TF to learn on which past position it needs to focus to predict a correct trajectory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>This work is partially supported by the Italian MIUR through PRIN 2017 -Project Grant 20172BH297: I-MALL -improving the customer experience in stores by intelligent computer vision, and by the project of the Italian Ministry of Education, Universities and Research (MIUR) "Dipartimenti di Eccellenza 2018-2022". REFERENCES [1] A. Sadeghian, V. Kosaraju, A. Gupta, S. Savarese, and A. Alahi, "Trajnet: Towards a benchmark for human trajectory prediction," arXiv preprint, 2018. Qualitative results: a) and b) showcasing failures of LSTM, c) and d) illustrating the trajectory distributions learned by TFq. Best viewed in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TRAJNET</head><label>I</label><figDesc>CHALLENGE RESULTS (WORLD PLANE HUMAN-HUMAN TRAJNET CHALLENGE, WEBSITES ACCESSED ON 26/07/2020). Blue italic indicates approaches proposed in this work.</figDesc><table><row><cell cols="2">Rank Method</cell><cell>Avg</cell><cell>FAD</cell><cell cols="2">MAD Context</cell><cell>Cit.</cell><cell>Year</cell></row><row><cell>2</cell><cell>TF</cell><cell cols="3">0.776 1.197 0.356</cell><cell>/</cell><cell>2020</cell></row><row><cell>3</cell><cell>REDv3</cell><cell cols="3">0.781 1.201 0.360</cell><cell>/</cell><cell>[14]</cell><cell>2019</cell></row><row><cell>4</cell><cell>REDv2</cell><cell cols="3">0.783 1.207 0.359</cell><cell>/</cell><cell>[14]</cell><cell>2019</cell></row><row><cell>6</cell><cell>RED</cell><cell cols="3">0.798 1.229 0.366</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>7</cell><cell>SR-LSTM</cell><cell cols="3">0.816 1.261 0.37</cell><cell>s</cell><cell>[30]</cell><cell>2019</cell></row><row><cell>9</cell><cell>S.Forces (EWAP)</cell><cell cols="3">0.819 1.266 0.371</cell><cell>s</cell><cell>[39]</cell><cell>1995</cell></row><row><cell>12</cell><cell cols="4">N-Lin. RNN-Enc-MLP 0.827 1.276 0.377</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>13</cell><cell>N-Lin. RNN</cell><cell cols="3">0.841 1.300 0.381</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>15</cell><cell cols="4">Temp. ConvNet (TCN) 0.841 1.301 0.381</cell><cell>/</cell><cell>[10]</cell><cell>2018</cell></row><row><cell>16</cell><cell>TFq</cell><cell cols="3">0.858 1.300 0.416</cell><cell>/</cell><cell>2020</cell></row><row><cell>17</cell><cell>N-Linear Seq2Seq</cell><cell cols="3">0.860 1.331 0.390</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>18</cell><cell>MX-LSTM</cell><cell cols="3">0.887 1.374 0.399</cell><cell>s</cell><cell>[40]</cell><cell>2018</cell></row><row><cell>21</cell><cell>Lin. RNN-Enc.-MLP</cell><cell cols="3">0.892 1.381 0.404</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>22</cell><cell>Lin. Interpolation</cell><cell cols="3">0.894 1.359 0.429</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>24</cell><cell>Lin. MLP (Off)</cell><cell cols="3">0.896 1.384 0.407</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>25</cell><cell>BERT</cell><cell cols="3">0.897 1.354 0.440</cell><cell>/</cell><cell>[16]</cell><cell>2020</cell></row><row><cell>26</cell><cell cols="4">BERT NLP pretrained 0.902 1.357 0.447</cell><cell>/</cell><cell>2020</cell></row><row><cell>27</cell><cell>S.Forces (ATTR)</cell><cell cols="3">0.904 1.395 0.412</cell><cell>s</cell><cell>[39]</cell><cell>1995</cell></row><row><cell>29</cell><cell>Lin. Seq2Seq</cell><cell cols="3">0.923 1.429 0.418</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>30</cell><cell>Gated TCN</cell><cell cols="3">0.947 1.468 0.426</cell><cell>/</cell><cell>[10]</cell><cell>2018</cell></row><row><cell>31</cell><cell>Lin. RNN</cell><cell cols="3">0.951 1.482 0.420</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>32</cell><cell>Lin. MLP (Pos)</cell><cell cols="3">1.041 1.592 0.491</cell><cell>/</cell><cell>[14]</cell><cell>2018</cell></row><row><cell>34</cell><cell>LSTM</cell><cell cols="3">1.140 1.793 0.491</cell><cell>/</cell><cell>[41]</cell><cell>2018</cell></row><row><cell>36</cell><cell>S-GAN</cell><cell cols="3">1.334 2.107 0.561</cell><cell>s</cell><cell>[5]</cell><cell>2018</cell></row><row><cell>40</cell><cell>Gauss. Process</cell><cell cols="3">1.642 1.038 2.245</cell><cell>/</cell><cell>[42]</cell><cell>2010</cell></row><row><cell>42</cell><cell>N-Linear MLP (Off)</cell><cell cols="3">2.103 3.181 1.024</cell><cell>/</cell><cell>[14] 2018</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>AGAINST SOA MODELS FOLLOWING THE BEST-OF-20 PROTOCOL. THE ENTIRETY OF SOA APPROACHES IS ROOTED ON LSTM, AND LEVERAGES ADDITIONAL INFORMATION (SOCIAL, SEGMENTED MAPS).THE MERE QUANTIZED TRANSFORMER TFq IS SUPERIOR TO ALL THE SOCIAL APPROACHES, SECOND ONLY TO TRAJECTRON++. ACTUALLY, ONLY S-GAN-IND [5] AND TFq HAVE THE SAME INPUT AND ARE DIRECTLY COMPARABLE; ALL OF THE OTHER PERFORMANCES ARE REPORTED AS REFERENCE, WRITTEN IN CURSIVE.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">LSTM-based</cell><cell></cell><cell>TF-based</cell></row><row><cell></cell><cell>Individual</cell><cell></cell><cell>Social</cell><cell>Soc.+ map</cell><cell>Ind.</cell></row><row><cell></cell><cell>S-GAN-ind</cell><cell>S-GAN</cell><cell cols="2">Trajectron++ Soc-BIGAT</cell><cell>TFq</cell></row><row><cell></cell><cell>[5]</cell><cell>[5]</cell><cell>[7]</cell><cell>[6]</cell><cell></cell></row><row><cell>ETH</cell><cell>0.81/1.52</cell><cell>0.87/1.62</cell><cell>0.35/0.77</cell><cell>0.69/1.29</cell><cell>0.61 / 1.12</cell></row><row><cell>Hotel</cell><cell>0.72/1.61</cell><cell>0.67/1.37</cell><cell>0.18/0.38</cell><cell>0.49/1.01</cell><cell>0.18 / 0.30</cell></row><row><cell>UCY</cell><cell>0.60/1.26</cell><cell>0.76/1.52</cell><cell>0.22/0.48</cell><cell>0.55/1.32</cell><cell>0.35 / 0.65</cell></row><row><cell>Zara1</cell><cell>0.34/0.69</cell><cell>0.35/0.68</cell><cell>0.14/0.28</cell><cell>0.30/0.62</cell><cell>0.22 / 0.38</cell></row><row><cell>Zara2</cell><cell>0.42/0.84</cell><cell>0.42/0.84</cell><cell>0.14/0.30</cell><cell>0.36/0.75</cell><cell>0.17 / 0.32</cell></row><row><cell>Avg</cell><cell>0.58/1.18</cell><cell>0.61/1.21</cell><cell>0.21/0.45</cell><cell>0.48/1.00</cell><cell>0.31 / 0.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>AGAINST SOA MODELS FOLLOWING THE SINGLE TRAJECTORY DETERMINISTIC PROTOCOL (NUMBERS OF OTHER APPROACHES ARE TAKEN FROM<ref type="bibr" target="#b5">[7]</ref> ). REGULARFONT INDICATES APPROACHES WHICH ARE COMPARABLE WITH OUR TRANSFORMER-BASED PREDICTORS, SINCE THEY USE A SINGLE INDIVIDUAL OBSERVED TRAJECTORY AS INPUT. THE OTHER APPROACHES HAVE PERFORMANCE IN ITALIC, AND ARE DISPLAYED AS A REFERENCE.</figDesc><table><row><cell></cell><cell>Linear</cell><cell></cell><cell></cell><cell>LSTM-based</cell><cell></cell><cell></cell><cell>TF-based</cell></row><row><cell></cell><cell>Individual</cell><cell cols="2">Individual</cell><cell cols="2">Social</cell><cell>Soc.+ map</cell><cell>Individual</cell></row><row><cell></cell><cell>Interpolat.</cell><cell>LSTM</cell><cell>S-GAN-ind</cell><cell>Social</cell><cell>Soc.</cell><cell cols="2">Trajectron++ Trasformer</cell></row><row><cell></cell><cell></cell><cell>[5]</cell><cell>[5]</cell><cell>LSTM [5]</cell><cell>Att. [6]</cell><cell>[7]</cell><cell>TF (ours)</cell></row><row><cell>ETH</cell><cell>1.33/2.94</cell><cell>1.09/2.94</cell><cell>1.13/2.21</cell><cell>1.09/2.35</cell><cell>0.39/3.74</cell><cell>0.50/1.19</cell><cell>1.03/2.10</cell></row><row><cell>Hotel</cell><cell>0.39/0.72</cell><cell>0.86/1.91</cell><cell>1.01/2.18</cell><cell>0.79/1.76</cell><cell>0.29/2.64</cell><cell>0.24/0.59</cell><cell>0.36/0.71</cell></row><row><cell>UCY</cell><cell>0.82/1.59</cell><cell>0.61/1.31</cell><cell>0.60/1.28</cell><cell>0.67/1.40</cell><cell>0.20/0.52</cell><cell>0.36/0.89</cell><cell>0.53/1.32</cell></row><row><cell>Zara1</cell><cell>0.62/1.21</cell><cell>0.41/0.88</cell><cell>0.42/0.91</cell><cell>0.47/1.00</cell><cell>0.30/2.13</cell><cell>0.29/0.72</cell><cell>0.44/1.00</cell></row><row><cell>Zara2</cell><cell>0.77/1.48</cell><cell>0.52/1.11</cell><cell>0.52/1.11</cell><cell>0.56/1.17</cell><cell>0.33/3.92</cell><cell>0.27/0.67</cell><cell>0.34/0.76</cell></row><row><cell>avg</cell><cell>0.79/1.59</cell><cell>0.70/1.52</cell><cell>0.74/1.54</cell><cell>0.72/1.54</cell><cell>0.30/2.59</cell><cell>0.34/0.84</cell><cell>0.54/1.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV MAD</head><label>IV</label><figDesc>AND FAD ERRORS WHEN LETTING THE T F AND THE LSTM MODELS PREDICT LONGER HORIZONS, I.E. FROM 12 TO 32 TIME STEPS. BOTH MODELS WERE TRAINED ON THE TRAJNET TRAIN SET, WHILE ERRORS ARE REPORTED OVER THE UNION OF ETH AND ZARA1 SEQUENCES (NOT PART OF THE TRAJNET TRAIN SET).OnTable IVare reported the average MAD and FAD values over the ETH-univ and UCY-zara1. Obviously, performances are generally decreasing. TF has a consistent advantage at every horizon Vs. LSTM and the decrease with the horizon of LSTM is approximately 25% worse, as LSTM degrades from 0.78 to 4.13 MAD, while TF degrades from 0.71 to 2.98 MAD.</figDesc><table><row><cell>Pred.</cell><cell>TF (ours)</cell><cell>LSTM [41]</cell></row><row><cell></cell><cell cols="2">MAD / FAD MAD / FAD</cell></row><row><cell>12</cell><cell>0.71/1.56</cell><cell>0.78/1.70</cell></row><row><cell>16</cell><cell>0.95/2.15</cell><cell>1.15/2.72</cell></row><row><cell>20</cell><cell>1.27/2.90</cell><cell>1.64/3.99</cell></row><row><cell>24</cell><cell>1.66/3.76</cell><cell>2.29/5.55</cell></row><row><cell>28</cell><cell>2.27/5.09</cell><cell>3.07/7.46</cell></row><row><cell>32</cell><cell>2.98/4.52</cell><cell>4.13/9.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V EVALUATION</head><label>V</label><figDesc>OF MISSING DATA RESULTS FOR TF ON TRAJNET. WE EXPERIMENT DROPPING A VARYING NUMBER OF MOST RECENT OBSERVED SAMPLES, EITHER INCLUDING OR EXCLUDING THE CURRENT FRAME. FOR EXAMPLE, IN THE CASE OF DROPPING 3 FRAMES, WE DROP T obs = {x AND T obs = {x</figDesc><table><row><cell cols="3">(i) t } 0 t=âˆ’2) (i) t } âˆ’1 t=âˆ’3) RESPECTIVELY.</cell></row><row><cell># most recent</cell><cell>Drop most recent obs.</cell><cell>Drop most recent obs.</cell></row><row><cell cols="3">frames dropped including current frame excluding current frame</cell></row><row><cell></cell><cell>(FAD/MAD)</cell><cell>(FAD/MAD)</cell></row><row><cell>0</cell><cell>1.197 / 0.356</cell><cell>1.197 / 0.356</cell></row><row><cell>1</cell><cell>1.305/ 0.389</cell><cell>1.267 / 0.373</cell></row><row><cell>2</cell><cell>1.409 / 0.429</cell><cell>1.29 / 0.38</cell></row><row><cell>3</cell><cell>1.602 / 0.495</cell><cell>1.303 / 0.384</cell></row><row><cell>4</cell><cell>1.787 / 0.557</cell><cell>1.313 / 0.387</cell></row><row><cell>5</cell><cell>1.897/ 0.593</cell><cell>1.327 / 0.329</cell></row><row><cell>6</cell><cell>2.128 / 0.669</cell><cell>1.377 / 0.406</cell></row><row><cell cols="3">the error, i.e. degrades performance by 91%, from 0.356 to</cell></row><row><cell cols="3">0.669 MAD. By contrast dropping 6 observed frames but</cell></row><row><cell cols="3">keeping the current one only degrades the performance by</cell></row><row><cell cols="3">16% (from 0.356 to 0.406 MAD), although the TF may now</cell></row><row><cell cols="3">only leverage 2 observations (farther and closest in time).</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t 10000 d/D for d even cos t 10000 d/D for d odd(2)In other words, each dimension of the positional encoding varies in time according to a sinusoid of different frequency, from 2Ï€ to 10000 Â· 2Ï€. This ensures a unique time stamp for</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Coordinate-based forecasting takes as input floor coordinates of people, and is different to image-based forecasting, where images are processed to extract bounding boxes locations on the image plane such as<ref type="bibr" target="#b41">[43]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Social LSTM: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cONF</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Social-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>MartÃ­n-MartÃ­n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Trajectron++: Multi-agent generative trajectory forecasting with heterogeneous data for control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03093</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The trajectron: Probabilistic multi-agent trajectory modeling with dynamic spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2375" to="2384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1349" to="1358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time endto-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3569" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What the constant velocity model can teach us about pedestrian motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>SchÃ¶ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aravantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Red: A simple but effective baseline predictor for the trajnet benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An evaluation of trajectory prediction approaches and notes on the trajnet benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>HÃ¼bner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07663</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformer attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bert pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey of vision-based trajectory learning and analysis for surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1114" to="1127" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generalized linear models, no. 37 in monograph on statistics and applied probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A unifying view of sparse approximate gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>QuiÃ±onero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1939" to="1959" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Prediction with gaussian processes: From linear regression to linear prediction and beyond,&quot; in Learning in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="599" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Priestley</surname></persName>
		</author>
		<title level="m">Spectral analysis and time series</title>
		<imprint>
			<publisher>Academic press</publisher>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fitting autoregressive models for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the institute of Statistical Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="243" to="247" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Social attention: Modeling attention in human crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01909</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Forecast the plausible paths in crowd scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/386</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/386" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2772" to="2778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowd scene understanding with coherent recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta a robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sr-lstm: State refinement for lstm towards pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="85" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Human motion trajectory prediction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06113</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pets2009: Dataset and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 Twelfth IEEE International Workshop on Performance Evaluation of Tracking and Surveillance</title>
		<imprint>
			<date type="published" when="2009-12" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Trajectory forecasts in unknown environments conditioned on grid-based plans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00735</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-growing spatial graph networks for pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1151" to="1159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene compliant trajectory forecast with agent-centric spatio-temporal grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ridel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2816" to="2823" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Safecritic: Collision-aware trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Der Heiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06673</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Social force model for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mx-lstm: mixing tracklets and vislets to jointly forecast trajectories and head poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsesmelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">LSTM MATLAB implementation</title>
		<ptr target="https://it.mathworks.com/help/deeplearning/ug/long-short-term-memory-networks.html" />
		<imprint>
			<biblScope unit="page" from="2019" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unfreezing the robot: Navigation in dense, interacting crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trautman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Confidence-based data association and discriminative deep appearance learning for robust online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="595" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online time series prediction with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Anava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2191" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A study of hybrid neural network approaches and the effects of missing data on traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grant-Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mussone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Montgomery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="286" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spectral modeling of time series with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. De</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Modelling</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4676" to="4684" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Robust training of recurrent neural networks to handle missing data for disease progression modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>SÃ¸rensen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05500</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The &quot;caterpillar&quot;-ssa method for analysis of time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Golyandina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Osipov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical planning and Inference</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2642" to="2653" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
