<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
							<email>huangqx@cs.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
							<email>xyxue@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<email>yichenw@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the task of 3D human pose estimation in the wild. This task is challenging due to lack of training data, as existing datasets are either in the wild images with 2D pose or in the lab images with 3D pose.</p><p>We propose a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a unified deep neutral network that presents two-stage cascaded structure. Our network augments a state-of-the-art 2D pose estimation sub-network with a 3D depth regression sub-network. Unlike previous two stage approaches that train the two sub-networks sequentially and separately, our training is end-to-end and fully exploits the correlation between the 2D pose and depth estimation sub-tasks. The deep features are better learnt through shared representations. In doing so, the 3D pose labels in controlled lab environments are transferred to in the wild images. In addition, we introduce a 3D geometric constraint to regularize the 3D pose prediction, which is effective in the absence of ground truth depth labels. Our method achieves competitive results on both 2D and 3D benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation problem has been heavily studied in computer vision. It has numerous important applications in human-computer interaction, virtual reality, and action recognition. Existing research works falls into two categories: 2D pose estimation and 3D pose estimation. Thanks to the availability of large-scale 2D annotated human poses and the emergence of deep neural networks, the 2D human pose estimation problem has gained tremendous success recently <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref>. State-of-the-art techniques are able to achieve accurate predictions across a wide range of settings (e.g., on images in the wild <ref type="bibr" target="#b1">[2]</ref>).</p><p>In contrast, advance in 3D human pose estimation re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head><p>In-the-wild images with 2D annotation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indoor images with 3D annotation</head><p>In-the-wild image 3D pose mains limited. This is partially due to the ambiguity of recovering 3D information from single images, and partially due to the lack of large scale 3D pose annotation dataset. Specifically, there is not yet a comprehensive 3D human pose dataset for images in the wild. The commonly used 3D datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref> were captured by mocap systems in controlled lab environments. Deep neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">33]</ref> trained on these datasets do not generalize well to other environments, such as in the wild.</p><p>There has been quite a few works on 3D human pose estimation in the wild. They usually proceed in two sequential steps <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. The first step estimates 2D joint locations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref>. The second step recovers a 3D pose from these 2D joints <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b0">1]</ref>. Training in the two steps are performed separately. Namely, 2D pose predictions are trained from 2D annotations in the wild, and 3D pose recovery from 2D joints is trained from existing 3D MoCap data. Such a sequential pipeline is clearly sub-optimal because the original in-the-wild 2D image information, which contains rich cues for 3D pose recovery, is discarded in the second step.</p><p>Recently, Mehta et al. <ref type="bibr" target="#b14">[15]</ref> have shown that 2D-to-3D knowledge transfer, i.e., using pre-trained 2D pose networks to initialize the 3D pose regression networks can significantly improve 3D pose estimation performance. This indicates that the 2D and 3D pose estimation tasks are inherently entangled and could share common representations.</p><p>Inspired by this work, we argue that the inverse knowledge transfer, i.e., from 3D annotations of indoor images to in-the-wild images, offers an effective solution for 3D pose prediction in the wild. In this work, we introduce a unified framework that can exploit 2D annotations of inthe-wild images as weak labels for the 3D pose estimation task. In other words, we consider a weakly-supervised transfer learning problem, where the source domain consists of fully annotated images in restricted indoor environment and the target domain consists of weakly-labeled images in the wild.</p><p>Similar to previous works <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, our network also consists of a 2D module and a 3D module. However, instead of merely feeding the output of the 2D module as input to the 3D module, our approach connect the 3D module with the intermediate layers of the 2D module. This allows us to share the common representations between the 2D and the 3D tasks. The network is trained end-to-end with both 2D and 3D data simultaneously. This distinguishes our work from all existing works.</p><p>To better regularize the learning of weakly-supervised 3D pose estimation, we introduce a geometric constraint for training the 3D module. The geometric constraint is based on the fact that relative bone length in a human skeleton remains approximately fixed. The effectiveness of this constraint is experimentally verified when adapting the 3D pose information from labeled images in indoor environments to unlabeled images in the wild.</p><p>This work makes the following contributions:</p><p>• For the first time, we propose an end-to-end 3D human pose estimation framework for in-the-wild images. It achieves state-of-the-art performance on several benchmarks.</p><p>• We propose a 3D geometric constraint for 3D pose es-timation from images with only 2D joint annotations. It has low cost in memory and computation. It improves the geometric validity of estimated poses.</p><p>Code is publicly available at https://github.com/ xingyizhou/pose-hg-3d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human pose estimation has been studied considerably in the past <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, and it is beyond the scope of this paper to provide a complete overview of the literature. In this section, we focus on previous works on 3D human pose estimation, which are most relevant to the context of this paper. We will also discuss related works on imposing weakly-/unsupervised constraints for training neural networks.</p><p>3D Human Pose Estimation. Given well labeled data (e.g., 3D joint locations of a human skeleton <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>), 3D human pose estimation can be formulated as a standard supervised learning problem. A popular approach is to train a neural network to directly regress joint locations <ref type="bibr" target="#b12">[13]</ref>. Recently, people have generalized this approach in different directions. Zhou et al. <ref type="bibr" target="#b33">[33]</ref> propose to explicitly enforce the bone-length constraints in the prediction, using a generative forward-kinematic layer; Tekin et al. <ref type="bibr" target="#b24">[25]</ref> embed a pretrained auto-encoder at the top of the network. In contrast these works, Pavlakos et al introduce a 3D approach, which regresses a volumetric representation of 3D skeleton <ref type="bibr" target="#b18">[19]</ref>. Despite the performance gain on standard 3D pose estimation benchmark datasets, the resulting networks do not generalize to images in the wild due to the domain difference between natural images and the specific capture environments utilized by these benchmark datasets.</p><p>A standard approach to address the domain difference between 3D human pose estimation datasets and images in the wild is to split the task into two separate subtasks <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref>. The first sub-task estimates 2D joint locations. This sub-task can utilize any existing 2D human pose estimation method (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref>) and can be trained from datasets of in-the-wild images. The second sub-task regresses the 3D locations of these 2D joints.</p><p>Since the input at this step is just a set of 2D locations, the 3D pose estimation network can be trained on any benchmark datasets and then adapted in other settings. Regarding 3D pose estimation from 2D joint locations, <ref type="bibr" target="#b35">[34]</ref> use an EM algorithm to compute a 3D skeleton by combining a sparse dictionary induced from the 2D heat-maps; <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> use 3D pose data and its 2D projection to train a heatmap-to-3D pose network without the original image; Bogo et al. <ref type="bibr" target="#b2">[3]</ref> optimize both the pose and shape terms of a linear 3D human model <ref type="bibr" target="#b13">[14]</ref> to best fit its 2D projection; Chen et al. <ref type="bibr" target="#b4">[5]</ref> use nearest-neighbor search to match the estimated 2D pose to a 3D pose as well as a camera-view which may produce a similar 2D projection from a large 3D pose library; finally,  <ref type="figure">Figure 2</ref>. Illustration of our framework: In testing, images go through the stacked hourglass network and turn into 2D heat-maps. The 2D heat-maps and with lower-layer images features are summed as the input of the following depth regression module. In training, images from both 2D and 3D datasets are mixed in a single batch. For the 3D data, the standard regression with Euclidean Loss is applied. For the 2D data, we propose a weakly-supervised loss based on its 2D annotation and prior knowledge of human skeleton.</p><formula xml:id="formula_0">+ + + + + (Ŷ dep )</formula><p>Tome et al. <ref type="bibr" target="#b25">[26]</ref> propose a pre-trained probabilistic 3D pose model layer that first generates plausible 3D human model from 2D heat-maps, and then refines these heat maps by combining 3D pose projection and image features. All these methods, however, share a common limitation: the 3D pose is only estimated from the 2D joints, which is known to produce ambiguous results. In contrast, our approach leverages both 2D joint locations as well as intermediate feature representations from the original image.</p><p>An alternative approach for 3D human pose estimation is to train from synthetic datasets which are generated from deforming a human template model with known 3D ground truth <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. This is indeed a viable solution, but the fundamental challenge is how to model the 3D environment so that the distribution of the synthesized images matches that of the natural images. It turns out state-of-the-art methods along this line are less competitive on natural images.</p><p>There are also other works utilizing mixed 2D and 3D data for 3D human pose estimation. Mehta et al. <ref type="bibr" target="#b14">[15]</ref> finetune a pre-trained 2D pose estimation network with 3D data. Popa et al. <ref type="bibr" target="#b19">[20]</ref> consider 3D human pose estimation as a multi-task learning of 2D and depth regression with different data. Ours is different from those work that we use a weakly-supervised loss that seamlessly integrates both 2D and 3D data in a unified framework.</p><p>Weakly-/un-supervised constraints. In the presence of insufficient training data, incorporating generic or weakly supervised constraints among the prediction serves as a powerful tool for performance boosting. This idea was usually utilized in image classification or segmentation. Pathak et al. <ref type="bibr" target="#b17">[18]</ref> propose a constrained optimization framework that utilizes a linear constraint over sum of label probabilities for weakly supervised semantic segmentation. Tzeng et al. <ref type="bibr" target="#b27">[28]</ref> propose a domain confusion loss to maximize the confusion between two datasets so as to encourage a domain-invariant feature. Recently, Hoffman et al. <ref type="bibr" target="#b9">[10]</ref> introduce an adversarial learning based global domain alignment method and utilize a weak label constraint to apply fully connected networks in the wild. In this paper, we show this general concept can be used for pose estimation as well. To best of our knowledge, our approach is the first to leverage geometry-guided constraint to regularize the pose estimation network for images in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given an RGB image I containing a human subject, we aim to estimate the 3D human pose Y ∈ Y 3D , represented by a set of 3D joint coordinates of the human skeleton, i.e. Y 3D ∈ R J×3 , where J is the number of joints. We follow the convention of representing each 3D coordinate in the local camera coordinate system associated with I, namely, the first two coordinates are given by image pixel coordinates (which define the corresponding 2D joint location), and the third coordinate is the joint depth in metric coordinates, e.g., millimeters in this work.</p><p>Our proposed network architecture is illustrated in <ref type="figure">Fig. 2</ref>. It consists of a 2D pose estimation module (Section 3.2) and a depth regression module (Section 3.3). They predict</p><formula xml:id="formula_1">the 2D joint locations Y 2D ∈ Y 2D , where Y 2D ⊂ R J×2 , and the depth values Y dep ∈ Y dep , where Y dep ⊂ R J×1 , respectively. The final output is the concatenation of Y 2D and Y dep .</formula><p>The network is trained from both images in the lab with 3D ground truth (for both Y 2D and Y dep ) and images in the wild with only 2D ground truth (for Y 2D ). In the reminder of this paper, the 3D and 2D training image sets are denoted as I 3D and I 2D , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">2D Pose Estimation Module</head><p>We adopt the state-of-the-art hourglass network architecture in <ref type="bibr" target="#b16">[17]</ref> as our 2D pose estimation module. The network output is a set of J low-resolution heat-maps. Each map Y HM ∈ R H×W represents a 2D probability distribution of one joint. The predicted joints in the 2D poseŶ 2D ∈ Y 2D are the peak locations on these heat-maps. This heat-map representation is convenient as it can be easily combined (concatenate or sum) with the other deep layer feature maps, e.g., as shown in <ref type="figure">Fig 2.</ref> To train this module, the loss function is</p><formula xml:id="formula_2">L 2D (Ŷ HM , Y 2D ) = H h W w (Ŷ (h,w) HM − G(Y 2D ) (h,w) ) 2 .</formula><p>(1) The loss measures the L 2 distance between the predicted heat-mapsŶ HM and the heat-maps G(Y 2D ) rendered from the ground truth Y 2D through a Gaussian kernel <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth Regression Module</head><p>Compared with previous methods that recover 3D joint locations from only 2D joint predictions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b0">1]</ref>, our approach innovates in terms of (i) the integration of 2D and 3D modules for end-to-end network training, and (ii) the usage of a 3D geometric constraint induced loss. They are elaborated below.</p><p>Integration of 2D and 3D modules. A key issue for depth estimation is how to effectively exploit image features. A widely used strategy in previous <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5]</ref> is to take the 2D joint locations as the only input for depth prediction as in this way the Mocap-only data can be utilized. However, this strategy is inherently ambiguous, as there typically exist multiple 3D interpretations of a single 2D skeleton. We propose to combine the 2D joint heatmaps and the intermediate feature representations in the 2D module as input to the depth regression module. These features, which extract semantic information at multiple levels for 2D pose estimation, provide additional cues for 3D pose recovery. This shared common feature learning is crucial in our approach.</p><p>3D geometric constraint induced loss. One challenge for depth learning is to how to integrate both fully-labeled and weakly-labeled images. For fully-annotated 3D dataset S 3D = {I 3D , Y 2D , Y dep }, the training loss can be simply the standard Euclidean Loss using ground-truth depth label. For weakly-labeled dataset S 2D = {I 2D , Y 2D }, we propose a novel loss induced from a geometric constraint. In the absence of ground truth depth label, this geometric constraint serves as effective regularization for depth prediction.</p><p>Overall, letŶ dep denote the predicted depth. The loss of the depth regression module is</p><formula xml:id="formula_3">L dep (Ŷ dep |I, Y 2D ) = λ reg ||Y dep −Ŷ dep || 2 , if I ∈ I 3D λ geo L geo (Ŷ dep |Y 2D ), if I ∈ I 2D</formula><p>(2) where λ reg and λ geo are the corresponding loss weights.</p><p>L geo (Ŷ dep |Y 2D ) is the proposed geometric loss. It is based on the fact that ratios between bone lengths remain relative fixed in a human skeleton (e.g., upper/lower arms have a fixed length ratio, left/right shoulder bones share the same length).</p><p>Specifically, let R i be a set of involved bones in a skeleton group i, e.g. R arm ={left upper arm, left lower arm, right upper arm, right lower arm}, let l e be the length of bone e, and let l e denote the length of bone e in a canonical skeleton (in our experiments, it is set as the average of all training subjects of Human 3.6M dataset). The ratio le le for each bone e in each group R i should remain fixed. The proposed loss measures the sum of variance among { le le } e∈Ri of each R i :</p><formula xml:id="formula_4">L geo (Ŷ dep |Y 2D ) = i 1 |R i | e∈Ri l e l e − r i 2 ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">r i = 1 |R i | e∈Ri l e l e .</formula><p>Note that the bone length is a function of joint locations, which are in turn functions of the predicted depths. Thus, L geo is continuous and differentiable with respect toŶ dep . The math details of forward and backward equations are provided in the supplemental material Also note that L geo is defined on the ground truth 2D position Y 2D instead of the predicted 2D positionŶ 2D . This makes the training easier as there is no back-propagation into the 2D module.</p><p>In our experiments, we consider 4 groups of bones: R arm = {left/right lower/upper arms}, R leg = { left/right lower/upper legs}, R shoulder = { left/right shoulder bones }, R hip = {left/right hip bones}. We do not include bones on the torso as we found them exhibit relatively high variance in bone lengths across different human shapes, which makes our constraint less valid. Note that bones in different sets do not affect each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Combining the losses in Eq. (1), (2), and (3), the overall loss for each training image I ⊂ I 2D ∪ I 3D is</p><formula xml:id="formula_6">L(Ŷ HM ,Ŷ dep |I) =L 2D (Ŷ HM , Y 2D )+ L dep (Ŷ dep |I, Y 2D ).<label>(4)</label></formula><p>Stochastic gradient descent optimization is used for training. Similar to <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b9">[10]</ref>, each mini-batch contains both the 2D and 3D training examples (half-half), which are randomly sampled.</p><p>In experiments, we found the direct end-to-end training of the whole network from scratch does not work well, likely because of the dependency between the two modules and highly non-linear property of the new geometric constraint induced loss. Thus, we propose a three-stage training scheme that we found is more stable and effective in practice. Note that the final stage is end-to-end.</p><p>Stage 1 initializes the 2D pose module using 2D annotated images, as described in <ref type="bibr" target="#b16">[17]</ref>. Stage 2 initializes the 3D pose estimation module and fine-tunes the 2D pose estimation module. Both 2D and 3D annotated data are used. The geometric constraint is not activated, by setting λ geo = 0 in Equation 2. Stage 3 fine-tunes the whole network with all data. The geometric constraint is activated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>To validate our approach, a single model is trained using Human3.6M data <ref type="bibr" target="#b11">[12]</ref> and MPII data <ref type="bibr" target="#b1">[2]</ref>. Evaluation is performed on three different testing datasets.</p><p>The evaluations are from two aspects: supervised 3D human pose estimation (Section 4.2) and transferred 3D human pose estimation in the wild(Section 4.3).</p><p>Qualitative results are summarized in <ref type="table">Table.</ref> 5. More qualitative results on MPII validation set can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Implementation Detail</head><p>Our method was implemented with torch7 <ref type="bibr" target="#b7">[8]</ref>. The hourglass component was based on the public code in <ref type="bibr" target="#b16">[17]</ref>. For fast training, we used a shallow version of stacked hourglass, i.e. 2 stacks with 2 residual modules <ref type="bibr" target="#b8">[9]</ref> for each hourglass. The depth regression module contains 4 sequential residual &amp; pooling modules, which can be regarded as a half hourglass. The same network architecture and training iterations are used in all of our experiments.</p><p>The first training stage in Section 3.4 took 240k with a batchsize of 6. This gave us a 2D pose estimation module with similar performance as in <ref type="bibr" target="#b16">[17]</ref>. Stage 2 and stage 3 took 200k and 40k iterations, respectively. The whole training procedure took about two days in one Titan X GPU with CUDA 8.0 and cudnn 5. A forward pass at testing is about 30ms. We set λ reg = 0.1 and λ geo = 0.01. We followed <ref type="bibr" target="#b16">[17]</ref> to set all the other hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Datasets &amp; Metrics</head><p>MPII-training. MPII dataset <ref type="bibr" target="#b1">[2]</ref> is used for training. It is a large scale in-the-wild human pose dataset. The images are collected from on-line videos and annotated by human for J = 16 2D joints. It contains 25k training images and 2957 validation images <ref type="bibr" target="#b26">[27]</ref>. The human subjects are annotated with bounding boxes. We use the training set of MPII to train the 2D pose estimation module. It also provides weak supervision for the depth regression module.</p><p>Human3.6M. Human 3.6M dataset <ref type="bibr" target="#b11">[12]</ref> is used both in training and testing. It is a widely used dataset for 3D human pose estimation. This dataset contains 3.6 millions of RGB images captured by a MoCap System in an indoor environment. We down-sampled the video from 50f ps to 10f ps for both the training and testing sets to reduce redundancy. Following the standard protocol in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b33">33]</ref>, we use 5 subjects(S1, S5, S6, S7, S8) for training and the rest 2 subjects(S9, S11) for testing. The evaluation metric is mean per joint position error(MPJPE) in mm after aligning the depths of the root joints. We use its projected 2D locations for training the 2D module and its depth annotation for depth regression module.</p><p>We use the ground truth 2D joint locations provided in the dataset in training (thus implicitly use the camera calibration information), for aligning the 3D and 2D poses. During testing, such calibration is not needed, by requiring that the sum of all 3D bones lengths is equal to that of a pre-defined canonical skeleton, as is done in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">35]</ref>. The converting formulation is as follows:</p><formula xml:id="formula_7">Y = (Y out − Y (root) out ) * AvgSumLen SumLen out + Y (root) GT</formula><p>Where Y out is the combined 2D and depth 3D joint, which is the output of the network; SumLen out is the calculated sum-of-skeleton-length of the output joints; and AvgSumLen is an constant, which is calculated as the average sum-of-skeleton-length of all the training subjects in Human 3.6M dataset.</p><formula xml:id="formula_8">MPI-INF-3DHP. MPI-INF-3DHP [15]</formula><p>is a newly proposed 3D human pose dataset. The images were captured by a MoCap system both in indoor and outdoor scenes. We only use its test set split for evaluation. The test set contains 2929 valid frames from 6 subjects, performing 7 actions. Following <ref type="bibr" target="#b14">[15]</ref>, we employ average PCK (with a threshold 150mm) and AUC as the evaluation metrics, i.e., after aligning the root joint (pelvis). Note that we assume the global scale is known for experimental evaluation. We observe that the definition of pelvis position in MPI-INF-3DHP is different from the one used in our training sets (i.e., Human 3.6M and MPII), so we moved the pelvis and hips towards neck in a fixed ratio (0.2) as post processing in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Directions Discussion Eating Greeting Phoning Photo Posing Purchases</head><p>Chen &amp; Ramanan <ref type="bibr" target="#b4">[5]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII-Validation.</head><p>Although MPII dataset does not provide 3D pose annotation, we use its validation subset <ref type="bibr" target="#b26">[27]</ref> in our evaluation for two purposes. It contains 2958 in-thewild images out of the training set.</p><p>First, we provide qualitative 3D pose estimation results. Many of them looks plausible and convincing. See more in supplementary material.</p><p>Second, we can still evaluate the geometric validity of the estimated 3D pose, which is improved by our proposed constraint. We use the symmetric bone lengths' difference (e.g., left and right upper arms) as the evaluation metric. To compute the metric, we normalize the 2D joints in 256×256 pixels (so that the predicted joints can be directly plotted in the input image). The depth is normalized by the same scale. We then compute the L1 distance between the left and right symmetric bones, e.g. for upper arms it is ||Y (lef t shoulder) − Y (lef t elbow) || − ||Y (right shoulder) − Y (right elbow) |||. This metric is applied for both MPI-INF-3DHP dataset and MPII-Validation set to evaluate the effectiveness of our proposed weakly-supervised geometric loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baselines for Ablation Study</head><p>We implemented three baseline methods and trained the baseline models in the same way as for proposed method.</p><p>3D/wo geo It only uses 3D labeled data to train the network in Stage2 and Stage3 of Sec. 3.4. The in-the-wild images are not used. Note that the 2D hourglass module is pre-trained on the 2D dataset in Stage1.</p><p>3D/w geo It adds the geometric constraint induced loss into the first baseline.</p><p>3D+2D/wo geo Its only difference from the proposed method is that the geometric constraint is not utilized for 2D labeled data when training the 3D module.</p><p>The proposed method is denoted as 3D+2D/w geo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Supervised 3D Human Pose Estimation</head><p>We first report and analyze the performance of our method on Human 3.6M dataset <ref type="bibr" target="#b11">[12]</ref>.</p><p>Baseline comparison. <ref type="table" target="#tab_1">Table 1</ref> compares the proposed approach with the three baselines. The average MPJPE of baseline 3D/wo geo is 82.44mm. This is already comparable to most state-of-the-art methods <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">35]</ref>. Note that this baseline is similar with Metha et al. <ref type="bibr" target="#b14">[15]</ref>, which finetuned 2D pose network <ref type="bibr" target="#b10">[11]</ref> with 3D data for information transfer. The difference is that we did not use 1000× learning rate decay for the transferred layers, which in our case yielded worse performance. Adding the geometric constraint, i.e., 3D/w geo, provides a decent performance gain.</p><p>Training with both 2D and 3D data (3D+2D/wo geo), provides significant performance gain -average MPJPE dropped to 64.90mm, which is superior to all previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. This verifies the effectiveness of combining data sources in our unified training.</p><p>Finally, the proposed approach 3D+2D/w geo achieves the best results. Note that the constraints are applied on the disjoint 2D dataset, showing that the provided prior knowledge is universal. We have also tested adding constraints on fully-supervised 3D data. The results are similar.</p><p>Comparisons to other in-the-wild methods. Our method is superior to other methods that are applicable to in-the-wild images. Comparing to two two-step methods, MPJPE of Chen &amp; Ramanan <ref type="bibr" target="#b4">[5]</ref> is 114.18mm and MPJPE of Zhou et al. <ref type="bibr" target="#b36">[35]</ref> is 79.9mm. Pavlakos et al. <ref type="bibr" target="#b18">[19]</ref> provided an alternative decoupled version which can also be applied in the wild, but its MPJPE increased to 78.1mm. MPJPE of our method is 64.90mm and significantly better.</p><p>Why combining 2D and 3D data is better? A reasonable question is that it is still unclear whether the benefit of combined training comes from better depth estimation, or just from more accurate 2D pose estimation.</p><p>To answer this question, we only evaluate the accu-racy of the 2D pose estimation, using the standard metric PCKh@0.5 (see <ref type="bibr" target="#b1">[2]</ref>). The results in Tab. 2 show that the 2D pose is very accurate in all the three baselines and the proposed method. This convincingly indicates that adding 2D data into training does not improve the 2D accuracy but mostly benefits the the depth regression module via shared deep feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transferred Human Pose In the Wild</head><p>We evaluate the generalization of our method on two datasets captured in different in-the-wild environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">MPI-INF-3DHP Dataset</head><p>It exhibits considerable domain shift from both MPII and Human 3.6M datasets. <ref type="table" target="#tab_2">Table 3</ref> compares the performance of various methods on MPI-INF-3DHP. In this case, the first two baseline methods, i.e., 3D/wo geo and 3D/w geo, have low performance. This is not surprising, as the 3D training set contains only indoor images. We note that even in this case, the geometric constraint is still effective (3D/wo geo is worse than 3D/w geo).</p><p>3D+2D/wo geo achieved 65.8 and 32.1 in PCK and AUC, respectively. These numbers are better than their counterparts (64.7 PCK and 31.7 AUC) in <ref type="bibr" target="#b14">[15]</ref> with Human 3.6M training data, again showing the advantage of our training scheme.</p><p>The proposed approach yields 69.2 in PCK and 32.5 in AUC. These numbers are close to the one that is derived from the original training data of MPI-INF-3DHP <ref type="bibr" target="#b14">[15]</ref>, which has 72.5 in PCK and 36.5 in AUC. Our result is strong even though we didn't use their training data. This confirms the ability of our method on in-the-wild images.</p><p>We also tested the left-right symmetry as described in Sec. 4.1.2. The results in <ref type="table">Table.</ref> 4 (Bottom) shows that using the geometric constraint considerably improves the geometric validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">MPII Validation Dataset</head><p>Finally, we evaluate our method on the most challenging in-the-wild MPII validation set. The qualitative 3D pose <ref type="table">Table 5</ref>. Qualitative results from different datasets. We show the 2D pose on the original image and 3D pose from a novel view. First line: Human 3.6M dataset; Second and third lines: MPI-INF-3DHP dataset; Fourth to seventh lines: MPII dataset. results in <ref type="table">Table 5</ref> are quite plausible.</p><p>Geometric validity. As explained in sec. 4.1.2, we evaluate the left-right symmetry metric. The results in <ref type="table">Table 4</ref> (Top) show that our approach is considerably better.</p><p>2D accuracy versus 3D accuracy. We note that our method has a slightly lower 2D joint accuracy than the original Hourglass model. This can be expected as our model learns the additional depth regression task. However, utilizing the geometric constraint improves the 2D joint accuracy as well. This indicates that our network is able to propagate this geometric constraint from the 3D module to the 2D module, which justifies the design goal of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future Work and Conclusions</head><p>In this paper, we introduced an end-to-end system that combines 2D pose labels in the wild and 3D pose labels in restricted environments for the challenge problem of 3D human pose estimation in the wild. In the future, we plan to explore more un-/weakly-supervised constraints for a better transfer, e.g., a domain alignment network as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. We hope this work can inspire more works on un-/weaklysupervised transfer learning and on 3D human pose estimation in the wild.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A schematic illustration of our method: transferring 3D annotation from indoor images to in-the-wild images. Top (Training): Both indoor images with 3D annotation (Right) and in-thewild images with 2D annotation (Left) are used to train the deep neural network. Bottom (Testing): The learned network can predict the 3D pose of the human in in-the-wild images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results of Human3.6M Dataset. The numbers are mean Euclidean distance(mm) between the ground-truth 3D joints and the estimations of different methods.</figDesc><table><row><cell></cell><cell>89.87</cell><cell>97.57</cell><cell cols="5">89.98 107.87 107.31 139.17 93.56</cell><cell>136.09</cell></row><row><cell>Tome et al. [26]</cell><cell>64.98</cell><cell>73.47</cell><cell>76.82</cell><cell>86.43</cell><cell cols="3">86.28 110.67 68.93</cell><cell>74.79</cell></row><row><cell>Zhou et al. [35]</cell><cell>87.36</cell><cell>109.31</cell><cell cols="5">87.05 103.16 116.18 143.32 106.88</cell><cell>99.78</cell></row><row><cell>Metha et al. [15]</cell><cell>59.69</cell><cell>69.74</cell><cell>60.55</cell><cell>68.77</cell><cell>76.36</cell><cell>85.42</cell><cell>59.05</cell><cell>75.04</cell></row><row><cell>Pavlakos et al. [19]</cell><cell>58.55</cell><cell>64.56</cell><cell>63.66</cell><cell>62.43</cell><cell>66.93</cell><cell>70.74</cell><cell>57.72</cell><cell>62.51</cell></row><row><cell>3D/wo geo</cell><cell>73.25</cell><cell>79.17</cell><cell>72.35</cell><cell>83.90</cell><cell>80.25</cell><cell>81.86</cell><cell>69.77</cell><cell>72.74</cell></row><row><cell>3D/w geo</cell><cell>72.29</cell><cell>77.15</cell><cell>72.60</cell><cell>81.08</cell><cell>80.81</cell><cell>77.38</cell><cell>68.30</cell><cell>72.85</cell></row><row><cell>3D+2D/wo geo</cell><cell>55.17</cell><cell>61.16</cell><cell>58.12</cell><cell>71.75</cell><cell>62.54</cell><cell>67.29</cell><cell>54.81</cell><cell>56.38</cell></row><row><cell>3D+2D/w geo</cell><cell>54.82</cell><cell>60.70</cell><cell>58.22</cell><cell>71.41</cell><cell>62.03</cell><cell>65.53</cell><cell>53.83</cell><cell>55.58</cell></row><row><cell></cell><cell cols="8">Sitting SittingDown Smoking Waiting WalkDog Walking WalkPair Average</cell></row><row><cell cols="2">Chen &amp; Ramanan [5] 133.14</cell><cell>240.12</cell><cell cols="2">106.65 106.21</cell><cell cols="3">87.03 114.05 90.55</cell><cell>114.18</cell></row><row><cell>Tome et al. [26]</cell><cell>110.19</cell><cell>172.91</cell><cell>84.95</cell><cell>85.78</cell><cell>86.26</cell><cell>71.36</cell><cell>73.14</cell><cell>88.39</cell></row><row><cell>Zhou et al. [35]</cell><cell>124.52</cell><cell>199.23</cell><cell cols="4">107.42 118.09 114.23 79.39</cell><cell>97.70</cell><cell>79.9</cell></row><row><cell>Metha et al. [15]</cell><cell>96.19</cell><cell>122.92</cell><cell>70.82</cell><cell>68.45</cell><cell>54.41</cell><cell>82.03</cell><cell>59.79</cell><cell>74.14</cell></row><row><cell>Pavlakos et al. [19]</cell><cell>76.84</cell><cell>103.48</cell><cell>65.73</cell><cell>61.56</cell><cell>67.55</cell><cell>56.38</cell><cell>59.47</cell><cell>66.92</cell></row><row><cell>3D/wo geo</cell><cell>98.41</cell><cell>141.60</cell><cell>80.01</cell><cell>86.31</cell><cell>61.89</cell><cell>76.32</cell><cell>71.47</cell><cell>82.44</cell></row><row><cell>3D/w geo</cell><cell>93.52</cell><cell>131.75</cell><cell>79.61</cell><cell>85.10</cell><cell>67.49</cell><cell>76.95</cell><cell>71.99</cell><cell>80.98</cell></row><row><cell>3D+2D/wo geo</cell><cell>74.79</cell><cell>113.99</cell><cell>64.34</cell><cell>68.78</cell><cell>52.22</cell><cell>63.97</cell><cell>57.31</cell><cell>65.69</cell></row><row><cell>3D+2D/w geo</cell><cell>75.20</cell><cell>111.59</cell><cell>64.15</cell><cell>66.05</cell><cell>51.43</cell><cell>63.22</cell><cell>55.33</cell><cell>64.90</cell></row><row><cell cols="3">3D/wo geo 3D/w geo 3D+2D/wo geo 3D+2D/w geo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>90.01% 90.57%</cell><cell>90.93%</cell><cell>91.62%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 2. 2D pose accuracy (PCKh@0.5) on Human 3.6M dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of MPI-INF-3DHP Dataset by scene. GS indicates green screen background. The results are shown in PCK and AUC.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Studio GS Studio no GS Outdoor ALL PCK AUC</cell></row><row><cell cols="3">Metha et al.(H36M+MPII) [15]</cell><cell>70.8</cell><cell>62.3</cell><cell>58.8</cell><cell>64.7</cell><cell>31.7</cell></row><row><cell cols="2">3D/wo geo</cell><cell></cell><cell>34.4</cell><cell>40.8</cell><cell>13.6</cell><cell>31.5</cell><cell>18.0</cell></row><row><cell cols="2">3D/w geo</cell><cell></cell><cell>45.6</cell><cell>45.1</cell><cell>14.4</cell><cell>37.7</cell><cell>20.9</cell></row><row><cell cols="2">3D+2D/wo geo</cell><cell></cell><cell>68.8</cell><cell>61.2</cell><cell>67.5</cell><cell>65.8</cell><cell>32.1</cell></row><row><cell cols="2">3D+2D/w geo</cell><cell></cell><cell>71.1</cell><cell>64.7</cell><cell>72.7</cell><cell>69.2</cell><cell>32.5</cell></row><row><cell cols="3">Metha et al.(MPI-INF-3DHP) [15]</cell><cell>84.1</cell><cell>68.9</cell><cell>59.6</cell><cell>72.5</cell><cell>36.9</cell></row><row><cell></cell><cell cols="2">3D+2D/wo geo 3D+2D/w geo</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upper arm</cell><cell>42.4mm</cell><cell>37.8mm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lower arm</cell><cell>60.4mm</cell><cell>50.7mm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upper leg</cell><cell>43.5mm</cell><cell>43.4mm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lower leg</cell><cell>59.4mm</cell><cell>47.8mm</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upper arm</cell><cell>6.27px</cell><cell>4.80px</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lower arm</cell><cell>10.11px</cell><cell>6.64px</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upper leg</cell><cell>6.89px</cell><cell>4.93px</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lower leg</cell><cell>8.03px</cell><cell>6.22px</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 4. Evaluation of left-right Symmetry of with and without</cell><cell></cell><cell></cell></row><row><cell cols="4">constraint on MPI-INF-3DHP(Up) and MPII-Validation set (Bot-</cell><cell></cell><cell></cell></row><row><cell cols="4">tom). Results shown in average L1 distance between left and right</cell><cell></cell><cell></cell></row><row><cell cols="2">bone in mm/3D pixels, respectively</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Dushyant Mehta and Helge Rhodin for helping about evaluating on MPI-INF-3DHP dataset and thank Danlu Chen for help with <ref type="figure">Fig. 2</ref>. Also, we thank Wei Zhang for helpful discussion. This work is supported in part by the National Natural Science Foundation of China (#U1611461, #61572138), Shanghai Municipal Science and Technology Commission (#16JC1420401).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06524</idno>
		<title level="m">3d human pose esti-mation= 2d pose estimation+ matching</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation using transfer learning and improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09813</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A survey of computer vision-based human motion capture. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="231" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1796" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07828</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08985</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3108" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">3d human pose estimation: A review of the literature and analysis of covariates. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05180</idno>
		<title level="m">Structured prediction of 3d human pose with deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lifting from the deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00295</idno>
	</analytic>
	<monogr>
		<title level="m">Convolutional 3d pose estimation from a single image</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d shape estimation from 2d landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Workshops</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02354</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
