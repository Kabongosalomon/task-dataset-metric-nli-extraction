<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Learning with Deep and Handcrafted Features for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
							<email>georgesculily@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Novustech Services</orgName>
								<address>
									<addrLine>12B Aleea Ilioara</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<email>raducu.ionescu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>21D Mircea Vodȃ</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
							<email>popescunmarius@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>21D Mircea Vodȃ</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Local Learning with Deep and Handcrafted Features for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach that combines automatic features learned by convolutional neural networks (CNN) and handcrafted features computed by the bag-of-visual-words (BOVW) model in order to achieve state-of-the-art results in facial expression recognition. To obtain automatic features, we experiment with multiple CNN architectures, pretrained models and training procedures, e.g. Dense-Sparse-Dense. After fusing the two types of features, we employ a local learning framework to predict the class label for each test image. The local learning framework is based on three steps. First, a k-nearest neighbors model is applied in order to select the nearest training samples for an input test image. Second, a one-versus-all Support Vector Machines (SVM) classifier is trained on the selected training samples. Finally, the SVM classifier is used to predict the class label only for the test image it was trained for. Although we have used local learning in combination with handcrafted features in our previous work, to the best of our knowledge, local learning has never been employed in combination with deep features. The experiments on the 2013 Facial Expression Recognition (FER) Challenge data set, the FER+ data set and the AffectNet data set demonstrate that our approach achieves state-of-the-art results. With a top accuracy of 75.42% on FER 2013, 87.76% on the FER+, 59.58% on AffectNet 8-way classification and 63.31% on AffectNet 7-way classification, we surpass the state-of-theart methods by more than 1% on all data sets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic facial expression recognition is an active research topic in computer vision, having many applications including human behavior understanding, detection of mental disorders, human-computer interaction, among others.</p><p>In the past few years, most works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> have focused on building and training deep neural networks in order to achieve state-of-the-art results. Engineered models based on handcrafted features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> have drawn very little attention, since such models usually yield less accurate results compared to deep learning models. In this paper, we show that we can surpass the current state-of-the-art systems by combining automatic features learned by convolutional neural networks (CNN) and handcrafted features computed by the bag-of-visual-words (BOVW) model, especially when we employ local learning in the training phase. In order to obtain automatic features, we experiment with multiple CNN architectures, such as VGG-face <ref type="bibr" target="#b28">[29]</ref>, VGG-f <ref type="bibr" target="#b4">[5]</ref> and VGG-13 <ref type="bibr" target="#b1">[2]</ref>, some of which are pre-trained on other computer vision tasks such as object class recognition <ref type="bibr" target="#b30">[31]</ref> or face recognition <ref type="bibr" target="#b28">[29]</ref>. We also fine-tune these CNN models using a novel training procedure known as Dense-Sparse-Dense (DSD) <ref type="bibr" target="#b11">[12]</ref>. To our knowledge, we are the first to successfully apply DSD to train CNN models for facial expression recognition. In order to obtain handcrafted features, we use a standard BOVW model, which is based on a variant of dense Scale-Invariant Feature Transform (SIFT) features <ref type="bibr" target="#b24">[25]</ref> extracted at multiple scales, known as Pyramid Histogram of Visual Words (PHOW) <ref type="bibr" target="#b2">[3]</ref>. We use automatic and handcrafted features both independently and together. For the independent models, we use either softmax (for the fine-tuned CNN models) or Support Vector Machines (SVM) based on the one-versus-all scheme. For the combined models, the one-versus-all SVM is used both as a global learning method (trained on all training samples) or as a local learning method (trained on a subset of training samples, selected specifically for each test sample using a nearest neighbors scheme). We combine the automatic and handcrafted features by concatenating the corresponding feature vectors, before the learning stage. For the combined models, we explore only global or local SVM alternatives. We perform a thorough experimental study on the 2013 Facial Expression Recognition (FER) Challenge data set <ref type="bibr" target="#b10">[11]</ref>, the FER+ data set <ref type="bibr" target="#b1">[2]</ref>, and the Affect-Net <ref type="bibr" target="#b26">[27]</ref> data set, comparing our combined deep and handcrafted models with recent and relevant state-of-the-art approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. We report top results on each and every data set with our combination of automatic and handcrafted features, especially when local SVM is employed in the learning phase. With a top accuracy of 75.42% on the FER 2013 data set, we surpass the state-of-the-art accuracy <ref type="bibr" target="#b5">[6]</ref> by 2.02%. We also surpass the best method <ref type="bibr" target="#b1">[2]</ref> on the FER+ data set by 2.77%, reaching the best accuracy of 87.76%. The evaluation on AffectNet is typically conducted using 8-way classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40]</ref> or 7-way classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref> (the class corresponding to contempt being removed). We attain the best results in both settings, surpassing Mollahosseini et al. <ref type="bibr" target="#b26">[27]</ref> by 1.58% in the 8-way classification task, and Hua et al. <ref type="bibr" target="#b13">[14]</ref> by 1.20% in the 7-way classification task. We also include ablation results in the paper, which indicate that the proposed model combination yields superior performance compare to each and every component.</p><p>Although automatic and handcrafted features have been combined before in the context of facial expression recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>, different from the related art, (1) we include various CNN architectures and a single handcrafted model, and (2) we employ a local learning strategy that leads to superior results. To the best of our knowledge, our previous work based on the BOVW model <ref type="bibr" target="#b14">[15]</ref> is the only one to explore local learning for facial expression recognition. In this paper, we extend our previous work <ref type="bibr" target="#b14">[15]</ref> and propose to combine local learning with automatic features learned by deep CNN models. Compared to the best accuracy reported in <ref type="bibr" target="#b14">[15]</ref> for FER 2013, which is 67.48%, we report an improvement of almost 8%. In summary, our contributions consist of (i) successfully training CNN models for facial expression recognition using Dense-Sparse-Dense <ref type="bibr" target="#b11">[12]</ref>, (ii) successfully combining automatic and handcrafted features with local learning, (iii) conducting an extensive empirical evaluation with various deep, engineered and combined facial expression recognition models, and (iv) reporting stateof-the-art results on three benchmark data sets.</p><p>The rest of the paper is organized as follows. We present recent related art in Section 2. We describe the automatic and handcrafted features, as well as the learning methods, in Section 3. We present the experiments on facial expression recognition in Section 4. Finally, we draw our conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The early works on facial expression recognition are mostly based on handcrafted features <ref type="bibr" target="#b34">[35]</ref>. After the success of the AlexNet <ref type="bibr" target="#b17">[18]</ref> deep neural network in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b30">[31]</ref>, deep learning has been widely adopted in the computer vision community. Perhaps some of the first works to propose deep learning approaches for facial expression recognition were presented at the 2013 Facial Expression Recognition (FER) Challenge <ref type="bibr" target="#b10">[11]</ref>. Interestingly, the top scoring system in the 2013 FER Challenge is a deep convolutional neural network <ref type="bibr" target="#b33">[34]</ref>, while the best handcrafted model ranked only on the fourth place <ref type="bibr" target="#b14">[15]</ref>. With only a few exceptions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, most of the recent works on facial expression recognition are based on deep learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Some of these recent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> proposed to train an ensemble of convolutional neural networks for improved performance, while others <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> combined deep features with handcrafted features such as SIFT <ref type="bibr" target="#b24">[25]</ref> or Histograms of Oriented Gradients (HOG) <ref type="bibr" target="#b7">[8]</ref>. While most works studied facial expression recognition from static images, some works tackled facial expression recognition in video <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>. Hasani et al. <ref type="bibr" target="#b12">[13]</ref> proposed a network architecture that consists of 3D convolutional layers followed by a Long-Short Term Memory (LSTM) network that together extract the spatial relations within facial images and the temporal relations between different frames in the video. Different from other approaches, Meng et al. <ref type="bibr" target="#b25">[26]</ref> and Liu et al. <ref type="bibr" target="#b23">[24]</ref> presented identity-aware facial expression recognition models. Meng et al. <ref type="bibr" target="#b25">[26]</ref> proposed to jointly estimate expression and identity features through a neural architecture composed of two identical CNN streams, in order to alleviate inter-subject variations introduced by personal attributes and to achieve better facial expression recognition performance. Liu et al. <ref type="bibr" target="#b23">[24]</ref> employed deep metric learning and jointly optimized a deep metric loss and the softmax loss. They obtained an identity-invariant model by using an identity-aware hard-negative mining and online positive mining scheme. Li et al. <ref type="bibr" target="#b21">[22]</ref> trained a CNN model using a modified back-propagation algorithm which creates a locality preserving loss aiming to pull the locally neighboring faces of the same class together. Li et al. <ref type="bibr" target="#b22">[23]</ref> proposed an end-to-end trainable Patch-Gated CNN that can automatically perceive occluded region of the face, making the recognition based on the visible regions. To find the visible regions of the face, their model decomposes an intermediate feature map into several patches according to the positions of related facial landmarks. Each patch is then reweighted by its importance, which is determined from the patch itself. Zeng et al. <ref type="bibr" target="#b39">[40]</ref> proposed a model that addresses the labeling inconsistencies across data sets. In their framework, images are tagged with multiple (pseudo) labels either provided by human annotators or predicted by learned models. Then, a facial expression recognition model is trained to fit the latent truth from the inconsistent pseudo-labels. Hua et al. <ref type="bibr" target="#b13">[14]</ref> proposed a deep learning algorithm consisting of three sub-networks of different depths. Each sub-network is based on an independently-trained CNN. Different from Hua et al. <ref type="bibr" target="#b13">[14]</ref>, we combine deep CNN features with handcrafted features and employ local learning.</p><p>Closer to our work are methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> that combine deep and handcrafted features or that employ local learning <ref type="bibr" target="#b14">[15]</ref> for facial expression recognition. While Ionescu et al. <ref type="bibr" target="#b14">[15]</ref> used local learning to improve the performance of a handcrafted model, we show that local learning can also improve performance when deep features are used in combination with handcrafted features. Remarkably, our top accuracy is almost 8% better than the accuracy reported in <ref type="bibr" target="#b14">[15]</ref>. Works that combine deep and handcrafted features usually employ a single CNN model and various handcrafted features, e.g. Connie et al. <ref type="bibr" target="#b5">[6]</ref> employed SIFT and dense SIFT, while Kaya et al. <ref type="bibr" target="#b15">[16]</ref> employed SIFT, HOG and Local Gabor Binary Patterns (LGBP). On the other hand, we employ a single type of handcrafted features and we include various CNN architectures in the combination. Another important difference from works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> that combine deep and handcrafted features is that we employ local learning in the training stage. With these key changes, the empirical results indicate that our approach achieves better performance than the approach of Connie et al. <ref type="bibr" target="#b5">[6]</ref>. We do not compare with Kaya et al. <ref type="bibr" target="#b15">[16]</ref>, since their approach is designed to work on video. Hence, they do not report results on static image data sets. In future work, we aim to extend our approach for video, which will enable a direct comparison to Kaya et al. <ref type="bibr" target="#b15">[16]</ref>. Nevertheless, our experiments on static images indicate that the proposed model combination achieves superior results than various state-ofthe-art approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep Models</head><p>We employ three CNN models in this work, namely VGG-face <ref type="bibr" target="#b28">[29]</ref>, VGG-f <ref type="bibr" target="#b4">[5]</ref> and VGG-13 <ref type="bibr" target="#b1">[2]</ref>. Among these three models, only VGG-13 is trained from scratch. For the other two CNN models, we use pre-trained as well as fine-tuned versions. In order to train or fine-tune the models, we use stochastic gradient descent using mini-batches of 512 images and the momentum rate set to 0.9. All models are trained using data augmentation, which is based on including horizontally flipped images. To prevent overfitting, we employ Dense-Spare-Dense (DSD) training <ref type="bibr" target="#b11">[12]</ref> to train our CNN models. The training starts with a dense phase, in which the network is trained as usual. When switching to the sparse phase, the weights that have lower absolute values are replaced by zeros after every epoch. A sparsity threshold is used to determine the percentage of weights that are replaced by zeros. The DSD learning process, typically ends with a dense phase. It is important to note that DSD can be applied several times in order to achieve the desired performance. VGG-face. With 16 layers, VGG-face <ref type="bibr" target="#b28">[29]</ref> is the deepest network that we fine-tune. Since VGG-face is pre-trained on a closely related task (face recognition), we freeze the weights in the convolutional (conv) layers and we train only the fully-connected (fc) layers to adapt the network for our task (facial expression recognition). We replace the softmax layer of 1000 units with a softmax layer of 7 or 8 units, depending on the data set, e.g. FER 2013 <ref type="bibr" target="#b10">[11]</ref> contains 7 classes of emotion, while FER+ <ref type="bibr" target="#b1">[2]</ref> contains 8 classes of emotion. We randomly initialize the weights in this layer, using a Gaussian distribution with zero mean and 0.1 standard deviation. We add a dropout layer after the first fc layer, with the dropout rate set to 0.7. We set the learning rate to 10 −4 and we decrease it by a factor of 10 when the validation error stagnates for more than 10 epochs. We finetune VGG-face using DSD training <ref type="bibr" target="#b11">[12]</ref>. We train the network for 200 epochs in a first dense phase. We then switch to a sparse phase and we carry on training for another 50 epochs, with the sparsity rate set to 0.6 for all fc layers. In the second dense phase, we train the network for 50 epochs. We train the network for another 50 epochs during a second sparse phase, without changing the sparsity rate. Finally, we train the network for another 50 epochs during a third dense phase. In total, the network is trained for 400 epochs. VGG-f. We also fine-tune the VGG-f <ref type="bibr" target="#b4">[5]</ref> network with 8 layers, which is pre-trained on ILSVRC <ref type="bibr" target="#b30">[31]</ref>. Since VGG-f is pre-trained on a distantly related task (object class recognition), we fine-tune all of its layers. We set the learning rate to 10 −4 and we decrease it by a factor of 10 when the validation error stagnates for more than 10 epochs. In the end, the learning rate drops to 10 −5 . After each fc layer, we add a dropout layer with the dropout rate set to 0.5. We also add dropout layers after the last two conv layers, setting their dropout rates to 0.35. In total, there are four dropout layers. As for VGG-face, we use the DSD training method to finetune the VGG-f model. However, we refrain from pruning the weights of the first two conv layers during the sparse phases, since pruning these layers has a higher negative impact on the validation accuracy of the network, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Based on the sensitivity analysis presented in <ref type="figure" target="#fig_0">Figure 1</ref>, we choose, for each layer, the highest sparsity rate in the set {0.3, 0.4, 0.5, 0.6} that does not affect the validation accuracy by more than 0.5%. We train this network for a total of 600 epochs using DSD. We start with a dense phase of 300 epochs, then we alternate between sparse and dense phases, each phase lasting for 50 epochs. VGG-13. The VGG-13 architecture was specifically designed by Barsoum et al. <ref type="bibr" target="#b1">[2]</ref> for the FER+ data set. Since the images in FER 2013 are of the same size, we consider that VGG-13 is an excellent choice for FER 2013 as well.  Although the model is not particularly adapted for the Af-fectNet data set <ref type="bibr" target="#b26">[27]</ref>, which contains larger images, we keep the same architecture for all data sets. The weights are randomly initialized, by drawing them from a Gaussian distribution with zero mean and 0.01 standard deviation. We use the same dropout rates as in the original paper <ref type="bibr" target="#b1">[2]</ref>. We set the initial learning rate to 10 −2.5 and we decrease it by a factor of 10 0.5 whenever the validation error stops decreasing. The last learning rate that we use is 10 −4 . We train VGG-13 for 100 epochs using dense training. We then switch to a sparse phase that lasts for 50 epochs, with the sparsity rate set to 0.1. The training ends with a second dense phase that lasts for 50 epochs. In total, the VGG-13 network is trained for 200 epochs. It is important to mention that, different from Barsoum et al. <ref type="bibr" target="#b1">[2]</ref>, we use the softmax loss instead of probabilistic label drawing to train VGG-13. Hence, the results of the individual VGG-13 model are slightly different than those reported in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Handcrafted Model</head><p>The BOVW model proposed for facial expression recognition is divided in two pipelines, one for training and one for testing. In the training pipeline, we build the feature representation by extracting dense SIFT descriptors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref> from all training images, and by later quantizing the extracted descriptors into visual words using k-means clustering <ref type="bibr" target="#b19">[20]</ref>. The visual words are then stored in a randomized forest of k-d trees <ref type="bibr" target="#b29">[30]</ref> to reduce search cost. After build- ing the vocabulary of visual words, the training and testing pipelines become equivalent. For each image in the training or testing sets, we record the presence or absence of each visual word in a binary feature vector. The standard BOVW model described so far ignores spatial relationships among visual words, but we can achieve better performance by including spatial information. Perhaps the most popular and straightforward approach to include spatial information is the spatial pyramid <ref type="bibr" target="#b18">[19]</ref>. Our spatial pyramid representation is obtained by dividing the image into increasingly fine sub-regions (bins) and by computing the binary feature vector corresponding to each bin. The final representation is a concatenation of all binary feature vectors. It is reasonable to think that dividing an image representing a face into bins is a good choice, since most features, such as the contraction of the muscles at the corner of the eyes, are only visible in a certain region of the face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Fusion and Learning</head><p>Model fusion. We combine the deep and handcrafted models before the learning stage, by concatenating the corre-sponding features. To extract deep features from the pretrained or fine-tuned CNN models, we remove the softmax classification layer and we consider the activation map of last remaining fc layer as the deep feature vector corresponding to the image provided as input to the network. The deep feature vectors are normalized using the L 2 -norm. The bag-of-visual-words representation is the only kind of handcrafted features that we employ. The BOVW feature vectors are also normalized using the L 2 -norm. Our full processing pipeline is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Global learning. We employ the linear Support Vector Machines (SVM) <ref type="bibr" target="#b6">[7]</ref> to learn a discriminative model based on all training examples. SVM is a binary classifier that tries to find the vector of weights and the bias term that define the hyperplane which maximally separates the feature vectors of the training examples belonging to the two classes. To extend the linear SVM classifier to our multi-class facial expression recognition problem, we employ the one-versusall scheme. Local learning. Local learning methods attempt to locally adjust the performance of the training system to the prop-erties of the training set, in each area of the input space. A local learning algorithm essentially works by (i) selecting a few training samples located in the vicinity of a given test sample, then by (ii) training a classifier with only these few examples and finally, by (iii) applying the classifier to predict the class label of the test sample.</p><p>It is interesting to note that the k-nearest neighbors (k-NN) model can be included in the family of local learning algorithms. Actually, the k-NN model is the simplest formulation of local learning, since the discriminant function is constant (there is no learning involved). What is even more interesting, however, is that almost any other classifier can be employed in the local learning paradigm. In our case, we employ the linear SVM classifier for the local classification problem.</p><p>It is important to mention that besides the classifier, a similarity or distance measure is also required to determine the neighbors located in the vicinity of a test sample. In our case, we use the cosine similarity.</p><p>An interesting remark is that a linear classifier, such as SVM, put in the local learning framework, becomes nonlinear, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In the standard approach, a single linear classifier trained at the global level (on the entire train set) produces a linear discriminative function. On the other hand, the discriminative function for a set of test samples is no longer linear in the local learning framework, since each prediction is given by a different linear classifier which is specifically trained for a single test sample. Moreover, the discriminative function cannot be determined without having the test samples beforehand, yet the local learning paradigm is able to rectify some limitations of linear classifiers, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Local learning has a few advantages over standard learning methods. First, it divides a hard classification problem into multiple simple sub-problems. Second, it reduces the variety of samples in the training set, by selecting the samples that are most similar to the test sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Sets</head><p>We conduct experiments on the FER 2013 <ref type="bibr" target="#b10">[11]</ref>, the FER+ <ref type="bibr" target="#b1">[2]</ref> and the AffectNet <ref type="bibr" target="#b26">[27]</ref> data sets. AffectNet. The AffectNet <ref type="bibr" target="#b26">[27]</ref> data set contains 287651 training images and 4000 validation images, which are manually annotated. Since the test set is not publicly available, researchers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40]</ref> evaluate their approaches on the validation set containing 500 images for each of the following 8 emotion classes: anger, contempt, disgust, fear, happiness, neutral, sadness, surprise. As the facial expression recognition task typically includes only 7 emotion classes (contempt is excluded), some works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref> report results on 3500 validation images from AffectNet, by removing the 500 images labeled with the contempt emotion. We evaluate our approach in both 8-way and 7-way classification settings, in order to provide a comprehensive comparison with related works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The input images are scaled to 224×224 pixels for VGGface and VGG-f, and to 64 × 64 pixels for VGG-13. We use the MatConvNet <ref type="bibr" target="#b36">[37]</ref> library to train the CNN models. To implement the BOVW model, we use functions from the VLFeat <ref type="bibr" target="#b35">[36]</ref> library. To generate the spatial pyramid representation for the BOVW model, we divide the images into 1 × 1, 2 × 2, 3 × 3 and 4 × 4 bins. At each level of the pyramid, we use vocabularies of 17000, 14000, 11000 and 8000 words, respectively. In the training phase, we employ the SVM implementation from LibSVM <ref type="bibr" target="#b3">[4]</ref>. We set the regularization parameter of SVM to C = 1 for individual models and to C = 100 for combined models. We employ the linear kernel, which does not require any additional parameters. In the local learning approach, we employ the cosine similarity to choose the nearest neighbors. We select 200 neighbors for training the local SVM. All parameters are tuned on the validation sets from FER 2013 and FER+. We transfer the parameter values to AffectNet, without further tuning.</p><p>To train our deep or handcrafted models on Affect-Net, we adopt the down-sampling setting proposed in <ref type="bibr" target="#b26">[27]</ref>, which solves, to some extent, the imbalanced nature of the facial expression recognition task. As Mollahosseini et al. <ref type="bibr" target="#b26">[27]</ref>, we select at most 15000 samples from each class. This leaves us with a training set of 88021 images.</p><p>We use the same model combination on all data sets. The proposed combination includes the BOVW representation and the deep features extracted with pre-trained VGG-face, fine-tuned VGG-face, fine-tuned VGG-f and VGG-13. The combination is obtained by concatenating the corresponding features. <ref type="table">Table 1</ref> includes the results of our combined models, one based on global SVM and another based on local SVM, on <ref type="table">Table 1</ref>: Results on the FER 2013 <ref type="bibr" target="#b10">[11]</ref>, the FER+ <ref type="bibr" target="#b1">[2]</ref> and the AffectNet <ref type="bibr" target="#b26">[27]</ref> data sets. Our combination based on pre-trained, fine-tuned and handcrafted models, with and without data augmentation (aug.), are compared with several state-of-the-art approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, which are listed in temporal order. The best result on each data set is highlighted in bold. <ref type="bibr">Model</ref> FER FER FER+ FER+ AffectNet AffectNet AffectNet AffectNet (aug.) (aug.) 8-way 8-way (aug.) 7-way 7-way (aug.) Ionescu et al. <ref type="bibr" target="#b14">[15]</ref> 67.48%  <ref type="figure">Figure 4</ref>: FER 2013 test images that are incorrectly predicted by the global SVM based on our combination of deep and handcrafted features, but are correctly predicted by the local SVM based on the same feature combination. three data sets: FER 2013, FER+ and AffectNet. We report results with and without data augmentation. Our models are compared with several state-of-the-art approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><formula xml:id="formula_0">- - - - - - - Tang [34] - 71.16% - - - - - - Yu et al. [39] - 72.00% - - - - - - Kim et al. [17] - 72.72% - - - - - - Barsoum et al. [2] - - - 84.99% - - - - Li et al. [21] - 70.66% - - - - - - Connie et al. [6] - 73.40% - - - - - - Mollahosseini et al. [27] - - - - - 58.00% - - Li et al. [23] - - - - - - 55.</formula><p>Results on FER 2013. With our combination of features, the local SVM classifier achieves an accuracy rate of 74.92% when the FER 2013 training set is not augmented with flipped images, and an accuracy rate of 75.42% when the training set is augmented with flipped images. In the latter case, the difference from the global SVM is 2.17%. We consider that the trade-off between accuracy and speed is acceptable, given that the local SVM finds the nearest neighbors and predicts the test labels in 40.28 seconds for all 3589 test images, while the global SVM predicts the labels in 23.93 seconds. The reported running times are measured on a computer with Intel Xeon 2.20 GHz Processor and 256 GB of RAM, using a single thread. <ref type="figure">Figure 4</ref> provides a handful of test images that are incorrectly labeled by the global SVM, but correctly labeled by the local SVM. We also tried to determine if applying SVM locally (on the selected nearest neighbors) is indeed helpful in comparison with a k-NN model. The k-NN model yields an accuracy of 70.33% with the same number of neighbors (200). We thus conclude that the local SVM approach provides a considerable improvement over the k-NN model. Moreover, with a top accuracy of 75.42%, we surpass the accuracy of the state-of-the-art model <ref type="bibr" target="#b5">[6]</ref> by 2.02%.</p><p>Results on FER+. In both settings (with and without data augmentation), the local SVM approach yields better accuracy rates than the global SVM approach on FER+, but the differences are not as high as for FER 2013. Without data augmentation, our combination of deep and handcrafted features attains an accuracy of 86.68% when the global SVM is employed in the training phase, and an accuracy of 87.76% when the local SVM is used instead of the global SVM. The local SVM classifier attains an accuracy improvement of 1.08% over the global SVM. Data augmentation does not seem to help us gain any performance improvements on FER+, but it is important to note that the local SVM still attains better performance than the global SVM. In the end, we surpass the state-of-the-art method <ref type="bibr" target="#b1">[2]</ref> on the FER+ data set by 2.77%, reaching the best accuracy of 87.76% using the local SVM without data augmentation.</p><p>Results on AffectNet 8-way. First, we note that Molla- hosseini et al. <ref type="bibr" target="#b26">[27]</ref> attained better results using weightedloss (58.00%) instead of down-sampling (50.00%) the Af-fectNet training set. Although we use down-sampling to train our models, we compare our results with the better (weighted-loss) version of Mollahosseini et al. <ref type="bibr" target="#b26">[27]</ref>. We are able to surpass their approach by 1.58%, reaching an accuracy of 59.58% on AffectNet with our local SVM based on the combination of deep and handcrafted features. We note that the local SVM attains superior results compared to the global SVM, in both settings, i.e. with and without data augmentation.</p><p>Results on AffectNet 7-way. Some researchers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref> reported results on AffectNet, by excluding the 500 images labeled as contempt. We include a comparison with these works in <ref type="table">Table 1</ref>. When we do not use data augmentation, we notice that the global SVM outperforms the local SVM. However, the local SVM approach is better than the global SVM, when data augmentation is included. Our best result on the AffectNet 7-way classification task (63.31%) is obtained by the local SVM that includes data augmentation. Our accuracy is 1.20% higher than the state-of-the-art accuracy reported in <ref type="bibr" target="#b13">[14]</ref>. Results overview. The empirical results presented in <ref type="table">Table 1</ref> show that the local SVM model based on our combination of deep and handcrafted features achieves superior performance compared to several recent and related works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. We also note that the local SVM generally attains better performance than the global SVM, in all but one case, proving that the idea of using local learning is indeed useful. Overall, the results demonstrate that our method based on local SVM and a combination of deep and handcrafted features, achieves top performance on all three data sets: FER 2013, FER+ and AffectNet. <ref type="table" target="#tab_1">Table 2</ref> includes the results of our combined models, one based on global SVM and another based on local SVM, in comparison with each and every individual component, on three data sets: FER 2013, FER+ and AffectNet. BOVW. The accuracy rates of our BOVW model, which is based on global SVM, are generally lower than the accuracy rates of the deep CNN models. It seems that the BOVW model is able to surpass VGG-13 on AffectNet. However, we believe that this happens only because the VGG-13 architecture is not specifically adapted to the larger AffectNet images. VGG-face. Although the pre-trained VGG-face <ref type="bibr" target="#b28">[29]</ref> is trained on a rather complementary task, face recognition, it achieves fairly good accuracy rates, e.g. 81.73% on FER+. Fine-tuning the VGG-face model using data augmentation improves its accuracy rates on all data sets, usually by a very large margin (up to 9% over the pre-trained VGG-face). VGG-f. Since VGG-f <ref type="bibr" target="#b4">[5]</ref> is pre-trained on a distantlyrelated task, object class recognition, we do not consider it as a viable model to be included in our combination. However, the fine-tuned VGG-f model reaches respectable accuracy rates (see <ref type="table" target="#tab_1">Table 2</ref>), even surpassing the fine-tuned VGG-face model on FER+. We also note that our VGG-f model trained on AffectNet using down-sampling attains an accuracy of 56.03%, surpassing the AlexNet model of Mollahosseini et al. <ref type="bibr" target="#b26">[27]</ref> trained using down-sampling, which attains an accuracy of 50.00%. Although the two networks, VGG-f and AlexNet, have fairly similar architectures, we believe that the significant performance difference between these models is due to the DSD training procedure, which we applied for training all our CNN models, including VGG-f. VGG-13. The VGG-13 [2] model, which is trained from scratch, achieves an accuracy of 66.51% on FER 2013 and an accuracy of 84.41% on FER+. Since the input of the VGG-13 architecture is 64 × 64 pixels in size, it seems to be better suited to the FER 2013 or the FER+ data sets, both containing images of 48 × 48 pixels, compared to the VGGface or the VGG-f architectures, which take as input images of 224 × 224 pixels. However, its lower performance compared to VGG-face or VGG-f can be explained by the fact that the other CNN models have a better starting point, since they are pre-trained on related computer vision tasks. It is interesting to note that our own implementation of the VGG-13 architecture of Barsoum et al. <ref type="bibr" target="#b1">[2]</ref> attains an accuracy of 84.41% on FER+, which is 0.58% less than the accuracy reported in <ref type="bibr" target="#b1">[2]</ref>. We believe that this difference is a consequence of using the standard softmax loss function instead of probabilistic label drawing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Results</head><p>Ablation study overview. With an accuracy of 72.11%, the best individual model on FER 2013 is the fine-tuned VGG-face. We note that all models obtain much better results on FER+ than on FER 2013, indicating that the FER+ curation process conducted by Barsoum et al. <ref type="bibr" target="#b1">[2]</ref> was indeed helpful. Although the fine-tuned VGG-face obtains better results than the other fine-tuned networks on FER 2013, it seems that the shallower VGG-f reaches the best performance (86.01%) among individual models, when it is fine-tuned on FER+. As for FER 2013, the best individual model on AffectNet is the fine-tuned VGG-face. In the Af-fectNet 8-way classification task, it achieves an accuracy of 58.93%, and in the AffectNet 7-way classification task, it achieves an accuracy of 62.66%. Overall, the ablation results presented in <ref type="table" target="#tab_1">Table 2</ref> show that our model combination provides better results than each individual counterpart. We thus conclude that our combination of deep and handcrafted features is indeed necessary to improve performance over each component of the combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a state-of-the-art approach for facial expression recognition, which is based on combining deep and handcrafted features and on applying local learning in the training phase. With a top accuracy of 75.42% on FER 2013, a top accuracy of 87.76% on FER+, a top accuracy of 59.58% on AffectNet 8-way classification and a top accuracy of 63.31% on AffectNet 7-way classification, our approach is able to surpass the best methods on these data sets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In future work, we aim to evaluate our approach on additional data sets and adapt our method for video. We also consider training our approach to distinguish between voluntary (deceptive) and involuntary (natural) facial expressions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Validation accuracy rates of the VGG-f network, which is fine-tuned on FER 2013, after pruning the smaller weights on each individual layer using several sparsity rates between 30% and 60%. The baseline represents the accuracy of the fine-tuned model without pruning, i.e. having a sparsity rate of 0%. The layers closer to the input are more sensitive to pruning. Best viewed in color. a A global linear classifier misclassifies the test samples depicted in red.b Local learning models based on an underlying linear classifier are able correctly classify the test samples depicted in red. The grey area around each test sample represents the neighborhood of the respective test sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Two classification models are used to solve the same binary classification problem. The two test samples depicted in red are misclassified by a global linear classifier (left-hand side). The local learning framework produces a non-linear decision boundary that fixes this problem (right-hand side). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Our processing pipeline based on automatic features learned by convolutional neural networks (VGG-13, VGG-f and VGG-face) and handcrafted features computed by the bag-of-visual-words model. After feature vector concatenation and L 2 -normalization, we employ a local learning model. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FER 2013 .</head><label>2013</label><figDesc>The FER 2013 data set contains 28709 training images, 3589 validation (public test) images and another 3589 (private) test images. All images are of 48 × 48 pixels in size. The images belong to 7 classes of emotion: anger, disgust, fear, happiness, neutral, sadness, surprise. FER+. The FER+ data set is a curated version of FER 2013 in which some of the original images are relabeled, while other images, e.g. not containing faces, are completely removed. Interestingly, Barsoum et al. [2] add contempt as the eighth class of emotion. The FER+ data set contains 25045 training images, 3191 validation images and another 3137 test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation results on the FER 2013<ref type="bibr" target="#b10">[11]</ref>, the FER+<ref type="bibr" target="#b1">[2]</ref> and the AffectNet<ref type="bibr" target="#b26">[27]</ref> data sets. Our combination of deep and handcrafted models is compared with each individual component of the combination. Results are reported with and without data augmentation (aug.). The best result on each data set is highlighted in bold.</figDesc><table><row><cell>Model</cell><cell>FER</cell><cell>FER</cell><cell>FER+</cell><cell>FER+</cell><cell>AffectNet</cell><cell>AffectNet</cell><cell>AffectNet</cell><cell>AffectNet</cell></row><row><cell></cell><cell></cell><cell>(aug.)</cell><cell></cell><cell>(aug.)</cell><cell>8-way</cell><cell>8-way (aug.)</cell><cell>7-way</cell><cell>7-way (aug.)</cell></row><row><cell>BOVW</cell><cell cols="2">65.70% 66.23%</cell><cell cols="2">79.60% 80.65%</cell><cell>47.53%</cell><cell>48.30%</cell><cell>51.51%</cell><cell>52.29%</cell></row><row><cell>pre-trained VGG-face</cell><cell cols="2">65.65% 65.78%</cell><cell cols="2">81.54% 81.73%</cell><cell>49.28%</cell><cell>50.08%</cell><cell>54.14%</cell><cell>54.94%</cell></row><row><cell>fine-tuned VGG-face</cell><cell cols="2">71.50% 72.11%</cell><cell cols="2">84.35% 84.79%</cell><cell>58.77%</cell><cell>58.93%</cell><cell>62.54%</cell><cell>62.66%</cell></row><row><cell>fine-tuned VGG-f</cell><cell cols="2">69.38% 70.30%</cell><cell cols="2">85.72% 86.01%</cell><cell>55.85%</cell><cell>56.03%</cell><cell>60.40%</cell><cell>60.51%</cell></row><row><cell>VGG-13</cell><cell cols="2">66.31% 66.51%</cell><cell cols="2">84.38% 84.41%</cell><cell>40.50%</cell><cell>41.75%</cell><cell>44.60%</cell><cell>44.57%</cell></row><row><cell cols="3">CNNs and BOVW + global SVM 73.34% 73.25%</cell><cell cols="2">86.68% 86.96%</cell><cell>59.20%</cell><cell>59.30%</cell><cell>63.20%</cell><cell>62.91%</cell></row><row><cell>CNNs and BOVW + local SVM</cell><cell cols="4">74.92% 75.42% 87.76% 87.25%</cell><cell>59.45%</cell><cell>59.58%</cell><cell>62.94%</cell><cell>63.31%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is partially supported by Novustech Services through Project 115788 funded under the Competitiveness Operational Programme POC-46-2-2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving Bag-of-Visual-Words Towards Effective Facial Expressive Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Chanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caplier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VISIGRAPP</title>
		<meeting>VISIGRAPP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training deep networks for facial expression recognition with crowdsourced label distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMI</title>
		<meeting>ICMI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image Classification using Random Forests and Ferns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LibSVM: A Library for Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno>2:27:1-27:27</idno>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm.6" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Connie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Shabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Cheah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MIWAI</title>
		<meeting>MIWAI</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">10607</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support-Vector Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facenet2expnet: Regularizing a deep face recognition net for expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FG</title>
		<meeting>FG</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning approaches for facial emotion recognition: A case study on fer-2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giannopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Perikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hatzilygeroudis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Hybridization of Intelligent Methods</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Challenges in Representation Learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ramaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Athanasakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Romaszko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICONIP</title>
		<meeting>ICONIP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8228</biblScope>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DSD: Dense-Sparse-Dense Training for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Facial expression recognition using enhanced deep 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2278" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HERO: Human Emotions Recognition for Realizing Intelligent Internet of Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local Learning to Improve Bag of Visual Words Model for Facial Expression Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Challenges in Representation Learning</title>
		<meeting>ICML Workshop on Challenges in Representation Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition in the wild using deep transfer learning and score fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gürpınar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical committee of deep convolutional neural networks for robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="189" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<editor>P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MRMR-based ensemble pruning for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2584" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Patch-Gated CNN for occlusion-aware facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2209" to="2214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive deep metric learning for identity-aware facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identity-aware convolutional neural network for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FG</title>
		<meeting>FG</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Affect-Net: A Database for Facial Expression, Valence, and Arousal Computing in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facial expression recognition from World Wild Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1509" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facial expressions classification and false label reduction using LDA and threefold SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D dynamic facial expression recognition using low-resolution videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="162" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Learning using Linear Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Challenges in Representation Learning</title>
		<meeting>ICML Workshop on Challenges in Representation Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Face Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="487" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">VLFeat: An Open and Portable Library of Computer Vision Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MatConvNet -Convolutional Neural Networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACMMM</title>
		<meeting>eeding of ACMMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ensemble of deep neural networks with probability-based fusion for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICMI</title>
		<meeting>ICMI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facial expression recognition with inconsistently annotated datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="222" to="237" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
