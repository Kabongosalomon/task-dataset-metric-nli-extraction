<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Topic Modeling with Bidirectional Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="department" key="dep3">§Department of Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">‡AI Labs -Didi Chuxing Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemeng</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="department" key="dep3">§Department of Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">‡AI Labs -Didi Chuxing Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="department" key="dep3">§Department of Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">‡AI Labs -Didi Chuxing Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
							<email>yulan.he@warwick.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="department" key="dep3">§Department of Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">‡AI Labs -Didi Chuxing Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Ye</surname></persName>
							<email>chenchenye@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="department" key="dep3">§Department of Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">‡AI Labs -Didi Chuxing Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
							<email>xuhaiyangsnow@didiglobal.com</email>
						</author>
						<title level="a" type="main">Neural Topic Modeling with Bidirectional Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a twoway projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models have been extensively explored in the Natural Language Processing (NLP) community for unsupervised knowledge discovery. Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b2">(Blei et al., 2003)</ref>, the * corresponding author most popular topic model, has been extended <ref type="bibr" target="#b18">(Lin and He, 2009;</ref><ref type="bibr" target="#b33">Zhou et al., 2014;</ref><ref type="bibr" target="#b4">Cheng et al., 2014)</ref> for various extraction tasks. Due to the difficulty of exact inference, most LDA variants require approximate inference methods, such as mean-field methods and collapsed Gibbs sampling. However, these approximate approaches have the drawback that small changes to the modeling assumptions result in a re-derivation of the inference algorithm, which can be mathematically arduous.</p><p>One possible way in addressing this limitation is through neural topic models which employ blackbox inference mechanism with neural networks. Inspired by variational autoencoder (VAE) <ref type="bibr" target="#b14">(Kingma and Welling, 2013)</ref>, <ref type="bibr" target="#b28">Srivastava and Sutton (2017)</ref> used the Logistic-Normal prior to mimic the simplex in latent topic space and proposed the Neural Variational LDA (NVLDA). Moreover, they replaced the word-level mixture in NVLDA with a weighted product of experts and proposed the ProdLDA <ref type="bibr" target="#b28">(Srivastava and Sutton, 2017)</ref> to further enhance the topic quality.</p><p>Although <ref type="bibr" target="#b28">Srivastava and Sutton (2017)</ref> used the Logistic-Normal distribution to approximate the Dirichlet distribution, they are not exactly the same. An illustration of these two distributions is shown in <ref type="figure" target="#fig_0">Figure 1</ref> in which the Logistic-Normal distribution does not exhibit multiple peaks at the vertices of the simplex as that in the Dirichlet distribution and as such, it is less capable to capture the multi-modality which is crucial in topic modeling <ref type="bibr" target="#b29">(Wallach et al., 2009)</ref>. To deal with the limitation, <ref type="bibr" target="#b30">Wang et al. (2019a)</ref> proposed the Adversarialneural Topic Model (ATM) based on adversarial training, it uses a generator network to capture the semantic patterns lying behind the documents. However, given a document, ATM is not able to infer the document-topic distribution which is useful for downstream applications, such as text clustering. Moreover, ATM take the bag-of-words assumption and do not utilize any word relatedness information captured in word embeddings which have been proved to be crucial for better performance in many NLP tasks <ref type="bibr" target="#b20">(Liu et al., 2018;</ref><ref type="bibr" target="#b17">Lei et al., 2018)</ref>.</p><p>To address these limitations, we model topics with Dirichlet prior and propose a novel Bidirectional Adversarial Topic model (BAT) based on bidirectional adversarial training. The proposed BAT employs a generator network to learn the projection function from randomly-sampled documenttopic distribution to document-word distribution. Moreover, an encoder network is used to learn the inverse projection, transforming a document-word distribution into a document-topic distribution. Different from traditional models that often resort to analytic approximations, BAT employs a discriminator which aims to discriminate between real distribution pair and fake distribution pair, thereby helps the networks (generator and encoder) to learn the two-way projections better. During the adversarial training phase, the supervision signal provided by the discriminator will guide the generator to construct a more realistic document and thus better capture the semantic patterns in text. Meanwhile, the encoder network is also guided to generate a more reasonable topic distribution conditioned on specific document-word distributions. Finally, to incorporate the word relatedness information captured by word embeddings, we extend the BAT by modeling each topic with a multivariate Gaussian in the generator and propose the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT).</p><p>The main contributions of the paper are:</p><p>• We propose a novel Bidirectional Adversarial Topic (BAT) model, which is, to our best knowledge, the first attempt of using bidirectional adversarial training in neural topic modeling;</p><p>• We extend BAT to incorporate the word re-latedness information into the modeling process and propose the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT);</p><p>• Experimental results on three public datasets show that BAT and Gaussian-BAT outperform the state-of-the-art approaches in terms of topic coherence measures. The effectiveness of BAT and Gaussian-BAT is further verified in text clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Our work is related to two lines of research, which are adversarial training and neural topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adversarial Training</head><p>Adversarial training, first employed in Generative Adversarial Network (GAN) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>, has been extensively studied from both theoretical and practical perspectives. Theoretically,  and <ref type="bibr" target="#b10">Gulrajani (2017)</ref> proposed the Wasserstein GAN which employed the Wasserstein distance between data distribution and generated distribution as the training objective. To address the limitation that most GANs <ref type="bibr" target="#b8">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b26">Radford et al., 2015)</ref> could not project data into a latent space, Bidirectional Generative Adversarial Nets (Bi-GAN) <ref type="bibr" target="#b5">(Donahue et al., 2016)</ref> and Adversarially Learned Inference (ALI) <ref type="bibr" target="#b6">(Dumoulin et al., 2016)</ref> were proposed.</p><p>Adversarial training has also been extensively used for text generation. For example, Seq-GAN <ref type="bibr" target="#b32">(Yu et al., 2017)</ref> incorporated a policy gradient strategy for text generation. RankGAN <ref type="bibr" target="#b19">(Lin et al., 2017)</ref> ranked a collection of human-written sentences to capture the language structure for improving the quality of text generation. To avoid mode collapse when dealing with discrete data, MaskGAN <ref type="bibr" target="#b7">(Fedus et al., 2018)</ref> used an actor-critic conditional GAN to fill in missing text conditioned on the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Topic Modeling</head><p>To overcome the challenging exact inference of topic models based on directed graph, a replicated softmax model (RSM), based on the Restricted Boltzmann Machines was proposed in <ref type="bibr" target="#b11">(Hinton and Salakhutdinov, 2009)</ref>. Inspired by VAE, <ref type="bibr" target="#b22">Miao et al. (2016)</ref> used the multivariate Gaussian as the prior distribution of latent space and proposed the fake distribution pair  <ref type="bibr" target="#b21">(Miao et al., 2017)</ref> which constructs the topic distribution using a Gaussian distribution followed by a softmax transformation was proposed based on the NVDM. Likewise, to deal with the inappropriate Gaussian prior of NVDM, <ref type="bibr" target="#b28">Srivastava and Sutton (2017)</ref> proposed the NVLDA which approximates the Dirichlet prior using a Logistic-Normal distribution. Recently, the Adversarial-neural Topic Model (ATM) <ref type="bibr" target="#b30">(Wang et al., 2019a)</ref> is proposed based on adversarial training, it models topics with Dirichlet prior which is able to capture the multi-modality compared with logistic-normal prior and obtains better topics. Besides, the Adversarial-neural Event (AEM) <ref type="bibr" target="#b31">(Wang et al., 2019b)</ref> model is also proposed for open event extraction by representing each event as an entity distribution, a location distribution, a keyword distribution and a date distribution.</p><formula xml:id="formula_0">S-dim d f Generator Network (G) Discriminator Network (D) D in D out µ f » Dir(μ f j®) (V+K)-dim</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Network(E)</head><p>Despite the extensive exploration of this research field, scarce work has been done to incorporate Dirichlet prior, word embeddings and bidirectional adversarial training into neural topic modeling. In this paper, we propose two novel topic modeling approaches, called BAT and Gaussian-BAT, which are different from existing approaches in the following aspects: (1) Unlike NVDM, GSM, NVLDA and ProdLDA which model latent topic with Gaussian or logistic-normal prior, BAT and Gaussian-BAT explicitly employ Dirichlet prior to model topics; (2) Unlike ATM which could not infer topic distribution of a given document, BAT and Gaussian-BAT uses a encoder to generate the topic distribution corresponding to the document; (3) Unlike neural topic models that only utilize word co-occurrence information, Gaussian-BAT models topic with multivariate Gaussian and incorporates the word relatedness into modeling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our proposed neural topic models are based on bidirectional adversarial training <ref type="bibr" target="#b5">(Donahue et al., 2016)</ref> and aim to learn the two-way non-linear projection between two high-dimensional distributions. In this section, we first introduce the Bidirectional Adversarial Topic (BAT) model that only employs the word co-occurrence information. Then, built on BAT, we model topics with multivariate Gaussian in the generator of BAT and propose the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT), which naturally incorporates word relatedness information captured in word embeddings into modeling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bidirectional Adversarial Topic model</head><p>As depicted in <ref type="figure">Figure 2</ref>, the proposed BAT consists of three components: (1) The Encoder E takes the V -dimensional document representation d r sampled from text corpus C as input and transforms it into the corresponding K-dimensional topic distribution θ r ; (2) The Generator G takes a random topic distribution θ f drawn from a Dirichlet prior as input and generates a V -dimensional fake word distribution d f ; (3) The Discriminator D takes the real distribution pair p r = [ θ r ; d r ] and fake distribution pair p f = [ θ f ; d f ] as input and discriminates the real distribution pairs from the fake ones. The outputs of the discriminator are used as supervision signals to learn E, G and D during adversarial training. In what follows, we describe each component in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Encoder Network</head><p>The encoder learns a mapping function to transform document-word distribution to document-topic distribution. As shown in the top-left panel of <ref type="figure">Figure 2</ref>, it contains a V -dimensional document-word distribution layer, an S-dimensional representation layer and a K-dimensional document-topic distribution layer, where V and K denote vocabulary size and topic number respectively.</p><p>More concretely, for each document d in text corpus, E takes the document representation d r as input, where d r is the representation weighted by TF-IDF, and it is calculated by:</p><formula xml:id="formula_1">tf i,d = n i,d v n v,d , idf i = log |C| |C i | tf -idf i,d = tf i,d * idf i , d i r = tf -idf i,d v tf -idf v,d</formula><p>where n i,d denotes the number of i-th word appeared in document d, |C| represents the number of documents in the corpus, and |C i | means the number of documents that contain i-th word in the corpus. Thus, each document could be represented as a V -dimensional multinomial distribution and the i-th dimension denotes the semantic consistency between i-th word and the document. With d r as input, E firstly projects it into an S-dimensional semantic space through the representation layer as follows:</p><formula xml:id="formula_2">h e s = BN(W e s d r + b e s ) (1) o e s = max( h e s , leak * h e s )<label>(2)</label></formula><p>where W e s ∈ R S×V and b e s are weight matrix and bias term of the representation layer, h e s is the state vector normalized by batch normalization BN(·), leak denotes the parameter of LeakyReLU activation and o e s represents the output of representation layer.</p><p>Then, the encoder transforms o e s into a Kdimensional topic space based on the equation below:</p><formula xml:id="formula_3">θ r = softmax(W e t o e s + b e t )<label>(3)</label></formula><p>where W e t ∈ R K×S is the weight matrix of topic distribution layer, b e t represents the bias term, θ r denotes the corresponding topic distribution of the input d r and the k-th (k ∈ {1, 2, ..., K}) dimension θ k r represents the proportion of k-th topic in document d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Generator network</head><p>The generator G is shown in the bottom-left panel of <ref type="figure">Figure 2</ref>. Contrary to encoder, it provides an inverse projection from document-topic distribution to document-word distribution and contains a K-dimensional document-topic layer, an S-dimensional representation layer and a Vdimensional document-word distribution layer.</p><p>As pointed out in <ref type="bibr" target="#b29">(Wallach et al., 2009)</ref>, the choice of Dirichlet prior over topic distribution is important to obtain interpretable topics. Thus, BAT employs the Dirichlet prior parameterized with α to mimic the multi-variate simplex over topic distribution θ f . It can be drawn randomly based on the equation below:</p><formula xml:id="formula_4">p( θ f | α) = Dir( θ f | α) 1 ∆( α) K k=1 θ k f α k −1 (4)</formula><p>where α is the K-dimensional hyper-parameter of Dirichlet prior, K is the topic number that should be set in BAT, θ k f ∈ [0, 1], follows the constrain that K k=1 θ k f = 1, represents the proportion of the k-th topic in the document, and normalization term</p><formula xml:id="formula_5">∆( α) is defined as K k=1 Γ(α k ) Γ( K k=1 α k )</formula><p>.</p><p>To learn the transformation from documenttopic distribution to document-word distribution, G firstly projects θ f into an S-dimensional representation space based on equations:</p><formula xml:id="formula_6">h g s = BN(W g s θ f + b g s ) (5) o g s = max( h g s , leak * h g s )<label>(6)</label></formula><p>where W g s ∈ R S×K is weight matrix of the representation layer, b g s represents bias term, h g s is the state vector normalized by batch normalization, Eq. 6 represents the LeakyReLU activation parameterized with leak, and o g s is the output of the representation layer.</p><p>Then, to project o g s into word distribution d f , a subnet contains a linear layer and a softmax layer is used and the transformation follows:</p><formula xml:id="formula_7">d f = softmax(W g w o g s + b g w )<label>(7)</label></formula><p>where W g w ∈ R V ×S and b g w are weight matrix and bias of word distribution layer, d f is the</p><formula xml:id="formula_8">word distribution correspond to θ f . For each v ∈ {1, 2, ..., V }, the v-th dimension d v f is the probability of the v-th word in fake document d f .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Discriminator network</head><p>The discriminator D is constituted by three layers (a V + K-dimensional joint distribution layer, an S-dimensional representation layer and an output layer) as shown in the right panel of <ref type="figure">Figure 2</ref>. It employs real distribution pair p r and fake distribution pair p f as input and then outputs D out to identify the input sources (fake or real). Concretely, a higher value of D out represents that D is more prone to predict the input as real and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BAT with Gaussian (Gaussian-BAT)</head><p>In BAT, the generator models topics based on the bag-of-words assumption as in most other neural topic models. To incorporate the word relatedness information captured in word embeddings <ref type="bibr">(Mikolov et al., 2013a,b;</ref><ref type="bibr" target="#b25">Pennington et al., 2014;</ref><ref type="bibr" target="#b12">Joulin et al., 2017;</ref><ref type="bibr" target="#b1">Athiwaratkun et al., 2018)</ref> into the inference process, we modify the generator of BAT and propose Gaussian-BAT, in which G models each topic with a multivariate Gaussian as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.  Concretely, Gaussian-BAT employs the multivariate Gaussian N ( µ k , Σ k ) to model the k-th topic. Here, µ k and Σ k are trainable parameters, they represent mean and covariance matrix respectively. Following its probability density, for each word v ∈ {1, 2, ..., V }, the probability in the k-th topic φ k,v is calculated by:</p><formula xml:id="formula_9">p( e v |topic = k) = N ( e v ; µ k , Σ k ) = exp(− 1 2 ( e v − µ k ) T Σ −1 k ( e v − µ k )) (2π) De |Σ k | (8) φ k,v = p( e v |topic = k) V v=1 p( e v |topic = k)<label>(9)</label></formula><p>where e v means the word embedding of v-th word, V is the vocabulary size, |Σ k | = det Σ k is the determinant of covariance matrix Σ k , D e is the dimension of word embeddings, p( e v |topic = k) is the probability calculated by density, and φ k is the normalized word distribution of k-th topic. With randomly sampled topic distribution θ f and the calculated topic-word distributions { φ 1 , φ 2 , ..., φ K }, the fake word distribution d f corresponding to θ f can be obtained by:</p><formula xml:id="formula_10">d f = K k=1 φ k * θ k<label>(10)</label></formula><p>where θ k is the topic proportion of the k-th topic.</p><p>Then, θ f and d f are concatenated to form the fake distribution pair p f as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. And encoder and discriminator of Gaussian-BAT are same as BAT, shown as <ref type="figure">Figure 2</ref>. In our experiments, the pre-trained 300-dimensional Glove (Pennington et al., 2014) embedding is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Objective and Training Procedure</head><p>In <ref type="figure">Figure 2</ref>, the real distribution pair p r = [ θ r ; d r ] and the fake distribution pair p f = [ θ f ; d f ] can be viewed as random samples drawn from two (K + V )-dimensional joint distributions P r and P f , each of them comprising of a K-dimensional Dirichlet distribution and a V -dimensional Dirichlet distribution. The training objective of BAT and Gaussian-BAT is to make the generated joint distribution P f close to the real joint distribution P r as much as possible. In this way, a two-way projection between document-topic distribution and document-word distribution could be built by the learned encoder and generator. To measure the distance between P r and P f , we use the Wasserstein-distance as the optimization objective, since it was shown to be more effective compared to Jensen-Shannon divergence :</p><formula xml:id="formula_11">Loss = E p f ∼P f [D( p f )] − E pr∼Pr [D( p r )] (11)</formula><p>where D(·) represents the output signal of the discriminator. A higher value denotes that the discriminator is more prone to consider the input as a real distribution pair and vice versa. In addition, we use weight clipping which was proposed to ensure the Lipschitz continuity  </p><formula xml:id="formula_12">of D.</formula><p>Algorithm 1 Training procedure for BAT and Gaussian-BAT Input: K, c, n d , m, α 1 , β1, β2 Output: The trained encoder E and generator G.</p><p>1: Initialize D, E and G with ω d , ω e and ω g 2: while ω e and ω g have not converged do 3:</p><p>for t = 1, ..., n d do 4:</p><p>for j = 1, ..., m do 5: The training procedure of BAT and Gaussian-BAT is given in Algorithm. 1. Here, c is the clipping parameter, n d represents the number of discriminator iterations per generator iteration, m is the batch size, α 1 is the learning rate, β 1 and β 2 are hyper-parameters of Adam (Kingma and Ba, 2014), and p a represents {α 1 , β 1 , β 2 }. In our experiments, we set the n d = 5, m = 64, α 1 = 1e−4, c = 0.01, β 1 = 0.5 and β 2 = 0.999.</p><formula xml:id="formula_13">Sample d r ∼ P d r , 6: Sample a random θ f ∼ Dir( θ f | α) 7: d f ← G( θ f ), θ r ← E( d r ) 8: p r = [ θ r ; d r ], p f = [ θ f ; d f ] 9: L (j) = D( p f ) − D( p r ) 10: end for 11: ω d ← Adam(∇ ω d 1 m m j=1 L (j) , ω d , p a ) 12: ω d ← clip(ω d , −c, c) 13: end for 14: ω g ← Adam(∇ ωg −1 m m j=1 D( p j f ), ω g ,<label>p</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Topic Generation and Cluster Inference</head><p>After model training, learned G and E will build a two-way projection between document-topic distribution and document-word distribution. Thus, G and E could be used for topic generation and cluster inference.</p><p>To generate the word distribution of each topic, we use ts (k) , a K-dimensional vector, as the onehot encoding of the k-th topic. For example, ts 2 = [0, 1, 0, 0, 0, 0] T in a six topic setting. And the word distribution of the k-th topic is obtained by:</p><formula xml:id="formula_14">φ k = G( ts (k) )<label>(12)</label></formula><p>Likewise, given the document representation d r , topic distribution θ r obtained by BAT/Gaussian-BAT could be used for cluster inference based on:</p><formula xml:id="formula_15">θ r = E( d r ); c r = arg max θ r<label>(13)</label></formula><p>where c r denotes the inferred cluster of d r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first present the experimental setup which includes the datasets used and the baselines, followed by the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We evaluate BAT and Gaussian-BAT on three datasets for topic extraction and text clustering, 20Newsgroups 1 , Grolier 2 and NYTimes 3 . Details are summarized below: 20Newsgroups <ref type="bibr" target="#b16">(Lang, 1995)</ref> is a collection of approximately 20,000 newsgroup articles, partitioned evenly across 20 different newsgroups. Grolier is built from Grolier Multimedia Encycopedia, which covers almost all the fields in the world.</p><p>NYTimes is a collection of news articles published between 1987 and 2007, and contains a wide range of topics, such as sports, politics, education, etc. We use the full datasets of 20Newsgroups 1 and Grolier 2 . For the NYTimes dataset, we randomly select 100,000 articles and remove the low frequency words. The final statistics are shown in  We choose the following models as baselines: LDA <ref type="bibr" target="#b2">(Blei et al., 2003)</ref> extracts topics based on word co-occurrence patterns from documents. We implement LDA following the parameter setting suggested in <ref type="bibr" target="#b9">(Griffiths and Steyvers, 2004)</ref>. NVDM <ref type="bibr" target="#b22">(Miao et al., 2016)</ref> is an unsupervised text modeling approach based on VAE. We use the original implementation of the paper 4 .  GSM <ref type="bibr" target="#b21">(Miao et al., 2017)</ref> is an enhanced topic model based on NVDM, we use the original implementation in our experiments 5 .</p><p>NVLDA <ref type="bibr" target="#b28">(Srivastava and Sutton, 2017)</ref>, also built on VAE but with the logistic-normal prior. We use the implementation provided by the author 6 .</p><p>ProdLDA <ref type="bibr" target="#b28">(Srivastava and Sutton, 2017)</ref>, is a variant of NVLDA, in which the distribution over individual words is a product of experts. The original implementation is used. ATM <ref type="bibr" target="#b30">(Wang et al., 2019a)</ref>, is a neural topic modeling approach based on adversarial training, we implement the ATM following the parameter setting suggested in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Coherence Evaluation</head><p>Topic models are typically evaluated with the likelihood of held-out documents and topic coherence. However, <ref type="bibr" target="#b3">Chang et al. (2009)</ref> showed that a higher likelihood of held-out documents does not correspond to human judgment of topic coherence. Thus, we follow <ref type="bibr" target="#b27">(Röder et al., 2015)</ref> and employ four topic coherence metrics (C P, C A, NPMI and UCI) to evaluate the topics generated by various models. In all experiments, each topic is represented by the top 10 words according to the topic-word probabilities, and all the topic coherence values are calculated using the Palmetto library 7 . We firstly make a comparison of topic coherence vs. different topic proportions. Experiments are 5 https://github.com/linkstrife/NVDM-GSM 6 https://github.com/akashgit/autoencoding vi for topic models 7 https://github.com/dice-group/Palmetto  conducted on the datasets with five topic number settings <ref type="bibr">[20,</ref><ref type="bibr">30,</ref><ref type="bibr">50,</ref><ref type="bibr">75,</ref><ref type="bibr">100]</ref>. We calculate the average topic coherence values among topics whose coherence values are ranked at the top 50%, 70%, 90%, 100% positions. For example, to calculate the average C P value of BAT @90%, we first compute the average C P coherence with the selected topics whose C P values are ranked at the top 90% for each topic number setting, and then average the five coherence values with each corresponding to a particular topic number setting. The detailed comparison is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. It can be observed that BAT outperforms the baselines on all the coherence metrics for NYTimes datasets. For Grolier dataset, BAT outperforms all the baselines on C P, NPMI and UCI metrics, but </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Topics</head><p>Gaussian-BAT voter campaign poll candidates democratic election republican vote presidential democrat song album music band rock pop sound singer jazz guitar film movie actor character movies director series actress young scenes flight airline passenger airlines aircraft shuttle airport pilot carrier planes BAT vote president voter campaign election democratic governor republican black candidates album band music rock song jazz guitar pop musician record film actor play acting role playing character father movie actress flight airline delay airlines plane pilot airport passenger carrier attendant LDA voter vote poll election campaign primary candidates republican race party music song band sound record artist album show musical rock film movie character play actor director movies minutes theater cast flight plane ship crew air pilot hour boat passenger airport ATM voter vote poll republican race primary percent election campaign democratic music song musical album jazz band record recording mp3 composer film movie actor director award movies character theater production play jet flight airline hour plane passenger trip plan travel pilot <ref type="table">Table 3</ref>: Topic examples extracted by models, italics means out-of-topic words. These topics correspond to 'election', 'music', 'film' and 'airline' respectively, and topic examples of other models are omitted due to poor quality.</p><p>gives slightly worse results compared to ATM on C A. For 20Newsgroups dataset, BAT performs the best on C P and NPMI, but gives slightly worse results compared to ProdLDA on C A, and LDA on UCI. By incorporating word embeddings through trainable Gaussian distribution, Gaussian-BAT outperforms all the baselines and BAT on four coherence metrics, often by a large margin, across all the three datasets except for Grolier dataset on C A when considering 100% topics. This may be attribute to the following factors: (1) The Dirichlet prior employed in BAT and Gaussian-BAT could exhibit a multi-modal distribution in latent space and is more suitable for discovering semantic patterns from text; (2) ATM does not consider the relationship between topic distribution and word distribution since it only carry out adversarial training in word distribution space; (3) The incorpora-tion of word embeddings in Gaussian-BAT helps generating more coherent topics. We also compare the average topic coherence values (all topics taken into account) numerically to show the effectiveness of proposed BAT and Gaussian-BAT. The results of numerical topic coherence comparison are listed in <ref type="table" target="#tab_5">Table 2</ref> and each value is calculated by averaging the average topic coherences over five topic number settings. The best coherence value on each metric is highlighted in bold. It can be observed that Gaussian-BAT gives the best overall results across all metrics and on all the datasets except for Grolier dataset on C A. To make the comparison of topics more intuitive, we provide four topic examples extracted by models in <ref type="table">Table 3</ref>. It can be observed that the proposed BAT and Gaussian-BAT can generate more coherent topics.</p><p>Moreover, to explore how topic coherence varies with different topic numbers, we also provide the comparison of average topic coherence vs. different topic number on 20newsgroups, Grolier and NYTimes (all topics taken into account). The detailed comparison is shown in <ref type="figure" target="#fig_5">Figure 5</ref>. It could be observed that Gaussian-BAT outperforms the baselines with 20, 30, 50 and 75 topics except for Grolier dataset on C A metric. However, when the topic number is set to 100, Gaussian-BAT performs slightly worse than LDA (e.g., UCI for 20Newsgroups and C A for NYTimes). This may be caused by the increased model complexity due to the larger topic number settings. Likewise, BAT can achieve at least the second-best results among all the approaches in most cases for NYTimes dataset. For Grolier, BAT also performs the second-best except on C A metric. However, for 20newsgroups, the results obtained by BAT are worse than ProdLDA (C A) and LDA (UCI) due to the limited training documents in the dataset, though it still largely outperforms other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Text Clustering</head><p>We further compare our proposed models with baselines on text clustering. Due to the lack of document label information in Grolier and NYTimes, we only use 20Newsgroups dataset in our experiments. The topic number is set to 20 (ground-truth categories) and the performance is evaluated by accuracy (ACC):</p><formula xml:id="formula_16">ACC = max map Nt i=1 ind(l i = map(c i )) N t<label>(14)</label></formula><p>where N t is the number of documents in the test set, ind(·) is the indicator function, l i is the groundtruth label of i-th document, c i is the category assignment, and map ranges over all possible oneto-one mappings between labels and clusters. The optimal map function can be obtained by the Kuhn-Munkres algorithm <ref type="bibr" target="#b15">(Kuhn, 1955</ref>  The comparison of text clustering results on 20Newsgroups is shown in <ref type="table" target="#tab_7">Table 4</ref>. Due to the poor performance of NVDM in topic coherence evaluation, its result is excluded here. Not surprisingly, NVLDA and ProdLDA perform worse than BAT and Gaussian-BAT that model topics with the Dirichlet prior. This might be caused by the fact that Logistic-Normal prior does not exhibit multiple peaks at the vertices of the simplex, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. Compared with LDA, BAT achieves a comparable result in accuracy since both models have the same Dirichlet prior assumption over topics and only employ the word co-occurrence information. Gaussian-BAT outperforms the second best model, BAT, by nearly 6% in accuracy. This shows that the incorporation of word embeddings is important to improve the semantic coherence of topics and thus results in better consistency between cluster assignments and ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have explored the use of bidirectional adversarial training in neural topic models and proposed two novel approaches: the Bidirectional Adversarial Topic (BAT) model and the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT). BAT models topics with the Dirichlet prior and builds a two-way transformation between document-topic distribution and document-word distribution via bidirectional adversarial training. Gaussian-BAT extends from BAT by incorporating word embeddings into the modeling process, thereby naturally considers the word relatedness information captured in word embeddings. The experimental comparison on three widely used benchmark text corpus with the existing neural topic models shows that BAT and Gaussian-BAT achieve improved topic coherence results. In the future, we would like to devise a nonparametric neural topic model based on adversarial training. Besides, developing correlated topic modelsis another promising direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustrated probability simplex with Logistic-Normal distribution and Dirichlet distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The generator of Gaussian-BAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ω e , p a ) 16: end while</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The comparison of average topic coherence vs. different topic proportion on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The comparison of average topic coherence vs. different topic number on 20Newsgroups, Grolier and NYTimes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell cols="3">#Doc (Train) #Doc (Test) #Words</cell></row><row><cell>20Newsgroups</cell><cell>11,259</cell><cell>7,488</cell><cell>1,995</cell></row><row><cell>Grolier</cell><cell>29,762</cell><cell>-</cell><cell>15,276</cell></row><row><cell>NYtimes</cell><cell>99,992</cell><cell>-</cell><cell>12,604</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The statistics of datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Average topic coherence on three datasets with five topic settings[20, 30, 50, 75, 100].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>). A larger accuracy value indicates a better text clustering results.</figDesc><table><row><cell cols="3">Dataset NVLDA ProdLDA</cell><cell>LDA</cell><cell>BAT</cell><cell>G-BAT</cell></row><row><cell>20NG</cell><cell>33.31%</cell><cell>33.82%</cell><cell cols="2">35.36% 35.66% 41.25%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Text clustering accuracy on 20Newsgroups (20NG). 'G-BAT' refers to 'Gaussian-BAT'. The best result is highlighted in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://qwone.com/ jason/20Newsgroups/ 2 https://cs.nyu.edu/∼roweis/data/ 3 http://archive.ics.uci.edu/ml/datasets/Bag+of+Words 4 https://github.com/ysmiao/nvdm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank anonymous reviewers for their valuable comments and helpful suggestions. This work was funded by the National Key Research and Development Program of China(2017YFB1002801) and the National Natural Science Foundation of China (61772132). And YH is partially supported by EPSRC (grant no. EP/T017112/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic fasttext for multi-sense word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Btm: Topic modeling over short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2928" to="2941" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew M</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07736</idno>
		<title level="m">Maskgan: better text generation via filling in the</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr Bojanowski Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">427</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Saan: A sentiment-aware attention network for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1197" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial ranking for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ting</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3155" to="3165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Content attention model for aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zufeng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1023" to="1032" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 World Wide Web Conference</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering discrete latent topics with neural variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM international conference on Web search and data mining</title>
		<meeting>the eighth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01488</idno>
		<title level="m">Autoencoding variational inference for topic models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking lda: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mc-Callum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<title level="m">Atm: Adversarial-neural topic model. Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">102098</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Open event extraction from online text using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="282" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A simple bayesian modelling approach to event extraction from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="700" to="705" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
