<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VARIATIONAL RECURRENT NEURAL NETWORKS FOR GRAPH CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Pineau</surname></persName>
							<email>edouard.pineau@telecom-paristech.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>De Lara</surname></persName>
							<email>nathan.delara@telecom-paristech.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telecom</forename><surname>Paristech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safran</forename></persName>
						</author>
						<title level="a" type="main">VARIATIONAL RECURRENT NEURAL NETWORKS FOR GRAPH CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Representation Learning on Graphs and Manifolds workshop, ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of graph classification based only on structural information. Inspired by natural language processing techniques (NLP), our model sequentially embeds information to estimate class membership probabilities. Besides, we experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many natural or synthetic systems have a natural graph representation where entities are described through their mutual connections: chemical compounds, social or biological networks, for example. Therefore, automatic mining of such structures is useful in a variety of applications.</p><p>Graph classification raises several difficulties to leverage standard machine learning algorithms. Indeed, most of these algorithms take vectors of fixed size as inputs. In the case of graphs, usual representations such as edge list or adjacency matrix do not match this constraint. The size of the representations is graph dependent (number of edges in the first case, number of nodes squared in the second) and these representations are index dependent: up to indexing of its nodes, a same graph admits several equivalent representations. In a classification task, the label of a graph is independent from the indices of its nodes, so the model used for prediction should be invariant to node ordering as well. Handling discrete inputs with variable size and ordering is a well known problem in natural language processing (NLP). This is why we adapt NLP techniques to tackle graph classification.</p><p>In this paper, we propose a method to sequentially embed graph information in order to perform classification. By construction, this recurrent graph classifier overcomes the common difficulties listed above. Besides, we propose to use an additional node prediction block to help the model to capture the intrinsic structure of the graphs. The complete model is denoted variational recurrent graph classifier (VRGC). Experiments show that this leads to better classification results for larger datasets. For clarity of the contribution of our work, we use neither node attributes nor edge attributes.</p><p>Related works use either graph kernels <ref type="bibr" target="#b14">(Nikolentzos et al., 2017a;</ref><ref type="bibr" target="#b19">Shervashidze et al., 2011;</ref><ref type="bibr" target="#b23">Yanardag and Vishwanathan, 2015)</ref>, sequential methods <ref type="bibr" target="#b1">(Callut et al., 2008;</ref><ref type="bibr" target="#b22">Xu et al., 2012;</ref><ref type="bibr" target="#b7">Jin and JaJa, 2018;</ref><ref type="bibr" target="#b24">You et al., 2018)</ref> or graph features <ref type="bibr" target="#b0">(Barnett et al., 2016;</ref><ref type="bibr" target="#b12">Narayanan et al., 2017;</ref><ref type="bibr" target="#b6">Gomez et al., 2017;</ref><ref type="bibr" target="#b5">Dutta and Sahbi, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head><p>We propose to use a sequential approach to embed graphs with a variable number of nodes and edges into a vector space of a chosen dimension. This latent representation is then used for classification. Node index invariance is approximated through specific pre-processing and aggregation.</p><p>Let G = (V, E) be an undirected and unweighted graph with V a set of nodes and E a set of edges. The graph G can be represented, modulo any permutation π over its nodes {n i } adjacency matrix A π such that A π ij = 1 if nodes indexed by i and j are connected in the graph and A π ij = 0 otherwise. We use this adjacency matrix as a raw representation of the graph. Our VRGC is composed of three main parts: node ordering and embedding, classification and regularization with variational auto-regression (VAR). See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration.</p><p>Node ordering and embedding Before being processed by the neural network, the adjacency matrix of a graph is transformed on-the-fly <ref type="bibr" target="#b24">(You et al., 2018)</ref>. First, a node is selected at random and used as root for a breadth first search (BFS) over the graph. The rows and columns of the adjacency matrix are then reordered according to the sequence of nodes returned by the BFS. Next, each row i (corresponding to the i th node in the BFS ordering) is truncated to keep only the connections of node n i with the min(i, d) nodes that preceded in the BFS. This way, each node is d-dimensional, and each truncated matrix is zero-padded in order to have dimensions (|V | max , d n ). Throughout the rest of the paper, we use the notation n G for |V | max .</p><p>After node ordering and pre-embedding, each graph is processed as a sequence of d-dimensional nodes by a gated recurrent unit (GRU) neural network . The GRU is a special RNN able to learn long term dependencies by solving vanishing gradient effect 1 . In order to help the recurrent network training, we propose to add a simple fully connected network between preembedding and recurrent embedding. Therefore, the node will be presented to the GRU in the shape of continuous vectors instead of binary adjacency vectors.</p><p>Finally the GRU sequentially embeds each node n i by using n i−1 and information contained in a memory cell h i−1 that theoretically embeds all previously seen information. The embedded node sequence {h i } n G i=1 then feeds both the VAR and the classifier as discussed in subsequent sections. See top line of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Classification After the embedding step, we use an additional GRU dedicated to classification that takes {h i } n G i=1 as input. Its last memory cell, denotedh n G , feeds a softmax multilayer perceptron (MLP) which performs class prediction. Formally, let c be the class index, the classifier is trained by minimizing the cross-entropy loss between ground-truth andp(G, r) the softmax class membership probability vector for a given graph G that has been sorted by a BFS rooted with node r. We call this objective term L classif . As discriminating patterns might be spread across the whole graph, the network is required to model long-term dependencies. By construction, GRUs have such ability. See middle line of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Regularization with variational auto-regression As the structure of a graph is the concatenation of the interactions between all nodes and their respective neighbors, learning a good representation without using node attributes requires for the model to capture the structure of the graph while classifying. Accordingly, we add an auto-regression block to our model: at each node, the network makes a prediction for the next node adjacency. Multi-task learning is a powerful leverage to learn rich representation <ref type="bibr" target="#b18">(Sanh et al., 2018)</ref> in NLP. In particular, such representation for sequence classification has already been used for sentiment analysis <ref type="bibr" target="#b11">(Latif et al., 2017;</ref><ref type="bibr" target="#b21">Xu et al., 2017)</ref>. We use a variational auto-encoder (VAE) <ref type="bibr" target="#b10">(Kingma and Welling, 2013)</ref> to learn a representation of each node n i given h i−1 . The first layers of the encoder are shared with the classifier and corresponds to the graphs preprocessing (blue part in <ref type="figure" target="#fig_0">Figures 1 and 2)</ref>. The subsequent encoder layers, the latent sampling and the decoder constitute the VAR. For each graph G with embedded nodes {n i } i∈ 1,n G , the fully connected variational auto-encoder takes h := {h i } i∈ 1,n G −1 as input. Let {z i } i∈ 1,n G be the latent random variables for the following model</p><formula xml:id="formula_0">p(G, z|h) = p(n 1 , z 1 ) n G i=2 p θ (n i |z i )q φ (z i |h i−1 ).</formula><p>In practice, p θ and q φ are modelled by neural networks parametrized by θ and φ, which require differentiable functions for training. However, p θ (n i |z i ) models a binary adjacency vector representing the connections between node n i and previously visited nodes n j&lt;i . Therefore, we use sigmoid continuous relaxation to train our model, and hard binary sampling at test time. We use a Gaussian variational posterior distribution. Training is done by maximizing the variational lower bound of the log-likelihood of the observation as in Kingma's VAE. The exact loss is displayed in Appendix A and denoted L pred .</p><p>The regularization part is illustrated in the bottom line of <ref type="figure" target="#fig_1">Figure 2</ref>. In the end, the model is trained by minimizing the total loss L = L classif + αL pred , where α is a hyper-parameter.</p><p>Aggregation of the results at test time The node ordering step introduces randomness to our model. On the one hand, it helps learn more general graph representations during the training phase, but on the other hand, it might produce different outputs for the same graph during the testing phase, depending on the root of the BFS. In order to counter this side effect, we add the following aggregation step for the testing phase. Each graph is processed N times by the model with N different roots for BFS ordering. The N class membership probability vectors are extracted and averaged. The average score vector is notedp and computed as follows with an element-wise sum: This soft vote is repeated K times resulting in K probability vectors {p .,k (G)} K k=1 for each graph G. The final class attributed to a graph corresponds to the highest probability among the K vectors. This second hard vote enables to choose the batch of votes for which the model is the most confident.</p><formula xml:id="formula_1">p(G) = 1 N N i=1 r∼U ( 1,n G )p (G,</formula><formula xml:id="formula_2">c(G) = arg max c∈ 1,C {||p c,. (G)|| ∞ }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>Datasets and results We evaluated our model against four standard datasets from biology: Mutag (MT), Enzymes (EZ), Proteins Full (PF) and National Cancer Institute (NCI1) . A detailed description of each dataset in provided in Appendix B.</p><p>We compare our results to those obtained by Earth Mover's Distance <ref type="bibr" target="#b15">(Nikolentzos et al., 2017b)</ref>   <ref type="bibr">(Verma and Zhang, 2017) (FGSD)</ref>. All values are directly taken from the aforementioned papers as they use a setup similar to ours. For algorithms presenting results with and without node features, we reported the results without node features. For those presenting results with several sets of hyper-parameters, we reported the results for the parameters that performed best on the largest number of datasets. Results are reported in <ref type="table">Table 1</ref>. We obtain state-of-the-art results on three out of the four datasets used for this paper and the second best result on the fourth one.</p><p>Node indexing invariance Our model is designed to be independent from node ordering of the graph with respect to different BFS roots. Inputs representing the same graph (up to node ordering) should be close from one another in the latent embedding space. As the preprocessing is performed on each graph at each epoch, a same graph is processed many times by the model during training with different embeddings. This creates a natural regularization for the network. Indeed, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, the projections corresponding to the same graphs form a heap in the low dimensional representation of the latent space.</p><p>Contribution of the VAR to classification The variational regularization term seems to help the model finding a more meaningful latent representation for classification while graph dataset becomes larger. Note that the extra cost of training the VAR is marginal with respect to the training of the RNNs. We provide an illustration of the output of VAR block in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>Conclusion and room for improvement This paper proposed a recurrent graph classifier with variational regularization. The invariance to node indexing is greedy learned from numerous iterations on randomly rooted BFS-ordered graph. For future work, we should investigate the impact VAR capacity (number and size of hidden layers) on classification accuracy or generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A VAR LOSS</head><p>The VAE-like loss for VAR regularization is the following:</p><formula xml:id="formula_4">L pred = E p d (G) n G i=2 KL (q φ (z i |h i−1 )||q(z i )) − E p d (G) n G i=2 E q φ (zi|hi−1) [log p θ (n i |z i )] ,</formula><p>which is a lower bound of the negative marginal log-likelihood E p d (G) [log p θ (G)]. p θ and q φ are the respective densities of n|z and z|h, whose distribution are parameterized by θ and φ respectively. KL denotes the Kullback-Leibler divergence, p d is the empirical distribution of G and q(z i ) is the density of the prior distribution of latent variables {z i } n G i=2 . We chose the standard Gaussian prior for q(z i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASET CHARACTERISTICS</head><p>All graphs represent chemical compounds, nodes are molecular substructures (typically atoms) and edges represent connections between these substructures (chemical bound or spatial proximity). In MT, the compounds are either mutagenic or not mutagenic. EZ contains tertiary structures of proteins from the 6 Enzyme Commission top level classes; it is the only multiclass dataset of this paper. PF is a subset of the Dobson and Doig dataset representing secondary structures of proteins being either enzyme or not enzyme. In NCI1, compounds either have an anti-cancer activity or do not.  The input size d n of the recurrent neural network is chosen for each dataset according to the algorithm described in <ref type="bibr" target="#b24">(You et al., 2018)</ref>, namely 11 for MT, 25 for EZ, 80 for PF and 11 for NCI1. α is set to 0.1. For training, batch size is set to 64, and the learning rate to 10 −3 , decreased by 0.3 at iterations 400 and 1000. We use the same hyper-parameters for every dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ILLUSTRATION OF EXPERIMENTAL RESULTS</head><p>The following table presents the hyperparametrization of our model, i.e. the neural network architectures for each part presented in Section 2.</p><p>Step    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>|V | i=1 , by its boolean Representation Learning on Graphs and Manifolds workshop, ICLR 2019 Macroscopic representation of VRGC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture for VRGC. Top: node ordering and embedding. Middle: classification. Bottom: regularization with VAR plus final aggregation. FC stands for fully-connected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>EMD), Pyramid Match (Nikolentzos et al., 2017b) (PM), Feature-Based (Barnett et al., 2016) (FB), Dynamic-Based Features (Gomez et al., 2017) (DyF), Stochastic Graphlet Embedding (Dutta and Sahbi, 2017) (SGE), Truncated Laplacian Spectrum (de Lara and Pineau, 2018) (TLS) and Family of Graph Spectral Distances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>TSNE projection of the latent state preceding classification for five graphs of EZ each initiated with 20 different BFS. Colors and markers represent the respective classes of the graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Left: Representation of the same graph after two differently rooted BFS ordering and truncation. Right: corresponding auto-regressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>r).</figDesc><table><row><cell></cell><cell>MT</cell><cell>EZ</cell><cell>PF</cell><cell>NCI1</cell></row><row><cell>EMD</cell><cell>86.1</cell><cell>36.8</cell><cell>-</cell><cell>72.7</cell></row><row><cell>PM</cell><cell>85.6</cell><cell>28.2</cell><cell>-</cell><cell>69.7</cell></row><row><cell>FB</cell><cell>84.7</cell><cell>29.0</cell><cell>70.0</cell><cell>62.9</cell></row><row><cell>DyF</cell><cell>86.3</cell><cell>26.6</cell><cell>73.1</cell><cell>66.6</cell></row><row><cell>SGE</cell><cell>87.3</cell><cell>40.7</cell><cell>71.9</cell><cell>-</cell></row><row><cell>TLS</cell><cell>88.4</cell><cell>43.7</cell><cell>73.6</cell><cell>75.2</cell></row><row><cell>FGSD</cell><cell>92.1</cell><cell>-</cell><cell>73.4</cell><cell>79.8</cell></row><row><cell>RGC</cell><cell>89.5</cell><cell>48.7</cell><cell>72.5</cell><cell>78.1</cell></row><row><cell>VRGC</cell><cell>86.3</cell><cell>48.4</cell><cell>74.8</cell><cell>80.7</cell></row><row><cell cols="5">Table 1: Experimental results of different models plus our own on four standard molecular datasets.</cell></row><row><cell cols="5">RGC stands for recurrent graph classifier, VRGC for variational RGC. All other acronyms are de-</cell></row><row><cell>fined in section 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Basic characteristics of the datasets. Bias indicates the proportion of the largest class.C FEATURES OF NETWORK ARCHITECTUREMT, EZ, PF and NCI1 are respectively divided into 10 folds such that the class proportions are preserved in each fold for all datasets. These folds are then used for cross-validation i.e., one fold serves as the testing set while the other ones compose the training set. Results are averaged over all testing sets. Our model is implemented in Pytorch<ref type="bibr" target="#b17">(Paszke et al., 2017)</ref> and trained with the Adam stochastic optimization method (Kingma and Ba, 2014) on a NVIDIA TitanXp GPU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Architecture BFS 1-layer FC. d n × 64 embedding 2-layer GRU. 64 × 128</figDesc><table><row><cell>VAR</cell><cell>Encoder</cell></row><row><cell></cell><cell>1-layer FC. 128 × 2 × 8</cell></row><row><cell></cell><cell>Gaussian sampling</cell></row><row><cell></cell><cell>Predictor</cell></row><row><cell></cell><cell>2-layer ReLU FC. 8 × d n</cell></row><row><cell>Classifier</cell><cell>2-layer GRU. 128 × 128 + DP(0.25)</cell></row><row><cell></cell><cell>2-layer ReLU FC. 128 × C + SF</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Generic architecture used in our experiments. ReLU FC stands for fully-connected network with ReLU activation. DP stands for dropout. SF stands for softmax.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The choice of GRU over Long Short Term Memory networks is arbitrary as they have equivalent long-term modeling power<ref type="bibr" target="#b3">(Chung et al., 2014)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Thomas Bonald and Sebastien Razakarivony for their comments and help. We also would like to thank NVIDIA and its GPU Grant Program for providing the hardware we used in our experiments. This work is supported by the company Safran through the CIFRE convention 2017/1317.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Feature-based classification of networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Kuijjer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Onnela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05868</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Callut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Françoisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
		<title level="m">Classification in graphs using discriminative random walks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple baseline algorithm for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Representation Learning Workshops (NIPS 2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">High order stochastic graphlet embedding for graph-based pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00156</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Delvenne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10817</idno>
		<title level="m">Dynamics based features for graph classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Jaja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07683</idno>
		<title level="m">Learning graph-level representations with gated recurrent neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Variational autoencoders for learning latent representations of speech emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08708</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno>abs/1707.05005</idno>
		<ptr target="http://arxiv.org/abs/1707.05005" />
		<title level="m">graph2vec: Learning distributed representations of graphs. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-P</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10689</idno>
		<title level="m">Kernel graph convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A degeneracy framework for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2595" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A hierarchical multi-task approach for learning embeddings from semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06031</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weisfeilerlehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variational autoencoder for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3358" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Protein classification using random walk on graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="180" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
		<title level="m">Graphrnn: A deep generative model for graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
