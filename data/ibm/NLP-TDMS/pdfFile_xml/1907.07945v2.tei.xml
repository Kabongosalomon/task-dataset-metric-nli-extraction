<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MintNet: Building Invertible Neural Networks with Masked Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
							<email>yangsong@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
							<email>chenlin@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MintNet: Building Invertible Neural Networks with Masked Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new way of constructing invertible neural networks by combining simple building blocks with a novel set of composition rules. This leads to a rich set of invertible architectures, including those similar to ResNets. Inversion is achieved with a locally convergent iterative procedure that is parallelizable and very fast in practice. Additionally, the determinant of the Jacobian can be computed analytically and efficiently, enabling their generative use as flow models. To demonstrate their flexibility, we show that our invertible neural networks are competitive with ResNets on MNIST and CIFAR-10 classification. When trained as generative models, our invertible networks achieve competitive likelihoods on MNIST, CIFAR-10 and ImageNet 32×32, with bits per dimension of 0.98, 3.32 and 4.06 respectively.</p><p>Using our composition rules and masked convolutions as the basic triangular building block, we construct a rich set of invertible modules to form a deep invertible neural network. The architecture of our proposed invertible network closely follows that of ResNet [10]-the state-of-the-art architecture *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Invertible neural networks have many applications in machine learning. They have been employed to investigate representations of deep classifiers <ref type="bibr" target="#b14">[15]</ref>, understand the cause of adversarial examples <ref type="bibr" target="#b13">[14]</ref>, learn transition operators for MCMC <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b18">18]</ref>, create generative models that are directly trainable by maximum likelihood <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b0">1]</ref>, and perform approximate inference <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Many applications of invertible neural networks require that both inverting the network and computing the Jacobian determinant be efficient. While typical neural networks are not invertible, achieving these properties often imposes restrictive constraints to the architecture. For example, planar flows <ref type="bibr" target="#b27">[27]</ref> and Sylvester flow <ref type="bibr" target="#b1">[2]</ref> constrain the number of hidden units to be smaller than the input dimension. NICE <ref type="bibr" target="#b4">[5]</ref> and Real NVP <ref type="bibr" target="#b5">[6]</ref> rely on dimension partitioning heuristics and specific architectures such as coupling layers, which could make training more difficult <ref type="bibr" target="#b0">[1]</ref>. Methods like FFJORD <ref type="bibr" target="#b8">[9]</ref>, i-ResNets <ref type="bibr" target="#b0">[1]</ref> have fewer architectural constraints. However, their Jacobian determinants have to be approximated, which is problematic if repeatedly performed at training time as in flow models.</p><p>In this paper, we propose a new method of constructing invertible neural networks which are flexible, efficient to invert, and whose Jacobian can be computed exactly and efficiently. We use triangular matrices as our basic module. Then, we provide a set of composition rules to recursively build more complex non-linear modules from the basic module, and show that the composed modules are invertible as long as their Jacobians are non-singular. As in previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">24]</ref>, the Jacobians of our modules are triangular, allowing efficient determinant computation. The inverse of these modules can be obtained by an efficiently parallelizable fixed-point iteration method, making the cost of inversion comparable to that of an i-ResNet <ref type="bibr" target="#b0">[1]</ref> block. of discriminative learning. We call our model Masked Invertible Network (MintNet). To demonstrate the capacity of MintNets, we first test them on image classification. We found that a MintNet classifier achieves 99.6% accuracy on MNIST, matching the performance of a ResNet with a similar architecture. On CIFAR-10, it achieves 91.2% accuracy, comparable to the 92.6% accuracy of ResNet. When using MintNets as generative models, they achieve the new state-of-the-art results of bits per dimension (bpd) on uniformly dequantized images. Specifically, MintNet achieves bpd values of 0.98, 3.32, and 4.06 on MNIST, CIFAR-10 and ImageNet 32×32, while former best published results are 0.99 (FFJORD <ref type="bibr" target="#b8">[9]</ref>), <ref type="bibr" target="#b2">3</ref>.35 (Glow <ref type="bibr" target="#b15">[16]</ref>) and 4.09 (Glow) respectively. Moreover, MintNet uses fewer parameters and less computational resources. Our MNIST model uses 30% fewer parameters than FFJORD <ref type="bibr" target="#b8">[9]</ref>. For CIFAR-10 and ImageNet 32×32, MintNet uses 60% and 74% fewer parameters than the corresponding Glow <ref type="bibr" target="#b15">[16]</ref> models. When training on dataset such as CIFAR-10, MintNet required 2 GPUs for approximately 5 days, while FFJORD <ref type="bibr" target="#b8">[9]</ref> used 6 GPUs for approximately 5 days, and Glow <ref type="bibr" target="#b15">[16]</ref> used 8 GPUs for approximately 7 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Consider a neural network f : R D → R L that maps a data point x ∈ R D to a latent representation z ∈ R L . When for every z ∈ R L there exists a unique x ∈ R D such that f (x) = z, we call f an invertible neural network. There are several basic properties of invertible networks. First, when f (x) is continuous, a necessary condition for f to be invertible is D = L. Second, if f 1 : R D → R D and f 2 : R D → R D are both invertible, f = f 2 • f 1 will also be invertible. In this work, we mainly consider applications of invertible neural networks to classification and generative modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification with invertible neural networks</head><p>Neural networks for classification are usually not invertible because the number of classes L is usually different from the input dimension D. Therefore, when discussing invertible neural networks for classification, we separate the classifier into two parts f = f 2 • f 1 : feature extraction z = f 1 (x) and classification y = f 2 (z), where f 2 is usually the softmax function. We say the classifier is invertible when f 1 is invertible. Invertible classifiers are arguably more interpretable, because a prediction can be traced down by inverting latent representations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative modeling with invertible neural networks</head><p>An invertible network f : x ∈ R D → z ∈ R D can be used to warp a complex probability density p(x) to a simple base distribution π(z) (e.g., a multivariate standard Gaussian) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Under the condition that both f and f −1 are differentiable, the densities of p(x) and π(z) are related by the following change of variable formula</p><formula xml:id="formula_0">log p(x) = log π(z) + log | det(J f (x))|,<label>(1)</label></formula><p>where J f (x) denotes the Jacobian of f (x) and we require J f (x) to be non-singular so that log | det(J f (x))| is well-defined. Using this formula, p(x) can be easily computed if the Jacobian determinant det(J f (x)) is cheaply computable and π(z) is known.</p><p>Therefore, an invertible neural network f θ (x) implicitly defines a normalized density model p θ (x), which can be directly trained by maximum likelihood. The invertibility of f θ is critical to fast sample generation. Specifically, in order to generate a sample x from p θ (x), we can first draw z ∼ π(z), and warp it back through the inverse of f θ to obtain x = f −1 θ (z). Note that multiple invertible models f 1 , f 2 , · · · , f K can be stacked together to form a deeper invertible model f = f K • · · · • f 2 • f 1 , without much impact on the inverse and determinant computation. This is because we can sequentially invert each component, i.e.,</p><formula xml:id="formula_1">f −1 = f −1 1 • f −1 2</formula><p>• · · · • f −1 K , and the total Jacobian determinant equals the product of each individual Jacobian determinant, i.e.,</p><formula xml:id="formula_2">| det(J f )| = | det(J f1 )|| det(J f2 )| · · · | det(J f K )|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Building invertible modules compositionally</head><p>In this section, we discuss how simple blocks like masked convolutions can be composed to build invertible modules that allow efficient, parallelizable inversion and determinant computation. To this <ref type="figure" target="#fig_4">Figure 1</ref>: Illustration of a masked convolution with 3 filters and kernel size 3 × 3. Solid checkerboard cubes inside each filter represent unmasked weights, while the transparent blue blocks represent the weights that have been masked out. The receptive field of each filter on the input feature maps is indicated by regions shaded with the pattern (the colored square) below the corresponding filter. end, we first introduce the basic building block of our models. Then, we propose a set of composition rules to recursively build up complex non-linear modules with triangular Jacobians. Next, we prove that these composed modules are invertible as long as their Jacobians are non-singular. Finally, we discuss how these modules can be inverted efficiently using numerical methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The basic module</head><p>We start from considering linear transformations f (</p><formula xml:id="formula_3">x) = Wx + b, with W ∈ R D×D , and b ∈ R D .</formula><p>For a general W, computing its Jacobian determinant requires O(D 3 ) operations. We therefore choose W to be a triangular matrix. In this case, the Jacobian determinant det(J f (x)) = det(W) is the product of all diagonal entries of W, and the computational complexity is reduced to O(D). The linear function f (x) = Wx + b with W being triangular is our basic module.</p><p>Masked convolutions. Convolution is a special type of linear transformation that is very effective for image data. The triangular structure of the basic module can be achieved using masked convolutions (e.g., causal convolutions in PixelCNN <ref type="bibr" target="#b22">[22]</ref>). We provide the formula of our masks in Appendix B and an illustration of a 3 × 3 masked convolution with 3 filters in <ref type="figure" target="#fig_4">Fig. 1</ref>. Intuitively, the causal structure of the filters (ordering of the pixels) enforces a triangular structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The calculus of building invertible modules</head><p>Complex non-linear invertible functions can be constructed from our basic modules in two steps. First, we follow several composition rules so that the composed module has a triangular Jacobian. Next, we impose appropriate constraints so that the module is invertible. To simplify the discussion, we only consider modules with lower triangular Jacobians here, and we note that it is straightforward to extend the analysis to modules with upper triangular Jacobians.</p><p>The following proposition summarizes several rules to compositionally build new modules with triangular Jacobians using existing ones. Proposition 1. Define F as the set of all continuously differentiable functions whose Jacobian is lower triangular. Then F contains the basic module in Section 3.1, and is closed under the following composition rules.</p><p>• Rule of addition.</p><formula xml:id="formula_4">f 1 ∈ F ∧ f 2 ∈ F ⇒ λf 1 + µf 2 ∈ F, where λ, µ ∈ R. • Rule of composition. f 1 ∈ F ∧ f 2 ∈ F ⇒ f 2 • f 1 ∈ F. A special case is f ∈ F ⇒ h • f ∈ F, where h(·)</formula><p>is a continuously differentiable non-linear activation function that is applied element-wise.</p><p>The proof of this proposition is straightforward and deferred to Appendix A. By repetitively applying the rules in Proposition 1, our basic linear module can be composed to construct complex non-linear modules having continuous and triangular Jacobians. Note that besides our linear basic modules, other functions with triangular and continuous Jacobians can also be made more expressive using the composition rules. For example, the layers of dimension partitioning models (e.g., NICE <ref type="bibr" target="#b4">[5]</ref>, Real NVP <ref type="bibr" target="#b5">[6]</ref>, Glow <ref type="bibr" target="#b15">[16]</ref>) and autoregressive flows (e.g., MAF <ref type="bibr" target="#b24">[24]</ref>) all have continuous and triangular Jacobians and therefore belong to F. Note that the rule of addition in Proposition 1 preserves triangular Jacobians but not invertibility. Therefore, we need additional constraints if we want the composed functions to be invertible.</p><p>Next, we state the condition for f ∈ F to be invertible, and denote the invertible subset of F as M. Theorem 1. If f ∈ F and J f (x) is non-singular for all x in the domain, then f is invertible.</p><p>Proof. A proof can be found in Appendix A.</p><p>The non-singularity of J f (x) constraint in Theorem 1 is natural in the context of generative modeling. This is because in order for Eq. (1) to make sense, log | det(J f )| has to be well-defined, which requires J f (x) to be non-singular.</p><p>In many cases, Theorem 1 can be easily used to check and enforce the invertibility of f ∈ F. For example, the layers of autoregressive flow models and dimension partitioning models can all be viewed as elements of F because they are continuously differentiable and have triangular Jacobians.</p><p>Since the diagonal entries of their Jacobians are always strictly positive and hence non-singular, we can immediately conclude that they are invertible with Theorem 1, thus generalizing their modelspecific proofs of invertibility.</p><p>In <ref type="figure" target="#fig_0">Fig. 2</ref>, we provide a Venn Diagram to illustrate the set of functions that satisfy the condition of Theorem 1. As depicted by the orange set labeled by det(J f ) = 0, Theorem 1 captures a subset of M where the Jacobians of functions are non-singular so that the change of variable formula is usable. Note the condition in Theorem 1 is sufficient but not necessary.</p><formula xml:id="formula_5">For example, f (x) = x 3 ∈ M is invertible, but J f (x = 0) = 3x 2 | x=0 = 0 is singular.</formula><p>Many previous invertible models with special architectures, such as NICE, Real NVP, and MAF, can be viewed as elements belonging to subsets of det(J f ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient inversion of the invertible modules</head><p>In this section, we show that when the conditions in Theorem 1 hold, not only do we know that f is invertible (f ∈ M), but also we have a fixed-point iteration method to invert f with strong theoretical guarantees and good performance in practice.</p><p>The pseudo-code of our proposed inversion algorithm is described in Algorithm 1. Theoretically, we can prove that this method is locally convergent-as long as the initial value is close to the true value, the method is guaranteed to find the correct inverse. We formally summarize this result in Theorem 2. Theorem 2. The iterative method of Algorithm 1 is locally convergent whenever 0 &lt; α &lt; 2.</p><p>Algorithm 1 Fixed-point iteration method for computing f −1 (z).</p><p>Require: T, α T is the number of iterations; 0 &lt; α &lt; 2 is the step size.</p><formula xml:id="formula_6">1: Initialize x 0 2: for t ← 1 to T do 3: Compute f (x t−1 ) 4: Compute diag(J f (x t−1 )) 5: x t ← x t−1 − α diag(J f (x t−1 )) −1 (f (x t−1 ) − z) 6: end for return x T Proof.</formula><p>We provide a more rigorous proof in Appendix A.</p><p>In practice, the method is also easily parallelizable on GPUs, making the cost of inverting f ∈ M similar to that of an i-ResNet <ref type="bibr" target="#b0">[1]</ref> layer. Within each iteration, the computation is mostly matrix operations that can be vectorized and run efficiently in parallel. Therefore, the time cost will be roughly proportional to the number of iterations, i.e., O(T ). As will be shown in our experiments, Algorithm 1 converges fast and usually the error quickly becomes negligible when T D. This is in stark contrast to existing methods of inverting autoregressive flow models such as MAF <ref type="bibr" target="#b24">[24]</ref>, where D univariate equations need to be solved sequentially, requiring at least O(D) iterations. There are also other approaches for inverting f . For example, the bisection method is guaranteed to converge globally, but its computational cost is O(D), and is usually much more expensive than Algorithm 1. Note that as discussed earlier, autoregressive flow models can also be viewed as special cases of our framework. Therefore, Algorithm 1 is also applicable to inverting autoregressive flow models and could potentially result in large improvements of sampling speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Masked Invertible Networks</head><p>We show that techniques developed in Section 3 can be used to build our Masked Invertible Network (MintNet). First, we discuss how we compose several masked convolutions to form the Masked Invertible Layer (Mint layer). Next, we stack multiple Mint layers to form a deep neural network, i.e., the MintNet. Finally, we compare MintNets with several existing invertible architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Building the Masked Invertible Layer</head><p>We construct an invertible module in M that serves as the basic layer of our MintNet. This invertible module, named Mint layer, is defined as</p><formula xml:id="formula_7">L(x) = t x + K i=1 W 3 i h K j=1 W 2 ij h(W 1 j x + b 1 j ) + b 2 ij + b 3 i ,<label>(2)</label></formula><p>where denotes the elementwise multiplication,</p><formula xml:id="formula_8">{W 1 i }| K i=1 , {W 2 ij }| 1≤i,j≤K , and {W 3 i }| K i=1</formula><p>are all lower triangular matrices with additional constraints to be specified later, and t &gt; 0. Additionally, Mint layers use a monotonic activation function h, so that h ≥ 0. Common choices of h include ELU <ref type="bibr" target="#b3">[4]</ref>, tanh and sigmoid. Note that every individual weight matrix has the same size, and the 3 groups of weights</p><formula xml:id="formula_9">{W 1 i }| K i=1 , {W 2 ij }| 1≤i,j≤K and {W 3 i }| K i=1</formula><p>can be implemented with 3 masked convolutions (see Appendix B). We design the form of L(x) so that it resembles a ResNet / i-ResNet block that also has 3 convolutions with K × C filters, with C being the number of channels of x. When using Algorithm 1 to invert Mint layers, we initialize x 0 = z 1 t . From Proposition 1 in Section 3.2, we can easily conclude that L ∈ F. Now, we consider additional constraints on the weights so that L ∈ M, i.e., it is invertible. Note that the analytic form of its Jacobian is</p><formula xml:id="formula_10">J L (x) = K i=1 W 3 i A i K j=1 W 2 ij B j W 1 j + t,<label>(3)</label></formula><formula xml:id="formula_11">with A i = diag(h K j=1 W 2 ij h(W 1 j x + b 1 j ) + b 2 ij ) ≥ 0, B j = diag(h (W 1 j x + b 1 j ))</formula><p>≥ 0, and t &gt; 0. Therefore, once we impose the following constraint</p><formula xml:id="formula_12">diag(W 3 i ) diag(W 2 ij ) diag(W 1 j ) ≥ 0, ∀1 ≤ i, j ≤ K,<label>(4)</label></formula><p>we have diag(J L (x)) &gt; 0, which satisfies the condition of Theorem 1 and as a consequence we know L ∈ M. In practice, the constraint Eq. (4) can be easily implemented.</p><formula xml:id="formula_13">For all 1 ≤ i, j ≤ K, we impose no constraint on W 3 i and W 1 j , but replace W 2 ij with V 2 ij = W 2 ij sign(diag(W 2 ij )) sign(diag(W 3 i W 1 j )). Note that diag(V 2 ij ) has the same signs as diag(W 3 i ) diag(W 1 j ) and therefore diag(W 3 i ) diag(V 2 ij ) diag(W 1 j ) ≥ 0. Moreover, V 2 ij is almost everywhere differentiable w.r.t. W 2</formula><p>ij , which allows gradients to backprop through.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Constructing the Masked Invertible Network</head><p>In this section, we introduce design choices that help stack multiple Mint layers together to form an expressive invertible neural network, namely the MintNet. The full MintNet is constructed by stacking the following paired Mint layers and squeezing layers.</p><p>Paired Mint layers. As discussed above, our Mint layer L(x) always has a triangular Jacobian. To maximize the expressive power of our invertible neural network, it is undesirable to constrain the Jacobian of the network to be triangular since this limits capacity and will cause blind spots in the receptive field of masked convolutions. We thus always pair two Mint layers together-one with a lower triangular Jacobian and the other with an upper triangular Jacobian, so that the Jacobian of the paired layers is not triangular, and blind spots can be eliminated.</p><p>Squeezing layers. Subsampling is important for enlarging the receptive field of convolutions. However, common subsampling operations such as pooling and strided convolutions are usually not invertible. Following <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b0">[1]</ref>, we use a "squeezing" operation to reshape the feature maps so that they have smaller resolution but more channels. After a squeezing operation, the height and width will decrease by a factor of k , but the number of channels will increase by a factor of k 2 . This procedure is invertible and the Jacobian is an identity matrix. Throughout the paper, we use k = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to other approaches</head><p>In what follows we compare MintNets to several existing methods for developing invertible architectures. We will focus on architectures with a tractable Jacobian determinant. However, we note that there are models (cf ., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b7">8]</ref>) that allow fast inverse computation but do not have tractable Jacobian determinants. Following <ref type="bibr" target="#b0">[1]</ref>, we also provide some comparison in Tab. 5 (see Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Models based on identities of determinants</head><p>Some identities can be used to speed up the computation of determinants if the Jacobians have special structures. For example, in Sylvester flow <ref type="bibr" target="#b1">[2]</ref>, the invertible transformation has the form</p><formula xml:id="formula_14">f (x) x + Ah(Bx + b), where h(·) is a nonlinear activation function, A ∈ R D×M , B ∈ R M ×D , b ∈ R M and M ≤ D. By Sylvester's determinant identity, det(J f (x)) can be computed in O(M 3 ), which is much less than O(D 3 ) if M D.</formula><p>However, the requirement that M is small becomes a bottleneck of the architecture and limits its expressive power. Similarly, Planar flow <ref type="bibr" target="#b27">[27]</ref> uses the matrix determinant lemma, but has an even narrower bottleneck.</p><p>The form of L(x) bears some resemblance to Sylvester flow. However, we improve the capacity of Sylvester flow in two ways. First, we add one extra non-linear convolutional layer. Second, we avoid the bottleneck that limits the maximum dimension of latent representations in Sylvester flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Models based on dimension partitioning</head><p>NICE <ref type="bibr" target="#b4">[5]</ref>, Real NVP <ref type="bibr" target="#b5">[6]</ref>, and Glow <ref type="bibr" target="#b15">[16]</ref> all depend on an affine coupling layer. Given d &lt; D, x is first partitioned into two parts x = [x 1:d ; x d+1:D ]. The coupling layer is an invertible transformation, defined as f : x → z, z 1:d = x 1:d , z d+1:D = x d+1:D exp(s(x 1:d )) + t(x 1:d ), where s(·) and t(·) are two arbitrary functions. However, the partitioning of x relies on heuristics, and the performance is sensitive to this choice (cf ., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>). In addition, the Jacobian of f is a triangular matrix with diagonal [1 d ; exp(s(x 1:d ))]. In contrast, the Jacobian of MintNets has more flexible diagonals-without being partially restricted to 1's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Models based on autoregressive transformations</head><p>By leveraging autoregressive transformations, the Jacobian can be made triangular. For example, MAF <ref type="bibr" target="#b24">[24]</ref> defines the invertible tranformation as f : x → z, z i = µ(x 1:i−1 ) + σ(x 1:i−1 )x i , where µ(·) ∈ R and σ(·) ∈ R + . Note that f −1 (z) can be obtained by sequentially solving x i based on previous solutions x 1:i−1 . Therefore, a naïve approach requires Ω(D) computations for inverting autoregressive models. Moreover, the architecture of f is only an affine combination of autoregressive functions with x. In contrast, MintNets are inverted with faster fixed-point iteration methods, and the architecture of MintNets is arguably more flexible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Free-form invertible models</head><p>Some work proposes invertible transformations whose Jacobians are not limited by special structures. For example, FFJORD <ref type="bibr" target="#b8">[9]</ref> uses a continuous version of change of variables formula <ref type="bibr" target="#b2">[3]</ref> where the determinant is replaced by trace. Unlike MintNets, FFJORD needs an ODE solver to compute its value and inverse, and uses a stochastic estimator to approximate the trace. Another work is i-ResNet <ref type="bibr" target="#b0">[1]</ref> which constrains the Lipschitz-ness of ResNet layers to make it invertible. Both i-ResNet and MintNet use ResNet blocks with 3 convolutions. The inverse of i-ResNet can be obtained efficiently by a parallelizable fixed-point iteration method, which has comparable computational cost as our Algorithm 1. However, unlike MintNets whose Jacobian determinants are exact, the log-determinant of Jacobian of an i-ResNet must be approximated by truncating a power series and estimating each term with stochastic estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Other models using masked convolutions</head><p>Emerging convolutions <ref type="bibr" target="#b12">[13]</ref> and MaCow <ref type="bibr" target="#b20">[20]</ref> improve the Glow architecture by replacing 1 × 1 convolutions in the original Glow model with masked convolutions similar to those employed in MintNets. Emerging convolutions and MaCow are both inverted using forward/back substitutions designed for inverting triangular matrices, which requires the same number of iterations as the input dimension. In stark contrast, MintNets use a fixed-point iteration method (Algorithm 1) for inversion, which is similar to i-ResNet and requires substantially fewer iterations than the input dimension. For example, our method of inversion takes 120 iterations to converge on CIFAR-10, while inverting emerging convolutions will need 3072 iterations. In other words, our inversion can be 25 times faster on powerful GPUs. Additionally, the architecture of MintNet is very different. The architectures of <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b20">[20]</ref> are both built upon Glow. In contrast, MintNet is a ResNet architecture where normal convolutions are replaced by causal convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate our MintNet architectures on both image classification and density estimation. We focus on three common image datasets, namely MNIST, CIFAR-10 and ImageNet 32×32. We also empirically verify that Algorithm 1 can provide accurate solutions within a small number of iterations. We provide more details about settings and model architectures in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification</head><p>To check the capacity of MintNet and understand the trade-off of invertibility, we test its classification performance on MNIST and CIFAR-10, and compare it to a ResNet with a similar architecture.</p><p>On MNIST, MintNet achieves a test accuracy of 99.6%, which is the same as that of the ResNet. On CIFAR-10, MintNet reaches 91.2% test accuracy while ResNet reaches 92.6%. Both MintNet and ResNet achieve 100% training accuracy on MNIST and CIFAR-10 datasets. This indicates that MintNet has enough capacity to fit all data labels on the training dataset, and the invertible representations learned by MintNet are comparable to representations learned by non-invertible networks in terms of generalizability. Note that the small degradation in classification accuracy is also observed in other invertible networks. For example, depending on the Lipschitz constant, the gap between test accuracies of i-ResNet and ResNet can be as large as 1.92% on CIFAR-10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Density estimation and verification of invertibility</head><p>In this section, we demonstrate the superior performance of MintNet on density estimation by training it as a flow generative model. In addition, we empirically verify that Algorithm 1 can accurately produce the inverse using a small number of iterations. We show that samples can be efficiently generated from MintNet by inverting each Mint layer with Algorithm 1.</p><p>Density estimation. In Tab. 1, we report bits per dimension (bpd) on MNIST, CIFAR-10, and ImageNet 32×32 datasets. It is notable that MintNet sets the new records of bpd on all three datasets. Moreover, when compared to previous best models, our MNIST model uses 30% fewer parameters than FFJORD, and our CIFAR-10 and ImageNet 32×32 models respectively use 60% and 74% fewer parameters than Glow. When trained on datasets such as CIFAR-10, MintNet requires 2 GPUs for approximately five days, while FFJORD is trained on 6 GPUs for five days, and Glow on 8 GPUs for seven days. Note that all values in Tab. 1 are with respect to the continuous distribution of uniformly dequantized images, and results of models that view images as discrete distributions are not directly comparable (e.g., PixelCNN <ref type="bibr" target="#b22">[22]</ref>, IAF-VAE <ref type="bibr" target="#b16">[17]</ref>, and Flow++ <ref type="bibr" target="#b11">[12]</ref>). To show that MintNet learns semantically meaningful representations of images, we also perform latent space interpolation similar to the interpolation experiments in Real NVP (see Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verification of invertibility.</head><p>We first examine the performance of Algorithm 1 by measuring the reconstruction error of MintNets. We compute the inverse of MintNet by sequentially inverting each Mint layer with Algorithm 1. We used grid search to select the step size α in Algorithm 1 and chose α = 3.5, 1.1, 1.15 respectively for MNIST, CIFAR-10 and ImageNet 32×32. An interesting fact is for MNIST, α = 3.5 actually works better than other values of α within (0, 2), even though it does not have the theoretical gurantee of local convergence. As <ref type="figure" target="#fig_2">Fig. 4a</ref> shows, the normalized L 2 reconstruction error converges within 120 iterations for all datasets considered. Additionally, <ref type="figure" target="#fig_2">Fig. 4b</ref> demonstrates that the reconstructed images look visually indistinguishable to true images.</p><p>Samples. Using Algorithm 1, we can generate samples efficiently by computing the inverse of MintNets. We use the same step sizes as in the reconstruction error analysis, and run Algorithm 1 for 120 iterations for all three datasets. We provide uncurated samples in <ref type="figure" target="#fig_1">Fig. 3</ref>, and more samples can be found in Appendix F. In addition, we compare our sampling time to that of the other models (see Tab. 6 in Appendix E). Our sampling method has comparable speed as i-ResNet. It is approximately 5 times faster than autoregressive sampling on MNIST, and is roughly 25 times faster on CIFAR-10 and ImageNet 32×32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a new method to compositionally construct invertible modules that are flexible, efficient to invert, and with a tractable Jacobian. Starting from linear transformations with triangular matrices, we apply a set of composition rules to recursively build new modules that are non-linear and more expressive (Proposition 1). We then show that the composed modules are invertible as long as their Jacobians are non-singular (Theorem 1), and propose an efficiently parallelizable numerical method  (Algorithm 1) with theoretical guarantees (Theorem 2) to compute the inverse. The Jacobians of our modules are all triangular, which allows efficient and exact determinant computation.</p><p>As an application of this idea, we use masked convolutions as our basic module. Using our composition rules, we compose multiple masked convolutions together to form a module named Mint layer, following the architecture of a ResNet block. To enforce its invertibility, we constrain the masked convolutions to satisfy the condition of Theorem 1. We show that multiple Mint layers can be stacked together to form a deep invertible network which we call MintNet. The architecture can be efficiently inverted using a fixed point iteration algorithm (Algorithm 1). Experimentally, we show that MintNet performs well on MNIST and CIFAR-10 classification. Moreover, when trained as a generative model, MintNet achieves new state-of-the-art performance on MNIST, CIFAR-10 and ImageNet 32×32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>Notations. Let J f (x) denote the Jacobian of f evaluated at x. We use [f (x)] i to denote the i-th component of the vector-valued function f , and [J f (x)] ij to denote the ij-th entry of J f (x). We further use x i to denote the i-th component of the input vector x ∈ R D , and ∂[f (x)]i ∂xj x=t to denote the partial derivative of [f (x)] i w.r.t. x j , evaluated at x = t. Proposition 1. Define F as the set of all continuously differentiable functions whose Jacobian is lower triangular. Then F contains the basic module in Section 3.1, and is closed under the following composition rules.</p><p>• Rule of addition.</p><formula xml:id="formula_15">f 1 ∈ F ∧ f 2 ∈ F ⇒ λf 1 + µf 2 ∈ F, where λ, µ ∈ R. • Rule of composition. f 1 ∈ F ∧ f 2 ∈ F ⇒ f 2 • f 1 ∈ F. A special case is f ∈ F ⇒ h • f ∈ F, where h(·)</formula><p>is a continuously differentiable non-linear activation function that is applied element-wisely.</p><p>Proof. Since the basic modules have the form f (x) = Wx+b, where W is a lower triangular matrix, we immediately know that f is continuously differentiable and J f is lower triangular, therefore f ∈ F. Next, we prove the closeness properties of F one by one.</p><p>• Rule of addition. f = λf 1 + µf 2 is continuously differentiable, and J f is lower triangular. This is because ∂f /∂x = ∂(λf1+µf2) /∂x = λ ∂f1 /∂x + µ ∂f2 /∂x, and both ∂f1 /∂x and ∂f2 /∂x are continuous and lower triangular.</p><p>• Rule of composition. f = f 2 • f 1 is continuously differentiable and has a lower triangular Jacobian. This is because ∂f /∂x = ∂(f2•f1) /∂x = ∂f2 /∂x x=f1(x) ∂f1 /∂x, and both ∂f2 /∂x and ∂f1 /∂x are continuous and lower triangular. As a special case, we choose f 1 = h, where h is a continuously differentiable univariate function. Since the Jacobian of h is diagonal and continuous, we have h ∈ F. Therefore h • f 2 ∈ F holds true for all f 2 ∈ F.</p><p>The following two lemmas will be very helpful for proving Theorem 1. Lemma 1. J f (x) is lower triangular for all x ∈ R D implies [f (x)] i is a function of x 1 , ..., x i , and does not depend on x i+1 , · · · , x D .</p><p>Proof. Due to the fact that J f (x) is lower triangular, we have [J f (x)] i,j = ∂[f (x)]i ∂xj = 0 for any j &gt; i. When x 1 , ..., x j−1 , x j+1 , ..., x D are fixed, we have</p><formula xml:id="formula_16">[f (x 1 , ..., x j−1 , x j , x j+1 , x D )] i = [f (x 1 , ..., x j−1 , 0, x j+1 , ..., x D )] i + xj 0 ∂[f (t)] i ∂t j dt j (5) = [f (x 1 , ..., x j−1 , 0, x j+1 , ..., x D )] i .<label>(6)</label></formula><p>This implies that [f (x)] i does not depend on x j for any j &gt; i. In other words, f (x) is only a function of x 1 , ..., x i .  Proof. Assume without loss of generality that J f (x) is lower triangular. We first prove that</p><formula xml:id="formula_17">Lemma 2. diag(J f (x)J f (0)) &gt; 0 implies that for any 1 ≤ i ≤ n, either (i) ∀x ∈ R D : [J f (x)] ii &gt; 0 or (ii) ∀x ∈ R D : [J f (x)] ii &lt; 0. That is, [f (x)] i is monotonic w.r.t. x i when x 1 , · · · , x i−1 are fixed. Proof. Clearly diag(J f (x)J f (0)) &gt; 0 is equivalent to ∀1 ≤ i ≤ n, x ∈ R D : [J f (x)] ii [J f (0)] ii = ∂[f (x)]i ∂xi ∂[f (x)]i ∂xi</formula><formula xml:id="formula_18">diag(J f (x)J f (0)) &gt; 0 by contradiction. Assuming diag(J f (x)J f (0)) ≤ 0, then ∃1 ≤ i ≤ n, x ∈ R D such that [J f (x )] ii [J f (0)] ii ≤ 0. Because J f (x)</formula><p>is always triangular and non-singular, we immediately conclude that</p><formula xml:id="formula_19">[J f (x )] ii [J f (0)] ii &lt; 0. Assume without loss of generality that [J f (0)] ii &gt; 0 and [J f (x )] ii &lt; 0.</formula><p>Then, by the intermediate value theorem, we know that ∃t ∈ (0, 1) such that [J f (tx )] ii = 0, which contradicts that fact that J f (x) is always non-singular.</p><p>Next, we prove that for all z in the range of f (x), there exists a unique x such that f (x) = z. To obtain x 1 , we only need to solve [f (x)] 1 = z 1 , which is an equation of variable x 1 , as concluded from Lemma 1. Since Lemma 2 implies that [f (x)] 1 is monotonic w.r.t. x 1 , we know that [f (x)] 1 has a unique inverse x 1 whenever z 1 is in the range of [f (x)] 1 . Now assume we have already obtained x 1 , ..., x k , where k ≥ 1. In this case, Lemma 1 asserts that [f (x)] k+1 = z k+1 is an equation of variable x k+1 . Again Lemma 2 implies that [f (x)] k+1 is a monotonic function of x k+1 given x 1 , · · · , x k , which implies further that [f (x)] k+1 = z k+1 has a unique solution x k+1 whenever z k+1 is in the range of [f (x)] k+1 . By induction, we can solve for x 1 , x 2 , · · · , x D by repetitively employing this procedure, which concludes that f −1 (z) = (x 1 , ..., x D ) exists, and can be determined uniquely.</p><p>Theorem 2. The iterative method of Algorithm 1 is locally convergent whenever 0 &lt; α &lt; 2.</p><p>Proof. Let z be any value in the range of f (x) and g(</p><formula xml:id="formula_20">x; α, z) x − α diag(J f (x)) −1 [f (x) − z],</formula><p>where diag(A) −1 denotes a diagonal matrix whose diagonal entries are the reciprocals of those of A. The iterative method of Algorithm 1 can be written as x t = g(x t−1 ; α, z). Because of Theorem 1, there exists a unique x * ∈ R D such that f (x * ) = z, in which case g(x * ; α, z) = x * . Applying the product rule, we have</p><formula xml:id="formula_21">J g (x * ; α, z) = I − α diag(J f (x * )) −1 J f (x * ),</formula><p>where J g (x * ; α, z) denotes the Jacobian of g(x; α, z) evaluated at x * . Since J f (x * ) is triangular, J g (x * ; α, z) will also be triangular. Therefore, the only eigenvalue of J g (x * ; α, z) is 1 − α, due to the fact that the only solution to the equation system det(λI − J g (x * ; α, z)) = (λ − 1 + α) D = 0 is λ = 1 − α. Since 0 &lt; α &lt; 2, the spectral radius of J g (x * ; α, z) satisfies ρ(J g (x * ; α, z)) = |1 − α| &lt; 1. Then the Ostrowski Theorem (cf ., Theorem 10.1.3. in <ref type="bibr" target="#b23">[23]</ref>) shows that the sequence {x 1 , x 2 , · · · , x t } obtained by x t = g(x t−1 ; α, z) converges locally to x * as t → ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Masked convolutions</head><p>Convolution is a special type of linear transformation that proves to be very effective for image data. The basic invertible module can be implemented using masked convolutions (e.g., causal convolutions in PixelCNN <ref type="bibr" target="#b22">[22]</ref>). Consider a 2D convolutional layer with C in input feature maps, C out filters, a kernel size of R × R and a zero-padding of R /2 . We assume R is an odd integer and C out = C in so that the input and output of the convolutional layer have the same shape. Let W ∈ R Cout×Cin×R×R be the weight tensor of this layer. We define a mask M ∈ {0, 1} Cout×Cin×R×R that satisfies</p><formula xml:id="formula_22">M[i, j, m, n] = 0, if i &lt; j or i = j ∧ m &gt; R /2 or i = j ∧ m = R /2 ∧ n &gt; R /2 , 1,</formula><p>Otherwise.</p><p>The masked convolution then uses M W as the weight tensor. In <ref type="figure" target="#fig_4">Fig. 1</ref>, we provide an illustration on a 3 × 3 masked convolution with 3 filters.</p><p>In MintNet, L(x) is efficiently implemented with 3 masked convolutional layers. The weights and masks are denoted as (W 1 , M 1 ), (W 2 , M 2 ) and (W 3 , M 3 ), which separately correspond to</p><formula xml:id="formula_24">{W 1 i } K i=1 , {W 2 ij } 1≤i,j≤K , {W 3 j } K j=1 in Eq. (2)</formula><p>. Let C be the number of input feature maps, and suppose the kernel size is R × R. The shapes of W 1 , W 2 and W 3 are respectively (KC, C, R, R), (KC, KC, R, R) and (C, KC, R, R). The masks of them are simple concatenations of copies of the mask in Eq. <ref type="bibr" target="#b6">(7)</ref>. For instance, M 1 consists of K copies of Eq. <ref type="bibr" target="#b6">(7)</ref>, and M 2 consists of K × K copies. Using masked convolutions, L(x) can be concisely written as</p><formula xml:id="formula_25">L(x) = t x + (W 3 M 3 ) h (W 2 M 2 ) h (W 1 M 1 ) x + b 1 + b 2 + b 3 ,<label>(8)</label></formula><p>where b 1 , b 2 , b 3 are biases, and denotes the operation of discrete 2D convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Interpolation of hidden representations</head><p>Given four images x 1 , x 2 , x 3 , x 4 in the dataset, let z i = f (x i ), where i = 1, 2, 3, 4, be the corresponding features in the feature domain. Similar to <ref type="bibr" target="#b5">[6]</ref>, in the feature domain, we define</p><formula xml:id="formula_26">z = cos(φ)(cos(φ )z 1 + sin(φ )z 2 ) + sin(φ)(cos(φ )z 3 + sin(φ )z 4 )<label>(9)</label></formula><p>where x-axis corresponds to φ , y-axis corresponds to φ, and both φ and φ range over {0, π 14 , ..., 7π 14 }. We then transform z back to the image domain by taking f −1 (z). Interpolation results are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experiment setup and network architecture</head><p>Hyperparameter tuning and computation infrastructure. We use the standard train/test split of MNIST, CelebA and CIFAR-10. We tune our models by observing its training bpd. For density estimation on CIFAR-10 and ImageNet 32×32, the models were run on two Titan XP GPUs. In other cases the model was run on one Titan XP GPU.</p><p>Classification setup. Following <ref type="bibr" target="#b0">[1]</ref>, we pad the images to 16 channels with zeros. This corresponds to the first convolution in ResNet which increases the number of channels to 16. Both ResNet and our MintNet are trained with AMSGrad <ref type="bibr" target="#b26">[26]</ref> for 200 epochs with the cosine learning rate schedule <ref type="bibr" target="#b19">[19]</ref> and an initial learning rate of 0.001. Both networks use a batch size of 128.</p><p>Classification architecture. The ResNet contains 38 pre-activation residual blocks <ref type="bibr" target="#b10">[11]</ref>, and each block has three 3 × 3 convolutions. The architecture is divided into 3 stages, with 16, 64 and 256 filters respectively. Our MintNet uses 19 grouped invertible layers, which include a total of 38 residual invertible layers, each having three 3 × 3 convolutions. Batch normalization is applied before each invertible layer. Note that batch normalization does not affect the invertibility of our network, because during test time it uses fixed running average and standard deviation and is an invertible operation. We use 2 squeezing blocks at the same position where ResNet applies subsampling, and matches the number of filters used in ResNet. To produce the logits for classification, both MintNet and ResNet first apply global average pooling and then use a fully connected layer (see Tab. 2).</p><p>Density estimation setup. We mostly follow the settings in <ref type="bibr" target="#b24">[24]</ref>. All training images are dequantized and transformed using the logit transformation. Networks are trained using AMSGrad <ref type="bibr" target="#b25">[25]</ref>.</p><p>On MNIST, we decay the learning rate by a factor of 10 at the 250th and 350th epoch, and train for 400 epochs. On CIFAR-10, we train with cosine learning rate decay for a total of 200 epochs. On ImageNet 32×32, we train with cosine learning rate decay for a total of 350k steps. All initial learning rates are 0.001.</p><p>Density estimation architecture. For density estimation on MNIST, we use 20 paired Mint layers with 45 filters each. For both CIFAR-10 and ImageNet 32×32, we use 21 paired Mint layers, each of which has 255 filters. For all the three datasets, two squeezing operations are used and are distributed evenly across the network (see Tab. 3 and Tab. 4).</p><p>Tuning the step size for sampling. We perform grid search to find hyperparamter α for Algorithm 1 using a minibatch of 128 images. More specifically, we start from α = 1 to 5 with a step size 0.5 for MNIST, CIFAR-10, and ImageNet 32×32, and compute the normalized L 2 reconstruction error with respect to the number of iterations. The normalized L 2 error is defined as x − y 2 2 /D, where x ∈ R D and y ∈ R D are two image vectors corresponding to the original and reconstructed images. We find that the algorithm converges most quickly when α is in intervals <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> for MNIST, CIFAR-10 and ImageNet 32×32 respectively. Then we perform a second round grid search on the corresponding interval with a step size 0.05. In this case, we are able to find the best α, that is α = 3.5, 1.1, 1.15 for the corresponding datasets.</p><p>Verification of invertibility. To verify the invertibility of MintNet, we study the normalized L 2 reconstruction error for MNIST, CIFAR-10 and ImageNet 32×32. The L 2 reconstruction error is computed for 128 images on all three datasets. We plot the exponential of the mean log reconstruction errors in <ref type="figure" target="#fig_2">Fig. 4</ref>. The shaded area corresponds to the exponential of the standard deviation of log reconstruction errors.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional tables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Samples</head><p>In this section, we provide more uncurated MintNet samples on MNIST, CIFAR-10 and ImageNet 32×32.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Venn Diagram relationships between invertible functions (I), the function sets of F and M, functions that meet the conditions of Theorem 1 (det(J f ) = 0), functions whose Jacobian is triangular and Jacobian diagonals are strictly positive (diag(J f ) &gt; 0), functions whose Jacobian is triangular and Jacobian diagonals are all 1s (diag(J f ) = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Uncurated samples on MNIST, CIFAR-10, and ImageNet 32×32 datasets. (a) Reconstruction error analysis. (b) Reconstructed images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy analysis of Algorithm 1 on MNIST, CIFAR-10, and ImageNet 32×32 datasets. Each curve in (a) represents the mean value of normalized reconstruction errors for 128 images. The 2nd, 4th and 6th rows in (b) are reconstructions, while other rows are original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>x=0 &gt; 0 .</head><label>0</label><figDesc>This means for any x ∈ R D , [J f (x)] ii = ∂[f (x)]i ∂xi = 0 and it shares the same sign with [J f (0)] ii = ∂[f (x)]i ∂xi x=0 , a constant that is either strictly positive or strictly negative. This further implies that when x 1 , · · · , x i−1 are fixed, ∂[f (x)]i ∂xi is either strictly positive or strictly negative for all x i ∈ R, and [f (x)] i is therefore monotonic w.r.t. x i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 1 .</head><label>1</label><figDesc>If f ∈ F and J f (x) is non-singular for all x in the domain, then f is invertible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>MintNet interpolation of hidden representation. Left: MintNet MNIST latent space interpolation. Middle: MintNet CIFAR-10 latent space interpolation. Right: MintNet ImageNet 32×32 latent space interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>MintNet MNIST samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>MintNet CIFAR-10 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>MintNet ImageNet 32×32 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MNIST, CIFAR-10, ImageNet 32×32 bits per dimension (bpd) results. Smaller values are better. † Result not directly comparable because ZCA preprocssing was used.</figDesc><table><row><cell>Method</cell><cell cols="3">MNIST CIFAR-10 ImageNet 32×32</cell></row><row><cell>NICE [5]</cell><cell>4.36</cell><cell>4.48  †</cell><cell>-</cell></row><row><cell>MAF [24]</cell><cell>1.89</cell><cell>4.31</cell><cell>-</cell></row><row><cell>Real NVP [6]</cell><cell>1.06</cell><cell>3.49</cell><cell>4.28</cell></row><row><cell>Glow [16]</cell><cell>1.05</cell><cell>3.35</cell><cell>4.09</cell></row><row><cell>FFJORD [9]</cell><cell>0.99</cell><cell>3.40</cell><cell>-</cell></row><row><cell>i-ResNet [1]</cell><cell>1.06</cell><cell>3.45</cell><cell>-</cell></row><row><cell>MintNet (ours)</cell><cell>0.98</cell><cell>3.32</cell><cell>4.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MintNet image classification network architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>MintNet MNIST density estimation network architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>MintNet CIFAR-10 and Imagenet 32×32 density estimation network architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison to some common invertible models.</figDesc><table><row><cell>Property</cell><cell>NICE Real-NVP Glow MaCow FFJORD i-ResNet MintNet</cell></row><row><cell>Analytic Forward</cell><cell></cell></row><row><cell>Analytic Inverse</cell><cell></cell></row><row><cell>Non-volume Preserving</cell><cell></cell></row><row><cell>Exact Likelihood</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Sampling time for 64 samples for MintNet, i-ResNet and autoregressive method on the same model architectures. The time is evaluated on a NVIDIA TITAN Xp.</figDesc><table><row><cell>Method</cell><cell cols="3">MNIST CIFAR-10 ImageNet 32×32</cell></row><row><cell>i-ResNet [1] (100 iterations)</cell><cell>11.56s</cell><cell>99.41s</cell><cell>92.53s</cell></row><row><cell>Autoregressive (1 iteration)</cell><cell>63.61s</cell><cell>2889.64s</cell><cell>2860.21s</cell></row><row><cell>MintNet (120 iterations) (ours)</cell><cell>12.81s</cell><cell>117.83s</cell><cell>120.78s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by Intel Corporation, Amazon AWS, TRI, NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00995</idno>
		<title level="m">Invertible residual networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05649</idno>
		<title level="m">Sylvester normalizing flows for variational inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NICE: non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2214" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2214" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ffjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">Free-form continuous dynamics for scalable reversible generative models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emerging convolutions for generative normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2771" to="2780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Excessive invariance causes adversarial vulnerability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">i-revnet: Deep invertible networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<editor>D. D. Lee, M. Sugiyama, U. V</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalizing hamiltonian monte carlo with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04208</idno>
		<title level="m">Macow: Masked convolutional generative flow</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reversible recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9029" to="9040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Iterative solution of nonlinear equations in several variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Rheinboldt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="volume">30</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the convergence proof of amsgrad and a new version</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Phong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03590</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>F. Bach and D. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A-nice-mc: Adversarial training for mcmc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5140" to="5150" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
