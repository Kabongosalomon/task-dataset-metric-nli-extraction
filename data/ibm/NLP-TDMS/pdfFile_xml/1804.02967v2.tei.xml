<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Dolz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Gopinath</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Lombaert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Desrosiers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><forename type="middle">Ben</forename><surname>Ayed</surname></persName>
						</author>
						<title level="a" type="main">HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>brain MRI</term>
					<term>segmentation</term>
					<term>3D CNN</term>
					<term>multi-modal imaging !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on 6-month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M ULTI-MODAL imaging is of primary importance for developing comprehensive models of pathologies and increasing the statistical power of current imaging biomarkers <ref type="bibr" target="#b0">[1]</ref>. In neuroimaging studies, different magnetic resonance imaging (MRI) modalities are often combined to overcome the limitations of independent imaging techniques. While T1-weighted images yield a good contrast between gray matter (GM) and white matter (WM) tissues, T2-weighted and proton density (PD) pulses help visualize tissue abnormalities like lesions. Likewise, fluid attenuated inversion recovery (FLAIR) images can enhance the image contrast of white matter lesions resulting from multiple sclerosis <ref type="bibr" target="#b1">[2]</ref>. In brain segmentation, considering multiple MRI modalities is essential to obtain accurate results. This is particularly true for the segmentation of infant brains, where tissue contrast is low ( <ref type="figure" target="#fig_0">Fig. 1)</ref>.</p><p>Advances in multi-modal imaging, however, come at the price of an inherently large amount of data, imposing a burden on disease assessments. Visual inspections of such an enormous amount of medical images are prohibitively time-consuming, prone to errors and unsuitable for largescale studies. Therefore, automatic and reliable multi-modal segmentation algorithms are of high interest to the clinical community.</p><p>Manuscript received XXX; revised XXX. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Prior work</head><p>Multi-modal image segmentation in brain-related applications has received a substantial research attention, for instance, brain tumors <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref>, brain tissues of both infant <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b16">[17]</ref> and adult <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, subcortical structures <ref type="bibr" target="#b19">[20]</ref>, among other problems <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Atlas-propagation approaches are commonly used in multi-modal scenarios <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. These methods rely on registering one or multiple atlases to the target image, followed by a propagation of manuals labels. When several atlases are considered, labels from individual atlases can be combined into a final segmentation via a label fusion strategy <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>. When relying solely on atlas fusion, the performance of such techniques might be limited and prone to registration errors. Parametric or deformable models <ref type="bibr" target="#b10">[11]</ref> can be used to refine prior estimates of tissue probability <ref type="bibr" target="#b13">[14]</ref>. For example, the study in <ref type="bibr" target="#b13">[14]</ref> investigated a patch-driven method for neonatal brain tissue segmentation, integrating the probability maps of a subjectspecific atlas into a level-set framework.</p><p>More recently, our community has witnessed a wide adoption of deep learning techniques, particularly, convolutional neural networks (CNNs), as an effective alternative to traditional segmentation approaches. CNN architectures are supervised models, trained end-to-end, to learn a hierarchy of image features representing different levels of abstraction. In contrast to conventional classifiers based on hand-crafted features, CNNs can learn both the features and classifier simultaneously, in a data-driven manner. They achieved state-of-the-art performances in a broad range of medical image segmentation problems <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, including multimodal tasks <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Fusion of multi-modal CNN feature representations</head><p>Most of the existing multi-modal CNN segmentation techniques followed an early-fusion strategy, which integrates the multi-modality information from the original space of lowlevel features <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. For instance, in <ref type="bibr" target="#b14">[15]</ref>, MRI T1, T2 and fractional anisotropy (FA) images are simply merged at the input of the network. However, as argued in <ref type="bibr" target="#b29">[30]</ref> in the context of multi-modal learning, it is difficult to discover highly non-linear relationships between the low-level features of different modalities, more so when such modalities have significantly different statistical properties. In fact, early-fusion methods implicitly assume that the relationship between different modalities are simple (e.g., linear). For instance, the early fusion in <ref type="bibr" target="#b14">[15]</ref> learns complementary information from T1, T2 and FA images. However, the relationship between the original T1, T2 and FA image data may be much more complex than complementarity, due to significantly different image acquisition processes <ref type="bibr" target="#b15">[16]</ref>. The work in <ref type="bibr" target="#b15">[16]</ref> advocated late fusion of highlevel features as a way that accounts better for the complex relationships between different modalities. They used an independent convolutional network for each modality, and fused the outputs of the different networks in higher-level layers, showing better performance than early fusion in the context infant brain segmentation. These results are in line with a recent study in the machine learning community <ref type="bibr" target="#b29">[30]</ref>, which investigated multimodal learning with deep Boltzmann machines in the context of fusing data from color images and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Dense connections in deep networks</head><p>Since the recent introduction of residual learning in <ref type="bibr" target="#b31">[32]</ref>, shortcut connections from early to late layers have become very popular in a breadth of computer vision problems <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Unlike traditional networks, these connections back-propagate gradients directly, thereby mitigating the gradient-vanishing problem and allowing deeper networks. Furthermore, they transform a whole network into a large ensemble of shallower networks, yielding competitive performances in various applications <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref>. DenseNet <ref type="bibr" target="#b37">[38]</ref> extended the concept of shortcut connections, with the input of each layer corresponding to the outputs from all previous layers. Such a dense network facilitates the gradient flow and the learning of more complex patterns, which yielded significant improvements in accuracy and efficiency for natural image classification tasks <ref type="bibr" target="#b37">[38]</ref>. Inspired by this success, recent works have included dense connections in deep networks for medical image segmentation <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>. However, these works have either considered a single modality <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> or have simply concatenated multiple modalities in a single stream <ref type="bibr" target="#b40">[41]</ref>. So far, the impact of dense connectivity across multiple network paths, and its application to multi-modal image segmentation, remains unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths; see the illustration in <ref type="figure" target="#fig_1">Fig. 2</ref>. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input (early fusion) or at the output (late fusion) of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstractions, which increases significantly the learning representation in comparison to early/late fusion. Furthermore, hyper-dense connections facilitate the learning as they improve gradient flow and impose implicit deep supervision. We report extensive evaluations over two different 1 and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available 2 .</p><p>A preliminary conference version of this work appeared at ISBI 2018 <ref type="bibr" target="#b41">[42]</ref>. This journal version is a substantial extension, including (1) a much broader, more informative/rigorous treatment of the subject in the general context of multi-modal segmentation; and (2) comprehensive experiments with additional baselines and publicly available benchmarks, as well as a thorough investigation of the practical usefulness and impact of hyper-dense connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS AND MATERIALS</head><p>Convolutional neural networks (CNNs) are deep models that can learn feature representations automatically from the training data. They consist of multiple layers, each processing the imaging data at a different level of abstraction, enabling segmentation algorithms to learn from large datasets and discover complex patterns that can be further employed for predicting unseen samples. The first attempts to use CNNs in segmentation problems followed a slidingwindow strategy, where the regions defined by the window are processed independently, which impedes segmentation accuracy and computational efficiency. To overcome these 1. iSEG 2017 focuses on 6-month infant data, whereas MRBrainS 2013 uses adult data. Therefore, there are significant differences between the two benchmarks in term of image data characteristics, e.g, the voxel spacing and number of available modalities.</p><p>2. https://www.github.com/josedolz/HyperDenseNet limitations, the network can be viewed as a single nonlinear convolution, which is trained end-to-end, a process known as fully CNN (FCNN) <ref type="bibr" target="#b42">[43]</ref>. The latter brings several advantages over standard CNNs. It can handle images of arbitrary sizes and avoid redundant convolution and pooling operations, enabling computationally efficient learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The proposed Hyper-Dense network</head><p>The concept of "the deeper the better" is considered as a key principle in deep learning <ref type="bibr" target="#b31">[32]</ref>. Nevertheless, one obstacle when dealing with deep architectures is the problem of vanishing/exploding gradients, which hampers convergence during training. To address these limitations in very deep architectures, the study in <ref type="bibr" target="#b37">[38]</ref> investigated densely connected networks. DenseNets are built on the idea that adding direct connections from any layer to all the subsequent layers in a feed-forward manner makes training easier and more accurate. This is motivated by three observations. First, there is an implicit deep supervision thanks to the short paths to all feature maps in the architecture. Second, direct connections between all layers help improving the flow of information and gradients throughout the entire network. Third, dense connections have a regularizing effect, which reduces the risk of over-fitting on tasks with smaller training sets. Inspired by the recent success of densely-connected networks in medical image segmentation works <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, we propose a hyper-dense architecture for multi-modal image segmentation that extends the concept of dense connectivity to the multi-modal setting: each imaging modality has a path, and dense connections occur not only between layers within the same path, but also between layers across different paths (see <ref type="figure" target="#fig_1">Fig. 2</ref> for an illustration).</p><p>Let x l be the output of the l th layer. In CNNs, this vector is typically obtained from the output of the previous layer x l−1 by a mapping H l composed of a convolution followed by a non-linear activation function:</p><formula xml:id="formula_0">x l = H l x l−1 .<label>(1)</label></formula><p>A densely-connected network concatenates all feature outputs in a feed-forward manner,</p><formula xml:id="formula_1">x l = H l [x l−1 , x l−2 , . . . , x 0 ] ,<label>(2)</label></formula><p>where [. . .] denotes a concatenation operation. Pushing this idea further, HyperDenseNet introduces a more general connectivity definition, in which we link the outputs from layers in different streams, each associated with a different image modality. In the multi-modal setting, our hyper-dense connectivity yields a much more powerful feature representation than early/late fusion as the network learns the complex relationships between the modalities within and in-between all the levels of abstractions. For simplicity, let us consider the scenario of two image modalities, although extension to N modalities is straightforward. Let x 1 l and x 2 l denote the outputs of the l th layer in streams 1 and 2, respectively. In general, the output of the l th layer in a stream s can then be defined as follows:</p><formula xml:id="formula_2">x s l = H s l [x 1 l−1 , x 2 l−1 , x 1 l−2 , x 2 l−2 , . . . , x 1 0 , x 2 0 ] .<label>(3)</label></formula><p>Shuffling and interleaving feature map elements in a CNN was recently found to enhance the efficiency and performance, while serving as a strong regularizer <ref type="bibr" target="#b43">[44]</ref>- <ref type="bibr" target="#b45">[46]</ref>. This is motivated by the fact that intermediate CNN layers perform deterministic transformations to improve the performance, however, relevant information might be lost during these operations <ref type="bibr" target="#b46">[47]</ref>. To overcome this issue, it is therefore beneficial for intermediate layers to offer a variety of information exchange while preserving the aforementioned deterministic functions. Motivated by this principle, we thus concatenate feature maps in a different order for each branch and layer:</p><formula xml:id="formula_3">x s l = H s l π s l ([x 1 l−1 , x 2 l−1 , x 1 l−2 , x 2 l−2 , . . . , x 1 0 , x 2 0 ]) , (4) with π s</formula><p>l being a function that permutes the feature maps given as input. For instance, in the case of two image modalities, we could have: <ref type="figure" target="#fig_1">Figure 2</ref> shows a section of the proposed architecture, where each gray region represents a convolutional block. For simplicity, we assume that the red arrows indicate convolution operations only, whereas the black arrows represent the direct connections between feature maps from different layers, within and in-between the different streams. Thus, the input of each convolutional block (maps before the red arrow) is the concatenation of the outputs (maps after the red arrow) of all the preceding layers from both paths.</p><formula xml:id="formula_4">x 1 l = H 1 l [x 1 l−1 , x 2 l−1 , x 1 l−2 , x 2 l−2 , . . . , x 1 0 , x 2 0 ] x 2 l = H 2 l [x 2 l−1 , x 1 l−1 , x 2 l−2 , x 1 l−2 , . . . , x 2 0 , x 1 0 ])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Baselines</head><p>To investigate thoroughly the impact of hyper-dense connections between different streams in multi-modal image segmentation, several baselines were considered. First, we extended the semi-dense architecture proposed in <ref type="bibr" target="#b16">[17]</ref> to a fully-dense one, by connecting the output of each convolutional layer to all subsequent layers. In this network, we follow an early-fusion strategy, in which MRI T1 and T2 are integrated at the input of the CNN and processed jointly along a single path ( <ref type="figure" target="#fig_2">Fig. 3, left)</ref>. The connectivity setting of this model corresponds to Eq. (2). Second, instead of merging both modalities at the input of the network, we considered a late-fusion strategy, where each modality is processed independently in different streams and learned features are fused before the first fully connected layer ( <ref type="figure" target="#fig_2">Fig.  3</ref>, middle). In this model, the dense connections are included within each path, assuming the connectivity definition of Eq. (2) for each stream.</p><p>As last baseline, we used an early fusion model which combines features from different streams after the first convolutional layer <ref type="figure" target="#fig_2">(Fig. 3</ref>, right). Since this non-linear combination of features is re-used in all subsequent layers, the resulting network is similar to our hyper-dense model of Eq. (3).</p><p>However, there are two important differences. First, each stream in our model processes its input differently, as shown by the stream-indexed function H s l in Eq. (3). Also, as described above, each stream performs a different shuffling of inputs, which can enhance robustness to the model and mitigate the risk of overfitting. Our experiments in Section 3 demonstrate empirically the advantages of our model compared to this baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network architecture</head><p>To have a large receptive field, FCNNs typically use full images as input. The number of parameters is then limited via pooling/unpooling layers. A problem with this approach is the loss of resolution from repeated down-sampling operations. In the proposed method, we follow the strategy in <ref type="bibr" target="#b4">[5]</ref>, where sub-volumes are used as input, avoiding pooling layers. While sub-volumes of size 27×27×27 are considered for training, we used 35 × 35 × 35 non-overlapping subvolumes during inference, as in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[26]</ref>. This strategy offers two considerable benefits. First, it reduces the memory requirements of our network, thereby removing the need for spatial pooling. More importantly, it substantially increases the number of training examples and, therefore, does not need data augmentation.  <ref type="table" target="#tab_1">Table 2</ref> summarizes the parameters of the baselines and the proposed HyperDenseNet. The network parameters are optimized via the RMSprop optimizer, using cross-entropy as cost function. Let θ denotes the network parameters (i.e., convolution weights, biases and a i from the parametric rectifier units), and y v s the label of voxel v in the s-th image segment. We optimize the following:</p><formula xml:id="formula_5">J(θ) = − 1 S ·V S s=1 V v=1 C c=1 δ(y v s = c) · log p v c (x s ),<label>(5)</label></formula><p>where p v c (x s ) is the softmax output of the network for voxel v and class c, when the input segment is x s .</p><p>To initialize the weights of the network, we adopted the strategy proposed in <ref type="bibr" target="#b47">[48]</ref>, which yields fast convergence for very deep architectures. In this strategy, a zero-mean Gaussian distribution of standard deviation 2/n l is used to initialize the weights in layer l, where n l denotes the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Dense Path</head><p>Dual Dense Path Disentangled modalities with early fusion number of connections to the units in that layer. Momentum was set to 0.6 and the initial learning rate to 0.001, being reduced by a factor of 2 after every 5 epochs (starting from epoch 10). The network was trained for 30 epochs, each composed of 20 subepochs. At each subepoch, a total of 1000 samples were randomly selected from the training images and processed in batches of size 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS AND RESULTS</head><p>The proposed HyperDenseNet architecture is evaluated on challenging multi-modal image segmentation tasks, using publicly available data from two challenges: infant brain tissue segmentation, iSEG <ref type="bibr" target="#b48">[49]</ref>, and adult brain tissue segmentation, MRBrainS 3 . Quantitative evaluations and comparisons with state-of-the-art methods are reported for each of these applications. First, to evaluate the impact of dense connectivity on performance, we compared the proposed HyperDenseNet to the baselines described in Section 2.2 on infant brain tissue segmentation. Then, our results, compiled by the iSEG challenge organizers on testing data, are compared to those from the other competing teams. Second, to juxtapose the performance of HyperDenseNet to other segmentation networks under the same conditions, we provide a quantitative analysis of the results of current state-of-the-art segmentation networks for adult brain tissue segmentation. This includes comparison to the participants the MRBrainS challenge. Finally, in Section 3.3, we report a comprehensive analysis of feature re-use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">iSEG Challenge</head><p>The focus of this challenge was to compare (semi-) automatic stat-of-the-art algorithms for the segmentation of 6month infant brain tissues in T1-and T2-weighted brain MRI scans. This challenge was carried out in conjunction with MICCAI 2017, with a total of 21 international teams participating in the first round <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Evaluation</head><p>The iSEG-2017 organizers used three metrics to evaluate the accuracy of the competing methods: Dice Similarity Coefficient (DSC) <ref type="bibr" target="#b49">[50]</ref>, Modified Hausdorff distance (MHD), where the 95-th percentile of all Euclidean distances is 3. http://mrbrains13.isi.uu.nl employed, and Average Surface Distance (ASD). The first measures the degree of overlap between the segmentation region and ground truth, whereas the other two evaluate boundary distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Results</head><p>Validation results: <ref type="table" target="#tab_2">Table 3</ref> reports the performance achieved by HyperDenseNet and the baselines introduced in Section 2.2, for CSF, GM and WM brain tissues. The results were generated by splitting the 10 available iSEG-2017 volumes into training, validation and testing sets containing 6, 1 and 3 volumes, respectively. To show that improvements do not come from the higher number of learned parameters in HyperDenseNet, we also investigated a widened version of all baselines, with a similar parameter size as HyperDenseNet. The number of learned parameters of all the tested models is given in <ref type="table" target="#tab_3">Table 4</ref>. A more detailed description of the tested architectures can be found in <ref type="table" target="#tab_7">Table  8</ref> of the Supplemental materials ('Supplementary materials are available in the supplementary files /multimedia tab.').</p><p>We observe that the late fusion of deeper-layer features in independent paths provides a clear improvement over the single-path version, with an increase on performance of nearly 5%. Fusing the feature maps from independent paths after the first convolutional layer (i.e., Dual-Single) outperformed the other two baselines by 1-2%, particularly for WM and GM, which are the most challenging structures to segment. Also, the results indicate that processing multimodal data in separate paths, while allowing dense connectivity between all the paths, increases performance over early and late fusion, as well as over disentangled modalities with fusion performed after the first convolutional block. Another interesting finding is that increasing the number of learned parameters does not bring an important boost in performance. Indeed, in some tissues (e.g., CSF for Single path and Dual-Single path architectures), the performance slightly decreased when widening the architecture. <ref type="figure" target="#fig_3">Figures 4 and 5</ref> compare the training and validation accuracy between the baselines and HyperDenseNet. In these figures, the mean DSC for the three brain tissues is evaluated during training (Top) and validation (Bottom). One can see that HyperDenseNet outperforms baselines in both cases, achieving better results than architectures with a similar number of parameters. Performance improvements seen in <ref type="table" target="#tab_2">Table 3</ref>, <ref type="figure" target="#fig_3">Fig. 4</ref> and <ref type="figure" target="#fig_4">Fig. 5</ref> might be due to two factors: the high number of direct connections between different layers, which facilitates back-propagation of the gradient to shallow layers, and the freedom of the network to explore more complex patterns thanks to the combination of several image modalities at any level of abstraction.  The computational efficiency of HyperDenseNet and baselines is compared in <ref type="table" target="#tab_3">Table 4</ref>. As expected, inference times are proportional to the number of model parameters. While the lightest architecture needs around 45 seconds to segment a whole 3D brain, HyperDenseNet performs the same task in less than 2 minutes. This is acceptable from a clinical point of view. <ref type="figure" target="#fig_5">Figure 6</ref> depicts visual results for the subject used in validation. It can be seen that, in most cases, HyperDenseNet typically recovers thin regions better than the baselines, which can explain the improvements observed for distancebased metrics. As confirmed in <ref type="table" target="#tab_2">Table 3</ref>, this effect is most prominent in the boundaries between the gray and white matter. Furthermore, HyperDenseNet produces fewer false positives for WM than the baselines, which tend to overestimate the segmentation in this region.</p><p>Challenge results: <ref type="table">Table 5</ref> compares the segmentation accuracy of HyperDenseNet to that of top-5 ranking methods in the first round of the iSEG Challenge, as well as to all the methods in the second round of submission. We observe that our network ranked among the top-3 methods in 6 out of 9 metrics, considering the results of the first and second rounds of submissions.</p><p>A noteworthy point is the general performance decrease of all the methods for the segmentation of GM and WM, with lower DSC and larger ASD values. This confirms that segmenting these tissues is more challenging due to the unclear boundaries between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MRBrainS Challenge</head><p>The MRBrainS challenge was initially proposed in conjunction with MICCAI 2013. It focuses on adult brain tissue segmentation in the context of aging, based on three modalities: MRI T1, MRI T1 Inversion Recovery (IR) and MR-FLAIR. To this day, a total of 47 international teams have participated in this challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Evaluation</head><p>The organizers used three types of evaluation measures: a spatial overlap measure (DSC), a boundary distance measure (MHD) and a volumetric measure (the percentage of absolute volume difference).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Architectures for comparison</head><p>We compare HyperDenseNet to three state-of-the-art networks for medical image segmentation. The first architecture is a 3D fully convolutional neural network with residual connections <ref type="bibr" target="#b50">[51]</ref>, which we denote as FCN Res3D. The second one, referred to as UNet3D, is a U-Net [52] model with residual connections in the encoder and 3D volumes as input. Finally, our comparison also includes DeepMedic <ref type="bibr" target="#b4">[5]</ref>, which showed an outstanding performance in brain lesion segmentation. The implementation details of these architectures are described in Supplemental materials (Supplementary materials are available in the supplementary files /multimedia tab).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Results</head><p>Validation results: We performed a leave-one-outcross-validation (LOOCV) on the 5 available MRBrainS datasets, using 4 subjects for training and one for validation. We trained and tested models three times, each time using a different subject for validation, and measured the average accuracy over these three folds. For this experiment, we  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 5</head><p>Results on the iSEG-2017 data for HyperDenseNet and the methods ranked in the top-5 at the first round of submissions (in alphabetical order). The bold fonts highlight the best performances. Note: The reported values were obtained from the challenge organizers at the time of submitting this manuscript, in February 2018. For an updated ranking, see the iSEG-2017 Challenge website for first (http://iseg2017.web.unc.edu/rules/results/) and second (http://iseg2017.web.unc.edu/evaluation-on-the-second-round-submission/) rounds of submission. The method referred to as LIVIA is a previous work from our team <ref type="bibr" target="#b16">[17]</ref>. we assessed the impact of integrating multiple imaging modalities on the performance of HyperDenseNet using all possible combinations of two modalities as input. <ref type="table">Table 6</ref> reports the mean DSC and standard-deviation values of tested models, with FCN Res3D exhibiting the lowest mean DSC. This performance might be explained by the transpose convolutions in FCN Res3D, which may cause voxel misclassification within small regions. Furthermore, the downsampling and upsampling operations in FCN Res3D could make the feature maps in hidden layers sparser than the original inputs, causing a loss of image details. A strategy to avoid this problem is having skip connections as in UNet3D, which propagate information at different levels of abstraction between the encoding and decoding paths. This can be be observed in the results, where UNet3D clearly outperforms FCN Res3D in all the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSF GM WM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DSC MHD ASD DSC MHD ASD DSC MHD ASD</head><p>Moreover, DeepMedic obtained better results than its competitors, yielding a performance close to the different two-modality configurations of HyperDenseNet. The dual multiscale path is an important feature of DeepMedic which gives the network a larger receptive field via two paths, one for the input image and the other processing a lowresolution version of the input. This, in addition to the removal of pooling operations in DeepMedic, could explain the increase in performance with respect to FCN Res3D and UNet3D.</p><p>Comparing the different modality combinations, the two-modality versions of HyperDenseNet yielded competitive performances, although there is a significant variability between the three configurations. Using only MRI T1 and FLAIR places HyperDenseNet first for two DSC measures (GM and WM), and second for the remaining measure (CSF), even though competing methods used all three modalities. However, HyperDenseNet with three modalities yields significantly better segmentations, with the highest mean DSC values for all three tissues.</p><p>Challenge results: The MRBrainS challenge organizers compiled the results and a ranking of 47 international teams <ref type="bibr" target="#b3">4</ref> . In <ref type="table" target="#tab_5">Table 7</ref>, we report the results of the top-10 methods. We see that HyperDenseNet ranks first among competing methods, obtaining the highest DSC and HD for GM and WM. Interestingly, the BCH CRL IMAGINE and MSL SKKU teams participated in both iSEG and MRBrains2013 challenges. While these two networks outperformed HyperDenseNet in the iSEG challenge, the performance of our Model was noticeably superior in the MRBrains challenge, with HyperDenseNet ranked 1 st , MSL SKKU ranked 4 th and BCH CRL IMAGINE ranked 18 th (Ranking of February 2018). Considering the fact that three modalities are employed in MRBrains, unlike the two modalities used in iSEG, these results suggest that Hyper-DenseNet has stronger representation-learning power as the number of modalities increases.</p><p>A typical example of segmentation results is depicted in <ref type="figure">Fig. 7</ref>. In these images, red arrows indicate regions where the two-modality versions of HyperDenseNet fail in comparison to the three-modality version. As expected, most errors of these networks occur at the boundary between the 4. http://mrbrains13.isi.uu.nl/results.php GM and WM (see images in <ref type="figure" target="#fig_0">Fig. 1, for example)</ref>. Moreover, we observe that HyperDenseNet using three modalities can handle thin regions better than its two-modality versions. <ref type="figure">Fig. 7</ref>. A typical example of the segmentations achieved by the proposed HyperDenseNet in a validation subject (Subject 1 in the training set) for 2 and 3 modalities. The red arrows indicate some of the differences between the segmentations. For instance, one can see here that Hyper-DenseNet with three modalities can handle thin regions better than its two-modality versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis of features re-use</head><p>Dense connectivity enables each network layer to access feature maps from all its preceding layers, strengthening feature propagation and encouraging feature re-use. To investigate the degree at which features are used in the trained network, we computed, for each convolutional layer, the average L 1 -norm of connection weights to previous layers in any stream. This serves as a surrogate for the dependency of a given layer on its preceding layers. We normalized the values between 0 and 1 to facilitate visualization. <ref type="figure" target="#fig_7">Figure 8</ref> depicts the weights of HyperDenseNet trained with two modalities, for both iSEG and MRBrainS challenges. As the MRBrainS dataset contains three modalities, we have three different two-modality configurations. The average weights for the case of three modalities are shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. A dark square in these plots indicates that the target layer (on x-axis) makes a strong use of the features produced by the source layer (on y-axis). An important observation that one can make from both figures is that, in most cases, all layers spread the importance of the connections over many previous layers, not only within the same path, but also from the other streams. This indicates that shallow layer features are directly used by deeper layers from both paths, which confirms the usefulness of hyper-dense connections for information flow and learning complex relationships between modalities within different levels of abstractions.</p><p>Considering challenge datasets separately, for Hyper-DenseNet trained on iSEG (top row of <ref type="figure" target="#fig_7">Fig 8)</ref>, immediate previous layers have typically higher impact on the connections from both paths. Furthermore, the connections having access to MRI T2 features typically have the strongest values, which may indicate that T2 is more discriminative than T1 in this particular situation. We can also observe some regions <ref type="bibr">TABLE 6</ref> Comparison to several state-of-the-art 3D networks on the MRBrainS challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mean  with high (&gt; 0.5) feature re-use patterns from shallow to deep layers. The same behaviour is seen for HyperDenseNet trained on two modalities from the MRBrainS challenge, where immediate previous layers have a high impact on the connections within and in-between the paths. The reuse of low-level features by deeper layers is more evident than in the previous case. For example, in HyperDenseNet trained with T1-IR and FLAIR, deep layers in the T1-IR path make a strong use of features extracted in shallower layers of the same path, as well as in the path corresponding to FLAIR. This strong re-use of early features from both paths occurred across all tested configurations. The same pattern is observed when using three modalities <ref type="figure" target="#fig_8">(Fig 9)</ref>, with a strong re-use of shallow features from the network's last layers. This reflects the importance of giving deep layers access to early-extracted features. Additionally, it suggests that learning how and where to fuse information from multiple sources is more effective than combining these sources in early or late stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>This study investigated a hyper-densely connected 3D fully CNN, HyperDenseNet, with applications to brain tissue segmentation in multi-modal MRI. Our model leverages dense connectivity beyond recent works <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, exploiting the concept in multi-path architectures. Unlike these works, dense connections occur not only within the stream of individual modalities, but also across differents streams.</p><p>This give the network total freedom to explore complex combinations between features of different modalities, within and in-between all levels of abstraction. We reported a comprehensive evaluation using the benchmarks of two highly competitive challenges, iSEG-2017 for 6-month infant brain segmentation and MRBrainS for adult data, and showed state-of-the-art performances of HyperDenseNet on both datasets. Our experiments provided new insights on the inclusion of short-cut connections in deep neural networks for segmentating medical images, particularly in multi-modal scenarios. In summary, this work demonstrated the potential of HyperDenseNet to tackle challenging medical image segmentation problems involving multimodal volumetric data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets iSEG</head><p>Images were acquired at the UNC-Chapel Hill on a Siemens head-only 3T scanner with a circular polarized head coil, and were randomly chosen from the pilot study of the Baby Connectome Project (BCP) <ref type="bibr" target="#b4">5</ref> . During scan, infants were asleep, unsedated and fitted with ear protection, with the head secured in a vacuum-fixation device. T1-weighted images were acquired with 144 sagittal slices using the following parameters: TR/TE = 1900/4.38 ms, flip angle = 7 • and resolution = 1×1×1 mm 3 . Likewise, T2-weighted images were obtained with 64 axial slices, TR/TE = 7380/119 ms, flip angle = 150 • and resolution =1.25×1.25×1.95 mm 3 . T2 images were linearly aligned onto their corresponding T1 images. All the images were resampled into an isotropic 1×1×1 mm 3 resolution. Standard image pre-processing steps were then applied using in-house tools, including skull stripping, intensity inhomogeneity correction, and removal of the cerebellum and brain stem. For this application, 9 subjects were employed for training and 1 for validation. To obtain manual annotations, the organizers used 24-month follow-up scans to generate an initial automatic segmentation for 6-month subjects by employing a publicly available software iBEAT <ref type="bibr" target="#b5">6</ref> . Then, based on the initial automatic contours, an experienced neuroradiologist corrected manually the segmentation errors (based on both T1 and T2 images) and geometric defects via ITK-SNAP, with surface rendering.</p><p>MRBrainS 20 subjects with a mean age of 71 ± 4 years (10 male, 10 female) were selected from an ongoing cohort study of older (65 − 80 years of age), functionally-independent individuals without a history of invalidating stroke or other brain diseases <ref type="bibr" target="#b55">[56]</ref>. To test the robustness of the segmentation algorithms in the context of aging-related pathology, the subjects were selected to have varying degrees of atrophy and white-matter lesions, and the scans with major artifacts were excluded. The following sequences were acquired and used for the evaluation framework: 3D T1 (TR: 7.9 ms, TE: 4.5 ms), T1-IR (TR: 4416 ms, TE: 15 ms, and TI: 400 ms) and T2-FLAIR (TR: 11000 ms, TE: 125 ms, and TI: 2800 ms). The sequences were aligned by rigid registration using Elastix <ref type="bibr" target="#b56">[57]</ref>, along with a bias correction performed using SPM8 <ref type="bibr" target="#b57">[58]</ref>. After the registration, the voxel size within all the provided sequences (T1, T1 IR, and T2 FLAIR) was 0.96×0.96×3.00 mm 3 . Five subjects that were representative for the overall data (2 male, 3 female and varying degrees of atrophy and white-matter lesions) were selected for training. The remaining fifteen subjects were provided as testing data. While ground truth was provided for the 5 training subjects, manual segmentations were unknown for the testing data set. The following structures were segmented and were available for training: (a) cortical gray matter, (b) basal ganglia, (c) white matter, (d) white matter 5. http://babyconnectomeproject.org 6. http://www.nitrc.org/projects/ibeat/ lesions, (e) peripheral cerebrospinal fluid, (f) lateral ventricles, (g) cerebellum and (h) brainstem. These structures can be merged into gray matter (a-b), white matter (c-d), and cerebrospinal fluid (e-f). The cerebellum and brainstem were excluded from the evaluation. Manual segmentations were drawn on the 3mm slice thickness scans by employing an in-house manual segmentation tool based on the contour segmentation objects tool in Mevislab 7 , starting with the inner most structures. While the outer border of the CSF was segmented using both T1 and T1 IR scans, the other regions were segmented on the T1 scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dice similarity coefficient (DSC)</head><p>Let V ref and V auto be, respectively, the reference and automatic segmentations of a given tissue class and for a given subject. The DSC for this subject is defined as</p><formula xml:id="formula_6">DSC V ref , V auto = 2 | V ref ∩ V auto | | V ref | + | V auto |<label>(6)</label></formula><p>DSC values are within a [0, 1] range, 1 indicating perfect overlap and 0 corresponding to a total mismatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average volume distance (AVD)</head><p>Using the same definitions for V auto and V ref , AVD corresponds to</p><formula xml:id="formula_7">AVD V ref , V auto = | V ref − V auto | V ref · 100<label>(7)</label></formula><p>Modified Hausdorff distance (MHD) </p><p>where d(q, P ) is the point-to-set distance defined by: d(q, P ) = min p∈P q − p , with . denoting the Euclidean distance. Low MHD values indicate high boundary similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average surface distance (ASD)</head><p>Using the same notation as the Hausdorff distance above, the ASD corresponds to</p><formula xml:id="formula_9">ASD P ref , Pauto = 1 |P ref | p ∈ P ref d(p, Pauto),<label>(9)</label></formula><p>where |.| denotes the cardinality of a set. In distance-based metrics, smaller values indicate higher proximity between two point sets and, thus, a better segmentation. <ref type="bibr" target="#b6">7</ref>. https://www.mevislab.de/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We extended our 3D FCNN architecture proposed in <ref type="bibr" target="#b25">[26]</ref>, which is based on Theano. The source code of this architecture is publicly available <ref type="bibr" target="#b7">8</ref> . Training and testing was performed on a server equipped with a NVIDIA Tesla P100 GPU with 16 GB of RAM memory. Training HyperDenseNet took around 70 min per epoch, and around 35 hours in total for the two-modality version. With three image modalities, training each epoch took nearly 3 hours. Inference on a whole 3D MR scan took on average from 70-80 to 250-270 seconds, for the two-and three-modality versions, respectively.</p><p>The number of kernels per layer in each of the baselines and the proposed network are detailed in <ref type="table" target="#tab_7">Table 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FCN Res3D</head><p>The architecture of FCN Res3D consists on 5 convolutional blocks with residual units on the encoder path, with 16, 64, 128, 256 and 512 kernels. The decoding path contains 4 convolutional upsampling blocks, each composed of 4 kernels, one per class. At each residual block, batch normalization and a Leaky ReLU with a leakage value of 0.1 are employed before the convolution. Instead of including max-pooling operations to re-size the images, stride values of 2 × 2 × 2 are used in layers 2, 3 and 4. Volume size at the input of the network is 64 × 64 × 24. The implementation of this network is provided in [53] 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNet3D</head><p>Although quite similar to FCN Res3D, UNet3D presents some differences, particularly in the decoding path. It contains 9 convolutional blocks in total, 4 in the encoding and 5 in the decoding path. The number of kernels in the encoding path are 32, 64, 128 and 256, with strides of 2 × 2 × 2 at layers 2, 3 and 4. In the decoding path, the number of kernels are 256, 128, 64, 32 and 4, from the first to the last layer. Furthermore, skip connections are added at the convolutional blocks of the same scale between the encoding and decoding paths. As in FCN Res3D, batch normalization and a Leaky ReLU with a leakage value of 0.1 are employed before the convolution at each block. Volume size at the input of the network is also 64 × 64 × 24. The implementation is provided in <ref type="bibr" target="#b52">[53]</ref>. <ref type="bibr" target="#b7">8</ref>. https://github.com/josedolz/SemiDenseNet 9. https://github.com/DLTK/DLTK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepMedic</head><p>We used the default architecture of DeepMedic in our experiments. This architecture includes two paths with 8 convolutional blocks: <ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50</ref>, 50 kernels of size 3×3×3. At the end of both paths, two fully connected convolutional layers with 150 1×1×1 filters each are added, before the last classification layer. The second path is used with a low-resolution version of the input at the first path, for a larger receptive field. The input patch size is 27×27×27 and 35×35×35 for training and segmentation, respectively. The official code 10 is employed to evaluate this architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example of data from a training subject. Neonatal isointense brain images from a mid-axial T1 slice (left), the corresponding T2 slice (middle), and manual segmentation (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A section of the proposed HyperDenseNet in the case of two image modalities. Each gray region represents a convolutional block. Red arrows correspond to convolutions and black arrows indicate dense connections between feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Section of baseline architectures: single-path dense (left), dual-path dense (middle) with disentangled modalities and disentangled modalities with early fusion in a single path (right). While both modalities are concatenated at the input of the network in the first case, each modality is analyzed independently in the second architecture with the features being fused at the end of the streams. Each gray region represents a convolutional block. Red arrows correspond to convolutions and black arrows indicate dense connections between feature maps. Dense connections are propagated through the entire network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Training accuracy plots for the proposed architecture and the baselines on the iSeg-2017 challenge data. The first point of each curve corresponds to the end of the first training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Validation accuracy plots for the proposed architecture and the baselines on the iSeg-2017 challenge data. The first point of each curve corresponds to the end of the first training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative results of segmentation achieved by the baselines and HyperDenseNet on two validation subjects (each row shows a different subject). The green squares indicate some spots, where HyperDenseNet successfully reproduced the ground-truth whereas the baselines failed. Some regions where HyperDenseNet yielded incorrect segmentations are outlined in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(0.0161) 0.8163 (0.0222) 0.8607 (0.0178) UNet3D [52] (3-Modalities) 0.8218 (0.0159) 0.8432 (0.0241) 0.8841 (0.0123) DeepMedic [5] (3-Modalities) 0.8292 (0.0094) 0.8522 (0.0193) 0.8884 (0.0137) HyperDenseNet (T1-FLAIR) 0.8259 (0.0133) 0.8620 (0.0260) 0.8982 (0.0138) HyperDenseNet (T1 IR-FLAIR) 0.7991 (0.0181) 0.8226 (0.0255) 0.8654 (0.0087) HyperDenseNet (T1-T1 IR) 0.8191 (0.0297) 0.8498 (0.0173) 0.8913 (0.0082) HyperDenseNet (3-Modalities) 0.8485 (0.0078) 0.8663 (0.0247) 0.9016 (0.0109)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Relative importance of connections in HyperDenseNet trained on the iSEG (top) and MRBrainS (from 2 nd to 4 th rows) challenges with two modalities. The color at each location encodes the average L1 norm of weights connecting a convolutional-layer source to a convolutional-layer target. These values were normalized between 0 and 1 by accounting for all the values within each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Relative importance of connections in HyperDenseNet trained on the MRBrainS challenge with three modalities (MRI T1, FLAIR and T1 IR). The color at each location encodes the average L1 norm of weights connecting a convolutional-layer source to a convolutional-layer target. These values were normalized between 0 and 1 by accounting for all the values within each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Overview of representative works on multi-modal brain segmentation.</figDesc><table><row><cell>Work</cell><cell>Modality</cell><cell>Target</cell><cell>Method</cell></row><row><cell>Prastawa et al., 2005 [7]</cell><cell>T1,T2</cell><cell>Infant brain tissue</cell><cell>Multi-atlas</cell></row><row><cell>Weisenfeld et al., 2006 [8]</cell><cell>T1,T2</cell><cell>Infant brain tissue</cell><cell>Multi-atlas</cell></row><row><cell>Deoni et al., 2007 [20]</cell><cell>T1,T2</cell><cell>Thalamic nuclei</cell><cell>K-means clustering</cell></row><row><cell>Anbeek et al., 2008 [9]</cell><cell>T2,IR</cell><cell>Infant brain tissue</cell><cell>KNN</cell></row><row><cell>Weisenfeld and Warfield, 2009 [10]</cell><cell>T1,T2</cell><cell>Infant brain tissue</cell><cell>Multi-atlas</cell></row><row><cell>Wang et al., 2011 [11]</cell><cell>T1,T2,FA</cell><cell>Infant brain tissue</cell><cell>Multi-atlas + Level sets</cell></row><row><cell>Srhoj et al., 2012 [12]</cell><cell>T1,T2</cell><cell>Infant brain tissue</cell><cell>Multi-atlas + KNN</cell></row><row><cell>Wang et al., 2012 [13]</cell><cell>T1,T2</cell><cell>Infant brain tissue</cell><cell>Multi-atlas</cell></row><row><cell>Wang et al., 2014 [31]</cell><cell>T1,T2,FA</cell><cell>Infant brain tissue</cell><cell>Multi-atlas + Level sets</cell></row><row><cell>Kamnitsas et al., 2015 [28]</cell><cell>Flair, DWI, T1, T2</cell><cell>Brain lesion</cell><cell>3D FCNN + CRF</cell></row><row><cell>Zhang et al., 2015 [15]</cell><cell>T1,T2,FA</cell><cell>Infant brain tissue</cell><cell>2D CNN</cell></row><row><cell>Havaei et al., 2016 [4]</cell><cell>T1,T1c,T2,FLAIR</cell><cell>Multiple Sclerosis/Brain tumor</cell><cell>2D CNN</cell></row><row><cell>Nie et al., 2016 [16]</cell><cell>T1,T2,FA</cell><cell>Infant brain tissue</cell><cell>2D FCNN</cell></row><row><cell>Chen et al., 2017 [19]</cell><cell>T1,T1-IR,FLAIR</cell><cell>Brain tissue</cell><cell>3D FCNN</cell></row><row><cell>Dolz et al., 2017 [17]</cell><cell>T1,T2</cell><cell>Infant brain tissue</cell><cell>3D FCNN</cell></row><row><cell>Fidon et al., 2017 [6]</cell><cell>T1,T1c,T2,FLAIR</cell><cell>Brain tumor</cell><cell>CNN</cell></row><row><cell>Kamnitsas et al., 2017 [5]</cell><cell>T1,T1c,T2,FLAIR MPRAGE,FLAIR,T2,PD</cell><cell>Brain tumour/lesions</cell><cell>3D FCNN + CRF</cell></row><row><cell>Kamnitsas et al., 2017 [22]</cell><cell>MPRAGE,FLAIR,T2,PD</cell><cell>Traumatic brain injuries</cell><cell>3D FCNN(Adversarial Training)</cell></row><row><cell>Valverde et al., 2017 [23]</cell><cell>T1, T2,FLAIR</cell><cell>Multiple-sclerosis</cell><cell>3D FCNN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>The layers used in the baselines and the proposed architecture and the corresponding values with an input of size 27×27×27. In the case of multi-modal images, the convolutional layers (conv x) are present in any network path. All the convolutional layers have a stride of one pixel.</figDesc><table><row><cell></cell><cell>Conv. kernel</cell><cell># kernels</cell><cell>Output Size</cell><cell>Dropout</cell></row><row><cell>conv 1</cell><cell>3×3×3</cell><cell>25</cell><cell>25×25×25</cell><cell>No</cell></row><row><cell>conv 2</cell><cell>3×3×3</cell><cell>25</cell><cell>23×23×23</cell><cell>No</cell></row><row><cell>conv 3</cell><cell>3×3×3</cell><cell>25</cell><cell>21×21×21</cell><cell>No</cell></row><row><cell>conv 4</cell><cell>3×3×3</cell><cell>50</cell><cell>19×19×19</cell><cell>No</cell></row><row><cell>conv 5</cell><cell>3×3×3</cell><cell>50</cell><cell>17×17×17</cell><cell>No</cell></row><row><cell>conv 6</cell><cell>3×3×3</cell><cell>50</cell><cell>15×15×15</cell><cell>No</cell></row><row><cell>conv 7</cell><cell>3×3×3</cell><cell>75</cell><cell>13×13×13</cell><cell>No</cell></row><row><cell>conv 8</cell><cell>3×3×3</cell><cell>75</cell><cell>11×11×11</cell><cell>No</cell></row><row><cell>conv 9</cell><cell>3×3×3</cell><cell>75</cell><cell>9×9×9</cell><cell>No</cell></row><row><cell>fully conv 1</cell><cell>1×1×1</cell><cell>400</cell><cell>9×9×9</cell><cell>Yes</cell></row><row><cell>fully conv 2</cell><cell>1×1×1</cell><cell>200</cell><cell>9×9×9</cell><cell>Yes</cell></row><row><cell>fully conv 3</cell><cell>1×1×1</cell><cell>150</cell><cell>9×9×9</cell><cell>Yes</cell></row><row><cell>Classification</cell><cell>1×1×1</cell><cell>4</cell><cell>9×9×9</cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Performance on the testing set, in terms of DSC, for the investigated baselines and the proposed architecture. The best performance is highlighted in bold.</figDesc><table><row><cell></cell><cell>Architectures</cell><cell>CSF</cell><cell>WM</cell><cell>GM</cell></row><row><cell>No connectivity</cell><cell>Single Path Single Path  *</cell><cell cols="3">0.9014 0.8518 0.8370 0.9010 0.8532 0.8401</cell></row><row><cell>between paths</cell><cell>Dual Path Dual Path  *</cell><cell cols="3">0.9482 0.9078 0.8875 0.9503 0.9089 0.8872</cell></row><row><cell>Connectivity between paths</cell><cell>Dual-Single Path Dual-Single Path  *  HyperDenseNet</cell><cell cols="3">0.9552 0.9142 0.9008 0.9541 0.9159 0.9017 0.9580 0.9183 0.9035</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Widened version.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Number of parameters (convolution, fully-connected and total) and inference times of the baselines and the proposed architecture. Widened versions of the baselines, which we denoted using superscript * , are also included.</figDesc><table><row><cell>Architecture</cell><cell></cell><cell cols="2">Nb. of parameters</cell><cell>Time (sec)</cell></row><row><cell></cell><cell>Conv.</cell><cell>Fully-conn.</cell><cell>Total</cell><cell></cell></row><row><cell>Single Path Single Path  *</cell><cell>2,380,050 9,518,850</cell><cell>290,600 470,600</cell><cell>2,670,650 9,989,450</cell><cell>43.67 (±8.37) 101.63 (±12.65)</cell></row><row><cell>Dual Path Dual Path  *</cell><cell>4,760,100 9,381,960</cell><cell>470,600 614,600</cell><cell>5,230,700 9,996,560</cell><cell>64.57 (±9.45) 104.31 (±11.65)</cell></row><row><cell>Dual-Single Path Dual-Single Path  *</cell><cell>2,666,760 9,518,850</cell><cell>300,600 470,600</cell><cell>2,968,200 9,989,450</cell><cell>47.33 (±8.74) 103.64 (±13.61)</cell></row><row><cell>HyperDenseNet</cell><cell>9,518,850</cell><cell>830,600</cell><cell cols="2">10,349,450 105.67 (±14.74)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Widened version.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7</head><label>7</label><figDesc>Results of the MRBrainS challenge of different methods (DSC, HD (mm) and AVD). Only the top-10 methods are included in this table. Note: The reported values were obtained from the challenge organizers after submitting our results, in February 2018. For an updated ranking, see the MRBrainS Challenge website (http://mrbrains13.isi.uu.nl/results.php).</figDesc><table><row><cell>Method</cell><cell>GM</cell><cell>WM</cell><cell>CSF</cell><cell>Sum</cell></row><row><cell></cell><cell>DSC HD AVD</cell><cell>DSC HD AVD</cell><cell>DSC HD AVD</cell><cell></cell></row><row><cell>HyperDenseNet (ours)</cell><cell>0.8633 1.34 6.19</cell><cell>0.8946 1.78 6.03</cell><cell>0.8342 2.26 7.31</cell><cell>48</cell></row><row><cell cols="2">VoxResNet [19] + Auto-context 0.8615 1.44 6.60</cell><cell>0.8946 1.93 6.05</cell><cell>0.8425 2.19 7.69</cell><cell>54</cell></row><row><cell>VoxResNet [19]</cell><cell>0.8612 1.47 6.42</cell><cell>0.8939 1.93 5.84</cell><cell>0.8396 2.28 7.44</cell><cell>56</cell></row><row><cell>MSL-SKKU</cell><cell>0.8606 1.52 6.60</cell><cell>0.8900 2.11 5.54</cell><cell>0.8376 2.32 6.77</cell><cell>61</cell></row><row><cell>LRDE</cell><cell>0.8603 1.44 6.05</cell><cell>0.8929 1.86 5.83</cell><cell>0.8244 2.28 9.03</cell><cell>61</cell></row><row><cell>MDGRU</cell><cell>0.8540 1.54 6.09</cell><cell>0.8898 2.02 7.69</cell><cell>0.8413 2.17 7.44</cell><cell>80</cell></row><row><cell>PyraMiD-LSTM2</cell><cell>0.8489 1.67 6.35</cell><cell>0.8853 2.07 5.93</cell><cell>0.8305 2.30 7.17</cell><cell>83</cell></row><row><cell>3D-UNet [52]</cell><cell>0.8544 1.58 6.60</cell><cell>0.8886 1.95 6.47</cell><cell>0.8347 2.22 8.63</cell><cell>84</cell></row><row><cell>IDSIA [54]</cell><cell>0.8482 1.70 6.77</cell><cell>0.8833 2.08 7.05</cell><cell>0.8372 2.14 7.09</cell><cell>100</cell></row><row><cell>STH [55]</cell><cell>0.8477 1.71 6.02</cell><cell>0.8845 2.34 7.67</cell><cell>0.8277 2.31 6.73</cell><cell>112</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Let P ref and P auto denote the sets of voxels within the reference and automatic segmentation boundary, respectively. MHD is given byMHD P ref , Pauto = max max</figDesc><table><row><cell>q∈P ref</cell><cell>d(q, Pauto), max q∈P auto</cell><cell>d(q, P ref ) ,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Number of kernels (in convolutional and fully-connected layers) of the baselines and the proposed architecture. The architecture with two paths have the same number of kernels in both paths for the same convolutional block.</figDesc><table><row><cell>Architecture</cell><cell>Conv. kernels</cell><cell>Fully-conn. kernels</cell></row><row><cell>Single Path Single Path  *</cell><cell>[25, 25, 25, 50, 50, 50, 75, 75, 75] [50, 50, 50, 75, 75, 75, 150, 150, 150]</cell><cell>[400, 200, 150] [400, 200, 150]</cell></row><row><cell>Dual Path Dual Path  *</cell><cell>[25, 25, 25, 50, 50, 50, 75, 75, 75] [40, 40, 40, 70, 70, 70, 100, 100, 100]</cell><cell>[400, 200, 150] [400, 200, 150]</cell></row><row><cell>Dual-Single Path Dual-Single Path  *</cell><cell>[25, 25, 25, 50, 50, 50, 75, 75, 75] [25, 50, 50, 100, 100, 100, 150, 150, 150]</cell><cell>[400, 200, 150] [400, 200, 150]</cell></row><row><cell>HyperDenseNet</cell><cell>[25, 25, 25, 50, 50, 50, 75, 75, 75]</cell><cell>[400, 200, 150]</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>* Widened version.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">. https://github.com/Kamnitsask/deepmedic</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the National Science and Engineering Research Council of Canada (NSERC), discovery grant program, and by the ETS Research Chair on Artificial Intelligence in Medical Imaging. The authors would like to thank both iSEG and MRBrainS organizers for providing data benchmarks and evaluations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hybrid imaging (SPECT/CT and PET/CT): improving therapeutic decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Delbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schöder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Wahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminars in nuclear medicine</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="308" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation of multiple sclerosis lesions in brain MRI: a review of automated approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lladó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freixenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramió-Torrentà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rovira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="185" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HeMIS: Hetero-modal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guizard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient multiscale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable multimodal convolutional networks for brain tumour segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Garcia-Peraza-Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ekanayake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="285" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic segmentation of MR images of the developing newborn brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation of newborn brain MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Weisenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mewes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomedical Imaging: Nano to Macro, 2006. 3rd IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="766" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic brain tissue segmentation in neonatal magnetic resonance imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groenendaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Osch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Grond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatric research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="163" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic segmentation of newborn brain MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Weisenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="564" to="572" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic segmentation of neonatal images using convex optimization and coupled level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="805" to="817" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic segmentation of neonatal brain MRI using atlas based segmentation and machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srhoj-Egekher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Kersbergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Isgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI Grand Challenge: Neonatal Brain Segmentation</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Proceedings of the MICCAI Grand Challenge: Neonatal Brain Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuklisova-Murgasova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
	<note>An atlasbased method for neonatal MR brain tissue segmentation</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segmentation of neonatal brain MR images using patch-driven level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for multi-modality isointense infant brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="214" to="224" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multi-modality isointense infant brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1342" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05319</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MRBrainS challenge: online evaluation framework for brain image segmentation in 3T MRI scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kuijf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breeuwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Bouvy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">De</forename><surname>Bresser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alansary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence and neuroscience</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VoxResNet: Deep voxelwise residual networks for brain segmentation from 3D MR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation of thalamic nuclei using a modified k-means clustering algorithm and high-resolution quantitative magnetic resonance imaging at 1.5T</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Deoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Rutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Parrent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="126" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">MSSEG Challenge proceedings: Multiple Sclerosis Lesions Segmentation Challenge using a data management and processing infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Commowick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cervenansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ameli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation in brain lesion segmentation with adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on IPMI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving automated multiple sclerosis lesion segmentation with a cascaded 3D convolutional neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valverde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Roura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>González-Villà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pareto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramió-Torrentà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Rovira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lladó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A review on brain structures segmentation in magnetic resonance imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>González-Villà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Valverde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zwiggelaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lladó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="45" to="69" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review on automatic fetal and neonatal brain MRI segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Counsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Esophagus segmentation in CT via 3D fully convolutional neural network and random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fechter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adebahr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-scale 3D convolutional neural networks for lesion segmentation in brain MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ischemic Stroke Lesion Segmentation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic segmentation of MR brain images with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moeskops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Benders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1252" to="1261" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2949" to="2980" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integration of sparse multi-modality representation and anatomical constraint for isointense infant brain MR image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="152" to="164" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on CVPR</title>
		<meeting>the IEEE conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">H-DenseUNet: Hybrid densely connected UNet for liver and liver tumor segmentation from CT volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07330</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic 3D cardiovascular MR segmentation with densely-connected volumetric convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dsouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Abidin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wismuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02427</idno>
		<title level="m">MRI tumor segmentation with densely connected 3D CNN</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Isointense infant brain segmentation with a Hyper-dense connected convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="616" to="620" />
		</imprint>
	</monogr>
	<note>Biomedical Imaging (ISBI)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interleaved group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4373" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Regularization of convolutional neural networks using shufflenode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICCV</title>
		<meeting>the IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Benchmark on automatic 6-month-old infant brain segmentation algorithms: The iseg-2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puybareau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P W</forename><surname>Moeskops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Sanroma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benkarim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casamitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vilaplana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">DLTK: State of the art reference implementations for deep learning on medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06853</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Parallel multi-dimensional LSTM, with application to fast biomedical volumetric image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2998" to="3006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automatic brain segmentation using artificial neural networks with shape context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahbod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Smedby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Disruption of the cerebral white matter network is related to slowing of information processing speed in patients with type 2 diabetes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Reijmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brundel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Kappelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Biessels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2112" to="2115" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Elastix: a toolbox for intensity-based medical image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="205" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Statistical parametric mapping: the analysis of functional brain images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Nichols</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
