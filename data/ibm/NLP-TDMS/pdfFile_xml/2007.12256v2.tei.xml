<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Recognizing Unseen Categories in Unseen Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
							<email>mancini@diag.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>4 Fondazione Bruno Kessler</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Politecnico di Torino</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Italian Instituite of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Recognizing Unseen Categories in Unseen Domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>2[0000−0001−8595−9955]</term>
					<term>Zeynep Akata 2[0000−0002−1432−7747]</term>
					<term>Elisa Ricci 3</term>
					<term>4[0000−0002−0228−1147]</term>
					<term>and Barbara Caputo 5</term>
					<term>6[0000−0001−7169−0158] Keywords: Zero-Shot Learning</term>
					<term>Domain Generalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current deep visual recognition systems suffer from severe performance degradation when they encounter new images from classes and scenarios unseen during training. Hence, the core challenge of Zero-Shot Learning (ZSL) is to cope with the semantic-shift whereas the main challenge of Domain Adaptation and Domain Generalization (DG) is the domain-shift. While historically ZSL and DG tasks are tackled in isolation, this work develops with the ambitious goal of solving them jointly, i.e. by recognizing unseen visual concepts in unseen domains. We present CuMix (Curriculum Mixup for recognizing unseen categories in unseen domains), a holistic algorithm to tackle ZSL, DG and ZSL+DG. The key idea of CuMix is to simulate the test-time domain and semantic shift using images and features from unseen domains and categories generated by mixing up the multiple source domains and categories available during training. Moreover, a curriculum-based mixing policy is devised to generate increasingly complex training samples. Results on standard ZSL and DG datasets and on ZSL+DG using the DomainNet benchmark demonstrate the effectiveness of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite their astonishing success in several applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>, deep visual models perform poorly for the classes and scenarios that are unseen during training. Most existing approaches are based on the assumptions that (a) training and test data come from the same underlying distribution, i.e. domain shift, and (b) the set of classes seen during training constitute the only classes that will be seen at test time, i.e. semantic shift. These assumptions rarely hold in practice and, in addition to depicting different semantic categories, training and test images may differ significantly in terms of visual appearance in the real world.</p><p>To address these limitations, research efforts have been devoted to designing deep architectures able to cope with varying visual appearance <ref type="bibr" target="#b6">[7]</ref> and with novel semantic concepts <ref type="bibr" target="#b46">[47]</ref>. In particular, the domain-shift problem <ref type="bibr" target="#b14">[15]</ref> has been addressed by proposing domain adaptation (DA) models <ref type="bibr" target="#b6">[7]</ref> that assume the availability target domain data during training. To circumvent this assumption, a recent trend has been to move to more complex scenarios where the adaptation problem must be either tackled online <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>, with the help of target domain descriptions <ref type="bibr" target="#b22">[23]</ref>, auxiliary data <ref type="bibr" target="#b31">[32]</ref> or multiple source domains <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. For instance, domain generalization (DG) methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5]</ref> aim to learn domainagnostic prediction models and to generalize to any unseen target domain.</p><p>Regarding semantic knowledge, multiple works have designed approaches for extending deep architectures to handle new categories and new tasks. For instance, continual learning methods <ref type="bibr" target="#b17">[18]</ref> attempt to sequentially learn new tasks while retaining previous knowledge, tackling the catastrophic forgetting issue.</p><p>Similarly, in open-world recognition <ref type="bibr" target="#b3">[4]</ref> the goal is to detect unseen categories and successfully incorporate them into the model. Another research thread is Zero-Shot Learning (ZSL) <ref type="bibr" target="#b46">[47]</ref>, where the goal is to recognize objects unseen during training given external information about the novel classes provided in forms of semantic attributes <ref type="bibr" target="#b16">[17]</ref>, visual descriptions <ref type="bibr" target="#b1">[2]</ref> or word embeddings <ref type="bibr" target="#b26">[27]</ref>.</p><p>Despite these significant efforts, an open research question is whether we can tackle the two problems jointly. Indeed, due to the large variability of visual concepts in the real world, in terms of both semantics and acquisition conditions, it is impossible to construct a training set capturing such variability. This calls for a holistic approach addressing them together. Consider for instance the case depicted in <ref type="figure">Fig. 1</ref>. A system trained to recognize elephants and horses from realistic images and cartoons might be able to recognize the same categories in another visual domain, like art paintings <ref type="figure">(Fig. 1, bottom)</ref> or it might be able to describe other quadrupeds in the same training visual domains <ref type="figure">(Fig. 1, top)</ref>.</p><p>On the other hand, how to deal with the case where new animals are shown in a new visual domain is not clear.</p><p>To our knowledge, our work is the first attempt to answer this question, proposing a method that is able to recognize unseen semantic categories in unseen domains. In particular, our goal is to jointly tackle ZSL and DG (see <ref type="figure">Fig.1</ref>). ZSL algorithms usually receive as input a set of images with their associated semantic descriptions, and learn the relationship between an image and its semantic attributes. Likewise, DG approaches are trained on multiple source domains and at test time are asked to classify images, assigning labels within the same set of source categories but in an unseen target domain. Here we want to address the scenario where, during training, we are given a set of images of multiple domains and semantic categories and our goal is to build a model able to recognize images of unseen concepts, as in ZSL, in unseen domains, as in DG.</p><p>To achieve this, we need to address challenges usually not present when these two classical tasks, i.e. ZSL and DG, are considered in isolation. For instance, while in DG we can rely on the fact that the multiple source domains permit to disentangle semantic and domain-specific information, in ZSL+DG we have no guarantee that the disentanglement will hold for the unseen semantic categories at test time. Moreover, while in ZSL it is reasonable to assume that the learned mapping between images and semantic attributes will generalize also to test images of the unseen concepts, in ZSL+DG we have no guarantee that this will happen for images of unseen domains.</p><p>To overcome these issues, during training we simulate both the semantic and the domain shift we will encounter at test time. Since explicitly generating images of unseen domains and concepts is an ill-posed problem, we sidestep this issue and we synthesize unseen domains and concepts by interpolating existing ones. To do so, we revisit the mixup <ref type="bibr" target="#b52">[53]</ref> algorithm as a tool to obtain partially unseen categories and domains. Indeed, by randomly mixing samples of different categories we obtain new samples which do not belong to a single one of the available categories during training. Similarly, by mixing samples of different domains, we obtain new samples which do not belong to a single source domain available during training.</p><p>Under this perspective, mixing samples of both different domains and classes allows to obtain samples that cannot be categorized in a single class and domain of the one available during training, thus they are novel both for the semantic and their visual representation. Since higher levels of abstraction contain more taskrelated information, we perform mixup at both image and feature level, showing experimentally the need for this choice. Moreover, we introduce a curriculumbased mixing strategy to generate increasingly complex training samples. We show that our CuMix (Curriculum Mixup for recognizing unseen categories in unseen domains) model obtains state-of-the-art performances in both ZSL and DG in standard benchmarks and it can be effectively applied to the combination of the two tasks, recognizing unseen categories in unseen domains. <ref type="bibr" target="#b6">7</ref> To summarize, our contributions are as follows. (i) We introduce the ZSL+DG scenario, a first step towards recognizing unseen categories in unseen domains. (ii) Being the first holistic method able to address ZSL, DG, and the two tasks together, our method is based on simulating new domains and categories during training by mixing the available training domains and classes both at image and feature level. The mixing strategy becomes increasingly more challenging during training, in a curriculum fashion. (iii) Through our extensive evaluations and analysis, we show the effectiveness of our approach in all three settings: namely ZSL, DG and ZSL+DG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Domain Generalization (DG). Over the past years the research community has put considerable efforts into developing methods to contrast the domain shift. Opposite to domain adaptation <ref type="bibr" target="#b6">[7]</ref>, where it is assumed that target data are available in the training phase, the key idea behind DG is to learn a domain agnostic model to be applied to any unseen target domain.</p><p>Previous DG methods can be broadly grouped into four main categories. The first category comprises methods which attempt to learn domain-invariant feature representations <ref type="bibr" target="#b27">[28]</ref> by considering specific alignment losses, such as maximum mean discrepancy (MMD), adversarial loss <ref type="bibr" target="#b21">[22]</ref> or self-supervised losses <ref type="bibr" target="#b4">[5]</ref>. The second category of methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15]</ref> develop from the idea of creating deep architectures where both domain-agnostic and domain-specific parameters are learned on source domains. After training, only the domain-agnostic part is retained and used for processing target data. The third category devises specific optimization strategies or training procedures in order to enhance the generalization ability of the source model to unseen target data. For instance, in <ref type="bibr" target="#b19">[20]</ref> a meta-learning approach is proposed for DG. Differently, in <ref type="bibr" target="#b20">[21]</ref> an episodic training procedure is presented to learn models robust to the domain shift. The latter category comprises methods which introduce data and feature augmentation strategies to synthesize novel samples and improve the generalization capability of the learned model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42]</ref>. These strategies are mostly based on adversarial training <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Our work is related to the latter category since we also generate synthetic samples with the purpose of learning more robust target models. However, differently from previous methods, we specifically employ mixup to perturb feature representations. Recently, works have considered mixup in the context of domain adaptation <ref type="bibr" target="#b50">[51]</ref> to e.g. reinforce the judgments of a domain discrimination. However, we employ mixup from a different perspective i.e. simulating semantic and domain shift we will encounter at test time. To this extent, we are not aware of previous methods using mixup for DG and ZSL. Zero-Shot Learning (ZSL). Traditional ZSL approaches attempt to learn a projection function mapping images/visual features to a semantic embedding space where classification is performed. This idea is achieved by directly predicting image attributes e.g. <ref type="bibr" target="#b16">[17]</ref> or by learning a linear mapping through margin-based objective functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Other approaches explored the use of non-linear multi-modal embeddings <ref type="bibr" target="#b44">[45]</ref>, intermediate projection spaces <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref> or similarity-based interpolation of base classifiers <ref type="bibr" target="#b5">[6]</ref>. Recently, various methods tackled ZSL from a generative point of view considering Generative Adversarial Networks <ref type="bibr" target="#b47">[48]</ref>, Variational Autoencoders (VAE) <ref type="bibr" target="#b37">[38]</ref> or both of them <ref type="bibr" target="#b49">[50]</ref>. While none of these approaches explicitly tackled the domain shift, i.e. visual appearance changes among different domains/datasets, various methods proposed to use domain adaptation technique, e.g. to refine the semantic embedding space, aligning semantic and projected visual features <ref type="bibr" target="#b37">[38]</ref> or, in transductive scenarios, to cope with the inherent domain shift existing among the appearance of attributes in different categories <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. For instance, in <ref type="bibr" target="#b37">[38]</ref> a distance among visual and semantic embedding projected in the VAE latent space is minimized. In <ref type="bibr" target="#b15">[16]</ref> the problem is addressed through a regularised sparse coding framework, while in <ref type="bibr" target="#b8">[9]</ref> a multi-view hypergraph label propagation framework is introduced.</p><p>Recently, works have considered also coupling ZSL and DA in a transductive setting. For instance, in <ref type="bibr" target="#b55">[56]</ref> a semantic guided discrepancy measure is employed to cope with the asymmetric label space among source and target domains. In the context of image retrieval, multiple works addressed the sketch-based image retrieval problem <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b7">8]</ref>, even across multiple domains. In <ref type="bibr" target="#b39">[40]</ref> the authors proposed a method to perform cross-domain image retrieval by training domainspecific experts. While these approaches integrated DA and ZSL, none of them considered the more complex scenario of DG, where no target data are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first formalize the Zero-Shot Learning under Domain Generalization (ZSL+DG). We then describe our approach, CuMix , which, by performing curriculum learning through mixup, simulates the domain-and semantic-shift the network will encounter at test time, and can be holistically applied to ZSL, DG and ZSL+DG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>In the ZSL+DG problem, the goal is to recognize unseen categories (as in ZSL) in unseen domains (as in DG). Formally, let X denote the input space (e.g. the image space), Y the set of possible classes and D the set of possible domains. During training, we are given a set</p><formula xml:id="formula_0">S = {(x i , y i , d i )} n i=1 where x i ∈ X , y i ∈ Y s and d i ∈ D s . Note that Y s ⊂ Y and D s ⊂ D and, as in standard DG, we have multiple source domains (i.e. D s = ∪ m j=1 d j , with m &gt; 1) with different distributions i.e. p X (x|d i ) = p X (x|d j ), ∀i = j.</formula><p>Given S our goal is to learn a function h mapping an image x of domains D u ⊂ D to its corresponding label in a set of classes Y u ⊂ Y. Note that in standard ZSL, while the set of train and test domains are shared, i.e. D s ≡ D u , the label sets are disjoint i.e. Y s ∩ Y u ≡ ∅, thus Y u is a set of unseen classes. On the other hand, in DG we have a shared output space, i.e. Y s ≡ Y u , but a disjoint set of domains between training and test i.e. D s ∩ D u ≡ ∅, thus D u is a set of unseen domains. Since the goal of our work is to recognize unseen classes in unseen domains, we unify the settings of DG and ZSL, considering both semantic-and domain-shift at test time</p><formula xml:id="formula_1">i.e. Y s ∩ Y u ≡ ∅ and D s ∩ D u ≡ ∅.</formula><p>In the following we divide the function h into three parts: f , mapping images into a feature space Z, i.e. f : X → Z, g going from Z to a semantic embedding space E, i.e. g : Z → E, and an embedding function ω :</p><formula xml:id="formula_2">Y t → E where Y t ≡ Y s during training and Y t ≡ Y u at test time.</formula><p>Note that ω is a learned classifier for DG while it is a fixed semantic embedding function in ZSL, mapping classes into their vectorized representation extracted from external sources. Given an image x, the final class prediction is obtained as follows:</p><formula xml:id="formula_3">y * = argmax y ω(y) g(f (x)).<label>(1)</label></formula><p>In this formulation, f can be any learnable feature extractor (e.g. a deep neural network), while g any ZSL predictor (e.g. a semantic projection layer, as in <ref type="bibr" target="#b45">[46]</ref> or a compatibility function among visual features and labels, as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>). The first solution to address the ZSL+DG problem could be training a classifier using the aggregation of data from all source domains. In particular, for each sample we could minimize a loss function of the form:</p><formula xml:id="formula_4">L AGG (x i , y i ) = y∈Y s (ω(y) g(f (x i )), y i )<label>(2)</label></formula><p>with an arbitrary loss function, e.g. the cross-entropy loss. In the following, we show how we can use the input to Eq. (2) to effectively recognize unseen categories in unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simulating Unseen Domains and Concepts through Mixup</head><p>The fundamental problem of ZSL+DG is that, during training, we have neither access to visual data associated to categories in Y u nor to data of the unseen domains D u . One way to overcome this issue in ZSL is to generate samples of unseen classes by learning a generative function conditioned on the semantic embeddings in W = {ω(y)|y ∈ Y s } <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>. However, since no description is available for the unseen target domain(s) in D u , this strategy is not feasible in ZSL+DG. On the other hand, previous works on DG proposed to synthesize images of unseen domains through adversarial strategies of data augmentation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39]</ref>. However, these strategies are not applied to ZSL since they cannot easily be extended to generate data for unseen semantic categories Y u . To circumvent this issue, we introduce a strategy to simulate, during training, novel domains and semantic concepts by interpolating from the ones available in D s and Y s . Simulating novel domains and classes allows to train the network to cope with both semantic-and domain-shift, the same situation our model will face at test time. Since explicitly generating inputs of novel domains and categories is a complex task, in this work we propose to achieve this goal, by mixing images and features of different classes and domains, revisiting the popular mixup [53] strategy. In practice, given two elements a i and a j of the same space (e.g. a i , a j ∈ X ), mixup <ref type="bibr" target="#b52">[53]</ref> defines a mixing function ϕ as follows:</p><formula xml:id="formula_5">ϕ(a i , a j ) = λ · a i + (1 − λ) · a j<label>(3)</label></formula><p>with λ sampled from a beta distribution, i.e. λ ∼ Beta(β, β), with β an hyperparameter. Given two samples (x i , y i ) and (x j , y j ) randomly drawn from a training set T , a new loss term is defined as:</p><formula xml:id="formula_6">L MIXUP ((x i , y i ), (x j , y j )) = L AGG (ϕ(x i , x j ), ϕ(ȳ i ,ȳ j ))<label>(4)</label></formula><p>whereȳ i ∈ |Y s | is the one-hot vectorized representation of label y i . Note that, when mixing two samples and label vectors with ϕ, a single λ is drawn and applied within ϕ in both image and label spaces. The loss defined in Eq.(4) forces the network to disentangle the various semantic components (i.e. y i and y j ) contained in the mixed inputs (i.e. x i and x j ) plus the ratio λ used to mix them. This auxiliar task acts as a strong regularizer that helps the network to e.g. being more robust against adversarial examples <ref type="bibr" target="#b52">[53]</ref>. Note however that the function ϕ creates input and targets which do not represent a single semantic concept in T but contains characteristics taken from multiple samples and categories, synthesising a new semantic concept from the interpolation of existing ones. For recognizing unseen concepts in unseen domains at test time, we revisit ϕ to obtain both cross-domain and cross-semantic mixes during training, simulating both semantic-and domain-shift. While simulating the semantic-shift is a by-product of the original mixup formulation, here we explicitly revisit ϕ in order to perform cross-domain mixups. In particular, instead of considering a pair of samples from our training set, we consider a triplet (x i , y i , d i ), (x j , y j , d j ) and (x k , y k , d k ). Given (x i , y i , d i ), the other two elements of the triplet are randomly sampled from S, with the only constraint that d i = d k , i = k and d j = d i . In this way, the triplet contains two samples of the same domain (i.e. d i ) and a third of a different one (i.e. d j ). Then, our mixing function φ is defined as follows:</p><formula xml:id="formula_7">φ(a i , a j , a k ) = λa i + (1 − λ)(γa j + (1 − γ)a k )<label>(5)</label></formula><p>with γ sampled from a Bernoulli distribution γ ∼ B(α) and a representing either the input x or the vectorized version of the label y, i.e.ȳ. Note that we introduced a term γ which allows to perform either intra-domain (with γ = 0) or cross-domain (with γ = 1) mixes.</p><p>To learn a feature extractor f and a semantic projection layer g robust to domain-and semantic-shift, we propose to use φ to simulate both samples and features of novel domains and classes during training. Namely, we simulate the semantic-and domain-shift at two levels, i.e. image and class levels. Given a sample (x i , y i , d i ) ∈ S we define the following loss:</p><formula xml:id="formula_8">L M-IMG (x i , y i , d i ) = L AGG (φ(x i , x j , x k ), φ(ȳ i ,ȳ j ,ȳ k )).<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">(x i , y i , d i ),(x j , y j , d j ),(x k , y k , d k ) are randomly sampled from S, with d i = d k and d j = d k .</formula><p>The loss term in Eq. (6) enforces the feature extractor to effectively process inputs of mixed domains/semantics obtained through φ. Additionally, to also act at classification level, we design another loss which forces the semantic consistency of mixed features in E. This loss term is defined as:</p><formula xml:id="formula_10">L M-F (x i , y i , d i ) = y∈Y s ω(y) g φ(f (x i ), f (x j ), f (x k )) , φ(ȳ i ,ȳ j ,ȳ k )<label>(7)</label></formula><p>where, as before, (x j , y j , d j ), (x k , y k , d k ) ∼ S, with d i = d k , i = k and d j = d k and is a generic loss function e.g. the cross-entropy loss. This second loss term forces the classifier ω and the semantic projection layer g to be robust to features with mixed domains and semantics. While we can simply use a fixed mixing function φ, as defined in Eq. (5), for Eq. (6) and Eq. <ref type="formula" target="#formula_10">(7)</ref>, we found that it is more beneficial to devise a dynamic φ which changes its behaviour during training, in a curriculum fashion. Intuitively, minimizing the two objectives defined in Eq.(6) and Eq. <ref type="formula" target="#formula_10">(7)</ref> requires our model to correctly disentangle the various semantic components used to form the mixed samples. While this is a complex task even for intra-domain mixes (i.e. when only the semantic is mixed), mixing samples across domains makes the task even harder, requiring to isolate also domain specific factors. To effectively tackle this task, we choose to act on the mixing function φ. In particular, we want our φ to create mixed samples with progressively increased degree of mixing both with respect to content and domain, in a curriculum-based fashion.</p><p>During training we regulate both α (weighting the probability of crossdomain mixes) and β (modifying the probability distribution of the mix ratio λ), changing the probability distribution of the mixing ratio λ and of the crossdomain mix γ. In particular, given a warm-up step of N epochs and being s the current epoch we set β = min( s N β max , β max )), with β max as hyperparameter, while α = max(0, min( s−N N , 1). As a consequence, the learning process is made of three phases, with a smooth transition among them. We start by solving the plain classification task on a single domain (i.e. s &lt; N ,α = 0,β = s N β max ,). In the subsequent step (N ≤ s &lt; 2N ) samples of the same domains are mixed randomly, with possibly different semantics (i.e. α = s−N N , β = β max ). In the third phase (s ≥ 2N ), we mix up samples of different domains (i.e. α = 1), simulating the domain shift the predictor will face at test time. <ref type="figure" target="#fig_0">Figure 2</ref>, shows a representation of how φ varies during training (top, white block). Final objective. The full training procedure, is represented in <ref type="figure" target="#fig_0">Figure 2</ref>. Given a training sample (x i , y i , d i ), we randomly draw other two samples, (x j , y j , d j ) and (x k , y k , d k ), with d i = d k , i = k and d j = d i , feed them to φ and obtain the first mixed input. We then feed x i , x j , x k and the mixed sample through f , to extract their respective features. At this point we use features extracted from other two randomly drawn samples (in the figure, and just for simplicity, x j and x k with same mixing ratios λ and γ), to obtain the feature level mixed features needed to build the objective in Eq. <ref type="bibr" target="#b6">(7)</ref>. Finally, the features of x i and the two mixed variants at image and feature level, are fed to the semantic projection layer g, which maps them to the embedding space E. At the same time, the labels in Y s are projected in E through ω. Finally, the objectives defined in Eq.(2),Eq. <ref type="formula" target="#formula_8">(6)</ref> and Eq. <ref type="formula" target="#formula_10">(7)</ref> functions are then computed in the semantic embedding space. Our final objective is:</p><formula xml:id="formula_11">L CuMIX (S) = |S| −1 (xi,yi,di)∈S L AGG (x i , y i ) + η I L M-IMG (x i , y i , d i ) + η F L M-F (x i , y i , d i )<label>(8)</label></formula><p>with η I and η F hyperparameters weighting the importance of the two terms. As (x, y) in both L AGG , L M-IMG and L M-F , we use the standard cross-entropy loss, even if any ZSL objective can be applied. Finally, we highlight that the optimization is performed batch-wise, thus also the sampling of the triplet considers the current batch and not the full training set S. Moreover, while in <ref type="figure" target="#fig_0">Figure 2</ref> we show for simplicity that the same samples are drawn for L M-IMG and L M-F , in practice, given a sample, the random sampling procedure of the other two members of the triplet is held-out twice, one at the image level and one at the feature level. Similarly, the sampling of the mixing ratios λ and cross domain factor γ of φ is held-out sample-wise and twice, one at image level and one at feature level. As in Eq. (3), λ and γ are kept fixed across mixed inputs/features and their respective targets in the label space. Discussion. We present similarities between our framework with DG and ZSL methods. In particular, presenting the classifier with noisy features extracted by a non-domain specialist network, has a similar goal as the episodic strategy for DG described in <ref type="bibr" target="#b20">[21]</ref>. On the other hand, here we sidestep the need to train domain experts by directly presenting as input to our classifier features of novel domains that we obtain by interpolating the available sources samples. Our method is also linked to mixup approaches developed in DA <ref type="bibr" target="#b50">[51]</ref>. Differently from them, we use mixup to simulate unseen domains rather then to progressively align the source to the given target data.</p><p>Our method is also related to ZSL frameworks based on feature generation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>. While the quality of our synthesized samples is lower since we do not exploit attributes for conditional generation, we have a lower computational cost. In fact, during training we simulate the test-time semantic shift without generating samples of unseen classes. Moreover, we do not require additional training phases on the generated samples or the availability of unseen class attributes to be available beforehand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and implementation details</head><p>We assess CuMix in three scenarios: ZSL, DG and the proposed ZSL+DG setting. ZSL. We conduct experiments on four standard benchmarks: Caltech-UCSD-Birds 200-2011 (CUB) <ref type="bibr" target="#b43">[44]</ref>, SUN attribute (SUN) <ref type="bibr" target="#b30">[31]</ref>, Animals with Attributes (AWA) <ref type="bibr" target="#b16">[17]</ref> and Oxford Flowers (FLO) <ref type="bibr" target="#b28">[29]</ref>. CUB contains 11,788 images of 200 bird species, with 312 attributes, SUN 14,430 images of 717 scenes annotated with 102 attributes, and AWA 30,475 images of 50 animal categories with 85 attributes. Finally, FLO is a fine-grained dataset of flowers, containing 8,189 images of 102 categories. As semantic representation, we use the visual descriptions of <ref type="bibr" target="#b34">[35]</ref>, following <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b45">46]</ref>. For each dataset, we use the train, validation and test split provided by <ref type="bibr" target="#b46">[47]</ref>. In all the settings we employ features extracted from the second-last layer of a ResNet-101 <ref type="bibr" target="#b12">[13]</ref> pretrained on ImageNet as image representation. For CuMix , we consider f as the identity function and as g a simple fully connected layer, perform our version of mixup directly at the feature-level while applying our alignment loss in the embedding space. All hyperparameters have been set following <ref type="bibr" target="#b46">[47]</ref>. DG. We perform experiments on the PACS dataset <ref type="bibr" target="#b18">[19]</ref>with 9,991 images of 7 semantic classes in 4 different visual domains, art paintings, cartoons, photos and sketches. For this experiment we use the standard train and test split defined in <ref type="bibr" target="#b18">[19]</ref>, with the same validation protocol. We use as base architecture a ResNet-18 <ref type="bibr" target="#b12">[13]</ref> pretrained on ImageNet. For our model, we consider f to be the ResNet-18 while g to be the identity function. We use the same training hyperparameters and protocol of <ref type="bibr" target="#b20">[21]</ref>. ZSL+DG. Since no previous work addressed the problem of ZSL+DG, there is no benchmark on this task. As a valuable benchmark, we choose Domain-Net <ref type="bibr" target="#b32">[33]</ref>, a recently introduced dataset for multi-source domain adaptation <ref type="bibr" target="#b32">[33]</ref> with a large variety of domains, visual concepts and possible descriptions. It contains approximately 600'000 images from 345 categories and 6 domains, clipart, infograph, painting, quickdraw, real and sketch.</p><p>To convert this dataset from a DA to a ZSL scenario, we need to define an unseen set of classes. Since our method uses a network pretrained on ImageNet <ref type="bibr" target="#b36">[37]</ref>, the set of unseen classes can not contain any of the classes present in ImageNet following the good practices in <ref type="bibr" target="#b48">[49]</ref>. We build our validation + test set with 100 classes that contain at least 40 images per domain and that has no overlap with ImageNet. We reserve 45 of these classes for the unseen test set, matching the number used in <ref type="bibr" target="#b39">[40]</ref>, and the remaining 55 classes for the unseen validation set. The remaining 245 classes are used as seen classes during training.</p><p>We set the hyperparameters of each method by training on all the images of the seen classes on a subset of the source domains and validating on all the images of the validation set from the held-out source domain. After the hyperparameters are set, we retrain the model on the training set, i.e. 245 classes, and validation set, i.e. 55 classes, of a total number of 300 classes. Finally, we report the final results on the 45 unseen classes. As semantic representation we use word2vec embeddings <ref type="bibr" target="#b26">[27]</ref> extracted from the Google News corpus and L2 -normalized, following <ref type="bibr" target="#b39">[40]</ref>. For all the baselines and our method, we employ as base architecture a ResNet-50 <ref type="bibr" target="#b12">[13]</ref> pretrained on ImageNet, using the same number of epochs and SGD with momentum as optimizer, with the same hyperparameters of <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>ZSL. In the ZSL scenario, we choose as baselines standard inductive methods plus more recent approaches. In particular we report the results of ALE <ref type="bibr" target="#b0">[1]</ref>, SJE <ref type="bibr" target="#b1">[2]</ref>, SYNC <ref type="bibr" target="#b5">[6]</ref>, GFZSL <ref type="bibr" target="#b40">[41]</ref> and SPNet <ref type="bibr" target="#b45">[46]</ref>. ALE <ref type="bibr" target="#b0">[1]</ref> and SJE <ref type="bibr" target="#b1">[2]</ref> are linear compatibility methods using a ranking loss and the structural SVM loss respectively. SYNC <ref type="bibr" target="#b5">[6]</ref> learns a mapping from the feature space and the semantic embedding space by means of phantom classes and a weighted graph. GFZSL <ref type="bibr" target="#b40">[41]</ref> employs a generative framework where each class-conditional distribution is modeled as a multivariate Gaussian. Finally, SPNet <ref type="bibr" target="#b45">[46]</ref> learns a semantic projection function from the feature space through the image embedding space by minimizing the standard cross-entropy loss.</p><p>Our results grouped by datasets are reported in <ref type="figure">Figure 3</ref>. Our model achieves performance either superior or comparable to the state-of-the-art in all benchmarks but AWA. We believe that in AWA learning a better alignment between visual features and attributes may not be as effective as improving the quality of the visual features. Especially, although the names of the test classes do not appear in the training set of ImageNet, for AWA being a non-fine-grained dataset, the information content of the test classes is likely represented by the ImageNet training classes. Moreover, for non-fine-grained datasets, finding labeled training data may not be as challenging as it is in fine-grained datasets. Hence, we argue that zero-shot learning is of higher practical interest in fine-grained settings. Indeed our proposed model is effective in fine-grained scenarios (i.e. CUB, SUN, FLO) where it consistently outperforms the state-of-the-art approaches.</p><p>These results show that our model based on mixup achieves competitive performances on ZSL by simulating the semantic shift the classifier will experience at test time. To this extent, our approach is the first to show that mixup can be a powerful regularization strategy for the challenging ZSL setting. DG. The second series of experiments consider the standard DG scenario. Here we test our model on the PACS dataset using a ResNet-18 architecture. As baselines for DG we consider the standard model trained on all source domains together (AGG), the adversarial strategies in <ref type="bibr" target="#b10">[11]</ref> (DANN) and <ref type="bibr" target="#b38">[39]</ref>, the meta learning-based strategy MLDG <ref type="bibr" target="#b19">[20]</ref> and MetaReg <ref type="bibr" target="#b2">[3]</ref>. Moreover we consider the episodic strategy presented in <ref type="bibr" target="#b20">[21]</ref> (Epi-FCR).</p><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, our model achieves competitive results comparable to the state-of-the-art episodic strategy Epi-FCR <ref type="bibr" target="#b20">[21]</ref>. Remarkable is the gain obtained with respect to the adversarial augmentation strategy CrossGrad <ref type="bibr" target="#b38">[39]</ref>. Indeed, synthesizing novel domains for domain generalization is an ill-posed problem, since the concept of unseen domain is hard to capture. However, with CuMix we are able to simulate inputs/features of novel domains by simply interpolating the information available in the samples of our sources. Despite containing information available in the original sources, our approach allows to produce a model more robust to domain shift.</p><p>Another interesting comparison is against the self-supervised approach JiGen <ref type="bibr" target="#b4">[5]</ref>. Similarly to <ref type="bibr" target="#b4">[5]</ref> we employ an additional task to achieve higher generalization abilities to unseen domains. While in <ref type="bibr" target="#b4">[5]</ref> the JigSaw puzzles <ref type="bibr" target="#b29">[30]</ref> are used as a secondary self-supervised task, here we employ the mixed samples/features in the same manner. The improvement in the performances of our method highlights that recognizing the semantic of mixed samples acts as a more powerful secondary task to improve robustness to unseen domains.</p><p>Finally, it is worth noting that CuMix performs a form of episodic training, similar to Epi-FCR <ref type="bibr" target="#b20">[21]</ref>. However, while Epi-FCR considers multiple domainspecific architectures (to simulate the domain experts needed to build the episodes), we require a single domain agnostic architecture. We build our episodes by mak- ing the mixup among images/features of different domains increasingly more drastic. Despite not requiring any domain experts, CuMix achieves comparable performances to Epi-FCR, showing the efficacy of our strategy to simulate unseen domain shifts. Ablation study. In this section, we ablate the various components of our method. We performed the ablation on the PACS benchmark for DG, since this allows us to show how different choices act on the generalization to unseen domains. In particular, we ablate the following implementation choices: 1) mixing samples at the image level, feature level or both 2) impact of our curriculum-based strategy for mixing features and samples. As shown in <ref type="table">Table 5</ref>, mixing samples at feature level produces a clear gain on the results with respect to the baseline, while mixing samples only at image level can even harm the performance. This happens particularly in the sketch domain, where mixing samples at feature level produces a gain of 2% while at image level we observe a drop of 10% with respect to the baseline. This could be explained by mixing samples at image level producing inputs that are too noisy for the network and not representative of the actual shift experienced at test time. Mixing samples at feature level instead, after multiple layers of abstractions, allows to better synthesize the information contained in the different samples, leading to more reliable features for the classifier. Using both of them allows to obtain higher results in almost all domains.</p><p>Finally, we analyze the impact of the curriculum-based strategy for mixing samples and features. As the table shows, adding the curriculum strategy allows to boost the performances for the most difficult cases (i.e. sketches) producing a further accuracy boost. Moreover, applying this strategy allows to stabilize the training procedure, as demonstrated experimentally. ZSL+DG. On the proposed ZSL+DG setting we use the DomainNet dataset, training on five out of six domains and reporting the average per-class accuracy on the held-out one. We report the results for all possible target domains but one, i.e. real photos, since our backbone has been pretrained on ImageNet, thus the photo domain is not an unseen one. Since no previous methods addressed the ZSL+DG problem, in this work we consider simple baselines derived from the literature of both ZSL and DG. The first baseline is a standard ZSL model without any DG algorithm (i.e. the standard AGG): as ZSL method we consider SPNet <ref type="bibr" target="#b45">[46]</ref>. The second baseline is a DG approach coupled with a ZSL algorithm. To this extent we select the state-of-the-art Epi-FCR as the DG ap- proach, coupling it with SPNet. As a reference, we also evaluate the performance of standard mixup coupled with SPNet. As shown in <ref type="table" target="#tab_4">Table 3</ref>, our method achieves competitive performances in ZSL+DG setting when compared to a state-of-the-art approach for DG (Epi-FCR) coupled with a state-of-the-art one for ZSL (SPNet), outperforming this baseline in almost all settings but sketch and, in average by almost 1%. Particularly interesting are the results on the infograph and quickdraw domains. These two domains are the ones where the shift is more evident as highlighted by the lower results of the baseline. In these scenarios, our model consistently outperforms the competitors, with a remarkable gain of more than 1.5% in average accuracy per class with respect to the ZSL only baseline. We want to highlight also that DomainNet is a challenging dataset, where almost all standard DA approaches are ineffective or can even lead to negative transfer <ref type="bibr" target="#b32">[33]</ref>. Our method however is able to overcome the unseen domain shift at test time, improving the performance of the baselines in all scenarios. Our model consistently outperforms SPNet coupled with the standard mixup strategy in every scenario. This demonstrates the efficacy of the choices in CuMix for revisiting mixup in order to recognize unseen categories in unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose the novel ZSL+DG scenario. In this setting, during training, we are given a set of images of multiple domains and semantic categories and our goal is to build a model able to recognize unseen concepts, as in ZSL, in unseen domains, as in DG. To solve this problem we design CuMix, the first algorithm which can be holistically and effectively applied to DG, ZSL and ZSL+DG. CuMix is based on simulating inputs and features of new domains and categories during training by mixing the available source domains and classes, both at image and feature level. Experiments on public benchmarks show the effectiveness of CuMix, achieving state-of-the-art performances in almost all settings in all tasks. Future works will investigate the use of alternative data-augmentation schemes in the ZSL+DG setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameter choices</head><p>In this section, we will detail the hyperparameter choices and validation protocols that, for lack of space, we did not include in the main paper. ZSL. For each dataset, we use the train, validation and test split provided by <ref type="bibr" target="#b46">[47]</ref>. In all the settings we employ features extracted from the second-last layer of a ResNet-101 <ref type="bibr" target="#b12">[13]</ref> pretrained on ImageNet as image representation, without end-to-end training. For CuMix , we consider f as the identity function and as g a simple fully connected layer, performing the mixing directly at the featurelevel while applying our alignment loss in the embedding space (i.e. L M-IMG and L M-F coincide in this case and are applied only once.) All hyperparameters have been set dataset-wise following <ref type="bibr" target="#b46">[47]</ref>, using the available validation sets. For all the experiments, we use SGD as optimizer with an initial learning rate equal to 0.1, momentum equal to 0.9, a weight-decay set to 0.001 for all settings but AWA, where is set 0. The learning-rate is downscaled by a factor of ten after 2/3 of the total number of epochs and N = 30. In particular, for CUB and FLO we train our model for 90 epochs, setting β max = 0.8 and η I = η F = 10.0 for CUB, and β max = 0.4 and η I = η F = 4.0 for FLO. For AWA, we train our network for 30 epochs, with β max = 0.2 and η I = η F = 1.0. For SUN, we train our network for 60 epochs, with β max = 0.8 and η I = η F = 10. In all settings, the batch-size is set to 128. DG. We use as base architecture a ResNet-18 <ref type="bibr" target="#b12">[13]</ref> pretrained on ImageNet. For our model, we consider f to be the ResNet-18, g to be the identity function and ω will be a learned, fully-connected classifier. We use the same training hyperparameters and protocol of <ref type="bibr" target="#b20">[21]</ref>, setting β max = 0.6, η I = 0.1, η F = 3 and N = 10. ZSL+DG. For all the baselines and our method we employ as base architecture a ResNet-50 <ref type="bibr" target="#b12">[13]</ref> pretrained on ImageNet, using SGD with momentum as optimizer, with a learning rate of 0.001 for the ZSL classifier and 0.0001 for the ResNet-50 backbone, a weight decay of 5 · 10 −5 and momentum 0.9. We train the models for 8 epochs (each epoch counted on the smallest source dataset), with a batch-size containing 24 sample per domain. We decrease the learning rates by a factor of 10 after 6 epochs. For our model, we consider the backbone as f and a simple fully-connected layer as g. We set N = 2, η I = 10 −3 for all the experiments, while β max in {1, 2} and η F in {0.5, 1, 2} depending on the scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ZSL+DG: analysis of additional baselines</head><p>In <ref type="table" target="#tab_4">Table 3</ref> of the main paper, we showed the performance of our method in the new ZSL+DG scenario on the DomainNet dataset <ref type="bibr" target="#b32">[33]</ref>, comparing it with three baselines: SPNet <ref type="bibr" target="#b45">[46]</ref>, simple mixup <ref type="bibr" target="#b52">[53]</ref> coupled with SPNet and SPNet coupled with EpiFCR <ref type="bibr" target="#b20">[21]</ref>, an episodic-based method for DG. We reported the results of these baselines to show 1) the performance of a state-of-the-art ZSL method (SPNet), 2) the impact of mixup alone (mixup+SPNet) and 3) the results obtained by coupling state-of-the-art models for DG and for ZSL together (EpiFCR+SPNet). We chose SPNet and EpiFCR as state-of-the-art references for ZSL and DG respectively due to their high performances on their respective scenarios, plus because they are very recent approaches.</p><p>In this section, we motivate our choices by showing that other baselines of ZSL and DG achieve lower performances in this new scenario. In particular we show the performances of two standard ZSL methods, ALE <ref type="bibr" target="#b0">[1]</ref> and DEVISE [?] and a standard DG/DA method, DANN <ref type="bibr" target="#b10">[11]</ref>. We choose DANN since it is a strong baseline for DG on residual architectures, as shown in <ref type="bibr" target="#b20">[21]</ref>. As in the main paper, we show the performances of the ZSL methods alone, ZSL methods coupled with DANN, and with EpiFCR. For all methods, we keep the same training hyperparameters, tuning only the method-specific ones. The results are reported in <ref type="table">Table 4</ref>. As the table shows, CuMix achieves superior performances even compared to these new baselines. Moreover, these baselines achieve lower results than the EpiFCR method coupled with SPNet, as expected. This motivates our choices of the main paper. It is also worth highlighting how coupling ZSL methods with DANN for DG achieves lower performances than the ZSL methods alone in this scenario. This is in line with the results reported in <ref type="bibr" target="#b32">[33]</ref>, where standard domain alignment-based methods are shown to be not effective in the DomainNet dataset, leading also to negative transfer in some cases <ref type="bibr" target="#b32">[33]</ref>.</p><p>Finally, we want to underline that coupling EpiFCR with any of the ZSL baselines, is not a straightforward approach, but requires to actually adapt this method, re-structuring the losses. In particular, we substitute the classifier originally designed for EpiFCR with the classifier specific of the ZSL method we apply on top of the backbone. Moreover, we additionally replace the classification loss with the loss devised for the particular ZSL method. For instance, for EpiFCR+SPNet, we use as classifier the semantic projection network, using the cross-entropy loss in <ref type="bibr" target="#b45">[46]</ref> as classification loss. Similarly, for EpiFCR+DEVISE and EpiFCR+ALE, we use as classifier a bi-linear compatibility function <ref type="bibr" target="#b46">[47]</ref>  <ref type="table">Table 4</ref>. ZSL+DG scenario on the DomainNet dataset with ResNet-50 as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Target Domain DG ZSL clipart infograph painting quickdraw sketch avg.</p><p>coupled with a pairwise ranking objective [?] and with a weighted pairwise ranking objective <ref type="bibr" target="#b0">[1]</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ZSL+DG: ablation study</head><p>In order to further investigate our design choices on the ZSL+DG setting, we conducted experiments on a challenging scenario where we consider just two domains as sources, i.e. Real and Painting. The results are shown in <ref type="table">Table C</ref>. On average our model improves SPNet by 2% and SPNet + Epi-FCR by 1.1%. Our approach without curriculum largely outperforms standard image-level mixup <ref type="bibr" target="#b52">[53]</ref> (more than 2%). Applying mixup at both feature and image level but without curriculum is effective but achieves still lower results with respect to our CuMix strategy (as in Tab. 2). Interestingly, if we apply the curriculum strategy but switching the order of semantic and domain mixing (CuMix reverse), this achieves lower performances with respect to CuMix, which considers domain mixing harder than semantic ones. This shows that, in this setting, it is important to correctly tackle intra-domain semantic mixing before including inter-domain ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ZSL results</head><p>In this section, we report the ZSL results in tabular form. The results are shown in <ref type="table" target="#tab_5">Table 6</ref>. With respect to <ref type="figure">Figure 3</ref> of the main paper, in the table, we also report the results of a baseline which uses just the cross-entropy loss term (similarly to <ref type="bibr" target="#b45">[46]</ref>), without the mixing term employed in our CuMix method. As the table shows, our baseline is weak, performing below most of the ZSL methods in all scenarios but FLO. However, adding our mixing strategy allows to boost the performances in all scenarios, achieving state-of-the-art performances in most of them. We also want to highlight that in <ref type="table" target="#tab_5">Table 6</ref>, as in the main paper, we do not report the results of methods based on generating features of unseen classes for ZSL <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>. This choice is linked to the fact that these methods can be used as data augmentation strategies to improve the performances of any ZSL method, as shown in <ref type="bibr" target="#b47">[48]</ref>. While using them can improve the results of all the baselines as well as CuMix , this falls out of the scope of this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Our CuMix Framework. Given an image (bottom, horse, photo), we randomly sample one image from the same (middle, photo) and one from another (top, cartoon) domain. The samples are mixed through φ (white blocks) both at image and feature level, with their features and labels projected into the embedding space E (by g and ω respectively) and there compared to compute our final objective. Note that φ varies during training (top part), changing the mixing ratios in and across domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 1. Our ZSL+DG problem. During training we have images of multiple categories (e.g. elephant,horse) and domains (e.g. photo, cartoon). At test time, we want to recognize unseen categories (e.g. dog, giraffe), as in ZSL, in unseen domains (e.g. paintings), as in DG, exploiting side information describing seen and unseen categories.</figDesc><table><row><cell></cell><cell>Train</cell><cell>Test</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ZSL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>= domain</cell></row><row><cell>elephant</cell><cell>horse</cell><cell>dog</cell><cell>giraffe</cell><cell>≠ categories</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ZSL+DG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>≠ domain</cell></row><row><cell></cell><cell></cell><cell>dog</cell><cell>giraffe</cell><cell>≠ categories</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>≠ domain</cell></row><row><cell>elephant</cell><cell>horse</cell><cell>elephant</cell><cell>horse</cell><cell>= categories</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ZSL results on CUB, SUN, AWA and FLO datasets with ResNet-101 features.</figDesc><table><row><cell>Accuracy</cell><cell>55 60 65 70</cell><cell>54.0</cell><cell>68.3</cell><cell>66.2</cell><cell>59.9</cell><cell>65.6</cell><cell>64.0</cell><cell>56.3</cell><cell>56.5 SYNC</cell><cell>54.9</cell><cell>53.9 GFZSL 60.4</cell><cell>55.6</cell><cell>60.6 SPNet</cell><cell>60.7</cell><cell>58.1</cell><cell>53.7 ALE</cell><cell>62.4</cell><cell>SJE</cell><cell>53.4</cell><cell>59.7 CuMix</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>45</cell><cell></cell><cell></cell><cell cols="2">AWA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CUB</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SUN</cell><cell></cell><cell></cell><cell></cell><cell cols="2">FLO</cell></row><row><cell cols="2">Fig. 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Domain Generalization accuracies on PACS with ResNet-18.</figDesc><table><row><cell></cell><cell cols="8">AGG DANN MLDG CrossGrad MetaReg JiGen Epi-FCR CuMix</cell></row><row><cell>Target</cell><cell></cell><cell>[11]</cell><cell>[20]</cell><cell>[39]</cell><cell>[3]</cell><cell>[5]</cell><cell>[21]</cell><cell></cell></row><row><cell>Photo</cell><cell>94.9</cell><cell>94.0</cell><cell>94.3</cell><cell>94.0</cell><cell>94.3</cell><cell>96.0</cell><cell>93.9</cell><cell>95.1</cell></row><row><cell>Art</cell><cell>76.1</cell><cell>81.3</cell><cell>79.5</cell><cell>78.7</cell><cell>79.5</cell><cell>79.4</cell><cell>82.1</cell><cell>82.3</cell></row><row><cell>Cartoon</cell><cell>73.8</cell><cell>73.8</cell><cell>77.3</cell><cell>73.3</cell><cell>75.4</cell><cell>75.3</cell><cell>77.0</cell><cell>76.5</cell></row><row><cell>Sketch</cell><cell>69.4</cell><cell>74.3</cell><cell>71.5</cell><cell>65.1</cell><cell>72.2</cell><cell>71.4</cell><cell>73.0</cell><cell>72.6</cell></row><row><cell>Average</cell><cell>78.5</cell><cell>80.8</cell><cell>80.7</cell><cell>80.7</cell><cell>77.8</cell><cell>80.4</cell><cell>81.5</cell><cell>81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation on PACS dataset with ResNet-18 as backbone.</figDesc><table><row><cell cols="2">LAGG LM-IMG LM-F Curriculum Art Cartoon Photo Sketch Avg.</cell></row><row><cell>76.1 73.8</cell><cell>94.9 69.4 78.5</cell></row><row><cell>78.4 72.7</cell><cell>94.7 59.5 76.3</cell></row><row><cell>81.8 76.5</cell><cell>94.9 71.2 81.1</cell></row><row><cell>82.7 75.4</cell><cell>95.4 71.5 81.2</cell></row><row><cell>82.3 76.5</cell><cell>95.1 72.6 81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>ZSL+DG scenario on the DomainNet dataset with ResNet-50 as backbone.</figDesc><table><row><cell>Method</cell><cell cols="5">Clipart Infograph Painting Quickdraw Sketch Avg.</cell></row><row><cell>SPNet</cell><cell>26.0</cell><cell>16.9</cell><cell>23.8</cell><cell>8.2</cell><cell>21.8 19.4</cell></row><row><cell>mixup+SPNet</cell><cell>27.2</cell><cell>16.9</cell><cell>24.7</cell><cell>8.5</cell><cell>21.3 19.7</cell></row><row><cell cols="2">Epi-FCR+SPNet 26.4</cell><cell>16.7</cell><cell>24.6</cell><cell>9.2</cell><cell>23.2 20.0</cell></row><row><cell>CuMix</cell><cell>27.6</cell><cell>17.8</cell><cell>25.5</cell><cell>9.9</cell><cell>22.6 20.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>ZSL results.</figDesc><table><row><cell>Method</cell><cell cols="2">CUB SUN AWA1 FLO</cell></row><row><cell>ALE [1]</cell><cell cols="2">54.9 58.1 59.9 48.5</cell></row><row><cell>SJE [2]</cell><cell cols="2">53.9 53.7 65.6 53.4</cell></row><row><cell>SYNC [6]</cell><cell>56.3 55.6 54.0</cell><cell>-</cell></row><row><cell cols="2">GFZSL [41] 49.3 60.6 68.3</cell><cell>-</cell></row><row><cell cols="2">SPNet [46] 56.5 60.7 66.2</cell><cell>-</cell></row><row><cell>Baseline</cell><cell cols="2">52.4 58.2 62.5 58.4</cell></row><row><cell>CuMix</cell><cell cols="2">60.4 62.4 64.0 59.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We thank the ELLIS Ph.D. student program and the ERC grants 637076 -RoboExNovo (B.C.) and 853489 -DEXIM (Z.A.). This work has been partially funded by the DFG under Germanys Excellence Strategy EXC number 2064/1 Project number 390727645.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for attributebased classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards open world recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Comprehensive Survey on Domain Adaptation for Visual Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantically tied paired cycle consistency for zero-shot sketch-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5089" to="5098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2332" to="2345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Continuous manifold based adaptation for evolving visual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="867" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2452" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attribute-based classification for zeroshot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="453" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08383</idno>
		<title level="m">Continual learning: A comparative study on how to defy forgetting in classification tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to generalize: Metalearning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adagraph: Unifying predictive and continuous domain adaptation through graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6568" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kitting in the wild through online domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karaoguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1103" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inferring latent domains for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3771" to="3780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations Workshop Track Proceedings</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2751" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="764" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep representations of finegrained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation using feature-whitening and consensus loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9471" to="9480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8247" to="8255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Thong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08621</idno>
		<title level="m">Open cross-domain visual search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A simple exponential family framework for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="792" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Addressing model vulnerability to distributional shifts over image transformation sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7980" to="7989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<title level="m">Caltech-ucsd birds 200</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantic projection network for zero-and few-label semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8256" to="8265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zero-shot learninga comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Feature generating networks for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4582" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">f-vaegan-d2: A feature generating framework for any-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10275" to="10284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6502" to="6509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A zero-shot framework for sketch based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yelamarthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="316" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4166" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6034" to="6042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised open domain recognition by semantic discrepancy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="750" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<title level="m">Table 5. Results on DomainNet dataset with Real-Painting as sources and ResNet-50 as backbone</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Method/Target Clipart Infograph Sketch Quickdraw Avg</title>
		<imprint/>
	</monogr>
	<note>SPNet 21.5±0.6 14.1±0.2 17.3±0.3 4.8±0.4 14.4</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Epi-Fcr+spnet</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
