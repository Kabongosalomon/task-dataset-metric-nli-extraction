<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stacked Deconvolutional Network for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jing</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Hanqing</forename><forename type="middle">Lu</forename></persName>
						</author>
						<title level="a" type="main">Stacked Deconvolutional Network for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic Segmentation</term>
					<term>Deconvolutional Neural Network</term>
					<term>Dense Connection</term>
					<term>Hierarchical Supervision !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in semantic segmentation has been driven by improving the spatial resolution under Fully Convolutional Networks (FCNs). To address this problem, we propose a Stacked Deconvolutional Network (SDN) for semantic segmentation. In SDN, multiple shallow deconvolutional networks, which are called as SDN units, are stacked one by one to integrate contextual information and guarantee the fine recovery of localization information. Meanwhile, inter-unit and intra-unit connections are designed to assist network training and enhance feature fusion since the connections improve the flow of information and gradient propagation throughout the network. Besides, hierarchical supervision is applied during the upsampling process of each SDN unit, which guarantees the discrimination of feature representations and benefits the network optimization. We carry out comprehensive experiments and achieve the new state-of-the-art results on three datasets, including PASCAL VOC 2012, CamVid, GATECH. In particular, our best model without CRF post-processing achieves an intersection-over-union score of 86.6% in the test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S EMANTIC image segmentation has been one of the most important fields in computer vision, which is to predict the category of individual pixels in an image. Recently, Deep Convolutional Neural Networks (DCNNs) <ref type="bibr" target="#b0">[1]</ref> have strong learning ability to obtain high-level semantic features, and make remarkable advances in computer vision, including image classification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, object detection <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and keypoint prediction <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. For semantic segmentation tasks, DCNNs based methods mainly utilize the architecture of Full Convolutional Networks (FCNs) <ref type="bibr" target="#b8">[9]</ref> which usually adopt a certain pretrained classification network and output a probability map per class for an arbitrary-sized input. However, the classification network with downsampling operations sacrifices the spatial resolution of feature maps to obtain the invariance to image transformations. The resolution reduction results in poor object delineation and small spurious regions in segmentation outputs.</p><p>Many approaches have been proposed to solve the above problems. One way is to apply dilated convolutions <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. This type of solutions are to upsample the filters with different dilation factors, while the number of filter parameters per position remains unchanged. Accordingly, the receptive fields are enlarged and the larger contextual information is captured without losing the spatial resolution. However those methods output a coarse subsampling feature maps which still lose details of object delineation. Moreover, many methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> incorporate multi-scale or global feature maps to capture contextual information effectively and fit objects at multiple scales. Although this strategy further improves the receptive field and captures multi-scale information, it still outputs low-resolution feature maps which impede the generation of detailed boundaries.</p><p>Another type of methods are to recover the spatial resolution by an upsampling or deconvolutional path <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which can generate high-resolution feature maps for dense prediction. By learning the upsampling process, the low resolution of the feature maps can be restored to the input resolution for pixel-wise classification, which is useful for accurate boundary localization. In the above upsampling solutions, the deconvolutional and unpooling layers are appended with a symmetric structure of the corresponding convolutional and pooling layers. Consequently, the parameter size of the new network is twice as large as the original convolutional structure. Furthermore, the deconvolutional networks are usually built by simply stacking layers, leading to the degradation problem when the depth increases <ref type="bibr" target="#b3">[4]</ref>. To make the model easy to convergence, Wang et al. <ref type="bibr" target="#b15">[16]</ref> use the VGG16 network <ref type="bibr" target="#b2">[3]</ref> as pretrained weights to obtain better initial parameters of deconvolutional network, and Noh et al. <ref type="bibr" target="#b17">[18]</ref> use a two-stage training strategy on single object images and multi-object images, respectively. Due to the difficulties on network optimization, neither of the previous networks can be extended directly to the deeper models, e.g., RestNet <ref type="bibr" target="#b3">[4]</ref> and DenseNet <ref type="bibr" target="#b19">[20]</ref>, resulting in their limited learning ability.</p><p>To overcome the above issues, in this paper, we propose a Stacked Deconvolutional Network (SDN) for semantic image segmentation. SDN is a deeper deconvolutional network but easier to optimize compared with most deconvolutional solutions. Instead of building a single deep encoder-decoder network, we design an efficient shallow deconvolutional network (called as SDN unit), and stack multiple SDN units one by one with dense connections, thus making the proposed SDN capture more contextual information and easily optimized. The designing details of the proposed architecture are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. A SDN unit is an encoder-decoder network. The encoder module is operated as a downsampling process, aiming to exploit multi-scale features and capture contextual information as much as possible, while the decoder module is operated as an upsampling process to recover the spatial resolution, aiming for accurate boundary localization. To benefit from the impressive performance of the pretrained deep model on ImageNet <ref type="bibr" target="#b20">[21]</ref>, we adopt DenseNet-161 <ref type="bibr" target="#b19">[20]</ref> without its last downsampling operations as the encoder module of the first SDN unit, while other encoder and decoder modules consist of some simple downsampling blocks and upsampling blocks, respectively. Typically, each downsampling (upsampling) block includes a max-pooling layer (deconvolutional layer), several convolutional layers, and a compression layer.</p><p>As the number of the stacked SDN units increases, the difficulty on model training becomes a major problem. Two strategies have been taken to ameliorate the situation. First, hierarchical supervision is added to upsampling blocks of each SDN unit. Specifically, the compression layers are mapped to pixel-wise labeling maps by a classification layer. With this structure, the network could be learned in a more refined way with no loss on discrimination ability. Second, we import intra-unit and inter-unit connections to help the network optimization. The connections are shortcut paths from early layers to later layers, and they are beneficial to the flow of information and gradient propagation throughout the network. With in an upsampling or a downsampling block of a given SDN unit, the intra-unit connections is a short-range dense connection which is the direct link from the inputs of previous convolutional layers to the ones of back convolutional layers. The inter-unit connection is a long-range skip connection between certain two SDN units.</p><p>Considering the different intentions for information transmission, there are two forms of inter-unit connections. One is to link decoder and encoder modules between any two adjacent SDN units to promote the network optimization. The other is to connect the multi-scale feature maps from the encoder module of the first SDN unit to the corresponding decoder modules of each SDN unit, thus maintaining the low-level details for the high-resolution prediction. With the above two strategies, our proposed SDN can be trained efficiently and effectively, and achieves impressive performance on three popular benchmarks including PASCAL VOC 2012 dataset <ref type="bibr" target="#b21">[22]</ref>, CamVid dataset <ref type="bibr" target="#b22">[23]</ref> and GATECH dataset <ref type="bibr" target="#b23">[24]</ref>. In particular, a Mean IoU score of 86.6% without CRF post-processing on PASCAL VOC 2012 test set.</p><p>Our main contributions can be summarized as follows:</p><p>• We propose a novel network (Stacked Deconvolutional Network) for semantic segmentation, which stacks multiple shallow deconvolutional networks to capture multi-scale context, and guarantee the fine recovery of localization information.</p><p>• Our proposed SDN adopt intra-unit and inter-unit connections to enhance the flow of information and gradients throughout the network. In particular, inter-unit connections make the multi-scale information across different units efficient to reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The hierarchical supervision for each SDN unit results in better optimization of the proposed network, since early layers of the network can obtain more gradient feedback. Besides, it guarantees the discrimination of the feature maps from the upsampling process of each unit.</p><p>• Extensive experimental evaluations demonstrate that the proposed SDN model achieves the new state-ofthe-art performance on all the three benchmarks.</p><p>The remainder of this paper is organized as follows: In Section II, we review the related work about DCNNs based methods for semantic image segmentation and scan the basic architecture of DenseNet <ref type="bibr" target="#b19">[20]</ref>. In Section III, we will introduce our proposed Stacked Deconvolutional Network, including the designing details of a single SDN unit, the connections stacking multiple SDN units and the multi-scale hierarchical supervision. To verify the effectiveness of our work, the experimental evaluations and necessary analysis are presented in Section IV. Finally, we summarize our work in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recently, DCNNs based methods make great progress in semantic image segmentation, represented by FCNs <ref type="bibr" target="#b8">[9]</ref>. FCNs apply a fully convolutional structure and bilinear interpolation to realize pixel-wise prediction, which results in rough edges and object vanishing. Following the fully convolutional structure, many works try to alleviate these problems from the following sides.</p><p>Some networks import dilated convolutions and reduce down-sampleing operations, which enable context aggregation and retain more spatial information. Deeplab <ref type="bibr" target="#b9">[10]</ref> and DilatedNet <ref type="bibr" target="#b12">[13]</ref> adopt dilated convolutions to enlarge receptive fields and capture larger contextual information without losing resolution. <ref type="bibr" target="#b24">[25]</ref> employs hybrid dilated convolutions to further enlarge receptive fields of the network. Further on, Dai et al. <ref type="bibr" target="#b25">[26]</ref> proposes a deformable convolution to adjust the receptive fields according to the scale of objects. The above works remove some downsampling operations for increasing the resolution of outputs, which helps to generate detailed object delineation.</p><p>Some networks explore multi-scale or global features to capture contextual information for performance improvement. ParseNet <ref type="bibr" target="#b13">[14]</ref> employs global pooling operations to extract image-level information. Deeplabv2 <ref type="bibr" target="#b11">[12]</ref> proposes atrous spatial pyramid pooling (ASPP) to embed contextual information, which consists of parallel dilated convolutions with different dilated rates. PSPNet <ref type="bibr" target="#b14">[15]</ref> designs a pyramid pooling module to collect the effective contextual prior, containing information with different scales. Deeplabv3 <ref type="bibr" target="#b10">[11]</ref> proposes an augment ASPP module with image-level features to further capture global context.</p><p>Besides, some networks employ deconvolution operations in upsampling path, and thus they are able to realize high resolution prediction and obtain refined object delineation. OA-Seg <ref type="bibr" target="#b15">[16]</ref> and SegNet <ref type="bibr" target="#b16">[17]</ref> apply unpooling operations to unsample the low-resolution features and learn deconvolutional layers to improve the upsampling process. U-Net <ref type="bibr" target="#b26">[27]</ref> exploits multi-level features by skip connections in a deconvolutional network. RefineNet <ref type="bibr" target="#b27">[28]</ref> further refines coarse semantic features with fine-grained low-level features in a multi-path refined architecture. In human pose estimation tasks, Newell et al. <ref type="bibr" target="#b7">[8]</ref> stacks encoder-decoder architecture to capture multi-scale information, where the nearest neighbor interpolation is employed in its upsampling path. DCDN <ref type="bibr" target="#b28">[29]</ref>, which is our previous work, stacks many small deconvolutional networks for enhancing the learning ability of the network. However, the performances of the above deconvolutional solutions are limited by their difficulties on model optimization or simple network design.</p><p>In this work, we stack multiple shallow deconvolutional networks one by one to improve accurate boundary localization. In contrary to the work <ref type="bibr" target="#b7">[8]</ref>, we adopt pretrained classification network to encode images and deconvolutional layers to generate more refined recovery of the spatial resolution, and explore dense connection structure and hierarchical supervision in our network for easier network optimization. Meanwhile, we expand our previous work DCDN <ref type="bibr" target="#b28">[29]</ref> by redesigning deconvolutional network with intraunit and inter-unit connections, introducing hierarchical supervision, and inheriting the ImageNet <ref type="bibr" target="#b20">[21]</ref> pre-trained network. All these designs make our network produce more discriminative features and easy to optimize.</p><p>Our proposed SDN imports dense connections to make the stacked deep model easy to optimize. This is inspired by the work of DenseNet <ref type="bibr" target="#b19">[20]</ref>. Accordingly, we will overview the basic idea of DenseNet in the following.</p><p>DenseNet adopts dense connections to avoid the vanishing gradient problem and improve the flow of information, and it mainly consists of dense blocks and transition layers. The input of each convolutional layer within a dense block is the concatenation of all feature outputs of its previous layers at a given resolution. Consider x l is the output of the l th layer in a dense block, x l can be computed as follows:</p><formula xml:id="formula_0">x l = H l ([x 0 , x 1 , . . . , x l−1 ])<label>(1)</label></formula><p>where [x 0 , x 1 , . . . , x l−1 ] stands for the concatenation of the feature maps x 0 , x 1 , . . . , x l−1 , and x 0 is the inputs of the dense block. Meanwhile, H l is defined as a composite function of operations: BN, ReLU, a 1 × 1 convolution operation followed by BN, ReLU, a 3 × 3 convolution operation. Each dense block is followed by a transition layer, which do convolution and pooling to change the number and the size of feature maps. Finally, a softmax classifier is attached to make prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We propose a new framework called as Stacked Deconvolutional Network (SDN), which aims to capture more contextual information and recover high-resolution prediction progressively by stacking multiple shallow deconvolutional networks one by one. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, three (but not limited) deconvolutional networks, called as SDN units, are piled up from end to end, and the intra-unit connections and the interunit connections are jointly employed. Such a connected structure of the network enables efficient backward gradient propagation, and effective detailed boundaries restoration for full resolution labelling prediction. In order to obtain high-quality semantic representation, we adopt DenseNet-161, pre-trained on ImageNet <ref type="bibr" target="#b20">[21]</ref>, as the encoder module of the first deconvolutional network. A typical structure of a SDN unit is shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, which consists of two downsampling blocks and two upsampling blocks. Each downsampling (upsampling) block starts with a pooling (deconvolutional) layer, crosses a few of convolutional layers and ends with a compression layer. Besides, the hierarchical supervision for multi-level feature maps are jointly employed during each upsampling process to guarantee discrimination of the output prediction. In the inference step, we only use the highest-resolution results of the last unit as the final prediction. In the following subsections, we will elaborate the designing details of each SDN unit, intra-unit and inter-unit connections, and the hierarchical supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deconvolutional Network (SDN unit)</head><p>We design a shallow deconvolutional network referred to as a SDN unit to capture contextual information and refine poor object delineation, whose structure is illustrated in <ref type="figure" target="#fig_0">Fig.  1(a)</ref>. A SDN unit has an encoder module and a corresponding decoder module. In a encoder module, we stack two downsampling blocks to enlarge the receptive fields of the network, resulting in the low resolution of the feature maps. In a decoder module, upsampling blocks are used twice to achieve a more refined reconstruction of the feature maps.</p><p>For a given SDN unit, its encoder module takes the outputs of its previous unit as inputs and produces lowresolution feature maps with larger receptive fields. Here, we employ the downsampling blocks twice, resulting in <ref type="bibr">1 16</ref> spatial resolution of the input image. The structure of downsampling block is shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. A downsampling block consists of a max-pooling layer and 2 (or more) convolutional layers, and a compression layer. First, the feature map F i−1 n from the (i − 1) th block is fed into the max-pooling layer in the i th downsampling block of the n th unit and sub-sampled by a factor of 2 to produce a new feature map P i n . Second, behind the max-pooling layer, we cascade 2 convolutional layers with intra-unit connections. Concretely, the input of the convolutional layer is the concatenation of the input and output of its previous convolutional layer. Such a densely connected structure is beneficial to feature reuse, i.e., the multi-scale appearance of objects can be better captured to obtain effective semantic segmentation. The densely connected structure takes the feature map P i n and the output F i n−1 of the i th block in the (n − 1) th unit as inputs and outputs an feature map Q i n , where the i th block is the backward nearest block to the i th block in the same resolution. However, intra-unit connections bring the linear growth in the channel number of the feature maps, resulting in too much GPU memory demanding. Finally, in order to decrease the computational cost and the memory demanding, we employ a compression layer, which performs convolutions with fewer filters to reduce the channel number of the feature map Q i n , to generate an feature map F i n . In short, the i th downsampling block takes two feature maps F i−1 n and F i n−1 as its inputs, and outputs an new feature map F i n , the operations can be summarized as follows:</p><formula xml:id="formula_1">P i n = M ax(F i−1 n ), Q i n = T rans([P i n , F i n−1 ]), F i n = Comp(Q i n ).<label>(2)</label></formula><p>where [P i n , F i n−1 ] stands for the concatenation of the feature maps P i n and F i n−1 . M ax(·) denotes a max-pooling operation. T rans(·) denotes a transformation function of the densely connected structure, in which two sequences of BN, ReLU, a 3×3 convolution, dropout, and concatenation operations are performed. Comp(·) refers to a 3 × 3 convolution operation. It should be noted that our encoder module of the first SDN unit employs full convolutional DenseNet-161 to obtain high-semantic features, while the other SDN units adopt the encoder module described above.</p><p>In the decoder module, we apply upsampling blocks to progressively upsample feature maps to larger resolution. The upsampling blocks are also used twice to enlarge resolution back to <ref type="bibr">1 4</ref> spatial resolution of the input image. An upsampling block consists of a deconvolutional layer and several convolutional layers and a compression layer. As shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, in the i th upsampling block, we first apply deconvolutional operation on the output F i−1 n of the (i − 1) th block and produce a high-resolution feature map O i n . Then the feature map O i n is concatenated with the feature map H k 1 from the k th block in the encoder module of the first SDN unit, where the k th block have the same resolution with O i n . Finally, the concatenated feature maps are fed to the subsequent convolutional layers and compression layer, which adopt the similar connection structure as the downsampling block. The output F i n of the i th upsampling block can be computed as follow:</p><formula xml:id="formula_2">O i n = Deconv(F i−1 n ), Q i n = T rans([O i n , H k 1 ]), F i n = Comp(Q i n ).<label>(3)</label></formula><p>where Deconv(·) refers to a deconvolutional operation. For the last unsamlping block of the last SDN unit, we abandon its compression layer for better prediction. Note that, our highest resolution of SDN unit is set to a quarter of input images. One important reason for this design is that we can reduce GPU memory usage of a single SDN unit to stack more units. Contrary to traditional deconvolutional networks <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, which have difficulty in network training and require additional aids, our deconvolutional network achieves great improvements in network training. First, our deconvolutional network is shallow encoder-decoder framework, which only includes two simple downsampling and upsampling blocks. Then, intra-unit connections are performed between convolutional layers, and this enables effective backward propagation of the gradients through a SDN unit. All these design improves end-to-end training of all network blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Densely Connecting SDN units</head><p>To enhance the learning ability of network, we stack some shallow SDN units as introduced in above subsection into a very deep model. Meanwhile, the inter-unit connections are imported to make the multi-scale information across different units efficient to reuse. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, there are two types of inter-unit skip connections in the proposed framework. One is between any two adjacent SDN units, and the other is a kind of skip connections from the first SDN units to others. In the following, we will introduce them in detail. The skip connections between any two adjacent SDN units are used to promote the flows of high-level semantic information and improve the optimization of encoder modules. For a given SDN unit, its encoder module exploit the intermediate features of the decoder module in its previous unit by a shortcut path. Specifically, the feature map P i n from the i th block of the n th unit is concatenated with the feature map F i n−1 from the i th block of the (n − 1) th unit. Such a concatenation operation is shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. We adopt the skip connection twice according to the number of downsamlpling blocks. With such skip connections, the gradients can be directly propagated to the previous unit, thus promoting the optimization of network. Besides, the fusion of features from adjacent units would efficiently capture multi-scale information.</p><p>Meanwhile, we adopt the skip connections from the first SDN unit to others to fuse low-level representations and high-level semantic features, resulting in refined object segmentation edges. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we feed the low-level visual features from the k th block of the first encoder module into a convolutional layer, where the convolution operations generate a feature map H k 1 . Then the feature map H k 1 is concatenated with the outputs of the deconvolutional layer in the corresponding resolution. Here, we combine the features from the first encoder module with the upsampling features from each decoder module. This is because mid-level representations in first encoder module have more spatial visual information. Such interunit connections contribute to detailed boundaries for highresolution prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical Supervision</head><p>Deeper networks lead to better performance. However, the difficulty in training deeper network becomes a major problem. We stack multiple shallow deconvolutional networks with random initialization, which leads to additional optimization difficulty. According to the previous design, interunit and intra-unit connections are used to assist training. In order to alleviate this problems further, we add hierarchical supervision in each SDN unit. Specifically, the output F i n of the i th upsampling block is fed to a pixel-wise classification layer to obtain a feature map E i n with channel C, where C is the number of possible labels. Then E i n is upsampled to match the size of the input image with bilinear interpolation, and finally supervised with pixel-wise groundtruth. In this way, the hierarchical supervision helps optimize the learning process. The pixel-wise cross-entropy loss is applied to all predictions in the network. For the i th block with a pixelwise cross-entropy loss L i , the loss is computed as follows:</p><formula xml:id="formula_3">L i = (Bi(E i n ), G), E i n = Classif y(F i n )<label>(4)</label></formula><p>where denotes a cross-entropy loss function, Classif y(·) denotes a classifier with a 3 × 3 convolution operation, Bi(·) denotes a bilinear interpolation operation, and G refers to a ground truth map. In order to further improve the performance of the network, we enhance score map fusion before bilinear interpolation in the same resolution, depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>. In particular, for the n th SDN unit, the output E i n of the classification layer are fused with the features S i n−1 in the (n − 1) th unit by element-wise sum operation to produce an new fused feature S i n , and then the fused feature S i n is upsampled with bilinear interpolation, and also supervised with pixelwise groundtruth. Therefor, we can compute the loss L i as follows:</p><formula xml:id="formula_4">L i = (Bi(S i n ), G), S i n = E i n ⊕ S i n−1<label>(5)</label></formula><p>where ⊕ denotes element-wise sum. Such a same-scale sum fusion of score maps enhances information flows and improves segmentation results.</p><p>In the testing phase, we only use the highest-resolution result of the last unit as the final prediction. It can be found that, with the hierarchical supervision, the feature maps guarantee the discrimination and restrain the noise during upsampling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>In each SDN unit, we stack 4 convolutional layers in the block of the lowest resolution for better global perspective, while the other blocks have 2 convolutional layers. The convolutional layers in a block is composed of BN, ReLU, and a 3 × 3 convolution operation followed by dropout with the probability of 0.2. The filter numbers of the convolutional layers are all set to 48. In two downsampling blocks, the compression layers are 3 × 3 convolution operations and their filter numbers are set to 768 and 1024 respectively. Max-pooling with 2 × 2 window and stride 2 is preformed. Meanwhile, in two upsampling blocks, the compression layers also are 3 × 3 convolution operations and their filter number are set to 768 and 576 respectively. Upsampling operation can be done by a 4 × 4 deconvolutional operation with stride 2. The convolutional layer in inter-unit connections from the first SDN unit to others is composed of BN, ReLU, and a 3 × 3 convolution operation. We employ the full convolutional DenseNet-161 network, pre-trained on ImageNet, as the first encoder module. Meanwhile, the last downsampling operations is removed and dilation convolutions is used.</p><p>The proposed SDN is implemented with Caffe <ref type="bibr" target="#b29">[30]</ref>. Similar to <ref type="bibr" target="#b9">[10]</ref>, we optimize it using the "ploy" learning rate policy, with batchsize of 10. We set power to 0.9, momentum to 0.9 and weight decay to 0.0005. We apply data augmentation in the training step. Here, random crops of 320 × 320 are used, and horizontal flip is also applied. In the inference step, we pad images with mean value to make the length divisible by 16 before feeding full images into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To show the effectiveness of our approach, we carry out comprehensive experiments on PASCAL VOC 2012 dataset <ref type="bibr" target="#b21">[22]</ref>, CamVid dataset <ref type="bibr" target="#b22">[23]</ref> and GATECH dataset <ref type="bibr" target="#b23">[24]</ref>. Moreover, we perform a series of ablation evaluations to inspect the impact of various components on PASCAL VOC 2012 dataset. Experimental results demonstrate our proposed SDN achieves new state-of-the-art performance on 3 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dataset</head><p>PASCAL VOC 2012 : The dataset has 1,464 images for training, 1,449 images for validation and 1,456 images for testing, which involves 20 foreground object classes and one background class. Meanwhile, we augment the training set with extra labeled PASCAL VOC images provided by Semantic Boundaries Dataset <ref type="bibr" target="#b30">[31]</ref>, resulting in 10,582 images for training.</p><p>CamVid: The dataset is a street scene understanding dataset which consists of 5 video sequences. Following <ref type="bibr" target="#b16">[17]</ref>, we split the dataset into 367 training images, 100 validation images, and 233 test images. The resolution of each image is 360 × 480 and all images belong to 11 semantic categories. Compared with PASCAL VOC 2012 dataset, CamVid dataset has more strong spatial-relationship among different categories.</p><p>GATECH: The dataset is a large video set of outdoor scenes which consists of 63 videos with 12241 frames for training and 38 videos with 7071 frames for testing. The dataset is labeled with 8 semantic classes which are sky, ground, solid, porous, cars, humans, vertical mix, and main mix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics</head><p>We only report the results of Mean IoU on PASCAL VOC 2012 dataset for the common convention, while we evaluate other datasets with Global Avg and Mean IoU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on PASCAL VOC 2012 dataset</head><p>In this subsection, we first verify the effectiveness of each component in our approach, including the effects of stacking multiple SDN units, stacked network design and hierarchical supervision. Then we report the experimental results on PASCAL VOC 2012 test set. Here, we set the initial learning rate to 0.00025, and further apply data augmentation by randomly transforming the input images with 5 scales {0.6, 0.8, 1, 1.2, 1.4}, and 5 aspect rations {0.7, 0.85, 1, 1.15, 1.3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Stacking multiple SDN units</head><p>We stack multiple SDN units to refine the segmentation maps. The key of the stacked architecture is that each unit can capture more contextual information and recover the high-resolution features. To verify this intuition, we gradually increase the number of the SDN units and test the performance respectively. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, we refer to the network stacking k SDN units as SDN M k , and the number of the SDN units is up to 3.</p><p>The results are shown in <ref type="table" target="#tab_0">Table 1</ref>, it is observed that the performance consistently increases with the growth of SDN unit number. Specially, the performance of SDN M 1 network is only 78.2%. When we increase SDN unit number from 1 to 3, the performance improves from 78.2% to 79.2% to 79.9%. Meanwhile, we show some predicted semantic maps under different networks in <ref type="figure" target="#fig_4">Fig. 4</ref>. With the growth of SDN unit number, the object edges are ameliorated (the first and fourth column), and the object discriminate is enhanced (the second and third column). These factors bring improvement of the semantic maps. The noticeable trend indicates that, with increasing number of the stacked units, the model ben- efits from the deeper network. Moreover, stacking multiple SDN units makes a coarse-to-fine prediction process, thus improving the network performance. However, it also leads to more computational stress and GPU memory demand, thus we stack up to 3 SDN units as more units bring too slight improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Stacked network design</head><p>From the experiments above, the network depth and its parameter size increase with the number of stacked units. We want to explore that the improvements are mainly brought by more parameters or stacked network design. To address this problem, we design a single encoder-decoder network, named as SDN M 1 +, which has the same network depth and the size of parameters as the network stacking two SDN units, i.e., SDN M 2 . SDN M 1 + also employ DenseNet-161 as the encoder module, meanwhile, the decoder module consists of 6 trivial convolutional layers and 2 upsampling blocks, the number of layers in upsampling block are set to 11 and 7 respectively. The results for the performance comparison between SDN M 1 + and SDN M 2 are listed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>From the results we can see that the performance of SDN M 2 is 0.6 percent higher than SDN M 1 +. The comparison indicates that such a stacked network design, which captures more contextual information, improves the performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Hierarchical supervision</head><p>To further alleviate the difficulty on optimization, we add hierarchical supervision to the upsampling process in each unit. In order to explore the impact of hierarchical supervision, we take SDN M 1 as an example to test the performance of different supervision, Specifically, we modify SDN M 1 network by adding supervision at different upsampling scales, and refer corresponding settings to as SDN M 1 1 , SDN M 1 2 , SDN M 1 respectively. Here, we denote by up ratio the ratio of the input image spatial resolution to the output resolution of the block. In SDN M 1 network, we add supervision at up ratio = 16, 8, 4. And for SDN M 1 2 network, we add supervision at up ratio = 8, 4. In SDN M 1 1 network, we only add supervision at up ratio = 4. In the inference step, we output the prediction at different up ratio.</p><p>Results are shown in <ref type="table" target="#tab_1">Table 2</ref>, we can find that, in SDN M 1 network, the performance at up ratio = 4 is 2.3 percent higher than the performance at up ratio = 16, it is clear that the high-resolution prediction, which use more lowlevel visual features, performs better than the low-resolution prediction during upsampling process of the unit. Meanwhile, the performance keep increasing with the number of the supervision. the performance of SDN M 1 2 is 0.5 percent higher than SDN M 1 1 , and there is modest improvement in performance from 78.0 to 78.2 when we add supervision at all up ratio. The results prove the effectiveness of hierarchical supervision, and we employ hierarchical supervision during upsampling process of each unit. We also explore the effects of score map fusion, depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>. It is to compare the performance between networks with or without score map fusion. To this end, we stack two SDN units without score map connection, and refer to the corresponding network as SDN M 2 −. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>, the performance of SDN M 2 is 0.4 percent higher than SDN M 2 −. From the results we can see that the score map fusion is benefit for the segmentation result.  <ref type="bibr" target="#b31">[32]</ref> for the better performance. In order to compare with these models, we also pretrain the model of SDN M 2 * on MS-COCO dataset. We evaluate how each of these factors affects val set performance in <ref type="table" target="#tab_3">Table 4</ref>. Increasing an upsampling block gives 0.4% gain, and segmentation map fusion brings another 1.2% improvement. Moreover, when we pretrain the model of SDN M 2 * on MS-COCO dataset, the performance attains 84.8%. Compared with current well-known method Deeplabv3 <ref type="bibr" target="#b10">[11]</ref>(82.7% pre-trained on MS-COCO), our method of the SDN M 2 * outperforms theirs by 2.1%, which shows the strength of our proposed SDN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Comparing with the state-of-the-art methods</head><p>We further compare our method with the state-of-the-art methods on PASCAL VOC 2012 test set. Here, based on two settings, i.e., with or without pre-trained with MS-COCO dataset, we fine tune our the model of SDN M 3 on PASCAL VOC 2012 trainval set, and submit our test results to the official evaluation server. Results are shown in  <ref type="bibr" target="#b10">[11]</ref> by 0.9%, they both adopt multiscale or global features to capture contextual information. These comparisons indicate that our proposed SDN network can more effectively capture contextual information and generate accurate boundary localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on CamVid dataset</head><p>In this subsection, we carry out experiments on the CamVid <ref type="bibr" target="#b22">[23]</ref> dataset to further evaluate the effectiveness of our method. The experimental settings are as follows: we apply SDN M 2 * network, and train the network with 367 training images, and test it with 233 test images. The initial learning rate is changed to 5e-5. Meanwhile, we also adopt data augmentation to reduce overfitting by multi-scale translation. Note that we compare SDN with the previous state-of-theart methods on two settings, i.e., initializing the network with or without the model pretrained on PASCAL VOC 2012 <ref type="bibr" target="#b21">[22]</ref>.</p><p>Following <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b40">[41]</ref>, Mean IoU and Global Avg are employed to evaluate our method on this dataset. We compare our method with previous ones in <ref type="table" target="#tab_7">Table 6</ref>. Results show that our method on two settings both outperform other methods. The model, without pretrained on PASCAL VOC 2012(marked by SDN), achieves 69.6% in Mean IoU and 91.7% in Global Avg. Particularly, the classes, including Car, Sign, Pedestrian, Bicyclist, have a major boost in performance. When we initialize the network pretrained on PASCAL VOC dataset (marked by SDN+), the performance further improves by 2.2 percent in Mean IoU and 1 percent in Global Avg, and here nine out of eleven object categories achieve best performance in Mean IoU. Note that, the CamVid dataset sampling in video frames contains temporal information, and some works <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> mine temporal information to aid segmentation results. Our method still outperforms these works by a relative large margin. Besides, the spatio-temporal information of the dataset is complementary to our method and could bring additional improvements.</p><p>Some test images along with ground truth and our predicted semantic maps are shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. We can find that our network can well sketch multi-scale appearance of objects, including large-scale objects,i.e., building and car, and shape objects i.e., poles and pedestrians. All the results on the this dataset show that our network can capture more contextual information and learn better spatial-relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on GATECH dataset</head><p>In order to further verify the generalization of our models, we evaluate our network on GATECH <ref type="bibr" target="#b23">[24]</ref> dataset, which is much larger than CamVid dataset, but has a lot of noisy annotations. We employ the SDN M 2 * network with the same training strategy on CamVid. We also compare our models with previous state-of-the-art methods on two settings, i.e., initializing the network with (marked by SDN) or without (marked by SDN+) the model pretrained on PASCAL VOC 2012 <ref type="bibr" target="#b21">[22]</ref>.</p><p>Results are shown in <ref type="table" target="#tab_8">Table 7</ref>, we can find that our method on two settings both outperforms other methods. SDN yields 53.5% in mean IoU and 84.6% in Global Avg. and SDN+ improves the result significantly, which gives 1.7% gain in Global Avg and 2.4% in Mean IoU. Specially, our model, without using any temporal, performs better than the models <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b42">[43]</ref> which exploit spatio-temporal      relationships between video frames. All these comparisons confirm the proposed SDN a robust and effective model for coarse-annotation dataset. Some test images along with ground truth and our predicted semantic maps are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have presented a stacked deconvolutional network (SDN), a novel deep network architecture for semantic segmentation. We stack multiple SDN units to make network deeper and realize a coarse-to-fine learning process, meanwhile, intra-unit and inter-unit connections and hierarchical supervision are adopted to promote network optimization. The ablation experiments show that those designs effectively capture contextual information and recover the spatial resolution for accurate boundary localization, which benefit network performance. Our best model outperform all previous works on three public benchmarks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overall architecture of our approach. The upper part indicates the structure of proposed stacked deconvolutional network (SDN), the lower part indicates the detailed structure of the SDN unit (a), the downsampling block (b), the upsampling block (c). (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Hierarchical supervision with score map connections during upsampling process. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Global Avg: Percentage of correctly classified pixels over the whole dataset. i.e. i tii i Ti .• Mean IoU: Ratio of correctly classified pixels in a class over the union set of pixels predicted to this class and groundtruth, and then averaged over all classes. i.e. 1 N i tii Ti+ j tji−tii . where N is the number of semantic classes, and T i is the total number of pixels in class i, while t ij indicates the number of pixels which belong to class i and predicted to class j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Different stacked SDN structure. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Results on PASCAL VOC 2012 val set. For every column we list input images (A), the semantic segmentation results of SDN M 1 network (B), SDN M 2 network (C), SDN M 3 network (D), and Ground Truth (E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Results on PASCAL VOC 2012 dataset. The images in each row from left to right are: (1) input image (2) groundtruth (3) semantic segmentation result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Results on CamVid dataset. The images in each column from top to down are: (1) input image (2) semantic segmentation result(3)groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Results on GATECH dataset. The images in each column from top to down are: (1) input image (2) semantic segmentation result(3)groundtruth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>The performance comparison between different networks on PASCAL VOC 2012 val set.SDNM 1 SDNM 2 SDNM 3 SDNM 1+</figDesc><table><row><cell>Depth</cell><cell>169</cell><cell>185</cell><cell>201</cell><cell>185</cell></row><row><cell>Parameters (M)</cell><cell>84.9</cell><cell>161.7</cell><cell>238.5</cell><cell>161.7</cell></row><row><cell>Mean IoU</cell><cell>78.2</cell><cell>79.2</cell><cell>79.9</cell><cell>78.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 The</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">performance comparison between different supervision on</cell></row><row><cell></cell><cell cols="3">PASCAL VOC 2012 val set.</cell><cell></cell></row><row><cell></cell><cell cols="4">up ratio SDNM 1 1 SDNM 1 2 SDNM 1</cell></row><row><cell></cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>75.9</cell></row><row><cell>Mean IoU</cell><cell>8</cell><cell>-</cell><cell>77.1</cell><cell>77.7</cell></row><row><cell></cell><cell>4</cell><cell>77.5</cell><cell>78.0</cell><cell>78.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>The performance comparison networks with or without score map connections on PASCAL VOC 2012 val set.In the section, detailed evaluations are performed on PAS-CAL VOC 2012 val dataset. Here, we adopt several steps to improve segmentation performance further based on the SDN M 2 network: (1) UP: We further restore high resolution features by cascading a upsampling block, and we refer to the network as SDN M 2 * .</figDesc><table><row><cell></cell><cell cols="2">SDNM 2 SDNM 2−</cell></row><row><cell>Mean IoU</cell><cell>79.2</cell><cell>78.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 The</head><label>4</label><figDesc></figDesc><table><row><cell>performance comparison between different measures on PASCAL</cell></row><row><cell>VOC 2012 val set.</cell></row><row><cell>Up MS Flip COCO Mean IoU</cell></row><row><cell>79.2</cell></row><row><cell>79.6</cell></row><row><cell>80.7</cell></row><row><cell>84.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>In the two settings, our method both outperforms all the other methods. Trained with only PSACAL VOC 2012 data, we achieve a Mean IoU score of 83.5% 1 . When we pretrain the model of SDN M 3 on MS-COCO dataset, it reaches 86.6% 2 in Mean IoU. Specifically, our model outperforms the RefineNet [28] by 2.4%, and RefineNet employs deep encoder-decoder structure and demonstrates outstanding performance on several semantic segmentation dataset. Meanwhile, our model also outperforms the PSPNet [28] by 1.2% and Deeplabv3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1. http://host.robots.ox.ac.uk:8080/anonymous/Z9RDVZ.html 2. http://host.robots.ox.ac.uk:8080/anonymous/GRWV3B.html</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Experimental results on PASCAL VOC 2012 test set. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoU Only using VOC data FCN [9] 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 DeepLab [12] 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6 CRF-RNN [33] 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0 DeconvNet [18] 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 72.5 GCRF [34] 85.2 43.9 83.3 65.2 68.3 89.0 82.7 85.3 31.1 79.5 63.3 80.5 79.3 85.5 81.0 60.5 85.5 52.0 77.3 65.1 73.2 DPN [35] 87.7 59.4 78.4 64.9 70.3 89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0 83.5 82.3 60.5 83.2 53.4 77.9 65.0 74.1 Piecewise [36] 90.6 37.6 80.0 67.8 74.4 92.0 85.2 86.2 39.1 81.2 58.9 83.8 83.9 84.3 84.8 62.1 83.2 58.2 80.8 72.3 75.3 ResNet38 [37] 94.4 72.9 94.9 68.8 78.4 90.6 90.0 92.1 40.1 90.4 71.7 89.9 93.7 91.0 89.1 71.3 90.7 61.3 87.7 78.RNN [33] 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7 Dilation8 [13] 91.7 39.6 87.8 63.1 71.8 89.7 82.9 89.8 37.2 84.0 63.0 83.3 89.0 83.8 85.1 56.8 87.6 56.0 80.2 64.7 75.3 DPN [35] 89.0 61.6 87.7 66.8 74.7 91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8 85.6 85.4 63.6 87.3 61.3 79.4 66.4 77.5 Piecewise [36] 94.1 40.7 84.1 67.8 75.9 93.4 84.3 88.4 42.5 86.4 64.7 85.4 89.0 85.8 86.0 67.5 90.2 63.8 80.9 73.0 78.0 DeepLab [12] 92.6 60.4 91.6 63.4 76.3 95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1 87.0 87.4 63.3 88.3 60.0 86.8 74.5 79.7 RefineNet [28] 95.0 73.2 93.5 78.1 84.8 95.6 89.8 94.1 43.7 92.0 77.2 90.8 93.4 88.6 88.1 70.1 92.9 64.3 87.7 78.8 84.2 ResNet38 [37] 96.2 75.2 95.4 74.4 81.7 93.7 89.9 92.5 48.2 92.0 79.9 90.1 95.5 91.8 91.2 73.0 90.5 65.4 88.7 80.6 84.9 PSPNet [15] 95.8 72.7 95.0 78.9 84.4 94.7 92.0 95.7 43.1 91.0 80.3 91.3 96.3 92.3 90.1 71.5 94.4 66.9 88.8 82.0 85.4 DeepLabv3 [11] 96.4 76.6 92.7 77.8 87.6 96.7 90.2 95.4 47.5 93.4 76.3 91.4 97.2 91.0 92.1 71.3 90.9 68.9 90.8 79.3 85.</figDesc><table><row><cell>1 82.5</cell></row></table><note>7 SDN+ 96.9 78.6 96.0 79.6 84.1 97.1 91.9 96.6 48.5 94.3 78.9 93.6 95.5 92.1 91.1 75.0 93.8 64.8 89.0 84.6 86.6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Experimental results on CamVid test set.MethodBuilding Tree Sky Car Sign Road Pedestrian Fence Pole Sidewalk Bicyclist Mean IoU Global Avg</figDesc><table><row><cell>SegNet [17]</cell><cell>68.7</cell><cell cols="5">52.0 87.0 58.5 13.4 86.2</cell><cell>25.3</cell><cell cols="2">17.9 16.0</cell><cell>60.5</cell><cell>24.8</cell><cell>46.4</cell><cell>62.5</cell></row><row><cell>DeconvNet [18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.9</cell><cell>85.6</cell></row><row><cell>ReSeg [38]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.8</cell><cell>88.7</cell></row><row><cell>DeepLab-LFOV [10]</cell><cell>81.5</cell><cell cols="5">74.6 89.0 82.2 42.3 92.2</cell><cell>48.4</cell><cell cols="2">27.2 14.3</cell><cell>75.4</cell><cell>50.1</cell><cell>61.6</cell><cell>-</cell></row><row><cell>Bayesian SegNet [19]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.1</cell><cell>86.9</cell></row><row><cell>Dilation8 [13]</cell><cell>82.6</cell><cell cols="5">76.2 89.0 84.0 46.9 92.2</cell><cell>56.3</cell><cell cols="2">35.8 23.4</cell><cell>75.3</cell><cell>55.5</cell><cell>65.3</cell><cell>79.0</cell></row><row><cell>HDCNN-448+TL [39]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.6</cell><cell>90.9</cell></row><row><cell>Dilation8+FSO [40]</cell><cell>84.0</cell><cell cols="5">77.2 91.3 85.6 49.9 92.5</cell><cell>59.1</cell><cell cols="2">37.6 16.9</cell><cell>76.0</cell><cell>57.2</cell><cell>66.1</cell><cell>88.3</cell></row><row><cell>FC-DenseNet103 [41]</cell><cell>83.0</cell><cell cols="5">77.3 93.0 77.3 43.9 94.5</cell><cell>59.6</cell><cell cols="2">37.1 37.8</cell><cell>82.2</cell><cell>50.5</cell><cell>66.9</cell><cell>91.5</cell></row><row><cell>G-FRNet [42]</cell><cell>82.5</cell><cell cols="5">76.8 92.1 81.8 43.0 94.5</cell><cell>54.6</cell><cell cols="2">47.1 33.4</cell><cell>82.3</cell><cell>59.4</cell><cell>68.0</cell><cell>-</cell></row><row><cell>DCDN [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.4</cell><cell>91.4</cell></row><row><cell>SDN</cell><cell>84.45</cell><cell cols="5">76.9 92.2 88.2 51.6 93.4</cell><cell>62.4</cell><cell cols="2">37.0 37.5</cell><cell>77.8</cell><cell>64.4</cell><cell>69.6</cell><cell>91.7</cell></row><row><cell>SDN+</cell><cell>85.2</cell><cell cols="5">77.5 92.3 90.2 53.9 96.0</cell><cell>63.8</cell><cell cols="2">39.8 38.4</cell><cell>85.36</cell><cell>66.9</cell><cell>71.8</cell><cell>92.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Experimental results on GATECH test set.</figDesc><table><row><cell></cell><cell cols="3">Temporal Global Mean</cell></row><row><cell>Method</cell><cell>Info</cell><cell>Avg</cell><cell>IoU</cell></row><row><cell>3D-V2V-scratch [43]</cell><cell>Yes</cell><cell>66.7</cell><cell>-</cell></row><row><cell>3D-V2V-finetune [43]</cell><cell>Yes</cell><cell>76.0</cell><cell>-</cell></row><row><cell>FC-DenseNet103 [41]</cell><cell>No</cell><cell>79.4</cell><cell>-</cell></row><row><cell>HDCNN-448+TL [39]</cell><cell>Yes</cell><cell>82.1</cell><cell>48.2</cell></row><row><cell>DCDN [29]</cell><cell>No</cell><cell>83.5</cell><cell>49.0</cell></row><row><cell>SDN</cell><cell>No</cell><cell>84.6</cell><cell>53.5</cell></row><row><cell>SDN+</cell><cell>No</cell><cell>86.3</cell><cell>55.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Objectness-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometric context from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3081" to="3088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08502</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06211</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">densely connected deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks,&quot; in ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gaussian conditional random field network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellapa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3224" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reseg: A recurrent neural network-based model for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchically supervised deconvolutional network for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="437" to="445" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09326</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D B B</forename><surname>Md Amirul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep end2end voxel2voxel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
