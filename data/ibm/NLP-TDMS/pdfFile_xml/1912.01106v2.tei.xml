<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MnasFPN : Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam Quoc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MnasFPN : Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the blooming success of architecture search for vision tasks in resource-constrained environments, the design of on-device object detection architectures have mostly been manual. The few automated search efforts are either centered around non-mobile-friendly search spaces or not guided by on-device latency. We propose MnasFPN, a mobile-friendly search space for the detection head, and combine it with latency-aware architecture search to produce efficient object detection models. The learned MnasFPN head, when paired with MobileNetV2 body, outperforms MobileNetV3+SSDLite by 1.8 mAP at similar latency on Pixel. It is both 1 mAP more accurate and 10% faster than NAS-FPNLite. Ablation studies show that the majority of the performance gain comes from innovations in the search space. Further explorations reveal an interesting coupling between the search space design and the search algorithm, for which the complexity of MnasFPN search space is opportune 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing neural network architectures for efficient deployment on mobile devices is not an easy task: one has to judiciously trade off the amount of computation with accuracy, while taking into consideration the set of operations that are supported and favored by the devices. Neural architecture search (NAS, <ref type="bibr" target="#b32">[33]</ref>) provides the framework to automate the design process, where a RL controller will learn to generate fast and accuracy models within a user-specified search space. While the focus of NAS papers have been on improving the search algorithm, the search space design remains a critical performance factor that is less visited.</p><p>Despite the significant advances on NAS for image classification both in the server setting <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b24">25]</ref> and in the mobile setting <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref>, relatively fewer at-  <ref type="table">Table 1</ref>. MnasFPN variations compared with other mobile detection models on COCO test-dev. Latency numbers with '*' are remeasured in the same configuration (same benchmarker binary and same device) as MnasFPN models to ensure fairness of comparison. Models with † employs the channel-halving trick <ref type="bibr" target="#b8">[9]</ref>. Models with ‡ was obtained with a depth multiplier of 0.7 on both head and backbone. tempts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26]</ref> focus on object detection. This is in part because the additional complexity in the search space of the detection head relative to the backbone. The backbone is a feature extractor that sequentially extracts features at increasingly finer scales, which behaves the same way as the feature extractor for image classification. Therefore, current NAS approaches either repurpose classification feature extractors for detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, or search the backbone while fixing the detection head <ref type="bibr" target="#b3">[4]</ref>. Since the backbone is composed of a sequence of layers, its search space is sequential. In contrast, a detection head could be highly nonsequential. It needs to fuse and regenerate features across multiple scales for better class prediction and localization. The search space therefore includes what features to fuse, as well as how often and in what order to fuse them. This is a challenging task that few NAS frameworks have demonstrated the ability to handle. One exception is NAS-FPN <ref type="bibr" target="#b6">[7]</ref>, which was the first NAS paper that tackles the non-sequential search space of the detection head. It demonstrates state-of-the-art performance when optimized for accuracy only, and its manually designed variant called NAS-FPNLite performs competitively on mobile devices. However, NAS-FPNLite is limited in three aspects. 1) The search process that produces the architecture is not guided by computational complexity or on-device latency; 2) The architecture was manually adapted to work with mobile devices, of which the process may be further optimized; 3) The original NAS-FPN search space was not tailored towards mobile use cases.</p><p>Our work addresses the above limitations. We propose a search space called MnasFPN, which is specifically designed for mobile devices where depthwise convolutions are reasonably optimized. Our search space re-introduces the inverted residual block <ref type="bibr" target="#b21">[22]</ref>, which is proven to be effective for mobile CPUs, into the detection head. We conduct NAS on the search space that is guided by on-device latency signals. The search found an architecture that is remarkably simple yet highly performant.</p><p>Our contributions include: 1) A mobile-specific search space for the detection head; 2) The first attempt to conduct latency-aware search for object detection; 3) A set of detection head architectures that outperform SS-DLite <ref type="bibr" target="#b21">[22]</ref> and NAS-FPNLite <ref type="bibr" target="#b6">[7]</ref>; 4) Ablation studies showing that our search space design is judiciously chosen for the current NAS controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mobile Object Detection Models</head><p>The most common detection models on mobile devices are manually designed by experts. Among them are singleshot detectors such as YOLO <ref type="bibr" target="#b19">[20]</ref>, SqueezeDet <ref type="bibr" target="#b28">[29]</ref>, and Pelee <ref type="bibr" target="#b26">[27]</ref> as well as two-stage detectors, such as Faster RCNN <ref type="bibr" target="#b20">[21]</ref>, R-FCN <ref type="bibr" target="#b4">[5]</ref>, and ThunderNet <ref type="bibr" target="#b18">[19]</ref>.</p><p>SSDLite <ref type="bibr" target="#b21">[22]</ref> is the most popular light-weight detection head architecture. It replaces the expensive 3×3 full convolutions in the SSD head <ref type="bibr" target="#b15">[16]</ref> with separable convolutions to reduce computational burden on mobile devices. This technique is also employed by NAS-FPNLite <ref type="bibr" target="#b6">[7]</ref> to adapt NAS-FPN to mobile devices. SSDLite and NAS-FPNLite are paired with efficient backbones such as MobileNetV3 <ref type="bibr" target="#b8">[9]</ref> to produce state-of-the-art mobile detectors. Since we design mobile-friendly detection heads, both SSDLite and NAS-FPNLite are crucial baselines to showcase our effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Architecture Search for Mobile Models</head><p>Our NAS search is guided by latency signals that come from on-device measurements. Latency-aware NAS was first popularized by NetAdapt <ref type="bibr" target="#b30">[31]</ref> and AMC <ref type="bibr" target="#b7">[8]</ref> to learn channel sizes for a pre-trained model. A look-up table (LUT) was used to efficiently estimate the end-to-end latency of a network based on the latency sum of its parts. This idea was then extended in MnasNet <ref type="bibr" target="#b23">[24]</ref> to search for generic architecture parameters using the NAS framework <ref type="bibr" target="#b32">[33]</ref>, where a RL controller learns to generate efficient architectures after observing the latency and accuracy of thousands of architectures. This framework was successfully adopted by MobileNetV3 <ref type="bibr" target="#b8">[9]</ref> to produce the current state-of-the-art architectures for mobile CPU.</p><p>The MnasNet-style search was not accessible to researchers with limited resources. Therefore a large body of the NAS literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2]</ref> focus on improving the search efficiency. These methods capitalize on the idea of hypernetwork and weight-sharing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18]</ref> to boost search efficiency. Despite the success in mobile classification, these efficient search techniques have not been extended to highly non-sequential search spaces in resource-constrained cases, hence have not seen many applications in mobile object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Architecture Search for Object Detection</head><p>Due to the above-mentioned non-sequential nature of search in object detection, NAS work on object detection has generally been limited.</p><p>NAS-FPN <ref type="bibr" target="#b6">[7]</ref> was the pioneering work that tackles detection head search. It proposes an overarching search space based on feature pyramid networks <ref type="bibr" target="#b13">[14]</ref>. The design covers many popular detection heads. Our work is primarily inspired by NAS-FPN, but with the goal of innovating a search space that is more mobile-friendly.</p><p>Another pioneering work was Auto-Deeplab <ref type="bibr" target="#b14">[15]</ref>, which extended NAS searches to semantic segmentation. Our work faces the similar challenge of learning the connectivity pattern across feature resolutions.</p><p>DetNAS <ref type="bibr" target="#b3">[4]</ref> focuses on improving the efficiency of searching for the detection body. It deals with the unmanageable computation caused by the need for ImageNet pretraining for every sampled architecture during search. Our work instead searches for the head only.</p><p>More recently, NAS-FCOS <ref type="bibr" target="#b25">[26]</ref> extends weight-sharing to the detection head in order to accelerate the search process for object detection. Similar to NAS-FPN, their search space for the detection head is based on full convolutions and not targeted for mobile. Our work is complementary to theirs, in that our latency-aware search based on a mobile-friendly search space could be accelerated with their weight-sharing search strategy.</p><p>On the mobile side, object detection architectures are rarely optimized as a primary target. Rather, they are composed of a light-weight backbone designed for classification and a predefined detection head. A partial list of work that follows this design strategy is <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>. Our work takes a first step towards directly optimizing object detection architectures for mobile deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MnasFPN</head><p>We overload the term MnasFPN to mean both our proposed search space and the family of architectures found via NAS, and leave disambiguation to context. Both NAS-FPN(Lite) and MnasFPN construct a detection network from a feature extractor backbone and a repeatable cell structure that recursively generates new features by merging pairs of existing features. Each cell consumes a collection of feature maps at various resolutions, and outputs another collection at the same set of resolutions, thus enabling the structure to be applied repeatedly. A cell is comprised of a collection of blocks. Each block merges two feature maps at potentially different resolutions into an intermediate feature, which is processed by a separable convolution and outputted by the block. MnasFPN differs from NAS-FPN(Lite) mainly at the block-level, which we describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generalized Inverted Residual Block (IRB)</head><p>Inverted Residual Blocks (IRBs) <ref type="bibr" target="#b21">[22]</ref> are well known block architectures that are widely used in NAS search spaces <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref>. The key insight of IRBs is to communicate features in low-dimensions in order to reduce memory impact, and expand the feature dimensions for depthwise convolutions in order to exploit their light-weight nature in mobile CPUs. It has shown superior performance gains over the conventional block design based on separable convolutions. This motivates us to explore the possibility of adopting IRB-like designs in the NAS-FPN search space, where the main challenge and innovation reside in improvising with the non-linear structure in NAS-FPN blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expandable intermediate features:</head><p>In NAS-FPN, all feature maps share the same, searchable channel size C by design. By comparison, MnasFPN gives additional flexibility to the intermediate feature size F , which is both searchable and independent from C. By adjusting F and C, the intermediate feature can serve as either an expansion or a bottleneck. Such network with unequal input and merged feature sizes is an instance of asymmetric FPNs, as defined in <ref type="bibr" target="#b11">[12]</ref>. A 1 × 1 convolution is applied as needed on each input feature to transform their channel count from C to F .</p><p>Learn-able block count: In NAS-FPN, the number of blocks in a cell is predetermined. This comes from the feature recycling mechanism where if a block is not consumed by the cell's outputs, its intermediate feature will be added to the output feature with the same resolution and size. In MnasFPN, however, the intermediate features often do not have the same channel size F as the output C. As a result, unused blocks are frequently discarded, giving additional flexibility in navigating the latency-accuracy trade-off.</p><p>Cell-wide residuals: As the connectivity gets thinner, we found that it's helpful to augment the flow of information by adding residuals between every pair of input and output features at the same resolution. Similar to IRB, we add ReLU non-linearity for the intermediate features, but not the output features. This is because the input/output feature channel size C is intended to be small to lessen the burden on memory. Adding lossy non-linearities may unnecessarily throttle the information flow.</p><p>Given the design above, one can traverse a connected path between an input feature and an output feature and see that it resembles an IRB, as shown in <ref type="figure">Fig. 1</ref>.</p><p>We have not experimented with the MobileNetV3-styled IRB with hard-swish in the search space because their implementations were not optimized at the time of the experiment design for this paper. They are worth re-visiting once efficient kernels for hard-swish become available. We have explored Squeeze-Excite (SE) <ref type="bibr" target="#b9">[10]</ref>, but much to our surprise, it was not chosen by our NAS controller for topperforming candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Size Dependent Ordering (SDO)</head><p>Another innovation in MnasFPN is the dynamic reordering of its reshaping and convolution operations based on the input/output resolutions. We refer to this as Size Dependent Ordering (SDO). More specifically, if the input feature needs to be down-sampled, then down-sampling will happen prior to the 1 × 1 convolution. On the contrary, if the input feature requires up-sampling, then 1 × 1 convolution will precede the up-sampling operation.</p><p>This design minimizes compute. For notation simplicity we assume the feature maps are square, and use R to represent both the height and width. When merging feature maps, we need to apply reshaping and 1 × 1 convolutions when the resolution R 0 and channel count C of the input feature do not match the resolution R and channel count F of the intermediate feature.</p><p>If R 0 &gt; R (needs down-sampling), let R 0 = kR where k ≥ 2, and assume down-sampling is performed with k × k convolution with a stride also equals to k, the cost (in MACs) of down-sample-then-1 × 1 is:</p><formula xml:id="formula_0">Cost 1 =R × R × k × k × C + R × R × C × F (1)</formula><p>whereas the cost of 1 × 1-then-down-sample is:</p><formula xml:id="formula_1">Cost 2 =kR × kR × C × F + R × R × k × k × F (2)</formula><p>Assume reasonably that F ≥ 2, we have k 2 C(F − 1) ≥ k 2 CF/2 ≥ CF , therefore:</p><formula xml:id="formula_2">Cost 2 − Cost 1 = R 2 k 2 C(F − 1) + R 2 F − CF &gt; 0<label>(3)</label></formula><p>hence proving that the down-sample-then-1×1 is more economical. The case for R 0 &lt; R (up-sampling) can be proved similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MnasFPN Search</head><p>The feature generation process of MnasFPN and all the searchable components are illustrated in <ref type="figure">Fig. 1</ref>. For each feature generation block, we search for which two input features to merge, the target resolution R and channel count F of the merged feature, the merging operation (addition or SE), and the kernel sizes for the depthwise convolution post merging. For the entire network, we mandate that the input, output and generated features all share the same channel count C, which is also searched.</p><p>We adopt the architecture search framework in Mnas-Net <ref type="bibr" target="#b23">[24]</ref> to incorporate latency measurements into the search objective. We train an RL controller to propose network architectures to maximize a reward function defined as follows. An architecture m is trained and evaluated on a proxy task. The proxy task is a scaled-down version of the real task, with details in Sec. 4.2. The proxy task performance, measured in mean average precision mAP (m), as well as the network latency on-device LAT (m) are combined into the following reward function:</p><formula xml:id="formula_3">Reward(m) = mAP (m) × LAT (m) w<label>(4)</label></formula><p>where w &lt; 0 controls the tradeoff point between latency and accuracy. In theory, w is the slope of the tangent line that cuts the performance trade-off curve at the desired latency. In practice, we observe that architectures around the desired latency will also be optimized, and the performance frontier of our search spaces have similar curvatures, suggesting that w needs to be set only once.</p><p>The controller repeatedly proposes candidate architecture m, and trains itself based on reward feedback Reward(m) using Proximal Policy Optimization <ref type="bibr" target="#b22">[23]</ref>. After every search experiment, all the architectures sampled by the controller trace a performance frontier, as shown in <ref type="figure">Fig. 6</ref>. We can then deploy promising architectures along the frontier to the real task.</p><p>Connectivity-based LUT: We apply detection-specific adaptations to the latency look-up table <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24]</ref> to estimate LAT (m). Existing LUT approaches do not work for MnasFPN because the number of blocks and the connectivity pattern of the head is dynamic. Instead, we compute layer connectivity for each model at run-time to determine the layers to be included in the look-up. The connectivitybased LUT gives high fidelity with on-device measurements (R 2 &gt; 0.97).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Connectivity Search</head><p>Our design of the MnasFPN search space is deliberately compact. This is in consideration of the fact that current architecture search algorithms are imperfect <ref type="bibr" target="#b12">[13]</ref>, and larger search spaces do not always lead to better models. Therefore, search space design is as much about what to include as it is about what not to include.</p><p>One design we do not include is the search for more general connectivity patterns. It overburdens the MNAS controller but remains valuable as search algorithms continue to improve. Recent work on randomly-wired networks <ref type="bibr" target="#b29">[30]</ref> suggests that search quality may be hampered by design biases in network connectivity in addition to search efficacy. We therefore challenge the connection rule in NAS-FPN where only two features are chosen to be merged each time. Instead, we design a new search space Conn-Search that allows merging between 2 to D ≥ 2 distinct feature maps with addition (D = 4 in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present experimental results to showcase the effectiveness of the proposed MnasFPN search space. We report results on COCO object detection. We also added ablation studies to isolate the effectiveness of every component of the search space design as well as latency-aware search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Search Experiments and Models</head><p>We include the following experiments / models. All search spaces allow 5 internal blocks per cell.</p><p>MnasFPN: Our proposed search space with searchable MnasFPN blocks described in <ref type="figure">Fig. 1</ref>.</p><p>NAS-FPNLite <ref type="bibr" target="#b6">[7]</ref>: NAS-FPN models that are posthoc modified to be light-weight, where modification refers to replacing full convolutions in the head with separableconvolutions. These are the only set of models that are not searched via latency-sensitive NAS (Sec. 3.3).  Conn-Search: We enlarges the MnasFPN search space by allowing between 2 to D ≥ 2 distinct inputs per block. Merge operation is limited to addition only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS-FPNLite</head><p>A detailed comparison of all the search spaces in the ablation studies are listed in <ref type="table" target="#tab_2">Table 2</ref>. Their performance frontiers are shown in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>To ensure comparability we train all detection models with the same configuration and hyper-parameters. Ablation study results are reported on the 5k COCO val2017 dataset, whereas the final comparison is reported on the COCO test-dev dataset.</p><p>Training setup: Training setup for COCO val2017: Each detection model is trained for 150 epochs, or 277k steps with a batch size of 64 on COCO train2017 dataset. Training is synchronized with 8 replicas. Learning rate follows a step-wise procedure: it increases linearly from 0 to 0.04 in the first epoch then holds its value; The learning rate drops sharply to 0.1 of its value at epoch 120 and 140, respectively. Gradient-norm clipping at 10 was used to stabilize training. In ablation studies, models that use Mo-bileNetV2 as the backbone are warm-started from an Im-ageNet pre-trained checkpoint.</p><p>Training setup for COCO test-dev: Each model is trained for 100k steps from scratch with a batch size of 1024 over 32 synchronized replicas with a cosine schedule for the learning rate <ref type="bibr" target="#b16">[17]</ref>, which is decayed from 4 to 0. The schedule also comes with a linear warmup phase at the first 2k steps. Following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11]</ref> to ensure comparability, we merged COCO train2017 and val2017 as training data.</p><p>All training and evaluation use 320 × 320 input images. We do not employ drop-block or auto-augmentation or hyper-parameter tuning to avoid favoring a particular class of models in our comparison studies, and for fair comparison with some previous results in the literature.</p><p>Timing setup: All timing was performed on a Pixel 1 device with single-thread and a batch size of one using Ten-sorflowLite's latency benchmarker 2 . Following the convention in MobileNetV2 <ref type="bibr" target="#b21">[22]</ref>, each detection model is converted into TensorflowLite flatbuffer format where the outputs are the box and class predictors immediately before non-maxsuppression.</p><p>Architecture Search Setup: We follow the same controller setup as used in MNASNet <ref type="bibr" target="#b23">[24]</ref>. The controller samples about 10K child models, each taking ∼ 1 hour of a TPUv2 device. To train a child model, we split COCO train2017 randomly into a 111k-search-train dataset and a 7k-search-val dataset. We train for 20 epochs with a batch size of 64 on search-train and evaluate its mAP on searchval. Learning rate increases linearly from 0 to 0.04 in the first epoch, the follows a step-wise procedure that decays to 0.1 of its value at epoch 16. We used the same 320×320 resolution for proxy task training to ensure that the estimated latency between the proxy task and the main task are identical. For the reward objective (Eq. 4), we use w = −0.3, estimated from a few trial runs, for all search experiments.</p><p>After training, for MnasFPN we compute the performance frontier over all the sampled models, and fetch the top models at 166 ms, 173 ms and 180 ms simulated latency. Then we increase the repeats from 3 and 5 to generate a total of 3 × 3 = 9 models. Among them we extract the performance frontier by only keeping models that are not dominated in both latency and mAP by any other model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discovered Architectures</head><p>We inspect a top-performing MnasFPN architecture in <ref type="figure">Fig. 2</ref> and a NAS-FPNLite-S architecture in <ref type="figure" target="#fig_1">Fig. 3</ref>. Both models have a similar latency as NAS-FPNLite. The comparison shows that:</p><p>First, MnasFPN is the most compact. Despite both given 5 internal blocks, MnasFPN only uses one block, whereas NAS-FPNLite-S uses 5, and places all of them at the same resolution. MnasFPN's compactness may be a product of 1) its ability to prune unused blocks and 2) the expansions in IRB that increases the capacity for each block.</p><p>Second, the Squeeze-and-excite (SE) option to merge  features is never used. This is an interesting discovery as SE was quite popular in the classification backbone. Third, both MnasFPN and NAS-FPNLite-S favor the 20×20 resolution for the intermediate features. This choice was also persistent among multiple search runs and multiple variations of search spaces. <ref type="figure" target="#fig_3">Fig. 4</ref> shows a Conn-search architecture with D = 4.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Latency Breakdowns</head><p>We divide a MnasFPN architecture into the feature extractor backbone, the detection head, and the "predictor", which is a set of full convolutions followed by class predictors and box decoders. These full convolutions are C × C in size, where C is the same parameter that describes the channel size of MnasFPN 's outputs. Therefore, our search affects both the head and the predictor part of the network.</p><p>To put the improvement on the MnasFPN detection head into perspective, we plot the latency breakdown of two 200ms models, namely MnasFPN with 5 repeats (25.5 mAP) and NAS-FPNLite with 6 repeats (24.4 mAP).</p><p>As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, our search affects around 80 ms or 40% of the total running time. MnasFPN (C = 48) learns to allocate nearly 2× more computational resources towards the head than the predictor, whereas NAS-FPNLite (C = 64) allocates less resources towards the head than to the predictor. This suggests that more significance should be associated with early feature fusion in the detection head than with predictor capacity.</p><p>The analysis above also indicates that as the detection head becomes more efficient with MnasFPN , the backbone, totaling around 60% of run time, now becomes the performance bottleneck. Since joint search of backbone and head is outside the scope of this paper, it is reasonable to assess all improvements in the paper relative to the latency budget excluding the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation on IRB</head><p>To evaluate our primary contribution of re-introducing IRB into the detection head, we compare in <ref type="figure" target="#fig_5">Fig. 7</ref> MnasFPN with NAS-FPNLite-S and No-Expand.</p><p>MnasFPN and NAS-FPNLite-S share the use of latencyaware search and differ in the search space. We see that a MnasFPN at 187 ms is more accurate than a NAS-FPNLite-S model at 201 ms, suggesting that the overall design of the MnasFPN search space contributes to almost all the improvements over NAS-FPNLite.</p><p>MnasFPN and No-Expand differ only in the use of expansions in the MnasFPN block. No-Expand's performance is significantly below that of MnasFPN. A closer inspection of the learned architectures shows that the model reduces the channel size C to 16 while increasing the number of intermediate nodes. This is a sub-optimal design strategy, on which the NAS controllers got stuck repeatedly. As a result, the entire performance frontier (during search) seems sub-optimal compared to those of other searches <ref type="figure">(Fig. 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation on Latency-aware Search</head><p>Our work is the first to introduce latency-aware training in architecture search for object detection. To investigate the gain of the latency signal, we compare MnasFPN with NAS-FPNLite and NAS-FPNLite-S.</p><p>According to <ref type="figure" target="#fig_5">Fig. 7</ref>, MnasFPN shows a superior latency-accuracy tradeoff than NAS-FPNLite. At 187 ms, MnasFPN achieves 24.9 mAP that is unmatched even by the NAS-FPNLite model at 205 ms. While the latency differential constitutes a mere 9% in terms of end-to-end latency, it amounts to around 22% improvement considering the latency portion excluding the backbone.</p><p>NAS-FPNLite-S also performs better than NAS-FPNLite, but only by a moderate amount. This indicates that the MNASNet-styled latency-aware search is an effective strategy overall, but the primary factor of MnasFPN's <ref type="bibr">Figure 6</ref>. Proxy task performance vs. simulated latency frontiers of various search spaces. This figure represents the NAS controller's view on the problem, where latency is simulated using LUT and quality is computed on the proxy task, which correlates with but is not directly comparable to mAPs of the real task. success is instead the search space design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Connectivity Search</head><p>To assess whether the MnasFPN search space was sufficiently large, we compare with Conn-Search (Sec. 3.4) where each block can take a maximum of D = 4 inputs.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, despite having a larger search space that subsumes MnasFPN, Conn-Search has a suboptimal latency-accuracy tradeoff. In <ref type="figure">Fig. 6</ref> we see that its performance frontier on the proxy task is slightly worse than that of MnasFPN, suggesting that the controller is unable to sufficiently explore the search space.  <ref type="table">Table 3</ref>. Ablation study of SDO. SDO does not affects parameters that much but reduces both MAdds and latency.</p><p>NAS-FPN <ref type="bibr">(10 22</ref> ). This result reiterates the significance of the co-adaptation of search spaces and search algorithms. While it is tempting to believe that NAS eliminates the need for manual tuning, and that one only needs to innovate a sufficiently powerful search space that subsumes all search spaces, the reality is that the search algorithm is not yet powerful enough to address arbitrarily large search spaces. Therefore, iterative shrinking and co-adaptation of search spaces, as practiced in the original NAS paper <ref type="bibr" target="#b32">[33]</ref>, are still relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Ablation on SDO</head><p>To understand the impact of SDO, we disable SDO of the MnasFPN architectures with 4 and 5 repeats, respectively. Models with no SDO will perform 1 × 1 convolution before resizing, which will be less economical for down-sampling operations, and the discovered MnasFPN architecture as it is dominated by down-sampling operations <ref type="figure">(Fig. 2)</ref>.</p><p>Unsurprisingly, we see from <ref type="table">Table 3</ref> that disabling SDO does not affect the mAP, but would lead to a 8 to 11ms (4 − 6%) latency regression. Similarly if we consider the portion of the network without the backbone, this amounts to 12% to 14% of the latency that is "optimizable". Given this strict dominance we conclude that the effectiveness of SDO is sufficiently evident and do not conduct search experiments without SDO for the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Performance Comparison on COCO Test-dev</head><p>We compare MnasFPN under different backbones and with other state-of-the-art on-device detection heads. <ref type="bibr" target="#b2">3</ref> As shown in <ref type="table">Table 1</ref>, with the same MobileNetV2 backbone, MnasFPN achieves 1.0 mAP improvement over NAS-FPNLite. Furthermore, MnasFPN is 10% faster in end-to-end latency, or 25% faster in terms of latency incurred outside the backbone.</p><p>Since SSDLite is generally much faster than MnasFPN , we compare the two either by applying width-multipler or changing the backbone. With a 0.7 width-multiplier on both head and backbone, MnasFPN with MobileNetV2 achieves 1.8 higher mAP compared with SSDLite with MobileNetV3 at around 120 ms. Here the MobileNetV3 results use the channel-halving trick, which tends to reduce latency with no mAP degradation, while our results do not.</p><p>Removing this trick for both shows a further 20 ms latency advantage for MnasFPN.</p><p>When paired with MobileNetV3 backbone, MnasFPN is 3.4 mAP higher than SSDLite with MobileNetV2 at around 165 ms. It is both faster and 2.5 mAP higher than SSDLite with MnasNet-A1 backbone.</p><p>Therefore, we conclude that MnasFPN compares favorably to both SSDLite and NAS-FPNLite head in its ability to trade off latency with accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we show the benefits of treating object detection as a first-class citizen in NAS. Unlike previous work that transfers learned backbone from classification, our work directly searches for object detection architectures. Additionally, we design the search process and, more importantly, the search space to incorporate knowledge about the targeted platform. Our proposed MnasFPN search space has two innovations. First, MnasFPN incorporates inverted residual blocks into the detection head, which is proven to be favored on mobile CPUs. Second, MnasFPN restructured the reshaping and convolution operations in the head to facilitate efficient merging of information across scales.</p><p>Through detailed ablation studies, we've discovered that both innovations in the search space are necessary for the performance boost. On the other hand, further expanding the search space in feature map connectivity seems to overwhelm the NAS framework. As a result, we conclude that the proposed MnasFPN search space may be close to the capacity of this controller. As the controller becomes more powerful, the MnasFPN with connectivity search could become viable again.</p><p>On COCO test-dev MnasFPN leads to a 25% improvement in non-backbone latency over NAS-FPNLite. The improvements are so substantial that the rest of the network becomes the bottleneck for performance improvements. For example, the backbone, which currently occupies over 60% of the total latency, could be searched either conditioning on or jointly with the MnasFPN head. This seems promising with our anecdotal evidence in <ref type="table">Table 1</ref> that MnasFPN pairs well with MobileNetV3 and depth-multiplied MobileNetV2 backbones. While the cardinality of a joint-search of backbone and the head is challenging for our current controller, recent one-shot NAS methods are opening avenues for more ambitious search spaces, of which MnasFPN could be an ideal component.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-S: Modified NAS-FPN search space where full convolutions are replaced with separableconvolutions. A key distinction from NAS-FPNLite is that the modification is done on the search space, instead of post-hoc on the model. No-Exand: We remove and only remove expansion from the MnasFPN search space by enforcing F = C for all intermediate features. This serves as an ablation of the expansion in IRB. It differs from NAS-FPNLite-S in that it still retains all other MnasFPN designs such as SDO and cell-wide residual, as well as search-able options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of a NAS-FPNLite-S cell architecture found via latency-aware search on the NAS-FPNLite search space. Each rectangle describes the resolution R and merge operation (sum or SE) for the feature generation process. The channel sizes and kernel sizes are fixed to 64 and 3, respectively, according to NAS-FPNLite<ref type="bibr" target="#b6">[7]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of a Conn-Search cell architecture with maximum in-degree D = 4. Each rectangle describes the expansion size F , resolution R, and kernel size k for the feature generation process. The merge operation is fixed to be summation. Blue arrows indicate the additional connections compared to MnasFPN in Fig. 2 where all intermediate blocks are treated as one agglomerate block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Latency breakdown of MnasFPN (left) and NAS-FPNLite (right). Both models have around 200ms latency, out of which 40% is reserved for the detection head as well as the box and class predictors, which we optimize in this paper. The MnasFPN model is 1.1 mAP higher than the NAS-FPNLite model.First, similar to MnasFPN, the resolutions of the intermediate features all concentrate around 20 × 20. Second, almost in all cases only 2 or 3 features are merged. Therefore, either allowing 4 input connections was already excessive, or the current search space is at the limit of what the search algorithm can handle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Performance comparisons between MnasFPN and various ablation designs. Latency is measured on Pixel 1 and mAP is computed on COCO val2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Search space comparisons. The common search parameters (e.g. merge operations, feature resolutions etc.) are omitted.</figDesc><table><row><cell>Search spaces</cell><cell>Kernel sizes</cell><cell>Filter sizes C</cell><cell>Expansion sizes F</cell><cell cols="3">SDO Maximum in-degree Cardinality</cell></row><row><cell>NAS-FPNLite-S</cell><cell>3</cell><cell>64</cell><cell>-</cell><cell>N</cell><cell>2</cell><cell>2 × 10 22</cell></row><row><cell>No-Expand</cell><cell>{3, 5, 7}</cell><cell>{16, 32, 48, 64, 80, 96}</cell><cell>-</cell><cell>Y</cell><cell>2</cell><cell>2.4 × 10 27</cell></row><row><cell>MnasFPN</cell><cell>{3, 5, 7}</cell><cell cols="2">{16, 32, 48, 64, 80, 96} {16, 32, 64, 96, 128, 256, 512}</cell><cell>Y</cell><cell>2</cell><cell>10 31</cell></row><row><cell>Conn-Search</cell><cell>{3, 5, 7}</cell><cell cols="2">{16, 32, 48, 64, 80, 96} {16, 32, 64, 96, 128, 256, 512}</cell><cell>Y</cell><cell>4</cell><cell>3 × 10 42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Both the inputs and outputs, represented as boxes with rounded edges, consist of four feature maps at C3 to C6, respectively. Each rectangle box represents a MnasFPN block whose internal structure is outlined inFig. 1. The box also contains architectural parameters such as channel size F and resolution R for the intermediate feature, the merging operation Op, and the kernel size k of the depthwise convolution. Finally, all outputs receive cell-wide residuals (dashed arrows) from the input with the corresponding resolution. Note that although the search allows for a maximum of 5 intermediate blocks, only one was chosen.</figDesc><table><row><cell>Output blocks</cell><cell></cell><cell>F=128, R=10, Op=+, k=5</cell><cell></cell></row><row><cell></cell><cell>F=128, R=20,</cell><cell></cell><cell>F=96, R=5,</cell></row><row><cell></cell><cell>Op= +, k=3</cell><cell></cell><cell>Op=+, k=3</cell></row><row><cell>F=128, R=40,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Op= +, k=3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>F=256, R=20,</cell><cell>Intermediate</cell><cell></cell></row><row><cell></cell><cell>Op=+, k=3</cell><cell>block</cell><cell></cell></row><row><cell>40 x 40 x 48</cell><cell>20 x 20 x 48</cell><cell>10 x 10 x 48</cell><cell>5 x 5 x 48</cell></row><row><cell cols="4">Figure 2. Visualization of a MnasFPN cell architecture found via</cell></row><row><cell cols="2">R=20, Op = + Output R=20, Op= + latency-aware search. R=20, R=40, Op= + R=20, Op= + blocks</cell><cell cols="2">R=20, Op= SE R =20, Op= + R=10, Op = SE Intermediate R=5, Op= + Blocks</cell></row><row><cell></cell><cell>Op = +</cell><cell></cell><cell></cell></row><row><cell>40 x 40 x 64</cell><cell>20 x 20 x 64</cell><cell>10 x 10 x 64</cell><cell>5 x 5 x 64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>shows that the cardinality of Conn-Search is roughly 10 42 , greatly surpasses the cardinalities of the two known successful applications of the MnasNet framework: MnasNet (10 13 ) and</figDesc><table><row><cell>Model</cell><cell cols="5">Repeats mAP Latency (ms) MAdds (B) Params (M)</cell></row><row><cell>MnasFPN</cell><cell>4</cell><cell>24.9</cell><cell>187</cell><cell>0.90</cell><cell>2.5</cell></row><row><cell></cell><cell>5</cell><cell>25.5</cell><cell>196</cell><cell>0.94</cell><cell>2.6</cell></row><row><cell>No SDO</cell><cell>4</cell><cell>24.9</cell><cell>195</cell><cell>0.94</cell><cell>2.5</cell></row><row><cell></cell><cell>5</cell><cell>25.5</cell><cell>207</cell><cell>1.0</cell><cell>2.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Implementation is available at Tensorflow Object Detection API https://github.com/tensorflow/models/tree/master/ research/object_detection</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.tensorflow.org/lite/performance/ benchmarks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The COCO test-dev mAPs are produced from a more time-consuming setup (Sec. 4.2) than the COCO val2017 mAPs for internal comparisons, hence the performances differ slightly as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">i=0(i + 2)(i + 1)/2 9 ≈ 3 × 10 42</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Appendix A.1. Search space cardinality comparison NAS-FPNLite-S: There are 9 nodes in total, where the i-th node has two choices for the combine operation, and choose(i + 4, 2) choices for picking a pair of inputs. The first 5 are internal nodes, and each have 4 resolutions choices. The last 4 are output nodes, whose orders are permuted with permute(4) possibilities. This gives a total search space size of:</p><p>No-Expand: In addition to the NAS-FPNLite-S search space, No-Expand additionally grants 3 kernel sizes for each node. It also have 6 choices for the globally-shared channel size C, giving a total search space size of:</p><p>MnasFPN : In addition to the No-Expand search space, MnasFPN additionally searches for channel sizes for the merged features for all 9 nodes, each with 7 choices. The total search space size is: 2 × 10 27 × 7 9 ≈ 10 31</p><p>Conn-Search: Finally, connectivity search allows for choose(i + 4, 4) choices for each node, which is (i + 2)(i + 1)× more possibilities than that in MnasFPN. It does not search for combine operations, so each node has 2× fewer choices. Therefore the total search space size is:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10979</idno>
		<title level="m">Neural architecture search on object detection</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards efficient network design through platform-aware model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11398" to="11407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7310" to="7311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thundernet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11752</idno>
		<title level="m">Towards real-time generic object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Nas-fcos: Fast neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04423</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pelee: A real-time object detection system on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1963" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Squeezedet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01569</idno>
		<title level="m">Exploring randomly wired neural networks for image recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Netadapt: Platform-aware neural network adaptation for mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
