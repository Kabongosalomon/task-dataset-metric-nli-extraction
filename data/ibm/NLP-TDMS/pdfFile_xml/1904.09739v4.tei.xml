<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Switchable Whitening for Deep Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Switchable Whitening for Deep Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see <ref type="figure">Fig.1</ref>), making it well suited for a wide range of tasks without manual design. Second, by integrating the benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques.</p><p>We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-ofthe-art performance with 45.33% mIoU on the ADE20K dataset. Code is available at https://github.com/ XingangPan/Switchable-Whitening.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Normalization methods have been widely used as a basic module in convolutional neural networks (CNNs). In various applications, different normalization techniques like Batch Normalization (BN) <ref type="bibr" target="#b12">[13]</ref>, Instance Normalization (IN) <ref type="bibr" target="#b30">[31]</ref> and Layer Normalization (LN) <ref type="bibr" target="#b0">[1]</ref> are proposed. These normalization techniques generally perform standardization that centers and scales features. Nevertheless, the features are not decorrelated, hence their correlation still exists.  <ref type="figure">Figure 1</ref>. (a) SW outperforms its counterparts in a variety of benchmarks. (b) SW learns to select appropriate whitening or standardization methods in different tasks and datasets. The CNNs are ResNet50 for ImageNet and ADE20K, ResNet44 for CIFAR-10, and VGG16 for GTA5→Cityscapes. GTA5→Cityscapes indicates adapting from GTA5 to Cityscapes using domain adaptation.</p><p>Another type of normalization methods is whitening, which not only standardizes but also decorrelates features. For example, Decorrelated Batch Normalization (DBN) <ref type="bibr" target="#b16">[17]</ref>, or namely Batch Whitening (BW), whitens a mini-batch using its covariance matrix, which gives rise to better optimization efficiency than BN in image classification. Moreover, whitening features of an individual image is used in image style transfer <ref type="bibr" target="#b18">[19]</ref> to filter out information of image appearance. Here we refer to this operation as instance whitening (IW). Despite their successes, existing works applied these whitening techniques separately to different tasks, preventing them from benefiting each other. Besides, whitening and standardization methods are typically employed in different layers of a CNN, which complicates model design.</p><p>To address the above issues, we propose Switchable Whitening (SW). SW provides a general form that integrates different whitening techniques (e.g. BW, IW), as well as standardization techniques (e.g. BN, IN and LN). SW controls the ratio of each technique by learning their importance weights. It is able to select appropriate normalizers with respect to various vision tasks, as shown in <ref type="figure">Fig.1(b)</ref>. For example, semantic segmentation prefers BW and BN, while IW is mainly chosen to address image diversity in image classification. Compared to semantic segmentation, domain adaptation selects more IW and IN, which alleviates domain discrepancy in CNN features. In image style transfer, IW dominates to handle image style variance.</p><p>SW can be inserted into advanced CNN architectures and effectively boosts their performances. Owing to the rich statistics and selectivity of SW, models trained with SW consistently outperform other counterparts in a number of popular benchmarks, such as CIFAR-10/100 <ref type="bibr" target="#b15">[16]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref> for image classification, ADE20K <ref type="bibr" target="#b38">[39]</ref> and Cityscapes <ref type="bibr" target="#b3">[4]</ref> for semantic segmentation, domain adaptation between GTA5 <ref type="bibr" target="#b26">[27]</ref> and Cityscapes, and image style transfer on COCO <ref type="bibr" target="#b21">[22]</ref>. For example, when using ResNet50 <ref type="bibr" target="#b8">[9]</ref> for ImageNet, ADE20K, and Cityscapes, as well as using VGG16 <ref type="bibr" target="#b28">[29]</ref> for domain adaptation, SW significantly outperforms the BN-based baselines by 1.51%, 3.2%, 4.1%, and 3.0% respectively.</p><p>SW serves as a useful tool for analyzing the characteristics of these whitening or standardization techniques. This work answers two questions: (1) Is IW beneficial for highlevel vision tasks like classification and domain adaptation?</p><p>(2) Is standardization still necessary when whitening is presented? Our experiments suggest that (1) IW works extremely well for handling image appearance diversity and reducing domain gap, giving rise to better performance in high-level vision tasks; (2) Using BW+IW in SW performs comparably well compared to using all the normalizers mentioned above in SW, indicating that full whitening generally works well, and the requirement for standardization is marginal when whitening is presented.</p><p>Overall, our contributions are summarized as follows. (1) We propose Switchable Whitening (SW), which unifies existing whitening and standardization methods in a general form and learns to switch among them during training. (2) SW adapts to various tasks and is used as a new building block in advanced CNNs. We show that SW outperforms its counterparts in multiple challenging benchmarks. (3) SW could be used as a tool to analyze the effects and characteristics of different normalization methods, and the interactions between whitening and standardization. We will make the code of SW available and hope it would deepen our understanding on various normalization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Normalization. Existing normalization techniques generally performs standardization. For example, Batch Normalization (BN) <ref type="bibr" target="#b12">[13]</ref> centers and scales activations using the mean and variance estimated over a mini-batch, accelerating training and enhancing generalization. In contrast, Instance Normalization (IN) <ref type="bibr" target="#b30">[31]</ref> and Layer Normalization (LN) <ref type="bibr" target="#b0">[1]</ref> standardize activations with statistics computed over each individual channel and all channels of a layer respectively. IN is mainly used in image generation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref> while LN has been proved beneficial for training recurrent neural networks <ref type="bibr" target="#b0">[1]</ref>. The above three normalizers are combined in Switchable Normalization (SN) <ref type="bibr" target="#b23">[24]</ref> that learns the ratio of each one. The combination of BN and IN is also explored in IBN-Net <ref type="bibr" target="#b25">[26]</ref> and Batch-Instance Normalization <ref type="bibr" target="#b24">[25]</ref>. Besides, there have been other attempts to improve BN for small batch sizes such as Group Normalization <ref type="bibr" target="#b32">[33]</ref>, Batch Renormalization <ref type="bibr" target="#b11">[12]</ref>, and Batch Kalman Normalization <ref type="bibr" target="#b31">[32]</ref>. All these normalization methods perform centering and scaling to the activations, whereas the correlation between activations remains, leading to suboptimal optimization efficiency. Our work provides a general form that integrates both whitening and standardization techniques, having SN as a special case. Whitening. Another paradigm towards improving optimization is whitening. Desjardins et al. <ref type="bibr" target="#b5">[6]</ref> proposes Natural Neural Network, which implicitly whitens the activations to improve conditioning of the Fisher Information Matrix. This improves optimization efficiency of deep neural networks. Decorrelated Batch Normalization (DBN) <ref type="bibr" target="#b16">[17]</ref> whitens features using covariance matrix computed over a mini-batch. It extends BN by decorrelating features. In this paper, we refer to DBN as Batch Whitening (BW) for consistency. Moreover, in the field of image style transfer, whitening and coloring operations are used to manipulate the image appearance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>. This is because the appearance of an individual image is well encoded in the covariance matrix of its features. We call whitening of an individual image as instance whitening (IW). In this work, we make the first attempt to apply IW in high-level vision tasks like image classification and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Switchable Whitening (SW)</head><p>We first present a general form of whitening as well as standardization operations, and then introduce SW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A General Form</head><p>Our discussion is mainly based on CNNs, where the data have four dimensions. Let X ∈ R C×N HW be the data matrix of a mini-batch, where N, C, H, W indicate the number of samples, number of channels, height, and width respectively. Here N , H and W are viewed as a single dimension for convenience. Let matrix X n ∈ R C×HW be the nth sample in the mini-batch, where n ∈ {1, 2, ..., N }. Then the whitening transformation φ : R C×HW → R C×HW for a sample X n could be formulated as</p><formula xml:id="formula_0">φ(X n ) = Σ −1/2 (X n − µ · 1 T )<label>(1)</label></formula><p>where µ and Σ are the mean vector and the covariance matrix calculated from the data, and 1 is a column vector of all ones. Note that different whitening methods could be achieved by calculating µ and Σ using different sets of pixels. We discuss them in detail as below.</p><p>Batch Whitening (BW). In BW <ref type="bibr" target="#b16">[17]</ref>, the statistics are calculated in a mini-batch. Thus</p><formula xml:id="formula_1">µ bw = 1 N HW X · 1 Σ bw = 1 N HW (X − µ · 1 T )(X − µ · 1 T ) T + I (2)</formula><p>where &gt; 0 is a small positive number to prevent a singular Σ bw . In this way, the whitening transformation φ whitens the data of the entire mini-batch, i.e., φ(X)φ(X) T = I. Instance Whitening (IW). In contrast, for IW <ref type="bibr" target="#b18">[19]</ref>, µ and Σ are calculated within each individual sample,</p><formula xml:id="formula_2">µ iw = 1 HW X n · 1 Σ iw = 1 HW (X n − µ · 1 T )(X n − µ · 1 T ) T + I (3)</formula><p>for n in {1, 2, ..., N}. IW whitens each samples separately, i.e., φ(X n )φ(X n ) T = I.</p><p>Note that Eq.(1) also naturally incorporates standardization operations as its special cases. In the covariance matrix Σ, the diagonal elements are the variance for each channel, while the off-diagonal elements are the correlation between channels. Therefore, by simply setting the off-diagonal elements to zeros, the left multiplication of Σ −1/2 equals to dividing the standard variance, so that Eq.(1) becomes standardization.</p><p>Batch Normalization (BN). BN <ref type="bibr" target="#b12">[13]</ref> centers and scales data using the mean and standard deviation of a mini-batch. Hence its mean is the same as in BW i.e., µ bn = µ bw . As discussed above, since BN does not decorrelate data, the covariance matrix becomes Σ bn = diag(Σ bw ), which is a diagonal matrix that only preserves the diagonal of Σ bw .</p><p>Instance Normalization (IN). Similarly, in IN <ref type="bibr" target="#b30">[31]</ref> we have µ in = µ iw and Σ in = diag(Σ iw ).</p><p>Layer Normalization (LN). LN <ref type="bibr" target="#b0">[1]</ref> uses the mean and variance of all channels in a sample to normalize. Let µ ln and σ ln denote the mean and the variance, then µ ln = µ ln 1 and Σ ln = σ ln I. In practice µ ln and σ ln could be calculated efficiently from µ in and Σ in using the results in <ref type="bibr" target="#b23">[24]</ref>.</p><p>In Eq.(1), the inverse square root of the covariance matrix is typically calculated by using ZCA whitening,</p><formula xml:id="formula_3">Σ −1/2 = DΛ −1/2 D T<label>(4)</label></formula><p>where Λ = diag(σ 1 , ..., σ c ) and D = [d 1 , ..., d c ] are the eigenvalues and the eigenvectors of Σ, i.e., Σ = DΛD T , which is obtained via eigen decomposition. So far we have formulated different whitening and normalization transforms in a general form. In the next section, we introduce switchable whitening based on this formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Formulation of SW</head><p>For a data sample X n , a natural way to unify the aforementioned whitening and standardization transforms is to combine the mean and covariance statistics of those methods, and perform whitening using this unified statistics, giving rise to</p><formula xml:id="formula_4">SW (X n ) =Σ −1/2 (X n −μ · 1 T ) (5) whereμ = k∈Ω ω k µ k ,Σ = k∈Ω ω k Σ k<label>(6)</label></formula><p>Here Ω is a set of statistics estimated in different ways. In this work, we mainly focus on two cases, i.e., Ω = {bw, iw} and Ω = {bw, iw, bn, in, ln}, where the former switches between two whitening methods, while the later incorporates both whitening and standardization methods. ω k are importance ratios to switch among different statistics. In practice, ω k are generated by the corresponding control parameters λ k via softmax function, i.e., ω k = e λ k z∈Ω e λz . And ω k are defined similarly using another group of control parameters λ k . This relieves the constraint of consistency between mean and covariance, which is a more general form.</p><p>Note that the above formulation incorporates SN <ref type="bibr" target="#b23">[24]</ref> as its special case by letting Ω = {bn, in, ln}. Our formulation is more flexible and general in that it takes into account the whole covariance matrix rather than only the diagonal. This provides the possibility of producing decorrelated features, giving rise to either better optimization conditioning or style invariance. SW could be easily extended to incorporate some other normalization methods like Batch Renormalization <ref type="bibr" target="#b11">[12]</ref> or Group Normalization <ref type="bibr" target="#b32">[33]</ref>, which is out of the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>Switchable Whitening could be inserted extensively into a convolutional neural network (CNN). Let Θ be a set of parameters of a CNN, and Φ be a set of importance weights in SW. The importance weights are initialized uniformly, e.g. λ k = 1. During training, Θ and Φ are optimized jointly by minimizing a loss function L(Θ, Φ) using backpropagation. The forward calculation of our proposed SW is presented in Algorithm 1 while the backward pass is presented in Appendix. For clearance, we use Ω = {bw, iw} as an illustrative example.</p><p>In the training phase, µ bw and Σ bw are calculated within each mini-batch and used to update the running mean and running covariance as in Line 7 and 8 of Algorithm 1. During inference, the running mean and the running covariance are used as µ bw and Σ bw , while µ iw and Σ iw are calculated independently for each sample.</p><p>Algorithm 1 Forward pass of SW for each iteration. 1: Input: mini-batch inputs X ∈ R C×N HW , where the nth sample in the batch is Xn ∈ R C×HW , n ∈ {1, 2, ..., N }; importance weights λk and λ k , k ∈ {bw, iw}; expected mean µE and expected covariance ΣE. 2: Hyperparameters: , running average momentum α. 3: Output: the whitened activations {Xn, n = 1, 2, ..., N }. 4: calculate: ωbw, ωiw = Softmax(λbw, λiw), ω bw , ω iw = Softmax(λ bw , λ iw ). 5: calculate: µbw = 1 N HW X · 1. <ref type="bibr">13:</ref> execute eigenvalue decomposition:Σn = DΛD T . <ref type="bibr">14:</ref> calculate ZCA-whitening matrix: Un = DΛ −1/2 D T . <ref type="bibr">15:</ref> calculate ZCA-whitened output:Xn = Un(Xn −μn · 1 T ).</p><formula xml:id="formula_5">6: calculate: Σbw = 1 N HW (X − µ · 1 T )(X − µ · 1 T ) T + I. 7: update: µE ← (1 − α)µE + αµbw. 8: update: ΣE ← (1 − α)ΣE + αΣbw. 9: for n = 1 to N do 10: calculate: µ (n) iw = 1 HW Xn · 1. 11: calculate: Σ (n) iw = 1 HW (Xn − µ · 1 T )(Xn − µ · 1 T ) T + I. 12: calculate:μn = k ωkµ (n) k ,Σn = k ω k Σ (n) k , k ∈ {bw, iw}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16: end for</head><p>In practice, the scale and shift operations are usually added right after the normalization or whitening transform to enhance the model's representation capacity. For SW, we follow this design to introduce scale and shift parameters γ and β as in BN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Accelerates SW via Newton's Iteration</head><p>In practice, the GPU implementation of singular value decomposition (SVD) in current deep learning frameworks are inefficient, leading to much slower training and inference. To address this issue, we could resort to an alternative way to calculateΣ −1/2 , which is to use Newton's iteration, as in IterNorm <ref type="bibr" target="#b9">[10]</ref>. Following <ref type="bibr" target="#b9">[10]</ref>, we normalizeΣ viâ Σ N =Σ/tr(Σ). Then calculateΣ −1/2 N via the following iterations:</p><formula xml:id="formula_6">P 0 = I P k = 1 2 (3P k−1 − P 3 k−1Σ N ), k = 1, 2, ..., T<label>(7)</label></formula><p>where T is the iteration number, and P k will converge tô</p><formula xml:id="formula_7">Σ −1/2 N . Finally, we haveΣ −1/2 =Σ −1/2 N / tr(Σ).</formula><p>In this work, we set T = 5, which produces similar performance with the SVD version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Analysis and Discussion</head><p>We have introduced the formulation and training of SW.</p><p>Here we discuss some of its important properties and analyze its complexity. Instance Whitening for Appearance Invariance. In style transfer, researchers have found that image appearance information (i.e. color, contrast, style etc.) is well encoded in the covariance matrix of features produced by CNNs <ref type="bibr" target="#b18">[19]</ref>. In this work, we take the first attempt to induce appearance invariance by leveraging IW, which is beneficial for domain adaptation or high-level vision tasks like classification or semantic segmentation. Although IN also introduces invariance by standardizing each sample separately, the difference in correlation could be easily enlarged in highly non-linear deep neural networks. In IW, features of different samples are not only standardized but also whitened individually, giving rise to the same covariance matrix, i.e., identity matrix. Therefore, IW has better invariance property than IN. Switching between Whitening and Standardization. Our formulation of SW makes it possible to switch between whitening and standardization. For example, considering Ω = {bw, bn}, i.e.,Σ = ω bw Σ bw + ω bn Σ bn , (ω bw + ω bn = 1). As ω bn grows larger, the diagonal ofΣ would remain the same, while the off-diagonal would be weaken. This would make the features less decorrelated after whitening. This is beneficial when the extent of whitening requires careful adjustment, which is an important issue of BW as pointed out in <ref type="bibr" target="#b16">[17]</ref>. Group SW. Huang et al. <ref type="bibr" target="#b16">[17]</ref> uses group whitening to reduce complexity and to address the inaccurate estimation of large covariance matrices. In SW we follow the same design, i.e., the features are divided into groups along the channel dimension and SW is performed for each group. The importance weights λ k could be shared or independent for each group. In this work we let groups of a layer share the same λ k to simplify discussion.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate SW on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For each task, SW is compared with previous normalization methods. We also provide results of instance segmentation in the Appendix.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification</head><p>CIFAR-10, CIFAR-100 <ref type="bibr" target="#b15">[16]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref> are standard image classification benchmarks. Our training policies and settings are the same as in <ref type="bibr" target="#b8">[9]</ref>. Implementation. We evaluate different normalization methods based on standard ResNet <ref type="bibr" target="#b8">[9]</ref>. Note that introducing whitening after all convolution layers of ResNet is redundant and would incur a high computational cost, as also pointed out in <ref type="bibr" target="#b16">[17]</ref>. Hence we replace part of the BN layers in ResNet to the desired normalization layers. For CIFAR, we apply SW or other counterparts after the 1st and the {4n}th (n = 1,2,3,...) convolution layers. And for Ima-geNet, the normalization layers considered are those at the 1st and the {6n}th (n = 1,2,3,...) layers. The residual blocks with 2048 channels are not considered to save computation. More discussions for such choices could be found in supplementary material.</p><p>The normalization layers studied here are BN, SN, BW, and SW. For SW, we consider two cases: Ω = {bw, iw} and Ω = {bw, iw, bn, in, ln}, which are denoted as SW a and SW b respectively. In all experiments, we adopt group whitening with group size G = 16 for SW and BW. Since <ref type="bibr" target="#b22">[23]</ref> shows that applying early stop to the training of SN reduces overfitting, we stop the training of SN and SW at the 80th epoch for CIFAR and the 30th epoch for ImageNet. Results. The results are given in <ref type="table">Table.</ref> 2 and the training curves are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. In both datasets, SW a and SW b show better results and faster convergence than BN, SN, and BW over various network depth. Specifically, with only 7 SW b layers, the top1 and top5 error of ResNet50 on Ima-geNet is significantly reduced by 1.51% and 1.09%. This performance is comparable with the original ResNet152 which has 5.94% top5 error.</p><p>Our results reveal that combining different normalization methods in a suitable manner surpasses every single normalizer. For example, the superiority of SW b over SN attributes to the better optimization conditioning brought out by whitening. And the better performance of SW a over BW shows that instance whitening is beneficial as it introduces style invariance. Moreover, SW a and SW b perform comparably well, which indicates that full whitening generally performs well, and the need for standardization is marginal while whitening is presented. Discussions. SW has two groups of importance weights λ k and λ k . We observe that allowing λ k and λ k to share weight produces slightly worse results. For example, ResNet20 has 8.17% test error when using SW with shared importance weights. We conjecture that mean and covariance have different impacts in training, and recommend to maintain independent importance weights for mean and covariance.</p><p>Note that IW is not reported here because it generally produces worse results due to diminished feature discrimination. For example, ResNet20 with IW gives 12.57% test error on CIFAR-10, which is worse than other normalization methods. This also implies that SW borrows the benefits of different normalizers so that it could outperform any individual of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Segmentation</head><p>We further verify the scalability of our method on ADE20K <ref type="bibr" target="#b38">[39]</ref> and Cityscapes <ref type="bibr" target="#b3">[4]</ref>, which are standard and challenging semantic segmentation benchmarks. We evaluate SW based on ResNet and PSPNet <ref type="bibr" target="#b36">[37]</ref>. Implementation. We adopt the same ResNet architecture, training setting, and data augmentation scheme as in <ref type="bibr" target="#b36">[37]</ref>. The normalization layers considered are the 1st and the {3n}th (n = 1,2,3,...) layers except those with 2048 channels, resulting in 14 normalization layers for ResNet50. Since overfitting is not observed in these two benchmarks, early stop is not used here. The BN and BW involved are synchronized across multiple GPUs. Results. <ref type="table">Table.</ref>3 reports mIoU on the validation sets of the two benchmarks. For ResNet50, simply replacing part of BN with SW would significantly boost mIoU ss by 3.2% and 4.1% for ADE20K and Cityscapes respectively. SW also notably outperforms SN and BW, which is consistent with the results of classification. Furthermore, we show that SW could improve even the most advanced models for semantic segmentation. We apply SW to PSPNet101 <ref type="bibr" target="#b36">[37]</ref>, and compare with other methods on the ADE20K dataset. The results are shown in Table.4. Simply using some SW layers could improve the strong PSPNet by 1.74% on mIoU. And our final score, 45.33%, outperforms other more advanced semantic segmentation methods like PSANet <ref type="bibr" target="#b37">[38]</ref> and EncNet <ref type="bibr" target="#b35">[36]</ref>. Computational cost. While the above implementation of SW is based on SVD, they can be accelerated via Newton's iteration, as discussed in section 3.4. As shown in Table.5, the GPU running time is significantly reduced when using iterative whitening, while the performance is comparable to the SVD version. Note that in this <ref type="table">Table,</ref> the ResNet-50-SW a in Cityscapes has the same configuration as in ImageNet, i.e., has 7 SW layers. Compared with the 14 layer version, this further saves computation cost, while  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Domain Adaptation</head><p>The adaptive style invariance of SW making it suitable for handling appearance discrepancy between two image domains. To verify this, we evaluate SW on domain adaptation task. The datasets employed are the widely used GTA5 <ref type="bibr" target="#b26">[27]</ref> and Cityscapes <ref type="bibr" target="#b3">[4]</ref> datasets. GTA5 is a street view dataset generated semi-automatically from the computer game Grand Theft Auto V (GTA5), while Cityscapes contains traffic scene images collected from the real world. Implementation. We conduct our experiments based on the AdaptSegNet <ref type="bibr" target="#b29">[30]</ref> framework, which is a recent stateof-the-art domain adaptation approach. It adopts adversarial learning to shorten the discrepancy between two domains with a discriminator. The segmentation network is DeepLab-v2 [3] model with VGG16 <ref type="bibr" target="#b28">[29]</ref> backbone. The training setting is the same as in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Note that the VGG16 model has five convolutional groups, where the number of convolution layers for these groups are {2,2,3,3,3}. We add SW or its counterparts after the first convolution layer of each group, and report the results using different normalization layers. Results. Table.6 reports the results of adapting GTA5 to Cityscapes. The models with SW achieve higher performance when evaluated on a different image domain. Particularly, compared with BN, and SN, SW a improves the mIoU by 3.0%, and 1.6% respectively.</p><p>To understand how SW performs better under crossdomain evaluation, we analyze the maximum mean discrepancy (MMD) <ref type="bibr" target="#b6">[7]</ref> of deep features between the two datasets. MMD is a commonly used metric for evaluating domain discrepancy. Specifically, we use the MMD with Gaussian <ref type="table">Table 6</ref>. Results of adapting GTA5 to Cityscapes. mIoU of models with different normalization layers are reported. <ref type="table" target="#tab_2">road  sidewalk  building  wall  fence  pole  light  sign  veg  terrain  sky  person  rider  car  truck  bus  train  mbike  bike  mIoU   AdaptSetNet-BN 88.3</ref>   kernels as in <ref type="bibr" target="#b17">[18]</ref>. We calculate the MMD for features of the first 13 layers in VGG16 with different normalization layers. The results are shown in <ref type="figure" target="#fig_3">Fig.3</ref>. Compared with BN and SN, SW significantly reduces MMD for both shallow and deep features. This shows that the IW introduced effectively reduces domain discrepancy in the CNN features, making the model easier to generalize to other data domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Style Transfer</head><p>Thanks to the rich statistics, SW could work not only in high-level vision tasks, but also in low-level vision tasks like image style transfer. To show this, we employ a popular style transfer algorithm <ref type="bibr" target="#b14">[15]</ref>. It has an image stylizing network trained with the content loss and style loss calculated by a loss network. The MS-COCO dataset <ref type="bibr" target="#b21">[22]</ref> is used as content images while the style images selected are candy and starry night. We follow the same training policy as in <ref type="bibr" target="#b14">[15]</ref>, and adopt different normalization layers for the image stylizing network. Results. The training loss curve is shown in <ref type="figure">Fig.4</ref>. As revealed in former works, IW and IN perform better than BN. Besides, we observe that IW has smaller content loss and style loss than IN, which verifies that IW works better in manipulating image style. Although SW converges slower than IW at the beginning, it soon catches up with IW as SW learns to select IW as the normalizer. Moreover, SW has smaller content loss than IW when the training converges, as BW preserves important content information.</p><p>Qualitative examples of style transfer using different normalization layers are shown in <ref type="figure">Fig.5</ref>. BN produces poor stylization images, while IW gives satisfactory results. SW works comparably well with IW, showing that SW is able to select appropriate normalizer according to the task. More examples are provided in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis on SW</head><p>In order to understand the behavior of SW, in this section we study its learning dynamics and the learned importance ratios. Learning Dynamics. The importance ratios of SW is initialized to have uniform values, i.e. 0.5 for Ω = {bw, iw} and 0.2 for Ω = {bw, iw, bn, in, ln}. To see how the ratios of SW in different layers change during training, we plot the learning curves of ω k and ω k in <ref type="figure">Fig.6</ref> and <ref type="figure">Fig.7</ref>. It can be seen that the importance ratios shift quickly at the beginning and gradually become stable. There are several interesting observations. (1) The learning dynamics vary across different tasks. In CIFAR-10, SW mostly selects IW and occasionally selects BW, while in Cityscapes BW or BN is mostly chosen. (2) The learning behaviours of SW across different layers tend to be distinct rather than homogeneous. For example, in <ref type="figure">Fig.7 (a)</ref>, SW selects IW for layer {15, 21, 39}, and BW for the rest except for layer {6, 9} where the ratios keep uniform.   in training. Importance Ratios for Various Tasks.</p><p>We further analyze the learned importance ratios in SW for various tasks, as shown in <ref type="figure" target="#fig_7">Fig.8</ref>. The results are obtained by taking average over the importance ratios ω k of all SW layers in a CNN. The models are ResNet50 for Im-ageNet, ADE20K, and Cityscapes, ResNet44 for CIFAR-10, VGG16 for GTA5→Cityscapes, and a ResNet-alike network for style transfer as in <ref type="bibr" target="#b14">[15]</ref>. Both Ω = {bw, iw} and Ω = {bw, iw, bn, in, ln} are reported.</p><p>We make the following remarks: (1) For semantic segmentation, SW chooses mainly BW and BN, and partially the rest, while in classification more IW are selected. This is because the diversity between images is higher in classification datasets than in segmentation datasets. Thus more IW is required to alleviate the intra-dataset variance. (2) Semantic segmentation on Cityscapes tends to produce more IW and IN under domain adaptation setting than in the normal setting. Since domain adaptation introduces a domain discrepancy loss, more IW and IN would be beneficial for reducing the feature discrepancy between the two domains, i.e., GTA5 and Cityscapes. (3) In image style transfer, SW switches to IW aggressively. This phenomenon is consistent with the common knowledge that IW is well suited for style transfer, as image level appearance information is well encoded in the covariance of CNN features. Our experiments also verify that IW is a better choice than IN in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose Switchable Whitening, which integrates various whitening and standardization techniques in a general form. SW adapts to various tasks by learning to select appropriate normalizers in different layers of a CNN. Our experiments show that SW achieves consistent improvements over previous normalization methods in a number of computer vision tasks, including classification, segmentation, domain adaptation, and image style transfer. Investigation of SW reveals the importance of leveraging different whitening methods in CNNs. We hope that our findings in this work would benefit other research fields and tasks that employ deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Back-propagation of SW</head><p>The backward pass of our proposed SW is presented in Algorithm 2.</p><p>Algorithm 2 Backward pass of SW for each iteration. </p><formula xml:id="formula_8">N i=1 ∂L ∂μ i + ω iw HW ∂L ∂μn ) +[ 2ω bw (Xn−µ bw ) T N HW N i=1 ( ∂L ∂Σ i )sym + 2ω iw (Xn−µ iw ) T HW ( ∂L ∂Σn )sym]</formula><p>9: end for 10: calculate:</p><formula xml:id="formula_9">∂L ∂λ bw = ωbw(1 − ωbw) N n=1 ( ∂L ∂μn µbw) − ωiwωbw N n=1 ( ∂L ∂μn µ (n) iw ) ∂L ∂λ iw = ωiw(1 − ωiw) N n=1 ( ∂L ∂μn µ (n) iw ) − ωbwωiw N n=1 ( ∂L ∂μn µbw) ∂L ∂λ bw = ω bw (1 − ω bw ) N n=1 ∂L ∂Σn , Σbw F − ω iw ω bw N n=1 ∂L ∂Σn , Σ (n) iw F 1 ∂L ∂λ iw = ω iw (1 − ω iw ) N n=1 ∂L ∂Σn , Σ (n) iw F − ω bw ω iw N n=1 ∂L ∂Σn , Σbw F</formula><p>In the Next, we derive ∂L ∂Σn and ∂L ∂μn in the line 4, 5 of Algorithm 2, where we start with the forward pass of ZCA whitening. Forward Pass. Let X ∈ R C×HW be a sample of a minibatch. Here the subscript n is omitted for clearance. Given the integrated meanμ and the integrated covarianceΣ in SW, the ZCA whitening is as follows: in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>, ∂L ∂Σ and ∂L ∂μ can be calculated as follows:</p><formula xml:id="formula_10">Σ = DΛD T<label>(8)</label></formula><formula xml:id="formula_11">V = Λ −1/2 D T (9) X = V(X −μ · 1 T )<label>(10)</label></formula><formula xml:id="formula_12">∂L ∂X = ∂L ∂X D (12) ∂L ∂V = ∂L ∂X T (X −μ · 1 T ) T<label>(13)</label></formula><formula xml:id="formula_13">∂L ∂Λ = ∂L ∂V D(− 1 2 Λ −3/2 )<label>(14)</label></formula><formula xml:id="formula_14">∂L ∂D = ∂L ∂V Λ −1/2 ) + ∂L ∂XX T (15) ∂L ∂Σ = D{[K T (D T ∂L ∂D )] + ( ∂L ∂Λ ) diag }D T<label>(16)</label></formula><formula xml:id="formula_15">∂L ∂μ = ∂L ∂X (−V)<label>(17)</label></formula><p>where L is the loss calculated via a loss function, K ∈ R C×C is a 0-diagonal matrix with K ij = 1 σi−σj [i = j], and is element-wise matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion for Network Configurations</head><p>In our experiments, we replace part of the BN layers in ResNet to SW layers to save computation and to reduce redundancy. In this section we discuss the network configurations in detail. CIFAR-10/100. For CIFAR, the ResNet has two convolution layers in a residual module. Thus we apply SW or other counterparts after the 1st and the {4n}th (n = 1,2,3,...) convolution layers. For example, in ResNet20, the normalization layers considered are the {1,4,8,12,16}th layers. We consider the 1st layer because <ref type="bibr" target="#b16">[17]</ref> shows that it is effective to conduct whitening there. The last layer is not considered because it is a classifier where normalization is not needed. ADE20K and Cityscapes. For semantic segmentation, the ResNet50 has a bottleneck architecture with a period of three layers, where the second layer provides a compact embedding of the input features. Therefore we apply SW after the second convolution layer of the bottleneck. Then the normalization layers considered are those at the 1st and the {3n}th (n = 1,2,3,...) layers. The residual blocks with 2048 channels are not considered to save computation, which also follows the rule in <ref type="bibr" target="#b25">[26]</ref> that instance normalization should not be added in deep layers. Thus in ResNet50, the normalization layers considered are the {1,3,6,...,39}th layers, containing 14 layers in total. ImageNet. And for ImageNet, the network configuration is similar to ResNet50 in semantic segmentation, except that we consider the 1st and the {6n}th (n = 1,2,3,...) layers to further save computation. Thus the normalization layers considered are the {1,6,12,...,36}th layers, containing 7 layers in total.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Instance Segmentation</head><p>We further provide results on instance segmentation, where Mask-RCNN <ref type="bibr" target="#b7">[8]</ref> and COCO dataset <ref type="bibr" target="#b21">[22]</ref> are used to evaluate our method, and the implementation is based on mmdetection <ref type="bibr" target="#b1">[2]</ref>. We replace 7 normalization layers of the ResNet50 backbone with SW following the same way as in ImageNet, while the rest normalization layers of the backbone, FPN, and detection/mask head are SyncBN. The SW layers are synchronized across multiple GPUs. As shown in <ref type="figure">Fig.7</ref>, SW significantly outperforms SyncBN and GN <ref type="bibr" target="#b32">[33]</ref>, and also outperforms SN reported by <ref type="bibr" target="#b23">[24]</ref>, which replaces all normalization layers to SN. <ref type="figure" target="#fig_8">Fig.9</ref> provides visualization examples for image style transfer, where results of stylizing network with different normalization techniques are shown. It can be observed that the results of BN are worse than those of other methods, and SW produces comparably well stylizing images with IW. This shows that SW well adapts to the image style transfer task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Style Transfer Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>BN,IN,LN,SN O(N CHW ) O(N CHW ) BW O(C 2 max(N HW, C)) O(CGmax(N HW, G)) IW O(N C 2 max(HW, C)) O(N CGmax(HW, G)) SW O(N C 2 max(HW, C)) O(N CGmax(HW, G)) Complexity Analysis. The computational complexities for different normalization methods are compared in Table 1. The flop of SW is comparable with IW. And applying group whitening could reduce the computation by C/G times. Usually we have HW &gt; G, thus the computation cost of SW and BW would be roughly the same (i.e., O(CGN HW )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Training and validation error curve on CIFAR-10 and ImageNet. Models with different normalization methods are reported. Here SW has Ω = {bw, iw}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>MMD distance between Cityscapes and GTA5. still achieves satisfactory results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>42.7 74.9 22.0 14.0 16.5 17.8 4.2 83.5 34.3 72.1 44.8 1.7 76.9 18.0 6.7 0.0 3.0 0.1 32.7 AdaptSetNet-SN 87.0 41.6 77.5 21.2 20.0 18.3 20.9 8.3 82.4 35.4 72.6 48.4 1.4 81.1 18.7 5.2 0.0 8.4 0.0 34.1 AdaptSetNet-SW a 91.8 50.2 78.1 25.3 17.5 17.5 21.4 6.2 83.4 36.6 74.0 50.7 7.4 83.4 16.7 6.3 0.0 10.4 0.8 35.7 AdaptSetNet-SW b 91.8 50.5 78.4 23.5 16.5 17.2 19.8 5.5 83.6 38.4 74.6 48.9 5.3 83.6 17.6 3.9 0.1 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Training loss in style transfer and the learned importance ratios of SW b . The importance ratios are averaged over all SW b layers in the image stylizing network. Visualization of style transfer using different normalization layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 3 )Figure 6 .Figure 7 .</head><label>367</label><figDesc>The behaviors of ω k and ω k are mostly coherent and sometimes divergent. For instance, in layer {15, 21} of Fig.7, ω k chooses IW while ω k chooses BW or BN. This implies that µ and Σ are not necessarily have to be consistent, as they might have different impacts Learning curve of importance weights in ResNet56 on CIFAR-10. (a) and (b) show ω k and ω k in SW with Ω = {bw, iw}. (c) and (d) correspond to ω k and ω k in SW with Ω = {bw, iw, bn, in, ln}. 12 layer 15 layer 18 layer 21 layer 24 layer 27 layer 30 layer 33 layer 36 layer 39 BW BN IW IN LN Learning curve of importance weights in ResNet50 on Cityscapes. (a)(b)(c)(d) have the same meanings as in Fig.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Learned importance ratios of SW in various tasks. Above and below correspond to Ω = {bw, iw} and Ω = {bw, iw, bn, in, ln} respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of style transfer using different normalization layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of computational complexity. N, C, H, W are the number of samples, number of channels, height, and width of the input tensor respectively. G denotes the number of channels for each group in group whitening.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Test errors (%) on CIFAR-10/100 and ImageNet validation sets<ref type="bibr" target="#b15">[16]</ref>. For each model, we evaluate different normalization or whitening methods. SW a and SW b correspond to Ω = {bw, iw} and Ω = {bw, iw, bn, in, ln} respectively. Results on CIFAR are averaged over 5 runs.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>BN</cell><cell>SN</cell><cell>BW SW a SW b</cell></row><row><cell></cell><cell>ResNet20</cell><cell cols="3">8.45 8.34 8.28 7.64 7.75</cell></row><row><cell>CIFAR-10</cell><cell>ResNet44 ResNet56</cell><cell cols="3">7.01 6.75 6.83 6.27 6.35 6.88 6.57 6.62 6.07 6.25</cell></row><row><cell></cell><cell>ResNet110</cell><cell cols="3">6.21 5.97 5.99 5.69 5.78</cell></row><row><cell>CIFAR-100</cell><cell>ResNet20</cell><cell>32</cell><cell></cell><cell></cell></row></table><note>.09 32.28 32.44 31.00 30.87 ResNet110 27.32 27.25 27.76 26.64 26.48 ImageNet ResNet50 (top1) 23.58 23.10 23.31 22.10 22.07 ResNet50 (top5) 7.00 6.55 6.72 5.96 5.91</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on Cityscapes and ADE20K datasets. 'ss' and 'ms' indicate single-scale and multi-scale test respectively.</figDesc><table><row><cell>Method</cell><cell cols="4">ADE20K mIoU ss mIoU ms mIoU ss mIoU ms Cityscapes</cell></row><row><cell>ResNet50-BN</cell><cell>36.6</cell><cell>37.9</cell><cell>72.1</cell><cell>73.4</cell></row><row><cell>ResNet50-SN</cell><cell>37.8</cell><cell>38.8</cell><cell>75.0</cell><cell>76.2</cell></row><row><cell>ResNet50-BW</cell><cell>35.9</cell><cell>37.8</cell><cell>72.5</cell><cell>73.7</cell></row><row><cell cols="2">ResNet50-SW a 39.8</cell><cell>40.8</cell><cell>76.2</cell><cell>77.1</cell></row><row><cell cols="2">ResNet50-SW b 39.8</cell><cell>40.7</cell><cell>76.0</cell><cell>77.0</cell></row><row><cell cols="5">Table 4. Comparison with advanced methods on the ADE20K val-</cell></row><row><cell cols="3">idation set. * indicates our implementation.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="3">mIoU(%) Pixel Acc.(%)</cell></row><row><cell cols="2">DilatedNet [35]</cell><cell>32.31</cell><cell>73.55</cell><cell></cell></row><row><cell cols="2">CascadeNet [40]</cell><cell>34.90</cell><cell>74.52</cell><cell></cell></row><row><cell cols="2">RefineNet [21]</cell><cell>40.70</cell><cell>-</cell><cell></cell></row><row><cell cols="2">PSPNet101 [37]</cell><cell>43.29</cell><cell>81.39</cell><cell></cell></row><row><cell>SDDPN [20]</cell><cell></cell><cell>43.68</cell><cell>81.13</cell><cell></cell></row><row><cell cols="2">WiderNet [34]</cell><cell>43.73</cell><cell>81.17</cell><cell></cell></row><row><cell cols="2">PSANet101 [38]</cell><cell>43.77</cell><cell>81.51</cell><cell></cell></row><row><cell>EncNet [36]</cell><cell></cell><cell>44.65</cell><cell>81.69</cell><cell></cell></row><row><cell>PSPNet101*</cell><cell></cell><cell>43.59</cell><cell>81.41</cell><cell></cell></row><row><cell cols="2">PSPNet101-SW a</cell><cell>45.33</cell><cell>82.05</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Performance and running time of ResNet50 with different normalization layers on ImageNet and Cityscapes datasets. We report the GPU running time per iteration during training. The GPU we use is NVIDIA Tesla V100.</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell cols="2">Whitening</cell><cell cols="4">ImageNet error(%) time(s) mIoU(%) time(s) Cityscapes</cell></row><row><cell cols="3">ResNet50-BN</cell><cell>-</cell><cell></cell><cell cols="2">23.58 0.27</cell><cell>72.1</cell><cell>0.52</cell></row><row><cell cols="3">ResNet50-BW</cell><cell>svd</cell><cell></cell><cell cols="2">23.31 0.79</cell><cell>72.4</cell><cell>1.09</cell></row><row><cell cols="3">ResNet50-SW a</cell><cell>svd</cell><cell></cell><cell cols="2">22.10 1.04</cell><cell>75.7</cell><cell>1.24</cell></row><row><cell cols="7">ResNet50-SW a iterative 22.07 0.36</cell><cell>76.0</cell><cell>0.67</cell></row><row><cell>MMD</cell><cell>0.4 0.6 0.8</cell><cell cols="2">VGG16-BN VGG16-SN VGG16-SW a VGG16-SW b</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7 layer id</cell><cell>9</cell><cell>11</cell><cell>13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Mask R-CNN using ResNet50 and FPN with 2× LR schedule. Backbone FPN &amp; Head AP box AP mask</figDesc><table><row><cell>FrozenBN</cell><cell>-</cell><cell>38.5 35.1</cell></row><row><cell>SyncBN</cell><cell>SyncBN</cell><cell>39.6 35.6</cell></row><row><cell>GN</cell><cell>GN</cell><cell>39.6 35.8</cell></row><row><cell>SN</cell><cell>SN</cell><cell>41.0 36.5</cell></row><row><cell>SW a</cell><cell>SyncBN</cell><cell>41.2 37.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix provides (1) back-propagation of SW, (2) discussion for our network configurations, (3) results for instance segmentation, and (4) some style transfer visualization results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<title level="m">Iterative normalization: Beyond standardization towards efficient whitening. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training deep networks with structured layers by matrix backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Decorrelated batch normalization. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dawei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Universal style transfer via feature transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07727</idno>
		<title level="m">Do normalization layers in a deep convnet really need to be distinct? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Differentiable learning-tonormalize via switchable normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch-instance normalization for adaptively style-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Whitening and coloring transform for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00420</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch kalman normalization: Towards training deep neural networks with micro-batches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Group normalization. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Scene parsing through ade20k dataset. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through the ade20k dataset. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
