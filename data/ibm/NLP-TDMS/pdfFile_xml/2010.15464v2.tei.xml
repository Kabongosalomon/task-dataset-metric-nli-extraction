<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pretext-Contrastive Learning: Toward Good Practices in Self-supervised Video Representation Leaning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20051">AUGUST 2005 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Pretext-Contrastive Learning: Toward Good Practices in Self-supervised Video Representation Leaning</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20051">AUGUST 2005 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-supervised learning</term>
					<term>video representation</term>
					<term>video recognition</term>
					<term>video retrieval</term>
					<term>spatio-temporal convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, pretext-task based methods are proposed one after another in self-supervised video feature learning. Meanwhile, contrastive learning methods also yield good performance. Usually, new methods can beat previous ones as claimed that they could capture "better" temporal information. However, there exist setting differences among them and it is hard to conclude which is better. It would be much more convincing in comparison if these methods have reached as closer to their performance limits as possible. In this paper, we start from one pretexttask baseline, exploring how far it can go by combining it with contrastive learning, data pre-processing, and data augmentation. A proper setting has been found from extensive experiments, with which huge improvements over the baselines can be achieved, indicating a joint optimization framework can boost both pretext task and contrastive learning. We denote the joint optimization framework as Pretext-Contrastive Learning (PCL). The other two pretext task baselines are used to validate the effectiveness of PCL. And we can easily outperform current state-of-the-art methods in the same training manner, showing the effectiveness and the generality of our proposal. It is convenient to treat PCL as a standard training strategy and apply it to many other works in self-supervised video feature learning. Code will be made public at https://github.com/BestJuly/Pretext-Contrastive-Learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Recently, pretext-task based methods are proposed one after another in self-supervised video feature learning. Meanwhile, contrastive learning methods also yield good performance. Usually, new methods can beat previous ones as claimed that they could capture "better" temporal information. However, there exist setting differences among them and it is hard to conclude which is better. It would be much more convincing in comparison if these methods have reached as closer to their performance limits as possible. In this paper, we start from one pretexttask baseline, exploring how far it can go by combining it with contrastive learning, data pre-processing, and data augmentation. A proper setting has been found from extensive experiments, with which huge improvements over the baselines can be achieved, indicating a joint optimization framework can boost both pretext task and contrastive learning. We denote the joint optimization framework as Pretext-Contrastive Learning (PCL). The other two pretext task baselines are used to validate the effectiveness of PCL. And we can easily outperform current state-of-the-art methods in the same training manner, showing the effectiveness and the generality of our proposal. It is convenient to treat PCL as a standard training strategy and apply it to many other works in self-supervised video feature learning. Code will be made public at https://github.com/BestJuly/Pretext-Contrastive-Learning.</p><p>Index Terms-Self-supervised learning, video representation, video recognition, video retrieval, spatio-temporal convolution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the development of convolutional neural networks (CNNs) and the help of many large-scale labeled datasets, the computer vision community has witnessed unprecedented success in many tasks such as object classification, detection, segmentation, and action recognition. For both image-level and video-level tasks, pre-training on larger datasets such as ImageNet <ref type="bibr" target="#b0">[1]</ref> and Kinetics <ref type="bibr" target="#b1">[2]</ref> is important to ensure satisfactory performance.</p><p>However, the world is abundant in images and videos, and annotating large-scale datasets requires a wealth of resources. To leverage unlabeled data, many self-supervised learning methods have been proposed for efficient feature representation. These methods can be broadly divided into two categories, pretext task-based methods and contrastive learning methods.</p><p>Several tasks have been designed to constrain pretext taskbased models to learn effective and informative representations. These tasks include solving jigsaw puzzles <ref type="bibr" target="#b3">[4]</ref>, image inpainting <ref type="bibr" target="#b4">[5]</ref>, and detecting image rotation angles <ref type="bibr" target="#b5">[6]</ref>. For video data, some of these spatial tasks are also effective, together with temporal-related tasks such as predicting frame <ref type="figure">Fig. 1</ref>. A glance at the performance of our proposals. Our results in this figure are based on one pretext task, VCP <ref type="bibr" target="#b2">[3]</ref>, the performance of which is only 66%. Results of other methods are from corresponding papers and results using the same input sizes <ref type="bibr">(16 × 112 × 112)</ref> are used if provided, without using other data modalities such as optical flow, audio and text. orders <ref type="bibr" target="#b6">[7]</ref> or video clip orders <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, recognizing temporal transformations, and being sensitive to video playback speed <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b14">[15]</ref>. A suitable combination <ref type="bibr" target="#b15">[16]</ref> of such different tasks can help improve the performances of the methods in video retrieval and recognition tasks. However, even though high accuracy can be achieved, it seems to be endless because there can be new and "better" pretext tasks. Identifying which pretext task is more effective and why is theoretically difficult to explain.</p><p>In contrastive learning methods <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>, the solution is based on the comparison among different samples. The key idea is to distinguish one instance from another. Usually, different modalities and different spatial/temporal crops of the same video are treated as positives while samples from different videos are treated as negatives, even though they may belong to the same action category. Once the network can distinguish one instance from another, the learned features would be sufficient for downstream tasks such as video retrieval and recognition.</p><p>The combination of different pretext tasks and contrastive learning seems to be better than each on its own. Such kind of combination using one pretext task (pace prediction) has been firstly validated effective in video representation learning reported in a recent work <ref type="bibr" target="#b14">[15]</ref>. However, the reason why the combination can be effective lacks discussion and arXiv:2010.15464v2 [cs.CV] 4 Apr 2021 the generality of this combination is unsure whether this phenomenon happens only for a specific pretext task or not.</p><p>In this paper, based on the success of pretext tasks and contrastive learning, we want to explore what kind of combination can boost both. We propose Pretext-Contrastive Learning (PCL), which also facilitates the advantages of some data processing strategies such as residual clips <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and strong data augmentations <ref type="bibr" target="#b16">[17]</ref>. With PCL, huge improvements over the corresponding baselines can be achieved, as shown in <ref type="figure">Fig. 1</ref>. Better performance can be obtained over recent works while using much smaller (around 3.6%) data for pre-training. We should clarify that we are not proposing new pretext tasks, nor contrastive learning methods; instead, we want to explore the limits of pretext task and contrastive learning with comprehensive experimental investigation and discussion to find the best strategy facilitating the advantages of these technologies. And this paper is trying to set new baselines in self-supervised learning in videos.</p><p>To prove the effectiveness of our PCL, three pretext taskbased methods are set as baselines, together with the contrastive learning method. Different network backbones are tested to eliminate biases. Experimental results prove the effectiveness and the generality of our proposal. The proposed PCL is closer to a framework or a strategy rather than a simple method as it is flexible and can be applied to many existing solutions. And we have lifted benchmarks to a new level, setting new baselines in self-supervised video representation learning.</p><p>The contributions of this work can be summarized as:</p><p>• We propose a joint optimization framework, utilizing the advantage of both pretext tasks and contrastive learning, together with proper training settings. • Experiments demonstrate that huge improvements can be obtained by using our proposal, and we can also achieve state-of-the-art performances in two evaluation tasks on two benchmark datasets. • Our proposal is validated on the basis of three pretext task baselines and different network backbones, showing the effectiveness and the generality of our PCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pretext tasks</head><p>Self-supervised learning methods were first proposed for images. Spatial pretext tasks include solving jigsaw puzzles <ref type="bibr" target="#b3">[4]</ref>, detecting image rotations <ref type="bibr" target="#b5">[6]</ref>, image channel prediction <ref type="bibr" target="#b23">[24]</ref>, and image inpainting <ref type="bibr" target="#b4">[5]</ref>. Prior works also include image reconstruction using autoencoders <ref type="bibr" target="#b24">[25]</ref> and variational autoencoders <ref type="bibr" target="#b25">[26]</ref>.</p><p>For video data, some image-based pretext tasks can be directly applied or extended, such as detecting rotation angles <ref type="bibr" target="#b26">[27]</ref> and completing space-time cubic puzzles <ref type="bibr" target="#b27">[28]</ref>. Compared to image data, videos have an additional temporal dimension. Therefore, to utilize temporal information, many works have designed temporal-specific tasks. In <ref type="bibr" target="#b6">[7]</ref>, the network was trained to distinguish whether the input frames were in the correct order. <ref type="bibr" target="#b7">[8]</ref> trained their odd-one-out network (O3N) to identify unrelated or odd video clips. The order prediction network (OPN) <ref type="bibr" target="#b8">[9]</ref> was trained by predicting the correct order of shuffled frames. The video clip order prediction network <ref type="bibr" target="#b9">[10]</ref> used video clips together with a spatiotemporal CNN during training. Further, <ref type="bibr" target="#b2">[3]</ref> utilized spatial and temporal transformations to train the network. Many recent works have started to utilize the playback speed of the input video clips. SpeedNet <ref type="bibr" target="#b10">[11]</ref> was trained to detect whether a video is playing at a normal rate or a sped-up rate. <ref type="bibr" target="#b11">[12]</ref> trained a network to sort video clips according to the corresponding playback rates. The playback rate perception (PRP) <ref type="bibr" target="#b12">[13]</ref> used an additional reconstructing decoder branch to help train the model. <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b14">[15]</ref> also utilized additional transformations to help train the model.</p><p>All these pretext tasks can be set as the main branch and can be combined with our PCL for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contrastive learning</head><p>The success of contrastive learning also originated from image tasks <ref type="bibr" target="#b28">[29]</ref>. The key idea of contrastive learning is to minimize the distance within positive pairs in the feature space while maximizing the distance between negative pairs. After contrastive loss was proposed <ref type="bibr" target="#b29">[30]</ref>, contrastive learning has become the mainstream method for self-supervised learning of image data. Contrastive predictive coding (CPC) <ref type="bibr" target="#b19">[20]</ref> attempted to learn the future from the past by using sequential data. Deep InfoMax <ref type="bibr" target="#b30">[31]</ref> and Instance Discrimination <ref type="bibr" target="#b18">[19]</ref> were proposed to maximize information probability from the same sample. Contrastive multiview coding (CMC) <ref type="bibr" target="#b20">[21]</ref> used different views (e.g. different color spaces) from the same sample. Momentum Contrast (MoCo) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b31">[32]</ref> used a momentum-updated encoder to conduct contrastive learning. In SimCLR <ref type="bibr" target="#b16">[17]</ref>, different combinations of data augmentation methods were tested for paired samples. Bootstrap Your Own Latent (BYOL) <ref type="bibr" target="#b32">[33]</ref> trained the network without negative samples.</p><p>The above-mentioned methods mainly focus on image data. Some technologies have been successfully applied to video data. The concept of CMC can be easily adapted to videos by simply using video data as the model input. Similar to CPC, DPC <ref type="bibr" target="#b33">[34]</ref> and MemDPC <ref type="bibr" target="#b34">[35]</ref> were proposed to handle video data. IIC <ref type="bibr" target="#b35">[36]</ref> introduced intra-negative video samples to enhance contrastive learning. These methods are all based on visual data only. The contrastive learning concept can be extended to additional modalities of video, such as audio <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, text, and descriptive data <ref type="bibr" target="#b38">[39]</ref>.</p><p>Most of these contrastive learning methods utilize a noise contrastive estimation (NCE) loss <ref type="bibr" target="#b39">[40]</ref> for robust and effective training. Wang et al. <ref type="bibr" target="#b40">[41]</ref> explored the learned features and proposed a new loss function, align-uniform loss, which is a possible substitute for the NCE loss. In our PCL, we used the NCE loss for optimization. Other contrastive loss functions are also compatible in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Methods combinations</head><p>A combination of several pretext tasks with proper weights can yield better performances <ref type="bibr" target="#b15">[16]</ref> than when they are used alone. Many existing pretext task-based methods are beyond one simple pretext task and are already a combination of some particular tasks. We have listed many pretext tasks, and the potential combinations are extensive. These pretext tasks vary widely, and determining why one pretext task or one combination is better than another is difficult.</p><p>The combination of pretext tasks and contrastive learning has been attempted in a recent work <ref type="bibr" target="#b14">[15]</ref>. However, except for the reported results, few analyses have been conducted and the combination may be only effective on a specific task. In this paper, we address this issue and show the generality of the combination of pretext task and contrastive learning that it can boost performance of both. Improvements over three pretext task baselines also reveal that the effective settings can be generalized to a lot of pretext tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY A. Motivation</head><p>Pretext task methods and contrastive learning methods can have good performances on their own. And the questions arise. 1) Can a simple combination of a pretext task based method and a contrastive learning method boost each other and achieve better performance? 2) Will it be effective only for specific pretext task, or general enough for many pretext tasks?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PCL: pretext-contrastive learning</head><p>The goal for self-supervised video representation learning is to learn effective feature representations from videos using a backbone network f θ . The commonly used networks are based on spatio-temporal convolutions, where the input video v i is decoded to a sequence of frames and several frames are stacked to form video clips x vi . Video features can be generated by using f θ (x).</p><p>Pretext task. For pretext task-based methods, one or several tasks are used to train the network in a supervised manner. Most pretext tasks are classification tasks. For example, VCP <ref type="bibr" target="#b2">[3]</ref> used different transformations on the input video clip x and trained the network by distinguishing which transformation was conducted. 3DRotNet <ref type="bibr" target="#b26">[27]</ref> was trained by detecting the rotation angles of the input clip. VCOP <ref type="bibr" target="#b9">[10]</ref> shuffled video clips and trained the network by predicting the correct order class of the inputs. All these pretext tasks can be concluded as designing a proper classification task. The video clip x needs to be transformed by a specific transformation function t(x, y), where y is the label of the corresponding transformation. Then the optimization target of these pretext tasks becomes</p><formula xml:id="formula_0">minimize ∀vi L cls (g(f θ (t(x vi , y))), y),<label>(1)</label></formula><p>where g(·) is the post-process network to process extracted features and L cls is usually set as cross-entropy loss because the corresponding pretext tasks usually belong to classification tasks.</p><p>Contrastive learning. For contrastive learning methods, after extracting features from the backbone, a two-linearlayer multi-layer perceptron (MLP) is usually used to project features f θ (x) to another feature space. Let us denote the projector network as h(·). Positive pairs and negative pairs are required to constrain the network. x 1 vi is one video clip from the video v i , and when another video clip x 2 vi is from the same video, these two video clips are treated as a positive pair. Conversely, when a video clip x vj is from a different video, v j . Then x vi and x vj are a negative pair. The encoded feature in the projected feature space is h(f (x vi )), which is denoted as z vi for simplicity. Let us define D(z vi , z vj ) as the similarity distance between feature z vi and z vj ; then for video v i , the contrastive learning target is</p><formula xml:id="formula_1">minimize L vi N CE = L v 1 i N CE + L v 2 i N CE ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">L v 1 i N CE = − log D(z 1 vi , z 2 vi ) D(z 1 vi , z 2 vi ) + j =i D(z 1 vi , z 1 vj ) , L v 2 i N CE = − log D(z 1 vi , z 2 vi ) D(z 1 vi , z 2 vi ) + j =i D(z 2 vi , z 2 vj ) .<label>(3)</label></formula><p>In practice, video features (i.e., z vi ) are normalized in the feature space and the similarity distance D(z vi , z vj ) is calculated by the inner product. In contrastive learning, instances with different indexes can be treated as negative samples and at most N − 1 negative samples can be used, where N is the size of the dataset. To accelerate training, memory bank <ref type="bibr" target="#b18">[19]</ref> technologies are adopted to save extracted features from previous epochs and k negative samples are sampled from the corresponding memory banks. This procedure is similar to <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Joint optimization framework. As we can see from the optimization targets of pretext tasks (Eq. 1) and contrastive learning (Eq. 2 and Eq. 3), pretext task-based methods focus more within the sample while contrastive learning methods try to distinguish one instance from another. A combination of them may take the advantage of both, ensuring the network to have a local-global view.</p><p>There are a number of pretext tasks, and some tasks use only one video clip to conduct experiments such as 3DRotNet <ref type="bibr" target="#b26">[27]</ref>, which rotated the input video clip and trained the model by predicting the rotation angles. Some tasks use multiple video clips during training, such as VCOP <ref type="bibr" target="#b9">[10]</ref>, which shuffled the temporal order of several video clips. The training styles for almost all pretext tasks can be divided into two categories, single-clip methods and multi-clip methods.</p><p>We illustrate the use of our proposal in <ref type="figure">Fig. 2</ref>. For singleclip methods, the contrastive loss will use the encoded features from the backbone network. As contrastive loss requires positive pair and negative pairs to train, the encoding process is duplicated. The input video clip is generated from the same video as the original path, which can be treated as a positive pair. Negative pairs are taken directly from one batch of data because different samples are from different videos in one training batch.</p><p>For multi-clip methods, different video clips are set as inputs and they are encoded to features using a shared encoder. These features are natural positive pairs because they are from the same video. Negative pairs are also from video clips from one batch data.  <ref type="figure">Fig. 2</ref>. The use of PCL in pretext task-based methods. (a) For single-clip methods, two different clips from the same video will be processed and the contrastive loss will be calculated among one batch of data. (b) For multi-clip methods, different clips from the same video have been already processed and the contrastive loss can be easily calculated. The data pre-processing procedure includes strong data augmentation transformations and converting to residual clips.</p><p>It can be observed that it is very simple to construct a joint optimization framework based on any pretext task baseline method, and the final training loss becomes</p><formula xml:id="formula_3">L total = L pretext + αL contrast ,<label>(4)</label></formula><p>where α is a weight to balance losses between pretext tasks and contrastive learning. For L pretext and L contrast , they came from Eq. 1 and Eq. 3. For convenience, we rewrite them here.</p><formula xml:id="formula_4">L pretext = L cls (g(f θ (t(x vi , y))), y),<label>(5)</label></formula><formula xml:id="formula_5">L contrast = L v 1 i N CE + L v 2 i N CE (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data processing strategies</head><p>To further boost the performance, we mainly introduce two different kinds of processing strategies on data, residual clips and augmentation transformations.</p><p>Residual clips. Most video-based self-supervised learning methods use 3D convolutional networks to process data, and the corresponding input is video clips, which are stacked RGB frames. Residual clips were introduced in <ref type="bibr" target="#b21">[22]</ref> and have been used to self-supervised learning in <ref type="bibr" target="#b35">[36]</ref>. However, there are not many researches on whether it also functions well with several pretext tasks, nor its generality. We introduce residual clips here to show its effectiveness on different methods in self-supervised learning.</p><p>Here we use f rame i to represent the i th frame data, and F rame i∼j denotes the stacked frames from the i th frame to the j th frame. The process to get residual frames can be formulated as follows,</p><formula xml:id="formula_6">ResClip = F rame i+1∼j+1 − F rame i∼j ,<label>(7)</label></formula><p>where F rame i+1∼j+1 can be easily obtained by shifting frames along the temporal axis in video clips.</p><p>The ResClip here will be then directly fed into the network for feature extraction.</p><p>Augmentation transformations. It is widely acknowledged that data augmentation methods enhance the performance in most cases. However, in previous methods in video-based self-supervised learning, only few data augmentations were conducted such as random cropping in the spatial axis and temporal jittering. However, some recent works <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b32">[33]</ref> started to use strong augmentations in images, such as color distortion and Gaussian blur, and have achieved improvements over the corresponding baselines.</p><p>Though these data augmentations are conducted on images, we adopted this kind of processing and applied it to video frames. The motivation is that motion features should be similar even though frames are blurred or distorted by color. Another motivation is that we wonder whether it will also boost the performance of residual clips because there will be much fewer color information in residual clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To demonstrate the effectiveness of the proposed PCL framework, we used 1) three pretext task baselines and a contrastive learning baseline method, 2) four backbone networks, and 3) two evaluation tasks in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>We mainly used two benchmark datasets, UCF101 <ref type="bibr" target="#b41">[42]</ref> and HMDB51 <ref type="bibr" target="#b42">[43]</ref> in our experiments, as our baselines did <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The UCF101 dataset consists of 13,320 videos in 101 action categories. HMDB51 is comprised of 7,000 videos with a total of 51 action classes. The official splits only contain training set and testing set. We randomly selected 800 and 400 videos from the training splits for UCF101 and HMDB51 datasets, respectively, to form the validation set. The best performance on the validation set will be saved and evaluated in video retrieval and recognition. To further evaluate the effectiveness of our PCL, we also utilized Kinetics-400 dataset <ref type="bibr" target="#b43">[44]</ref> to train. Kinetics-400 consists 400 action classes and contains around 240k videos for training, 20k videos for validation and 40k videos for testing. We only used Kinetics-400 dataset in the pre-training process.</p><p>Because spatio-temporal convolutions were used to train our models, we followed <ref type="bibr" target="#b44">[45]</ref> and resized videos in size 128×171. Sixteen successive frames are sampled to form a video clip. Random spatial/temporal cropping was conducted to generate an input data of size 16 × 112 × 112, where the channel number 3 was ignored. In addition to random cropping, other augmentation transformations we used in our experiments include random color jittering, randomly converting to grayscale, Gaussian blur, and flipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>Because our PCL is a combination of pretext tasks and contrastive learning, the baselines should be set as the pretext task or contrastive learning.</p><p>There are several pretext task-based methods in selfsupervised video representation learning. We chose three works: 3DRotNet <ref type="bibr" target="#b26">[27]</ref>, VCOP <ref type="bibr" target="#b9">[10]</ref>, and VCP <ref type="bibr" target="#b2">[3]</ref>. 3DRotNet is trained by recognizing the rotated angles of the input video clip. VCOP aims to detect the correct orders of several input video clips. VCP conducts different types of transformations and the network is trained to distinguish which transformation has been performed. These pretext tasks, as well as the training styles, are different. For example, 3DRotNet is a one-clip method while VCOP and VCP use several video clips as input data. The other reason is that these three pretext tasks are from three different categories. 3DRotNet uses rotation, which is more related to spatial information. VCOP cares about temporal orders of input clips, which only uses temporal information. The processing which VCP chooses from is a mixture of temporal and spatial transformations. There exist many pretext tasks in video-based self-supervised learning and it is impossible for us to conduct all experiments. However, other pretext tasks can be easily classified into one of these three categories and we think the effectiveness of our proposal on these three pretext tasks can prove the generality of our PCL.</p><p>Contrastive learning is widely used in image-based selfsupervised learning and has been explored in videos in <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>. For a fair comparison, our contrastive learning baseline will use the same framework as our PCL while the network will be optimized only by L contrast in Eq. 4, without using L pretext .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network backbones</head><p>For the network backbone, there are several 3D CNNs such as C3D <ref type="bibr" target="#b44">[45]</ref>, R3D, ResNet-18-3D <ref type="bibr" target="#b45">[46]</ref>, and R(2+1)D <ref type="bibr" target="#b46">[47]</ref>. Different network backbones were used in our experiments to eliminate model biases. R3D and ResNet-18-3D are composed of 3D convolution instead of 2D convolution in the original ResNet <ref type="bibr" target="#b47">[48]</ref> while the numbers of convolutional layers in each residual block vary. To compare with the baselines, we used the same network architectures as them. It is possible to use other network architectures such as I3D <ref type="bibr" target="#b43">[44]</ref>, S3D <ref type="bibr" target="#b48">[49]</ref>, or other deeper networks, but we simply follow the baselines for fair comparisons.</p><p>A two-linear-layer multi-layer perceptron (MLP) is used to process features from the same backbone. Therefore, this part can be treated as the post-processing for the contrastive learning part, paralleling with the post-processing of pretext tasks. The MLP is in an fc-relu-fc style. After projection, feature dimensions are reduced to 128 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation tasks</head><p>To evaluate the performance of the trained models, two evaluation tasks were used: video retrieval and video recognition. After self-supervised training, the trained models can be evaluated directly in video retrieval tasks on both UCF101 and HMDB51. Note that the self-supervised learning part was only conducted on UCF101 split 1. Therefore, when conducting video retrieval on UCF101, the task-level generalization ability was tested. When conducting video retrieval on HMDB51 using the same model, both task-level and dataset-level generalization abilities were tested.</p><p>Video retrieval is conducted based on video-level features. 3D ConvNets can extract features from video clips, and features of video clips are averaged if they are from the same video. Thus, video-level features can be generated and knearest neighbors (kNN) algorithm is used to check whether the retrieved video has the same action category as the query video.</p><p>Action recognition is a fundamental task in video representation learning. Following previous works, we also conducted experiments by fine-tuning trained models on both UCF101 and HMDB51 datasets to check the transfer learning ability of the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental details</head><p>In all of our experiments, the batch size is set to 16 and the training lasts for 200 epochs. The initial learning rate is 0.01 for self-supervised learning. Models with the best performance on the validation datasets are saved then used to test the performance in the video retrieval task. For video recognition tasks, the same models are fine-tuned for 150 epochs and the initial learning rate is set to 0.001. The best performance on the validation dataset is evaluated on the corresponding test splits. Stochastic Gradient Descent (SGD) is used for optimization for both training periods. The hyper-parameter in Eq. 4, α is set to 0.5 to balance pretext task loss and contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In this section, we first compare our proposed method with baseline methods. To further prove the effectiveness of our PCL framework, we also compare our results with current state-of-the-art methods. We mainly used VCP as the baseline pretext task and used C3D, R3D, or R(2+1)D as the network backbone. For the other two methods, 3DRotNet and VCOP, we used the same mainstream backbones reported in the corresponding papers: ResNet-18-3D for 3DRotNet and C3D for VCOP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison with baselines</head><p>All models were pre-trained on UCF101 split 1 and tested on both UCF101 and HMDB51 datasets. Results are presented in <ref type="table" target="#tab_0">Table I</ref>. For the pretext task VCP with the C3D backbone, the baseline is only 17.3% in video retrieval and 68.5% in recognition on UCF101 dataset. When maintaining the main training architecture and used contrastive loss only, the performance can reach 38.9% in retrieval and 78.0% in video recognition. This performance is much higher than the pretext task. One reason is that it already benefits from our data processing strategies. Our PCL yielded 50.3% at top 1 retrieval accuracy on UCF101 dataset, which is 33.0% points above the C3D baseline for pretext task and also 11.4% points higher than our strong contrastive learning baseline. In video recognition, our PCL can also yield the best performance.</p><p>Similar results can be found when we used different network backbones on the basis of VCP. Our PCL can achieve more than double the performance of the corresponding baseline at top 1 retrieval accuracy and over 10% points improvement when we use R3D and R(2+1)D as the network backbone. These results show that our PCL can boost the performance of both VCP and contrastive learning.</p><p>When we look at other pretext baselines in <ref type="table" target="#tab_0">Table I</ref>, the same trend can be found. Our PCL can outperform the corresponding pretext task baselines, 3DRotNet and VCOP, by a large margin. These results reveal that the effectiveness of our PCL is not limited to only one pretext tasks, but general enough to other methods. Also, we want to mention that VCP cares much about spatial and temporal transformations, VCOP uses temporal information only and 3DRotNet uses rotation which is much more related to spatial information. The effectiveness of PCL on these three baselines reveals the potential that our PCL can boost the performance of other existing pretext task based methods in self-supervised video representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with the state-of-the-art methods</head><p>There are too many pretext tasks in video self-supervised learning and it is impossible for us to embed our proposal to all these methods. The baselines we used in our study are not currently state-of-the-art methods. Some very recent works have used new pretext tasks such as pace prediction <ref type="bibr" target="#b14">[15]</ref> or more complex temporal transformation recognition <ref type="bibr" target="#b13">[14]</ref> and achieved better performances. Here we compared our methods with state-of-the-art methods to demonstrate the effectiveness of our PCL. We want to clarify that there are some other works that used larger pre-trained datasets together with audio or text information of videos and achieved even higher performance <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. Here, we did not include them and only referred to these methods using similar settings for fair comparison.</p><p>The results for video retrieval in UCF101 and HMDB51 datasets are shown in <ref type="table" target="#tab_0">Table II and Table III, respectively.  From Table II</ref>, we can see that by combining with PCL, we can easily outperform other state-of-the-art methods, no matter which backbone is used. The solution of PacePred <ref type="bibr" target="#b14">[15]</ref> is already a combination of pretext task and contrastive learning. We can still outperform their results by a large margin based on three network backbones. The best top-1 video retrieval performance in UCF101 dataset is 55.1%, achieved by our PCL using Resnet-18-3D network backbone and the corresponding pretext task is VCP <ref type="bibr" target="#b2">[3]</ref>. Similar trend can be found in HMDB51 dataset in <ref type="table" target="#tab_0">Table III</ref>. We can lift the corresponding pretext baselines by a large margin.</p><p>The results for video recognition are shown in <ref type="table" target="#tab_0">Table IV</ref>. We can observe that without our proposal, the performances of the corresponding baseline methods are lower than those of recent state-of-the-art methods. However, with the proposed PCL, which only has minor changes in the baselines, the performances can be significantly improved. In most settings, PCL performs better than the state-of-the-art methods. From this table, we can also see that the settings of existing methods vary from one to another, such as using different sizes of input data, different network architectures, and different pretrained datasets. The total duration of Kinetics-400 dataset is around 28 days while it is about one day for UCF101 datasets. Larger datasets as well as input size will usually boost the performance. In our experiments, we only set the input size of 16 × 112 × 112 while we can achieve even better performance than methods such as SpeedNet <ref type="bibr" target="#b10">[11]</ref> and MemDPC <ref type="bibr" target="#b34">[35]</ref> when our PCL is pre-trained on UCF101 while they used larger pre-trained datasets, larger input size, and deeper networks. The best video recognition performance on UCF101 dataset is achieved when our PCL is pre-trained on Kinetics-400, reaching 85.7%. On HMDB51, our best performance (48.8%) is obtained by using VCP as the pretext task baseline and Resnet-18-3D network backbone, outperforming all other methods except for RTT <ref type="bibr" target="#b13">[14]</ref>. It may be claimed that in some papers, their proposed pretext task or contrastive learning methods were novel and could achieve state-of-the-art performance at that time. However, based on our experiments, we find there is much room for previous methods. Exploring the limits of each method and then conducting comparison may be a fair way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study: effectiveness of each part</head><p>Because we have a lot of changes on the basis of pretext tasks such as combining with contrastive learning, using residual clips, and data augmentation transformations, we want to find out how much impact each part contributes. We choose Data augmentation. We can see from Exp. 7 and Exp. 8, with strong data augmentation transformations, the top-1 performance in video retrieval can be lifted from 40.5% to 48.1%. And 1% point improvement can be obtained in video recognition. From Exp. 5 and Exp. 6, we can also find that strong data augmentation is effective.</p><p>Methods combination. We can see a comparison set {Exp. 1, Exp. 3, Exp. 5}, whose experimental settings do not use our data processing strategies, a combination of VCP and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study: loss weight balancing</head><p>We conducted several experiments on loss weight balancing to explore the impact of α in Eq. 4. Experiments are conducted on the basis of pretext task VCP and the network backbone is R3D. Results are reported on UCF101 split 1 in both video retrieval and recognition.</p><p>We can see from <ref type="table" target="#tab_0">Table VI</ref>, the retrieval performances are comparable when α is set to 0.5 or 1.0, higher than others. However, the best recognition result is achieved when α is set to 0.1. Compared with the setting α = 0.1, the top-1 retrieval accuracy is 4.9% points higher for α = 0.5 while its corresponding recognition accuracy is 0.2% points lower. To balance the performance in both video retrieval and recognition, we choose to set α to 0.5 for all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSIONS</head><p>In addition to the improvements on numbers, we would like to pose discussions on how a combination of pretext task and contrastive learning can yield better performance. In this section, we show some evidence and analysis towards the combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. General analysis</head><p>The mechanism of pretext tasks is not well explained in theory. Researchers aim to design tasks related to their final target tasks. For example, action retrieval and action recognition require temporal information to distinguish between samples. Thus, temporal related tasks have been proposed. However, for individual pretext task, it is not clear which is the best, except based on a particular performance metrics.</p><p>For contrastive learning, the basic idea is to distinguish one sample from another. However, determining why it functions well for motion representation extraction is difficult because spatial information may sometimes be enough. And same action clips in different instances will be treated as negatives during training.</p><p>Owing to many unclear issues, it is difficult to model the training target in a clear way. However, from the optimization target, we know that pretext tasks focus within the sample while contrastive learning methods try to distinguish one instance from another. By combining them together, the model can not only capture temporal information constrained by pretext tasks, but also learn discriminative features from samples constrained by contrastive learning. <ref type="figure">Fig. 3</ref>. Visualizations using t-SNE. The point number for PCL appears smaller because points with the same color (i.e., the same action labels) are more concentrated. The first ten categories (in alphabetical order) in UCF101 are visualized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature visualizations</head><p>To better understand learned features, we visualize them using t-SNE <ref type="bibr" target="#b49">[50]</ref> in <ref type="figure">Fig. 3</ref>. Four different methods are used here: 1) random initialization, 2) one pretext task method, 3) one contrastive learning method, and 4) our proposed PCL. All models are trained in a self-supervised manner, except for the random initialization because it is initialized without training. The first ten categories in UCF101 split 1 are visualized and each point represents one video.</p><p>As we can see from <ref type="figure">Fig. 3</ref>, without any training, features randomly distribute in the space. In the visualization of VCP and contrastive learning, features of the same class (in the same color) distribute more concentrated. With our PCL, it appears that the number of points is fewer because features of the same class are more close to each other and can be better clustered than the other three methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Case studies</head><p>To evaluate the advantage of pretext task, contrastive learning, and our PCL respectively, we use self-supervised trained models without changing parameters by fine-tuning. Therefore, video retrieval is used as the evaluation task. And all models are based on R3D network backbone.</p><p>One type of action categories in UCF101 dataset is Playing Musical Instruments, where many similar actions are classified into different classes because of the different instruments. Therefore, contrastive learning should have better performance because it is constrained by distinguishing one sample from another, mainly based on spatial differences. <ref type="figure">Fig. 4</ref> illustrates this trend that contrastive learning perform better than single pretext task, VCP. Though VCP utilized temporal transformations, the movements in many cases in this category are highly similar. Because our PCL is a combination of pretext task Retrieval accuracy pretext task: VCP contrastive learning PCL (ours) <ref type="figure">Fig. 4</ref>. Video retrieval performance on each class. All classes here belong to the Playing Musical Instruments category. Our PCL can take the advantage of contrastive learning and compensate for pretext task baseline.  <ref type="figure">Fig. 5</ref>. Video retrieval performance on each class. The classes here are those where pretext task method perform better than contrastive learning method.Our PCL can take the advantage of pretext task baseline and compensate for contrastive learning baseline. and contrastive learning, we can see that PCL can avoid the disadvantage of pretext task and even have better performance than contrastive learning method. Note that in <ref type="figure">Fig. 4</ref>, for category Playing Tabla and Playing Cello, the contrastive learning method have better performance than our PCL. We find that for category Playing Tabla, the total number of testing case is only 31, where 3 cases can cause around 10% points decrease. For category Playing Cello, 13.6% testing cases for our PCL are confused with category Nunchuncks, whose videos share similar composition with Playing Tabla. Though it is hard to say our PCL is the best for all cases, we can still say that our combination is generally better than pretext task or contrastive learning methods when they stand alone. Though from Table V, we can see that contrastive learning method can have much better performance than pretext task (44.7% vs 25.6%), there are still some advantage that pretext task has over contrastive learning. We visualize these classes where pretext task method perform better than contrastive learning. As we can see from <ref type="figure">Fig. 5</ref>, some of these categories are much more temporal related. For example, the action Clean and Jerk represents a series of movements <ref type="figure" target="#fig_2">(Fig. 6</ref>). Because the pretext task VCP contain temporal transformations, it enable the model to capture more temporal related features. And we can also see from the histogram that our PCL can achieve the better or comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Band</head><p>In the tiny experiments, our PCL can achieve the best performance in 72 out of 101 classes, and the best averaging results, revealing that a combination of pretext task and contrastive learning can take the advantage of both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this paper, we proposed Pretext-Contrastive Learning (PCL), a joint optimization framework facilitating both pretext tasks and contrastive learning, which is beyond a simple combination. Data processing strategies such as residual clips and strong data augmentations are used in our framework. Extensive ablation studies showed the effectiveness of each component in our proposal. Experiments using different pretext task baselines with different network backbones in different evaluation tasks on two benchmark datasets revealed the effectiveness and the generality of our proposal. With our PCL framework and the empirical settings, pretext tasks and contrastive learning can boost each other, and old benchmarking baselines can be lifted to a new level, which could provide a guideline for the self-supervised video representation community. Our proposed PCL is sufficiently flexible enough and can be easily applied to almost any existing pretext task or contrastive method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Pretext-Contrastive Learning in multi-clip methods (a) Pretext-Contrastive Learning in single-clip methods Repeat the process Data Pre-processing Process Process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Sample frames for action category Clean and Jerk, extracted from v CleanAndJerk g11 c03.avi in UCF101 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISONS</head><label>I</label><figDesc>WITH BASELINES. RESULTS ARE EVALUATED ON split 1 OF UCF101 AND HMDB51. Contrastive only REPRESENTS WE ONLY USE CONTRASTIVE LEARNING LOSS ONLY IN EQ. 4 FOR NETWORK OPTIMIZATION, WHICH ALREADY BENEFITS FROM OUR DATA PROCESSING STRATEGIES WHICH HAVE BEEN INTRODUCED IN SEC. III-C. BEST RESULTS IN EACH BLOCK ARE IN BOLD.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell></cell><cell>UCF101</cell><cell></cell><cell></cell><cell></cell><cell>HMDB51</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">Top1 Top5 Top10 Top20 Top50 Recognition Top1 Top5 Top10 Top20 Top50 Recognition</cell></row><row><cell>VCP (baseline) [3]</cell><cell>C3D</cell><cell>17.3 31.5 42.0</cell><cell>52.6</cell><cell>67.7</cell><cell>68.5</cell><cell>7.8 23.8 35.3</cell><cell>49.3</cell><cell>71.6</cell><cell>32.5</cell></row><row><cell>Contrastive only</cell><cell>C3D</cell><cell>38.9 56.9 65.7</cell><cell>74.4</cell><cell>84.3</cell><cell>78.0</cell><cell>15.1 34.9 47.2</cell><cell>61.5</cell><cell>82.1</cell><cell>45.5</cell></row><row><cell>PCL</cell><cell>C3D</cell><cell>50.3 67.3 75.7</cell><cell>83.4</cell><cell>91.2</cell><cell>79.8</cell><cell>19.6 41.5 44.8</cell><cell>70.2</cell><cell>85.9</cell><cell>46.1</cell></row><row><cell>VCP (baseline) [3]</cell><cell>R3D</cell><cell>18.6 33.6 42.5</cell><cell>53.5</cell><cell>68.1</cell><cell>66.0</cell><cell>7.6 24.4 36.3</cell><cell>53.6</cell><cell>76.4</cell><cell>31.5</cell></row><row><cell>Contrastive only</cell><cell>R3D</cell><cell>44.7 62.4 71.6</cell><cell>79.6</cell><cell>88.8</cell><cell>79.3</cell><cell>17.3 38.6 51.2</cell><cell>65.3</cell><cell>83.4</cell><cell>46.3</cell></row><row><cell>PCL</cell><cell>R3D</cell><cell>48.1 64.7 73.9</cell><cell>82.0</cell><cell>90.6</cell><cell>79.9</cell><cell>19.2 42.0 55.3</cell><cell>69.1</cell><cell>86.7</cell><cell>46.1</cell></row><row><cell>VCP (baseline) [3]</cell><cell cols="2">R(2+1)D 19.9 33.7 42.0</cell><cell>50.5</cell><cell>64.4</cell><cell>66.3</cell><cell>6.7 21.3 32.7</cell><cell>49.2</cell><cell>73.3</cell><cell>32.2</cell></row><row><cell>PCL</cell><cell cols="2">R(2+1)D 42.8 59.9 69.5</cell><cell>78.0</cell><cell>87.6</cell><cell>79.9</cell><cell>19.6 41.1 56.2</cell><cell>71.1</cell><cell>86.5</cell><cell>45.9</cell></row><row><cell cols="3">3DRotNet (baseline) [27] R3D-18 14.2 25.2 33.5</cell><cell>43.7</cell><cell>59.5</cell><cell>62.9</cell><cell>6.2 18.7 31.0</cell><cell>46.6</cell><cell>70.5</cell><cell>33.7</cell></row><row><cell>PCL</cell><cell cols="2">R3D-18 33.7 53.5 64.1</cell><cell>73.4</cell><cell>85.0</cell><cell>81.5</cell><cell>12.4 34.4 48.4</cell><cell>65.4</cell><cell>83.6</cell><cell>47.4</cell></row><row><cell>VCOP (baseline) [10]</cell><cell>C3D</cell><cell>12.5 29.0 39.0</cell><cell>50.6</cell><cell>66.9</cell><cell>65.6</cell><cell>7.4 22.6 34.4</cell><cell>48.5</cell><cell>70.1</cell><cell>28.4</cell></row><row><cell>PCL</cell><cell>C3D</cell><cell>39.0 59.1 67.5</cell><cell>76.8</cell><cell>87.4</cell><cell>79.2</cell><cell>14.9 35.9 48.9</cell><cell>63.6</cell><cell>82.8</cell><cell>42.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>WITH STATE-OF-THE-ART METHODS IN VIDEO RETRIEVAL ON UCF101. MOST RESULTS ARE FROM THE CORRESPONDING PAPERS.</figDesc><table><row><cell>Methods</cell><cell cols="3">Backbone Top1 Top5 Top10 Top20 Top50</cell></row><row><cell>MemDPC [35]</cell><cell cols="2">R2D3D 20.2 40.4 52.4 64.7</cell><cell>-</cell></row><row><cell cols="3">MemDPC-Flow [35] R2D3D 40.2 63.2 71.9 78.6</cell><cell>-</cell></row><row><cell>Random</cell><cell>C3D</cell><cell cols="2">16.7 27.5 33.7 41.4 53.0</cell></row><row><cell>VCOP [10]</cell><cell>C3D</cell><cell cols="2">12.5 29.0 39.0 50.6 66.9</cell></row><row><cell>VCP [3]</cell><cell>C3D</cell><cell cols="2">17.3 31.5 42.0 52.6 67.7</cell></row><row><cell>PRP [13]</cell><cell>C3D</cell><cell cols="2">23.2 38.1 46.0 55.7 68.4</cell></row><row><cell>PacePred [15]</cell><cell>C3D</cell><cell cols="2">31.9 49.7 59.2 68.9 80.2</cell></row><row><cell>IIC [36]</cell><cell>C3D</cell><cell cols="2">31.9 48.2 57.3 67.1 79.1</cell></row><row><cell>PCL (VCOP)</cell><cell>C3D</cell><cell cols="2">39.0 59.1 67.5 76.8 87.4</cell></row><row><cell>PCL (VCP)</cell><cell>C3D</cell><cell cols="2">50.3 67.3 75.7 83.4 91.2</cell></row><row><cell>Random</cell><cell>R3D</cell><cell cols="2">9.9 18.9 26.0 35.5 51.9</cell></row><row><cell>VCOP [10]</cell><cell>R3D</cell><cell cols="2">14.1 30.3 40.0 51.1 66.5</cell></row><row><cell>VCP [3]</cell><cell>R3D</cell><cell cols="2">18.6 33.6 42.5 53.5 68.1</cell></row><row><cell>PRP [13]</cell><cell>R3D</cell><cell cols="2">22.8 38.5 46.7 55.2 69.1</cell></row><row><cell>IIC [36]</cell><cell>R3D</cell><cell cols="2">36.5 54.1 62.9 72.4 83.4</cell></row><row><cell>PCL (VCOP)</cell><cell>R3D</cell><cell cols="2">38.9 57.8 66.6 76.1 86.0</cell></row><row><cell>PCL (VCP)</cell><cell>R3D</cell><cell cols="2">48.1 64.7 73.9 82.0 90.6</cell></row><row><cell>Random</cell><cell cols="3">R(2+1)D 10.6 20.7 27.4 37.4 53.1</cell></row><row><cell>VCOP [10]</cell><cell cols="3">R(2+1)D 10.7 25.9 35.4 47.3 63.9</cell></row><row><cell>VCP [3]</cell><cell cols="3">R(2+1)D 19.9 33.7 42.0 50.5 64.4</cell></row><row><cell>PRP [13]</cell><cell cols="3">R(2+1)D 20.3 34.0 41.9 51.7 64.2</cell></row><row><cell>PacePred [15]</cell><cell cols="3">R(2+1)D 25.6 42.7 51.3 61.3 74.0</cell></row><row><cell>IIC [36]</cell><cell cols="3">R(2+1)D 34.7 51.7 60.9 69.4 81.9</cell></row><row><cell>PCL (VCOP)</cell><cell cols="3">R(2+1)D 16.6 33.3 43.1 55.5 72.6</cell></row><row><cell>PCL (VCP)</cell><cell cols="3">R(2+1)D 42.8 59.9 69.5 78.0 87.6</cell></row><row><cell>Random</cell><cell cols="3">R3D-18 15.3 25.1 32.1 40.8 53.7</cell></row><row><cell>3DRotNet</cell><cell cols="3">R3D-18 14.2 25.2 33.5 43.7 59.5</cell></row><row><cell>VCP [3]</cell><cell cols="3">R3D-18 22.1 33.8 42.0 51.3 64.7</cell></row><row><cell>RTT [14]</cell><cell cols="3">R3D-18 26.1 48.5 59.1 69.6 82.8</cell></row><row><cell>PacePred [15]</cell><cell cols="3">R3D-18 23.8 38.1 46.4 56.6 69.8</cell></row><row><cell>IIC [36]</cell><cell cols="3">R3D-18 36.8 54.1 63.1 72.0 83.3</cell></row><row><cell cols="4">PCL (3DRotNet) R3D-18 33.7 53.5 64.1 73.4 85.0</cell></row><row><cell>PCL (VCP)</cell><cell cols="3">R3D-18 55.1 71.2 78.9 85.5 92.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>WITH STATE-OF-THE-ART METHODS IN VIDEO RETRIEVAL ON HMDB51. MOST RESULTS ARE FROM THE CORRESPONDING PAPERS.VCP as the pretext task baseline and R3D as the network backbone. Experiments are conducted on UCF101 split 1. Results are reported inTable V. Because there are a lot of combination settings, we use the experiment ID to refer to for convenience. There are totally 16 kinds of settings for all possible situations. Here, eight out of 16 are conducted because we think it is enough to show the effectiveness of each part in our proposal.Residual clips. As we can see from the comparison pair, Exp. 1 and Exp. 2, by using residual clips instead of original RGB video clips, improvements can be obtained in both video retrieval and recognition. Similar performance can be found between Exp. 5 and Exp. 7, or Exp. 6 and Exp. 8.</figDesc><table><row><cell>Methods</cell><cell cols="3">Backbone Top1 Top5 Top10 Top20 Top50</cell></row><row><cell>MemDPC [35]</cell><cell cols="2">R2D3D 7.7 25.7 40.6 57.7</cell><cell>-</cell></row><row><cell cols="3">MemDPC-Flow [35] R2D3D 15.6 37.6 52.0 65.3</cell><cell>-</cell></row><row><cell>Random</cell><cell>C3D</cell><cell cols="2">7.4 20.5 31.9 44.5 66.3</cell></row><row><cell>VCOP [10]</cell><cell>C3D</cell><cell cols="2">7.4 22.6 34.4 48.5 70.1</cell></row><row><cell>VCP [3]</cell><cell>C3D</cell><cell cols="2">7.8 23.8 35.3 49.3 71.6</cell></row><row><cell>PRP [13]</cell><cell>C3D</cell><cell cols="2">10.5 27.2 40.4 56.2 75.9</cell></row><row><cell>PacePred [15]</cell><cell>C3D</cell><cell cols="2">12.5 32.2 45.4 61.0 80.7</cell></row><row><cell>IIC [36]</cell><cell>C3D</cell><cell cols="2">11.5 31.3 43.9 60.1 80.3</cell></row><row><cell>PCL (VCOP)</cell><cell>C3D</cell><cell cols="2">14.9 35.9 48.9 63.6 82.8</cell></row><row><cell>PCL (VCP)</cell><cell>C3D</cell><cell cols="2">19.6 41.5 44.8 70.2 85.9</cell></row><row><cell>Random</cell><cell>R3D</cell><cell cols="2">6.7 18.3 28.3 43.1 67.9</cell></row><row><cell>VCOP [10]</cell><cell>R3D</cell><cell cols="2">7.6 22.9 34.4 48.8 68.9</cell></row><row><cell>VCP [3]</cell><cell>R3D</cell><cell cols="2">7.6 24.4 36.3 53.6 76.4</cell></row><row><cell>PRP [13]</cell><cell>R3D</cell><cell cols="2">8.2 25.8 38.5 63.3 75.9</cell></row><row><cell>IIC [36]</cell><cell>R3D</cell><cell cols="2">13.4 32.7 46.7 61.5 83.8</cell></row><row><cell>PCL (VCOP)</cell><cell>R3D</cell><cell cols="2">14.3 34.0 48.3 62.1 81.9</cell></row><row><cell>PCL (VCP)</cell><cell>R3D</cell><cell cols="2">19.2 42.0 55.3 69.1 86.7</cell></row><row><cell>Random</cell><cell cols="3">R(2+1)D 4.5 14.8 23.4 38.9 63.0</cell></row><row><cell>VCOP [10]</cell><cell cols="3">R(2+1)D 5.7 19.5 30.7 45.6 67.0</cell></row><row><cell>VCP [3]</cell><cell cols="3">R(2+1)D 6.7 21.3 32.7 49.2 73.3</cell></row><row><cell>PRP [13]</cell><cell cols="3">R(2+1)D 8.2 25.3 36.2 51.0 73.0</cell></row><row><cell>PacePred [15]</cell><cell cols="3">R(2+1)D 12.9 31.6 43.2 58.0 77.1</cell></row><row><cell>IIC [36]</cell><cell cols="3">R(2+1)D 12.7 33.3 45.8 61.6 81.3</cell></row><row><cell>PCL (VCOP)</cell><cell cols="3">R(2+1)D 7.9 23.8 35.9 51.0 74.7</cell></row><row><cell>PCL (VCP)</cell><cell cols="3">R(2+1)D 19.6 41.1 56.2 71.1 86.5</cell></row><row><cell>Random</cell><cell cols="3">R3D-18 7.1 19.3 29.6 44.2 68.6</cell></row><row><cell>3DRotNet</cell><cell cols="3">R3D-18 6.2 18.7 31.0 46.6 70.5</cell></row><row><cell>VCP [3]</cell><cell cols="3">R3D-18 10.9 25.2 36.8 51.5 71.8</cell></row><row><cell>PacePred [15]</cell><cell cols="3">R3D-18 9.6 26.9 41.1 56.1 76.5</cell></row><row><cell>IIC [36]</cell><cell cols="3">R3D-18 15.5 34.4 48.9 63.8 83.8</cell></row><row><cell cols="4">PCL (3DRotNet) R3D-18 12.4 34.4 48.4 65.4 83.6</cell></row><row><cell>PCL (VCP)</cell><cell cols="3">R3D-18 20.2 43.6 59.1 72.5 86.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISONS</head><label>IV</label><figDesc>WITH THE STATE-OF-THE-ART SELF-SUPERVISED METHODS IN VIDEO RECOGNITION ON UCF101 AND HMDB51 DATASET (PRE-TRAINEDON VIDEO MODALITY ONLY, WITHOUT USING OPTICAL FLOW DATA). PCL (VCP) REPRESENT OUR PCL WHICH IS BASED ON THE PRETEXT TASK VCP. AND RESULTS FROM OUR PCL ARE REPORTED WITH PROPOSED DATA PROCESSING STRATEGIES SUCH AS USING RESIDUAL CLIPS AND DATA AUGMENTATIONS. ABLATION STUDIES ON DIFFERENT KINDS OF COMBINATIONS. NETWORK ARCHITECTURE IS BASED ON R3D. RESULTS ARE REPORTED ON UCF101 split 1. Res MEANS USING RESIDUAL CLIP AS INPUT AND Aug REPRESENTS METHODS USING STRONG DATA AUGMENTATIONS.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="5">Date Pre-train ClipSize Network UCF HMDB</cell></row><row><cell>OPN [9]</cell><cell></cell><cell>2017 UCF</cell><cell>227 2</cell><cell></cell><cell cols="2">VGG 59.6 23.8</cell></row><row><cell>DPC [34]</cell><cell></cell><cell cols="5">2019 K400 16 × 224 2 R3D-34 75.7 35.7</cell></row><row><cell>CBT [39]</cell><cell></cell><cell cols="4">2019 K600+ 16 × 112 2 S3D</cell><cell>79.5 44.6</cell></row><row><cell cols="2">SpeedNet [11]</cell><cell cols="5">2020 K400 64 × 224 2 S3D-G 81.1 48.8</cell></row><row><cell cols="2">MemDPC [35]</cell><cell cols="5">2020 K400 40 × 224 2 R-2D3D 78.1 41.2</cell></row><row><cell cols="2">VCOP [10]</cell><cell cols="5">2019 UCF 16 × 112 2 C3D 65.6 28.4</cell></row><row><cell>VCP [3]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 C3D 68.5 32.5</cell></row><row><cell>PRP [13]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 C3D 69.1 34.5</cell></row><row><cell>RTT [14]</cell><cell></cell><cell cols="5">2020 K400 16 × 112 2 C3D 69.9 39.6</cell></row><row><cell cols="2">PCL (VCOP)</cell><cell cols="5">UCF 16 × 112 2 C3D 79.8 41.8</cell></row><row><cell cols="2">PCL (VCP)</cell><cell cols="5">UCF 16 × 112 2 C3D 81.4 45.2</cell></row><row><cell cols="2">VCOP [10]</cell><cell cols="5">2019 UCF 16 × 112 2 R3D 64.9 29.5</cell></row><row><cell>VCP [3]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 R3D 66.0 31.5</cell></row><row><cell>PRP [13]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 R3D 66.5 29.7</cell></row><row><cell>IIC [36]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 R3D 74.4 38.3</cell></row><row><cell cols="2">PCL (VCOP)</cell><cell cols="5">UCF 16 × 112 2 R3D 78.2 40.5</cell></row><row><cell cols="2">PCL (VCP)</cell><cell cols="5">UCF 16 × 112 2 R3D 81.1 45.0</cell></row><row><cell cols="2">VCOP [10]</cell><cell cols="5">2019 UCF 16 × 112 2 R(2+1)D 72.4 30.9</cell></row><row><cell>VCP [3]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 R(2+1)D 66.3 32.2</cell></row><row><cell>PRP [13]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 R(2+1)D 72.1 35.0</cell></row><row><cell>RTT [14]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 R(2+1)D 81.6 46.4</cell></row><row><cell cols="2">PacePred [15]</cell><cell cols="5">2020 UCF 16 × 112 2 R(2+1)D 75.9 35.9</cell></row><row><cell cols="2">PacePred [15]</cell><cell cols="5">2020 K400 16 × 112 2 R(2+1)D 77.1 36.6</cell></row><row><cell cols="2">PCL (VCOP)</cell><cell cols="5">UCF 16 × 112 2 R(2+1)D 79.2 41.6</cell></row><row><cell cols="2">PCL (VCP)</cell><cell cols="5">UCF 16 × 112 2 R(2+1)D 79.9 45.6</cell></row><row><cell cols="2">PCL (VCP)</cell><cell cols="5">K400 16 × 112 2 R(2+1)D 85.7 47.4</cell></row><row><cell cols="7">3D-RotNet [27] 2018 K400 16 × 112 2 R3D-18 62.9 33.7</cell></row><row><cell cols="2">ST-Puzzle [28]</cell><cell cols="5">2019 K400 16 × 112 2 R3D-18 65.8 33.7</cell></row><row><cell>DPC [34]</cell><cell></cell><cell cols="5">2019 K400 16 × 128 2 R3D-18 68.2 34.5</cell></row><row><cell>RTT [14]</cell><cell></cell><cell cols="5">2020 UCF 16 × 112 2 R3D-18 77.3 47.5</cell></row><row><cell>RTT [14]</cell><cell></cell><cell cols="5">2020 K400 16 × 112 2 R3D-18 79.3 49.8</cell></row><row><cell cols="2">PCL (3DRotNet)</cell><cell cols="5">UCF 16 × 112 2 R3D-18 82.8 47.2</cell></row><row><cell cols="2">PCL (VCP)</cell><cell cols="5">UCF 16 × 112 2 R3D-18 83.4 48.8</cell></row><row><cell cols="2">PCL (VCP)</cell><cell cols="5">K400 16 × 112 2 R3D-18 85.6 48.0</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Exp. Pretext Contrastive Res Aug Retrieval Recog.</cell></row><row><cell>1</cell><cell>VCP</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>18.6</cell><cell>66.0</cell></row><row><cell>2</cell><cell>VCP</cell><cell>×</cell><cell></cell><cell>×</cell><cell>25.6</cell><cell>77.0</cell></row><row><cell>3</cell><cell>×</cell><cell></cell><cell>×</cell><cell>×</cell><cell>34.0</cell><cell>61.2</cell></row><row><cell>4</cell><cell>×</cell><cell></cell><cell></cell><cell></cell><cell>44.7</cell><cell>79.3</cell></row><row><cell>5</cell><cell>VCP</cell><cell></cell><cell>×</cell><cell>×</cell><cell>35.0</cell><cell>65.9</cell></row><row><cell>6</cell><cell>VCP</cell><cell></cell><cell>×</cell><cell></cell><cell>40.3</cell><cell>68.9</cell></row><row><cell>7</cell><cell>VCP</cell><cell></cell><cell></cell><cell>×</cell><cell>40.5</cell><cell>78.9</cell></row><row><cell>8</cell><cell>VCP</cell><cell></cell><cell></cell><cell></cell><cell>48.1</cell><cell>79.9</cell></row></table><note>contrastive learning can boost the performance of each. For comparison pair, Exp. 4 and Exp. 8, improvements can be also obtained when contrastive learning is combined with VCP in both video retrieval and recognition.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDIES ON THE HYPER-PARAMETER α IN EQ. 4. NETWORK ARCHITECTURE IS BASED ONR3D AND THE PRETEXT TASK IS VCP.RESULTS ARE REPORTED ON UCF101 split 1.</figDesc><table><row><cell>α</cell><cell cols="6">Top1 Top5 Top10 Top20 Top50 Recog.</cell></row><row><cell>0.1</cell><cell>43.2</cell><cell>63.1</cell><cell>72.7</cell><cell>80.9</cell><cell>89.8</cell><cell>80.1</cell></row><row><cell>0.5</cell><cell>48.1</cell><cell>64.7</cell><cell>73.9</cell><cell>82.0</cell><cell>90.6</cell><cell>79.9</cell></row><row><cell>1.0</cell><cell>48.1</cell><cell>65.8</cell><cell>73.6</cell><cell>82.1</cell><cell>90.0</cell><cell>79.3</cell></row><row><cell>10</cell><cell>45.9</cell><cell>65.0</cell><cell>72.7</cell><cell>81.1</cell><cell>89.6</cell><cell>73.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially financially supported by the Grantsin-Aid for Scientific Research Numbers JP19K20289 and JP18H03339 from JSPS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video cloze procedure for self-supervised spatio-temporal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selfsupervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speednet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9922" to="9931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal representation learning using variable playback speed prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02692</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video playback rate perception for self-supervised spatio-temporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6548" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video representation learning by recognizing temporal transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning by pace prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ICML</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rethinking motion representation: Residual frames with 3d convnets for better action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05661</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Motion representation using residual frames with 3d cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1786" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning via video rotation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8545" to="8552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>International conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1483" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Memory-augmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning using inter-intra contrastive framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2193" to="2201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with selfsupervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7763" to="7774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hmdb: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
