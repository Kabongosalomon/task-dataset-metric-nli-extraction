<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Kumar Tomar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Debesh</forename><surname>Jha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Håvard</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Dag</forename><surname>Johansen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jens</forename><surname>Rittscher</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Pål</forename><surname>Halvorsen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Sharib</forename><surname>Ali</surname></persName>
						</author>
						<title level="a" type="main">FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Medical image segmentation</term>
					<term>deep learning</term>
					<term>feedback attention</term>
					<term>colon polyps</term>
					<term>skin lesion</term>
					<term>retinal vessels</term>
					<term>cell nuclei</term>
					<term>lung segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increase in available large clinical and experimental datasets, there has been substantial amount of work being done on addressing the challenges in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide a hard attention to the learnt feature maps at different convolutional layers. The network also allows to rectify the predictions in an iterative fashion during the test time. We show that our proposed feedback attention model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of the proposed FANet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-With the increase in available large clinical and experimental datasets, there has been substantial amount of work being done on addressing the challenges in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide a hard attention to the learnt feature maps at different convolutional layers. The network also allows to rectify the predictions in an iterative fashion during the test time. We show that our proposed feedback attention model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of the proposed FANet. I MAGE segmentation is one of the most studied problems in computer vision, where the main goal is to classify each pixel of an image to a specific class instance. This can either be a pixel of arbitrary objects such as cars or humans and pixels of healthy and sick medical findings. Substantial progresses have been made in imaging modalities such as X-ray, Computerized Tomography (CT), Magnetic Resonance Imaging (MRI), endoscopy imaging, fundus imaging, Electron Microscopy (EM), and histology imaging. While Machine Learning (ML) methods provide improved performance over the traditional computer vision methods, most of them require ground truth labels from domain experts, which comes with some challenges. For example, due to the poor image quality of most biomedical imaging data, expert interpretations can be affected. Moreover, complex and irregular shapes of the object-of-interest in these imaging data, such as cell nuclei in EM, can pose challenges for model training. Although it is difficult to address these biases and provide sample-specific priors, designing a model that is capable of pruning and self-rectification for each sample during training can help to improve model performance.</p><p>Current developments of Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and attention modules have improved automated methods in biomedical image analysis. While CNNs helps to overcome the limitations of handcrafted features used in traditional ML methods by learning features in a supervised end-to-end manner, they require the estimation of millions of parameters. As a result, a large and diverse training dataset is required to avoid overfitting. RNNs can be used to preserve the model compactness and can be effectively used in resource-constrained settings <ref type="bibr" target="#b0">[1]</ref>. However, RNNs are known for their memory inefficient memory-bandwidth-bound computation and model complexity as they hold their internal state to process the data <ref type="bibr" target="#b1">[2]</ref>. Additionally, in many applications, applying visual attention mechanism in deep learning has shown promising results and improvements in terms of model usability <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Attention allows networks to focus on a concrete class instance, thereby penalizing non-specific regions. Therefore, it is required to adhere with the knowledge transfer that integrates the past information, similar to recurrent networks, and use them to provide stronger acceptance of learned features belonging to the region-of-interest as an attention mechanism <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>While most deep learning-based segmentation methods are trained on large datasets to avoid overfitting, existing methods do not incorporate any sample-specific information learned in the previous training epochs. In current deep learning approaches, only aggregated average weights are used for information propagation through gradients. A mask-guided contrastive attention model was used by Song et al. <ref type="bibr" target="#b6">[7]</ref> to deal with the background clutter. Unlike classical training mechanism and motivated by the work of Song et al. <ref type="bibr" target="#b6">[7]</ref>, we propose to propagate the sample-specific mask output from the previous epoch to the successive epoch in a recursive fashion. Such a feedback mechanism can provide with prior information that can help to learn sample variability, thereby enabling to train effectively on diverse and small or large datasets. Here, iterative prediction can be used to prune arXiv:2103.17235v1 [cs.CV] 31 Mar 2021 the predicted masks during the inference (see <ref type="figure">Fig. 1</ref>). This allows to both locally and globally learn to rectify the mask output from the learnt weights. Unlike Test-Time Augmentation (TTA) <ref type="bibr" target="#b7">[8]</ref>, where different transforms are utilized to mimic the sample representations that are better learnt by the network, we interactively rectify masks during training. To our knowledge, FANet is the first deep learning model that has the ability to self-rectify its predictions without requiring transformations, ensemble strategies, and prior sample-specific knowledge. We use a single end-to-end trainable network that allows information propagation during both train and test time.</p><p>Including a feedback mechanism as a part of training is central to our novel FANet approach for semantic segmentation. The predicted map of each sample from the previous epoch is used to provide attention unified with the current state feature map. We call this architecture, Feedback Attention Network (FANet), as it uses an attention mechanism to different feature scales in the network, allowing it to capture variability in image samples. Additionally, our residual block with Squeeze and Excitation (SE) layer allows us to improve channel interdependencies, which can be critical to tackle image quality issues.</p><p>The main contributions made in this work can be summarized as follows:</p><p>1) Feedback attention learning -A novel mechanism to utilize the variability present in each training sample. The mask outputs are propagated from one to subsequent epochs to repress the unwanted feature clutter. 2) Iterative refining of prediction masks -Using feedback information helps in refining the predicted masks in training as well as inference. During testing, we iterate over the input image and keep updating the input mask with the predicted mask. 3) Embedded run-length encoding strategy -Binary mask outputs of each samples are efficiently compressed before being propagated to the next epoch. This provides a memory efficient mechanism for passing sample specific masks. 4) Systematic evaluation -Experiments on seven vastly different biomedical datasets suggest that FANet outperforms other state-of-the-art (SOTA) algorithms. 5) Efficient training -FANet achieves near SOTA performance with far fewer training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we summarize the most relevant advances in medical image segmentation, highlight advances in feedback attention networks, and summarise the recent contributions to iterative refinement methods for image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Biomedical image segmentation</head><p>The basis of most modern CNN-based semantic segmentation architectures are either Fully Convolutional Network (FCN) <ref type="bibr" target="#b8">[9]</ref> or an encoder-decoder architecture such as U-Net <ref type="bibr" target="#b9">[10]</ref> originally designed for cell segmentation <ref type="bibr" target="#b9">[10]</ref>. Various modifications of these networks have been proposed both for semantic segmentation of natural images <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> and biomedical image segmentation <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b21">[22]</ref>. In general, in the encoder, the image content is encoded using multiple convolutions to capture from low-level to high level features, whereas in the decoder part of the network the prediction masks are obtained by multiple upsampling mechanism and deconvolution. Methods like PSPNet <ref type="bibr" target="#b11">[12]</ref> and DeepLab <ref type="bibr" target="#b22">[23]</ref> incorporate convolutional feature maps of varying resolutions to effectively segment both small and large sized objects. While PSPNet used a pyramid pooling module, DeepLab used Atrous Spatial Pyramidal Pooling (ASPP) for encoding the multi-scale contextual information. Both PSPNet and DeepLab based architectures have been used widely in the medical imaging community <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>Recently, Cheng et al. <ref type="bibr" target="#b25">[26]</ref> proposed the Panoptic-DeepLap method with a class-agnostic regression, for instance, segmentation that uses a dual-ASPP and dual-decoder structure. Wang et al. <ref type="bibr" target="#b26">[27]</ref> proposed an attention-based panoptic instance segmentation showing improved performance than the former. High-Resolution Network (HRNet) <ref type="bibr" target="#b12">[13]</ref> employed exchange of information across different resolutions to resolve the issue of loss of high-resolution feature representations. Application of these architectures in the biomedical imaging community is, however, yet to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feedback attention networks</head><p>Visual attention has been widely used in computer vision for object detection <ref type="bibr" target="#b27">[28]</ref>, image segmentation <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> and pose estimation <ref type="bibr" target="#b30">[31]</ref>. Attention mechanisms have also been utilized for posing explicit focus on the target region in medical imaging <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Attention U-Net <ref type="bibr" target="#b20">[21]</ref> used a gated operation in the U-Net architecture to focus on the target abdominal regions of CT datasets. Feedback mechanism for attention using two U-Net architectures with shared weights was used for cell segmentation <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. The latter used a standard U-Net architecture with the second U-Net incorporating Con-vLSTM <ref type="bibr" target="#b34">[35]</ref> to store the feature map (input-to-state) from the first U-Net network. However, feedback is only applied to the same epoch with state-to-state transitions. On the contrary, our approach utilizes a feedback mechanism that propagates information flow from the previous epoch to the current epoch in an attention mechanism setting. We employ the predicted masks from the previous epoch as hard attention to prune the segmentation output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Iterative refinement for segmentation</head><p>Inherent variation in object shape and changes in scale pose significant challenges in biomedical imaging. An iterative refinement of the segmentation mask by feeding the input image and the predicted segmentation mask to a modified U-Net architecture was done by Mosinska et al. <ref type="bibr" target="#b35">[36]</ref>. Similarly, iterative update of latent space and minimization of the Structure Similarity Index Measure (SSIM) loss was used to refine the predicted segmentation maps during test time in <ref type="bibr" target="#b36">[37]</ref>.</p><p>Recently, iterative refinement strategies have also been used for pose estimation <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref> that used consecutive modules for refinement of the predictions with a loss function for the evaluation of output in each module. These iterative refinement <ref type="figure">Fig. 1</ref>: Semantic segmentation using our FANet architecture. Otsu threshold is used for generating the initial mask used during 0 th iteration. Then the predictions are iteratively updated with the predicted mask. It can be observed that already at the 2 nd iteration, the results converge. The corresponding feature maps before and after feedback attention at the last decoder layer of our FANet are shown as color images on the right. processes show improved predictions and are able to handle domain shifts or object shape variability without requiring very deep networks <ref type="bibr" target="#b36">[37]</ref>. A major bottleneck in these methods is the number of iterations required for convergence. Unlike these methods, our proposed FANet provides attention to the specific region-of-interest and can prune the segmentation predictions in less than ten iterations without requiring any optimization scheme, i.e., simple weight updates are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>This section introduces the main components required to build the proposed architecture. We present the overall architecture used along with the proposed feedback attention learning mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SE-Residual block</head><p>Deeper networks improve the performance of the model significantly, but, increase in depth can cause either a vanishing or exploding gradients problem <ref type="bibr" target="#b48">[49]</ref>. To deal with this, we take advantage of shortcut connections between layers in the residual learning paradigm. Our SE-Residual block uses two 3×3 convolutions and an identity mapping, where each convolution layer is followed by a Batch Normalization (BN) layer and a Rectified Linear Unit (ReLU) non-linear activation function. The identity mapping is used to connect the input and the output of the convolution layer ( <ref type="figure" target="#fig_1">Fig. 2 a)</ref>.</p><p>Similar to the work by Hu et al. <ref type="bibr" target="#b49">[50]</ref>, we add a SE layer in the residual network. The SE layer acts as a content-aware mechanism that re-weights each channel accordingly to create robust representations. Hence, it allows the network to become more sensitive to significant features while suppressing irrelevant features. This goal is accomplished in two steps. First, the feature maps are squeezed by using the global average pooling to get a global understanding of each channel. The squeeze operation results in a feature vector of size n, where n refers to the number of channels. In the second step: excitation, this feature vector is feed through a two-layered feed-forward neural network, where the number of features is first reduced and then expanded to the original size n. Now, this n sized vector represents the weight of the original feature maps, which is used to scale each channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MixPool block</head><p>The proposed MixPool block is used in multiple layers of our FANet architecture. This block facilitates the flow of samplewise feedback information between consecutive epochs providing a hard attention to the learnt features in our SE-Residual block. The layer provides focus to the relevant features in both contraction path and expansion path layers. Here, first feature maps of each layer from the SE-Residual block F l is passed through a 3 × 3 convolution followed by BN and a ReLU activation function resulting in F l . Then, we apply a 1 × 1 convolution and a sigmoid activation function σ(·) with a threshold of 0.5 to obtain the binary mask M l to contribute to the spatial attention map generation given by:</p><formula xml:id="formula_0">M l = σ conv (F l ) = 1, if σ(·) ≥ 0.5 0, otherwise.<label>(1)</label></formula><p>We then apply appropriate max pooling on the input mask (from the previous epoch) to resize it to the size of the spatial attention map. After that, we apply a union operation on both the resized input mask and the spatial attention map. This confirms that we get the feature from both the feedback and the spatial attention maps to further create a new spatial attention map. Next, we apply the new combined spatial attention map to the original feature map by an element-wise multiplication operation that suppresses the irrelevant features and enhances the important ones. The enhanced and the original feature maps are then followed by a 3 × 3 convolution, BN, and a ReLU. Finally, we concatenate the output of both activation functions, which constitutes the output of our MixPool block given by:</p><formula xml:id="formula_1">Output MixPool = F l F l ⊗ (M l ∪ M l ) ,<label>(2)</label></formula><p>where denotes the concatenation operator, ⊗ is elementwise multiplication and ∪ represents the union operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proposed FANet architecture</head><p>The block diagram of FANet is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. It uses an encoder-decoder design common to many semantic segmentation architectures. We combine the strength of a residual network enhanced with SE as SE-Residual block, MixPool block that facilitates the attention and propagation of information flow from the current learning paradigm and that of the previous epoch. We implement a recurrent learning Retina Vessel † Lesion Boundary Segmentation mechanism in both encoder and decoder layers that allows to achieve efficient segmentation. The MixPool block uses the previous segmentation map, which contains the information from prior training and uses it to improve the semantic representation of the feature maps.</p><p>The proposed network architecture is a Fully Convolutional Neural Network (FCNN) consisting of four encoder and four decoder blocks. The encoder takes the input image, downsamples it gradually, and encodes it in a compact representation. Then, the decoder takes this compact representation and tries to reconstruct the semantic representation by gradually upsampling it and combining the features from the encoder. Finally, we receive a pixel-wise categorization of the input image. Both the encoder and the decoder are built using the SE-Residual block, and an additional concatenation of the original resolution feature representation in the encoder is added at each resolution scale. This mechanism minimizes the loss of feature representations during downscaling and upscaling processes.</p><p>Each encoder network starts with two SE-Residual blocks, which consist of two 3 × 3 convolutions and a shortcut connection known as identity mapping, connecting the input and output of the two convolution layers. Each convolution is followed by a BN and a ReLU activation function. The output of the second SE-Residual block acts as skip connection for the corresponding decoder block. After that, it is followed by the MixPool block, which has the previous epoch segmentation mask and provides a hard-attention over the incoming feature maps. This process is repeated for each of the downscaled layers.</p><p>Each decoder network starts with a 4 × 4 transpose convolution that doubles the spatial dimensions of the incoming feature maps. These feature maps are concatenated with feature maps from the corresponding encoder block through skip connections. The skip connections help to propagate the information from the upper layers, which are sometimes lost due to the depth of the network. The skip connections are followed by two SE-Residual blocks, which help to eliminate the problem of vanishing gradient. The MixPool block that utilizes the segmentation mask from the previous epoch is then applied creating a hard-attention over the learnt feature maps. Next, we concatenate the feature maps from the last decoder block and the segmentation mask from the previous epoch. Finally, we apply a 1 × 1 convolution with the sigmoid activation function. The output of this is used to both minimize the training loss, using a combined binary cross-entropy and dice loss, and to generate segmentation masks that are stored as a run-length encoded (RLE) compression for each sample and propagated during the next epoch. The RLE is updated every epoch. Similarly, the network learns to adapt the weights in iterative training, this mechanism is also utilized during the test time. As shown in <ref type="figure">Fig. 1</ref>, test results are pruned in a few iterations during the test time. Unlike many methods in literature <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, here we utilize the same network and without any complimentary loss function optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Dataset and Evaluation Metrics:</head><p>To evaluate the proposed architecture, we have selected seven datasets that capture different segmentation tasks in biomedical imaging. The details of each dataset can be found in <ref type="table" target="#tab_0">Table I</ref>. The dataset images contain the images of organs and lesions acquired under different imaging protocols. For the retina vessel segmentation task, we use DRIVE and CHASE-DB1 datasets. These two datasets are aimed at various diseases related to diseases of retina vessels, such as hypertensive retinopathy, retinal vein occlusion, and retinal artery occlusion. The ISIC 2018 dataset, which is a dermoscopy dataset that is useful in the diagnosis of skin cancer, is the third dataset focused on medical imaging data. This dataset contains a wide variety of skin cancer images of different sizes and shapes, which helps in a better understanding of the disease. We have further included Kvasir-SEG and CVC-ClinicDB colonoscopy datasets. These datasets contain the image frames extracted from different colonoscopy interventions and are focused on colorectal polyps that are one of the cancer precursors in the colon and rectum. It highly increases the chance of avoiding lethal cancer by early detection. In addition, we have included two datasets acquired from biological imaging and are aimed at understanding of the cellular processes. These include the 2018 Data Science Bowl and the EM datasets. The 2018 Data Science Bowl dataset contains images with a large number of variable shaped nuclei acquired from different cell types, magnification, and imaging modalities. This dataset is designed for automated nuclei segmentation. Similarly, the EM dataset contains the transmission electron microscopy images of the neural structures of the Drosophila nerve cord. This dataset is aimed at the automated segmentation of the neural structures.</p><p>To evaluate SOTA deep learning methods and our proposed FANet, we have used standard evaluation metrics that includes Dice Coefficient (DSC) (a.k.a. F1), mean Intersection over Union (mIoU), precision, and recall. We have additionally calculated specificity for those datasets where this metric was previously used for benchmarking.</p><p>2) Implementation details: All the experiments are performed on a Volta 100 GPU and an NVIDIA DGX-2 system using the PyTorch 1.6. framework. Our model is trained for 100 epochs (empirically set) using an Adam optimizer with a learning rate of 1e −4 for all the experiments except for the Digital Retinal Images for Vessel Extraction (DRIVE) and the CHASE-DB1 dataset where the learning rate was adjusted to 1e −3 due to the small size of the training dataset. Datasets were chosen such that the efficiency of our model could be compared to the SOTA methods. A combination of binary cross-entropy and dice loss has been used as the loss function. ReduceLROnPlateau callback was used to monitor the learning rate and adjust it to obtain optimal training performance. All the images used in the study were resized to 512 × 512 except for the 2018 Data Science Bowl and the CVC-ClinicDB dataset, where images were resized to 256 × 256. Data augmentation, such as random crop, flipping, rotation, elastic transformation, grid distortion, optical distortion, grayscale conversion, random brightness, contrast, channel, and course dropout were used.</p><p>3) Ablation study: In order to demonstrate the strength of our proposed FANet architecture, we perform a thorough ablation study. For this, we have used all seven datasets and evaluated on several metrics for baseline (FANet without MixPool), baseline with MixPool, and the combination of baseline, MixPool, and feedback (proposed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Below we present quantitative results on seven different biomedical imaging datasets and compare with corresponding SOTA methods.</p><p>1) Results on Kvasir-SEG: Kvasir-SEG <ref type="bibr" target="#b40">[41]</ref> is a publicly available polyp segmentation dataset acquired from clinical colonoscopy (endoscopy) procedures. This dataset has been widely used for algorithm benchmarking. We have trained our model and compared it with recent SOTA methods on Kvasir-SEG. A comparison with widely accepted segmentation methods with different backbones (see <ref type="table" target="#tab_0">Table II</ref>) shows that our approach is improved performance compared to the SOTA methods (on the same train-test split). Our FANet outperforms all the SOTA methods on almost all metrics. While outperforming most U-Net and its variants, it can be observed  that FANet achieved an F1 score of 0.8803, which is 1.6% and 3.57% better than the most accurate DeepLabv3+ with ResNet101 backbone and the recent HRNet.</p><p>2) Results on CVC-ClinicDB dataset: CVC-ClinicDB is another commonly used dataset for colonoscopy image analysis. FANet architecture outperforms all the SOTA methods on this dataset by a large margin with F1 of 0.9355, mIoU of 0.8937, recall of 0.9339, and precision of 0.9401 (see <ref type="table" target="#tab_0">Table III</ref>). FANet achieves the best trade-off between recall and precision compared to the ResUNet-based architectures <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b50">[51]</ref>. The strength of the FANet can be observed by the large improvement of 23.17% in the recall and 5.24% in the precision over the SOTA ResUNet++ <ref type="bibr" target="#b16">[17]</ref>. The recall suggests that our method is more clinically preferable than the SOTA. A higher recall is desired in the systems used for clinical diagnosis <ref type="bibr" target="#b53">[54]</ref>.</p><p>3) Results on 2018 Data Science Bowl: Cell nuclei segmentation in microscopy imaging is a common task in the biological image analysis <ref type="bibr" target="#b42">[43]</ref>. We used the publicly available 2018 Data Science Bowl (DSB) challenge dataset and compared our results with the SOTA methods. <ref type="table" target="#tab_0">Table IV</ref> shows that FANet produces an F1 of 0.9176, mIoU of 0.8569, and recall of 0.9222 with an improvement of 2.02% in F1 with respect to SOTA UNet++ <ref type="bibr" target="#b15">[16]</ref> and 28.15% improvement in recall compared to the best performing DoubleU-Net <ref type="bibr" target="#b17">[18]</ref>. In general, FANet achieves the best trade-off between precision and recall compared to the SOTA methods resulting in the highest F1 score (0.9176). The qualitative results with 2018 DSB also show that the predicted FANet produces high-quality segmentation masks for cell nuclei with respect to the ground truth (see <ref type="figure" target="#fig_2">Figure 3</ref>). 4) Results on ISIC 2018 dataset: Skin cancer is one of the most commonly diagnosed cancers in the US. Early detection of melanoma can improve the five-year survival rate and help prevent it in 99% of the cases <ref type="bibr" target="#b56">[57]</ref>. <ref type="table" target="#tab_4">Table V</ref> shows the results on the publicly available International Skin Imaging Collaboration (ISIC) 2018 dataset. FANet outperformed all the methods on almost all evaluation metrics (F1, mIoU, and recall). FANet achieved 0.8731 on F1 and recall of 0.8650 with an improvement of 2.21% and 8.00%, respectively, over the most accurate SOTA BCDU-Net (d=3) method. A competitive  <ref type="bibr" target="#b20">[21]</ref> 0.6650 0.5660 0.7170 0.9670 -R2U-Net <ref type="bibr" target="#b54">[55]</ref> 0.6790 0.5810 0.7920 0.9280 -Attention R2U-Net <ref type="bibr" target="#b54">[55]</ref> 0.6910 0.5920 0.7260 0.9710 -BCDU-Net (d=1) <ref type="bibr" target="#b55">[56]</ref> 0.8470 -0.7830 0.9800 -BCDU-Net (d=3) <ref type="bibr" target="#b55">[56]</ref> 0.8510 -0.7850 0.9820 -FANet 0.8731 0.8023 0.8650 0.9611 0.9235  specificity and precision were also recorded. From the qualitative results in <ref type="figure" target="#fig_2">Figure 3</ref>, we can see that the input mask produced by Otsu thresholding shows under segmentation, which is improved significantly using FANet. The masks produced by FANet have smooth boundaries. 5) Results on DRIVE dataset: The automated segmentation of vessels in fundus images can assist in the diagnosis and treatment of diabetic retinopathy. The quantitative result on the publicly available DRIVE dataset is presented in <ref type="table" target="#tab_0">Table VI</ref>. We can observe that the proposed FANet achieves an F1 score of 0.8183, mIoU of 0.6927, recall of 0.8215, and precision of 0.8189. The proposed method achieves an improvement of 4.24% in the recall over SOTA IterNet <ref type="bibr" target="#b58">[59]</ref>. Although the F1 of the IterNet is 0.35% higher than FANet, the recall is relatively lower, and other metrics such as mIoU and precision are not presented. For our proposed FANet, the precision of 0.8189 is well balanced with the obtained recall. The higher recall produced by FANet shows that our method is more clinically relevant. The quality of the segmentation masks in Figure3 demonstrates the efficiency of FANet.</p><p>6) Results on CHASE-DB1 dataset: CHASE-DB1 is the second retinal image segmentation dataset used to evaluate our method. For this dataset, there is no official training and test split. We have used 20 images to train our model and 8 images to test as reported in the work of Li et al. <ref type="bibr" target="#b58">[59]</ref>. From Table VII, we can observe that our method achieved the highest F1 of 0.8108, mIoU of 0.6820, and the highest recall   IX: Detailed ablation study of the FANet architecture. Flop is calculated in terms of GMac. "Rec" stands for Recall, "Prec" stands for precision, "Spec" stands for Specificity, "Acc" stands for Accuracy, and "Param" stands for total number parameters. B1 -B4 denotes different network configurations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative results</head><p>The qualitative results on all seven datasets are presented in <ref type="figure" target="#fig_2">Fig. 3</ref>. It can be observed that for colonoscopy datasets (Kvasir-SEG and CVC-ClinicDB), even though the initial input mask covers the entirety of the image, our model is able to prune and provide accurate masks. The same can be observed for the two retina vessel segmentation datasets, DRIVE and CHASE-DB1. It can be observed that our model is able to segment the challenging retinal vessels, including small retinal vessel bifurcations, and it well resembles the ground truth mask. For the 2018 DSB, ISIC-2018, and EM cell data, again, the input masks are finely rectified, achieving close to ground truth results by the proposed FANet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study</head><p>In this section, we ablate our model architecture and present extensive experimental results that show the effectiveness of proposed FANet. To evaluate the contribution of the MixPool block and the feedback, we created the following configurations: and decoder blocks and the feedback mechanism is used during the inference. <ref type="table" target="#tab_0">Table IX</ref> presents the ablation results on these four configurations performed on all seven datasets. Below we provide detailed analyses of the use of different model architectural settings and validate them with the above described four network configurations (B1-B4):</p><p>1) Effectiveness of MixPool block: The MixPool block is an essential part of the proposed FANet architecture. It uses the previously predicted mask as the attention to improve the semantically meaningful features and allows higher-level abstractions. The effectiveness of the MixPool block can be evaluated by comparing the network configurations B1 and B4.</p><p>From the experiments in <ref type="table" target="#tab_0">Table IX</ref>, we can conclude that the B4 outperforms the B1 on all the datasets. On the F1 metric, B4 shows an improvement of 2.87% on the Kvasir-SEG dataset, 1.89% improvement on the CVC-ClinicDB, 0.55% improvement on the 2018 Data Science Bowl dataset, 0.84% improvement on the ISIC 2018 dataset, 0.11% improvement on the DRIVE dataset, 2.92% improvement on the CHASE-DB1 dataset, and a 0.03% improvement on the EM dataset. These performance gains are significant and thus demonstrate the effectiveness of the use of MixPool block in the proposed FANet.</p><p>2) Optimum position of Mixpool block in FANet architecture: The positioning of the MixPool is an important factor determining the performance of the model. In the FANet (B4), we integrate the MixPool block in all the encoder blocks and the decoder blocks. In B3, we integrate MixPool block in the first encoder block and the last decoder block only. To evaluate the effectiveness of the integrating MixPool block, we compare B3 with B4 in <ref type="table" target="#tab_0">Table IX</ref>. It can be observed that out of the seven datasets, on three datasets, i.e., Kvasir-SEG, CVC-ClinicDB, and 2018 Data Science Bowl, a significant improvement in B4 is observed as compared to the B3. On the F1 metric, we can observe that B4 achieves an improvement of 3.43 % on Kvasir-SEG, 1.93 % on CVC-ClinicDB, 0.11 % on the 2018 Data Science Bowl, and 0.5 % on the EM dataset.</p><p>3) Significance of feedback during evaluation: The proposed architecture uses the feedback information (input mask) while training. This feedback mechanism is also used during the evaluation. To check the effectiveness of the feedback mechanism, we compare the B2 (FANet without feedback) with the B4 (FANet with feedback) in <ref type="table" target="#tab_0">Table IX</ref>. On all the datasets, we used feedback during inference and compared its performance with the model without feedback. We can observe that the majority of performance gains in mIoU and F1. For Kvasir-SEG, B4 shows a 17.75 % improvement in the mIOU, 15.01 % improvement in the F1, and a 20.76 % improvement in the Recall. Likewise, on the CVC-ClinicDB, we can see that B4 has 3.96 % improvement in mIoU and 2.27 % improvement in the F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>With the FANet architecture, we proposed a novel approach for biomedical image segmentation that can self-rectify the predicted masks. By introducing a feedback mechanism, we achieved an improvement on seven publicly available biomedical datasets when compared with existing SOTA methods. Our approach required far fewer epochs for training and is well-suited to diverse biomedical imaging datasets. The feedback mechanism integrated in FANet design effectively acts as hard attention that is used with the existing feature maps to boost the strength of feature representations. The experimental results demonstrated that the proposed architecture achieves accurate and consistent segmentation results across several biomedical imaging datasets despite its simple and straightforward network architecture. The ablation study also revealed that the FANet requires less training time to achieve near SOTA performance. In the future, we will use a contrastive learning approach to improve the performance of FANet further on additional multimodal biomedical images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Index Terms-Medical image segmentation, deep learning, feedback attention, colon polyps, skin lesion, retinal vessels, cell nuclei, lung segmentation I. INTRODUCTION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Feedback attention network with squeeze and excite residual (SE-Residual) block and MixPool block. (a) SE-Residual block integrated with squeeze and excite layer uses 1 × 1 convolution to concatenate the high-resolution feature representation with the the encoded feature vector. (b) MixPool block represents attention mechanism in our network. The input mask is downscaled to the corresponding layer feature map size M l which is fused with the masked feature map representation M l for hard attention of input feature in that layer F l . Finally, the attenuated feature map F M l and the feature maps F l are both concatenated. c) Proposed FANet showing the complete network architecture. Encoder-decoder architecture with skipconnections (in dotted arrows) from SE-Residual blocks to preserve high-and intermediate resolution feature representations and MixPool block connections (with solid arrows) that allow to feedback the previous mask predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results of FANet on seven biomedical image segmentation datasets. The initial "input mask" is an output from Otsu thresholding for which a threshold of 0.5 was applied. The final "output mask" is the predicted segmentation mask from FANet model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Details of the biomedical datasets used in our experiments. "Train", "Train after aug." and "Test" denote the number of training image sample, number of training sample after image augmentation, and number of test sample used in the study.</figDesc><table><row><cell>Dataset</cell><cell>Images</cell><cell>Size</cell><cell>Train</cell><cell>Train after Aug.</cell><cell>Test</cell><cell>Application</cell></row><row><cell>Kvasir-SEG [41]</cell><cell>1000</cell><cell>Variable</cell><cell>880</cell><cell>16720</cell><cell>120</cell><cell>Colonoscopy</cell></row><row><cell>CVC-ClinicDB [42]</cell><cell>612</cell><cell>384 × 288</cell><cell>490</cell><cell>14210</cell><cell>61</cell><cell>Colonoscopy</cell></row><row><cell>2018 Data Science Bowl [43]</cell><cell>670</cell><cell>256 × 256</cell><cell>335</cell><cell>10720</cell><cell>134</cell><cell>Nuclie</cell></row><row><cell>ISIC 2018  † [44], [45]</cell><cell>2596</cell><cell>Variable</cell><cell>1815</cell><cell>39930</cell><cell>259</cell><cell>Dermoscopy</cell></row><row><cell>EM dataset [46]</cell><cell>30</cell><cell>512 × 512</cell><cell>24</cell><cell>384</cell><cell>3</cell><cell>Cell</cell></row><row><cell>DRIVE Database [47]</cell><cell>40</cell><cell>584 × 565</cell><cell>20</cell><cell>640</cell><cell>20</cell><cell>Retina Vessel</cell></row><row><cell>CHASE-DB1 [48]</cell><cell>28</cell><cell>584 × 565</cell><cell>20</cell><cell>640</cell><cell>8</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Results on the Kvasir-SEG<ref type="bibr" target="#b40">[41]</ref> </figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Prec.</cell></row><row><cell>U-Net [10]</cell><cell>-</cell><cell>0.5969</cell><cell>0.4713</cell><cell>0.6171</cell><cell>0.6722</cell></row><row><cell>ResUNet [51]</cell><cell>-</cell><cell>0.6902</cell><cell>0.5721</cell><cell>0.7248</cell><cell>0.7454</cell></row><row><cell>ResUNet++ [17]</cell><cell>-</cell><cell>0.7143</cell><cell>0.6126</cell><cell>0.7419</cell><cell>0.7836</cell></row><row><cell>FCN8 [9]</cell><cell>VGG 16</cell><cell>0.8310</cell><cell>0.7365</cell><cell>0.8346</cell><cell>0.8817</cell></row><row><cell>HRNet [13]</cell><cell>-</cell><cell>0.8446</cell><cell>0.7592</cell><cell>0.8588</cell><cell>0.8778</cell></row><row><cell>DoubleU-Net [18]</cell><cell>VGG 19</cell><cell>0.8129</cell><cell>0.7332</cell><cell>0.8402</cell><cell>0.8611</cell></row><row><cell>PSPNet [12]</cell><cell>ResNet50</cell><cell>0.8406</cell><cell>0.7444</cell><cell>0.8357</cell><cell>0.8901</cell></row><row><cell>DeepLabv3+ [52]</cell><cell>MobileNet</cell><cell>0.8425</cell><cell>0.7575</cell><cell>0.8377</cell><cell>0.9014</cell></row><row><cell>DeepLabv3+ [52]</cell><cell>ResNet50</cell><cell>0.8572</cell><cell>0.7759</cell><cell>0.8616</cell><cell>0.8907</cell></row><row><cell>DeepLabv3+ [52]</cell><cell>ResNet101</cell><cell>0.8643</cell><cell>0.7862</cell><cell>0.8592</cell><cell>0.9064</cell></row><row><cell>U-Net [10]</cell><cell>VGG19</cell><cell>0.7535</cell><cell>0.6571</cell><cell>0.7364</cell><cell>0.8565</cell></row><row><cell>FANet</cell><cell>-</cell><cell>0.8803</cell><cell>0.8153</cell><cell>0.9058</cell><cell>0.9005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Results on the CVC-ClinicDB<ref type="bibr" target="#b41">[42]</ref> </figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>U-Net (MICCAI'15) [10]</cell><cell>0.8230</cell><cell>0.7550</cell><cell>-</cell><cell>-</cell></row><row><cell>U-Net++ (TMI'19) [15]</cell><cell>0.7940</cell><cell>0.7290</cell><cell>-</cell><cell>-</cell></row><row><cell>ResUNet-mod [51]</cell><cell>0.7788</cell><cell>0.4545</cell><cell>0.6683</cell><cell>0.8877</cell></row><row><cell>ResUNet++ [17]</cell><cell>0.7955</cell><cell>0.7962</cell><cell>0.7022</cell><cell>0.8785</cell></row><row><cell>SFA (MICCAI'19) [53]</cell><cell>0.7000</cell><cell>0.6070</cell><cell>-</cell><cell>-</cell></row><row><cell>PraNet [14]</cell><cell>0.8990</cell><cell>0.8490</cell><cell>-</cell><cell>-</cell></row><row><cell>FANet</cell><cell>0.9355</cell><cell>0.8937</cell><cell>0.9339</cell><cell>0.9401</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Results on the 2018 Data Science Bowl<ref type="bibr" target="#b42">[43]</ref> </figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Prec.</cell></row><row><cell>U-Net [10]</cell><cell>ResNet101</cell><cell>0.7573</cell><cell>0.9103</cell><cell>-</cell><cell>-</cell></row><row><cell>U-Net++ [16]</cell><cell>ResNet101</cell><cell>0.8974</cell><cell>0.9255</cell><cell>-</cell><cell>-</cell></row><row><cell>DoubleU-Net [18]</cell><cell>VGG19</cell><cell>0.7683</cell><cell>0.8407</cell><cell>0.6407</cell><cell>0.9596</cell></row><row><cell>FANet</cell><cell>None</cell><cell>0.9176</cell><cell>0.8569</cell><cell>0.9222</cell><cell>0.9194</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Results on the ISIC 2018 (Skin Caner Segmentation)<ref type="bibr" target="#b43">[44]</ref>,<ref type="bibr" target="#b44">[45]</ref> </figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Spec.</cell><cell>Prec.</cell></row><row><cell>U-Net [10]</cell><cell>0.6740</cell><cell>0.5490</cell><cell>0.7080</cell><cell>0.9640</cell><cell>-</cell></row><row><cell>Attention U-Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Results on the DRIVE dataset<ref type="bibr" target="#b46">[47]</ref> </figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Spec.</cell><cell>Prec.</cell></row><row><cell>U-Net [10]</cell><cell>0.8174</cell><cell>-</cell><cell>0.7822</cell><cell>0.9808</cell><cell>-</cell></row><row><cell>Residual U-Net [55]</cell><cell>0.8149</cell><cell>-</cell><cell>0.7726</cell><cell>0.9820</cell><cell>-</cell></row><row><cell>Recurrent U-Net [55]</cell><cell>0.8155</cell><cell>-</cell><cell>0.7751</cell><cell>0.9816</cell><cell>-</cell></row><row><cell>R2U-Net [55]</cell><cell>0.8171</cell><cell>-</cell><cell>0.7792</cell><cell>0.9813</cell><cell>-</cell></row><row><cell>DenseBlock-UNet 1</cell><cell>0.8146</cell><cell>-</cell><cell>0.7928</cell><cell>0.9776</cell><cell>-</cell></row><row><cell>DUNet [58]</cell><cell>0.8190</cell><cell>-</cell><cell>0.7863</cell><cell>0.9805</cell><cell>-</cell></row><row><cell>IterNet [59]</cell><cell>0.8218</cell><cell>-</cell><cell>0.7791</cell><cell>0.9831</cell><cell>-</cell></row><row><cell>IterNet(Patched) [59]</cell><cell>0.8205</cell><cell>-</cell><cell>0.7235</cell><cell>0.9838</cell><cell>-</cell></row><row><cell>FANet</cell><cell>0.8183</cell><cell>0.6927</cell><cell>0.8215</cell><cell>0.9826</cell><cell>0.8189</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>Results on the CHASE-DB1 dataset<ref type="bibr" target="#b47">[48]</ref> </figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Spec.</cell><cell>Prec.</cell></row><row><cell>U-Net [10]</cell><cell>0.7993</cell><cell>-</cell><cell>0.7840</cell><cell>0.9880</cell><cell>-</cell></row><row><cell>DenseBlock-UNet 2</cell><cell>0.8005</cell><cell>-</cell><cell>0.8177</cell><cell>0.9848</cell><cell>-</cell></row><row><cell>DUNet [58]</cell><cell>0.8000</cell><cell>-</cell><cell>0.7858</cell><cell>0.9880</cell><cell>-</cell></row><row><cell>IterNet [59]</cell><cell>0.8072</cell><cell>-</cell><cell>0.7969</cell><cell>0.9881</cell><cell>-</cell></row><row><cell>FANet</cell><cell>0.8108</cell><cell>0.6820</cell><cell>0.8544</cell><cell>0.9830</cell><cell>0.7722</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>Results on the EM dataset<ref type="bibr" target="#b45">[46]</ref> EM dataset aims to develop an automatic ML algorithm for the segmentation of the neural structures so that difficulties due to manual labeling can be resolved.Table VIIIshows the quantitative results on the EM dataset. The proposed FANet also obtains F1 of 0.9547,</figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>mIoU</cell><cell>Recall</cell><cell>Specificity</cell><cell>Prec.</cell></row><row><cell>U-Net [10]</cell><cell>-</cell><cell>0.8830</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wide U-Net [15]</cell><cell>-</cell><cell>0.8837</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>U-Net++ [16]</cell><cell>-</cell><cell>0.8933</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FANet</cell><cell>0.9547</cell><cell>0.9134</cell><cell>0.9568</cell><cell>0.8096</cell><cell>0.9529</cell></row><row><cell cols="6">of 0.8544. FANet achieved an improvement of 3.67% in the</cell></row><row><cell cols="5">recall compared to the SOTA DenseBlock-UNet.</cell><cell></cell></row><row><cell cols="3">7) Results on EM dataset:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recurrent unet for resource-constrained segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2142" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1803.01271</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural network with attention mechanism for punctuation restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alumäe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTER-SPEECH</title>
		<meeting>INTER-SPEECH</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention gated networks: Learning to leverage salient regions in medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="65" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI, 2020</title>
		<meeting>MICCAI, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UNet++: A nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DLMIAMLCDS</title>
		<meeting>DLMIAMLCDS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">UNet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ResUnet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISM</title>
		<meeting>ISM</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DoubleU-Net: a deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CBMS</title>
		<meeting>CBMS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SA-UNet: spatial attention u-net for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szemenyei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03696</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the U-Net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Attention U-Net: Learning where to look for the pancreas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Boundary-aware context neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Evaluation of Deep Segmentation Models for the Extraction of Retinal Lesions from Multimodal Retinal Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Usman</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Colorectal polyp segmentation by u-net with dilation convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMLA</title>
		<meeting>ICMLA</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="851" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5669" to="5678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An overview of deep learning in medical imaging focusing on MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Lundervold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lundervold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Medizinische Physik</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="127" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feedback attention for cell image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCVW</title>
		<meeting>ECCVW</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feedback U-Net for cell image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Worksh</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Worksh</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="974" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional LSTM Network: A machine learning approach for precipitation Nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond the pixel-wise loss for topology-aware delineation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koziński</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images Through Generative Latent Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pandey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="413" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MMM, 2020</title>
		<meeting>MMM, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Med Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nucleus segmentation across imaging experiments: the 2018 data science bowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1247" to="1253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (ISBI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISBI</title>
		<meeting>ISBI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180161</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An integrated micro-and macroarchitectural analysis of the Drosophila brain by computer-assisted serial section electron microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1000502</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Measuring retinal vessel tortuosity in 10-year-old children: validation of the computer-assisted image analysis of the retina (CAIAR) program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Owen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ARVO</publisher>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote. Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Selective feature aggregation network with area-boundary constraints for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The missing pieces of artificial intelligence in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gilvary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Madhukar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elkhader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elemento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Pharmacol. Sci</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="555" to="564" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent residual convolutional neural network based on U-Net (r2u-net) for medical image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bidirectional ConvLSTM U-Net with densely connected convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asadi-Aghbolaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCVW</title>
		<meeting>ICCVW</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Cancer facts &amp; figures 2018</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Society</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">DUNet: a deformable network for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">IterNet: retinal image segmentation utilizing structural redundancy in vessel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WCACV, 2020</title>
		<meeting>WCACV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="3656" to="3665" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
