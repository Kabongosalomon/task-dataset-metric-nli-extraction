<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BMN: Boundary-Matching Network for Temporal Action Proposal Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
							<email>lintianwei01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao12@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
							<email>wenshilei@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BMN: Boundary-Matching Network for Temporal Action Proposal Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve stateof-the-art temporal action detection performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the number of videos in Internet growing rapidly, video content analysis methods have attracted widespread attention from both academia and industry. Temporal action detection is an important task in video content analysis area, which aims to locate action instances in untrimmed long videos with both action categories and temporal boundaries. Akin to object detection, temporal action detection method can be divided into two stages: temporal action proposal generation and action classification. Although convincing classification accuracy can be achieved by action recognition methods, the detection performance is still low in mainstream benchmarks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref>. Therefore, many recent methods work on improving the quality of temporal action proposals. Besides being used in temporal action detection task, temporal proposal generation methods also have wide applications in many areas such as video recommendation, video highlight detection and smart surveillance.</p><p>To achieve high proposal quality, a proposal generation method should (1) generate temporal proposals with flexible duration and precise boundaries to cover groundtruth action instances precisely and exhaustively; (2) generate reliable confidence scores so that proposals can be retrieved properly. Most existing proposal generation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref> adopted a "top-down" fashion to generate proposals with multi-scale temporal sliding windows in regular interval, and then evaluate confidence scores of proposals respectively or simultaneously. The main drawback of these methods is that generated proposals are usually not temporally precise or not flexible enough to cover ground-truth action instances of varies duration. Recently, Boundary-Sensitive Network (BSN) <ref type="bibr" target="#b16">[17]</ref> adopted a "bottom-up" fashion to generate proposals in two stages:</p><p>(1) locate temporal boundaries and combine boundaries as proposals and (2) evaluate confidence score of each proposal using constructed proposal feature. By exploiting local clues, BSN can generate proposals with more precise boundaries and more flexible duration than existing topdown methods. However, BSN has three main drawbacks: <ref type="bibr" target="#b0">(1)</ref> proposal feature construction and confidence evaluation procedures are conducted to each proposal respectively, leading to inefficiency; (2) the proposal feature constructed in BSN is too simple to capture enough temporal context;</p><p>(3) BSN is multiple-stage but not an unified framework.</p><p>Can we evaluate confidence for all proposals simultaneously with rich context? Top-down methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref> can achieve this easily with anchor mechanism, where proposals are pre-defined as non-continuous distributed anchors. However, since the boundary and duration of proposals are much more flexible, anchor mechanism is not suitable for bottom-up methods such as BSN. To address these difficulties, we propose the Boundary-Matching (BM) mechanism for confidence evaluation of densely distributed proposals. In BM mechanism, a proposal is denoted as a matching pair of its starting and ending boundaries, and then all BM pairs are combined as a two dimensional BM confidence map to represent densely distributed proposals with continuous starting boundaries and temporal duration. Thus, we can generate confidence scores for all proposals simultaneously via the BM confidence map. A BM layer is proposed to generate BM feature map from temporal feature sequence, and the BM confidence map can be obtained from the BM feature map using a series of conv-layers. BM feature map contains rich feature and temporal context for each proposal, and gives the potential for exploiting context of adjacent proposals.</p><p>In summary, our work has three main contributions:</p><p>1. We introduce the Boundary-Matching mechanism for evaluating confidence scores of densely distributed proposals, which can be easily embedded in network. 2. We propose an efficient, effective and end-to-end temporal action proposal generation method Boundary-Matching Network (BMN). Temporal boundary probability sequence and BM confidence map are generated simultaneously in two branches of BMN, which are trained jointly as an unified framework. 3. Extensive experiments show that BMN can achieve significantly better proposal generation performance than other state-of-the-art methods, with remarkable efficiency, great generalizability and great performance on temporal action detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action Recognition. Action recognition is a fundamental and important task of video understanding area. Handcrafted features such as HOG, HOF and MBH are widely used in earlier works, such as improved Dense Trajectory (iDT) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. Recently, deep learning models have achieved significantly performance promotion in action recognition task. The mainstream networks fall into two categories: two-stream networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref> exploit appearance and motion clues from RGB image and stacked optical flow separately; 3D networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21]</ref> exploit appearance and motion clues directly from raw video volume. In our work, by convention, we adopt action recognition models to extract visual feature sequence of untrimmed video.</p><p>Correlation Matching. Correlation matching algorithms are widely used in many computer vision tasks, such as image registration, action recognition and stereo matching. Specifically, stereo matching aims to find corresponding pixels from stereo images. For each pixel in left image of a rectified image pair, the stereo matching method need to find corresponding pixel in right image along horizontal direction, or we can say finding right pixel with minimum cost. Thus, the cost minimization of all left pixels can be denoted as a cost volume, which denotes each leftright pixel pair as a point in volume. Based on cost volume, many recent works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref> achieve end-to-end network via generating cost volume directly from combining two feature maps, using correlation layer <ref type="bibr" target="#b19">[20]</ref> or feature concatenation <ref type="bibr" target="#b5">[6]</ref>. Inspired by cost volume, our proposed BM confidence map contains pairs of temporal starting and ending boundaries as proposals, thus can directly generate confidence scores for all proposals using convolutional layers. We propose BM layer to efficiently generate BM feature map via sampling feature among starting and ending boundaries of each proposal simultaneously.</p><p>Temporal Action Proposal Generation. As aforementioned, the goal of temporal action detection task is to detect action instances in untrimmed videos with temporal boundaries and action categories, which can be divided into temporal proposal generation and action classification stages. These two stages are taken apart in most detection methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>, and are taken together as single model in some methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref>. For proposal generation task, most previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref> adopt top-down fashion to generate proposals with pre-defined duration and interval, where the main drawback is the lack of boundary precision and duration flexibility. There are also some methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17]</ref> adopt bottom-up fashion. TAG <ref type="bibr" target="#b34">[35]</ref> generates proposals using temporal watershed algorithm, but lack confidence scores for retrieving. Recently, BSN <ref type="bibr" target="#b16">[17]</ref> generates proposals via locally locating temporal boundaries and globally evaluating confidence scores, and achieves significant performance promotion over previous proposal generation methods. In this work, we propose the Boundary-Matching mechanism for proposal confidence evaluation, which can largely simplify the pipeline of BSN and bring significant promotion in both efficiency and effectiveness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We can denote an untrimmed video X as frame sequence X = {x n } lv n=1 with l v frames, where x n is the n-th RGB frame of video X. The temporal annotation set of X is composed by a set of temporal action instances as Ψ g = {ϕ n = (t s,n , t e,n )} Ng n=1 , where N g is the amount of ground-truth action instances, t s,n is the starting time of action instance ϕ n and t e,n is the ending time. Unlike temporal action detection task, categories of action instances are not taken into account in proposal generation task. During inference, proposal generation method should generate proposals Ψ p which cover Ψ g precisely and exhaustively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Encoding.</head><p>Following recent proposal generation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>, we construct BMN model upon visual feature sequence extracted from raw video. In this work, we adopt twostream network <ref type="bibr" target="#b23">[24]</ref> for feature encoding since it achieves great action recognition precision and is widely used in many video analysis methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref>. Concatenating the output scores of top fc-layer in two-stream network, we can get encoded visual feature f tn ∈ R C around frame x tn , where C is the dimension of feature. Therefore, given an untrimmed video X of length l v , we can extract a visual</p><formula xml:id="formula_0">feature sequence F = {f tn } l f n=1 ∈ R C×l f with length l f .</formula><p>To reduce the computation cost, we extract feature in a regular frame interval σ, thus l f = l v /σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Boundary-Matching Mechanism</head><p>In this section, we introduce the Boundary-Matching (BM) mechanism to generate confidence scores for densely distributed proposals. First we denote a temporal proposal ϕ as a matching pair of its starting boundary t s and ending boundary t e . Then, as shown in <ref type="figure" target="#fig_1">Fig 2,</ref> the goal of BM mechanism is to generate the two dimensional BM confidence . map M C , which is constructed by BM pairs with different starting boundary and temporal duration. In BM confidence map, the value of point M C (i, j) is denoted as the confidence score of proposal ϕ i,j with starting boundary t s = t j , duration d = t i and ending boundary t e = t j + t i . Thus, we can generate confidence scores for densely distributed proposals via generating BM confidence map. Boundary-Matching Layer. How can we generate two dimensional BM confidence map from temporal feature sequence? In BM mechanism, we introduce the BM layer to generate BM feature map M F ∈ R C×N ×D×T from temporal feature sequence S F ∈ R C×T , and then use M F to generate BM confidence map M C ∈ R D×T with a series of convolutional layers, where D are pre-defined maximum proposal duration. The goal of BM layer is to uniformly sample N points in S F between starting boundary t s and ending boundary t e of each proposal ϕ i,j , and get proposal feature m f i,j ∈ R C×N with rich context. And we can generate BM feature map M F via conducting this sampling procedure for all proposals simultaneously.</p><p>There are two difficulties to achieve this feature sampling procedure: (1) how to sample feature in non-integer point and (2) how to sample feature for all proposals simultaneously. As shown in <ref type="figure" target="#fig_2">Fig 3,</ref> we achieve this via dot product between temporal feature sequence S F ∈ R C×T and sampling mask weight W ∈ R N ×T ×D×T in temporal dimension. In detail, first, for each proposal ϕ i,j , we construct weight term w i,j ∈ R N ×T via uniformly sampling N points between expanded temporal region [t s − 0.25d, t e + 0.25d]. For a non-integer sampling point t n , we define its corresponding sampling mask w i,j,n ∈ R T as where dec and f loor is decimal and integer fraction functions separately. Thus, for proposal ϕ i,j , we can get weight term w i,j ∈ R N ×T . Second, we conduct dot product in temporal dimension between S F and w i,j</p><formula xml:id="formula_1">w i,j,n [t] =      1 − dec(t n ) if t = f loor(t n ) dec(t n ) if t = f loor(t n ) + 1, 0 if t = others<label>(1)</label></formula><formula xml:id="formula_2">m f i,j [c, n] = T t=1 S f [c, t] · w i,j [n, t].<label>(2)</label></formula><p>Via expanding w i,j ∈ R N ×T to W ∈ R N ×T ×D×T for all proposals in BM confidence map, we can generate BM feature map M F ∈ R C×N ×D×T using dot product. Since the sampling mask weight W is the same for different videos and can be pre-generated, the inference speed of BM layer is very fast. BM feature map contains rich feature and temporal context for each proposal, and gives the potential for exploiting context of adjacent proposals. Boundary-Matching Label. During training, we denote the BM label map as G C ∈ R D×T with the same shape of BM confidence map M C , where g c i,j ∈ [0, 1] represents the maximum IoU between proposal ϕ i,j and all groundtruth action instances. Generally, in BM mechanism, we use BM layer to efficiently generate BM feature map M F from temporal feature sequence S F , and then use a series of convolutional layers to generate BM confidence map M C , which is trained under supervision of BM label map G C . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Boundary-Matching Network</head><p>Different with the multiple-stage framework of BSN <ref type="bibr" target="#b16">[17]</ref>, BMN generates local boundary probabilities sequence and global proposal confidence map simultaneously, while the whole model is trained in an unified framework. As demonstrated in <ref type="figure" target="#fig_3">Fig 4</ref>, BMN model contains three modules: Base Module handles the input feature sequence, and outputs feature sequence shared by the following two modules; Temporal Evaluation Module evaluates starting and ending probabilities of each location in video to generate boundary probability sequences; Proposal Evaluation Module contains the BM layer to transfer feature sequence to BM feature map, and contains a series of 3D and 2D convolutional layers to generate BM confidence map. Base Module. The goal of the base module is to handle the input feature sequence, expand the receptive field and serve as backbone of network, to provide a shared feature sequence for TEM and PEM. Since untrimmed videos have uncertain temporal length, we adopt a long observation window with length l ω to truncate the untrimmed feature sequence with length l f . We denote an observation window as ω = {t ω,s , t ω,e , Ψ ω , F ω }, where t ω,s and t ω,e are the starting and ending time of ω separately, Ψ ω and F ω are annotations and feature sequence within the window separately. The window length l ω = t ω,e − t ω,s is set depending on the dataset. The details of base module is shown in <ref type="table" target="#tab_0">Table  1</ref>, including two temporal convolutional layers. Temporal Evaluation Module (TEM). The goal of TEM is to evaluate the starting and ending probabilities for all temporal locations in untrimmed video. These boundary probability sequences are used for generating proposals during post processing. The details of TEM are shown in <ref type="table" target="#tab_0">Table  1</ref>, where conv1d 4 layer with two sigmoid activated filters output starting probability sequence P S,ω = p s tn lω n=1 and ending probability sequence P E,ω = p e tn lω n=1 separately for an observation window ω. Proposal Evaluation Module (PEM). The goal of PEM is to generate Boundary-Matching (BM) confidence map, which contains confidence scores for densely distributed proposals. To achieve this, PEM contains BM layer and a series of 3d and 2d convolutional layers.</p><p>As introduced in Sec. 3.3, BM layer transfers temporal feature sequence S to BM feature map M F via matrix dot product between S and sampling mask weight W in temporal dimension. In BM layer, the number of sample points N is set to 32, and the maximum proposal duration D is set depending on dataset. After generating BM feature map M F , first we conduct conv3d 1 layer in sample dimension to reduce dimension length from N to 1, and increase hidden units from 128 to 512. Then, we conduct conv2d 1 layer with 1 × 1 kernel to reduce the hidden units, and conv2d 2 layer with 3 × 3 kernel to capture context of adjacent proposals. Finally, we generate two types of BM confidence map M CC , M CR ∈ R D×T with sigmoid activation, where M CC and M CR are trained using binary classification and regression loss function separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training of BMN</head><p>In BMN, TEM learns local boundary context and PEM pattern global proposal context. To jointly learn local pattern and global pattern, an unified multi-task framework is exploited for optimization. The training details of BMN are introduced in this section. Training Data Construction. Given an untrimmed video X, we can extract feature sequence F with length l f . Then, we use observation windows with length l ω to truncate feature sequence with 50% overlap, where windows containing at least one ground-truth action instance are kept for training. Thus, a training set Ω = {ω n } Nω n=1 is constructed with N ω observation windows. Label Assignment. For TEM, we need to generate temporal boundary label sequence G S , G E ∈ R T . Following BSN <ref type="bibr" target="#b16">[17]</ref>, for a ground-truth action instance ϕ g = (t s , t e ) with duration d g = t e − t s in annotation set Ψ ω , we denote its starting and ending regions as r S = [t s − d g /10, t s + d g /10] and r E = [t e − d g /10, t e + d g /10] separately. Then, for a temporal location t n within F ω , we denote its local region as r tn = [t n −d f /2, t n +d f /2], where d f = t n −t n−1 is the temporal interval between two locations. Then we calculate overlap ratio IoR of r tn with r S and r E separately, and denote maximum IoR as g s tn and g e tn separately, where IoR is defined as the overlap ratio with groundtruth proportional to the duration of this region. Thus we can generate G S,ω = g s tn lω n=1 and G E,ω = g e tn lω n=1 as label of TEM. For PEM, we need to generate BM label map G C ∈ R D×T . For a proposal ϕ i,j = (t s = t j , t e = t j + t i ), we calculate its Intersection-over-Union (IoU ) with all ϕ g in Ψ ω , and denote the maximum IoU as g c i,j . Thus we can generate G C = g c i,j D,lω i,j=1 as label of PEM. Loss of TEM. With generated boundary probability sequence P S,ω , P E,ω and boundary label sequence G S,ω , G E,ω , we can construct the loss function of TEM as the sum of staring and ending losses</p><formula xml:id="formula_3">L T EM = L bl (P S , G S ) + L bl (P E , G E ).<label>(3)</label></formula><p>Following BSN <ref type="bibr" target="#b16">[17]</ref>, we adopt weighted binary logistic regression loss function L bl for both starting and ending losses, where L bl (P, G) is denoted as: <ref type="bibr" target="#b3">(4)</ref> where b i = sign(g i − θ) is a two-value function used to convert g i from [0, 1] to {0, 1} based on overlap threshold θ = 0.5. Denoting l + = b i and l − = l ω − l + , the weighted terms are α + = lw l + and α − = lw l − . Loss of PEM. With generated BM confidence map M CC , M CR and BM label map G C , we can construct the loss function of PEM, which is the sum of binary classification loss and regression loss:</p><formula xml:id="formula_4">1 lω lω i=1 α + · bi · log(pi) + α − · (1 − bi) · log(1 − pi) ,</formula><formula xml:id="formula_5">L P EM = L C (M CC , G C ) + λ · L R (M CR , G C ).<label>(5)</label></formula><p>where we adopt L bl for classification loss L C and L2 loss for regression loss L R , and set the weight term λ = 10.</p><p>To balance the ratio between positive and negative samples in L R , we take all points with g c i,j &gt; 0.6 as positive and randomly sample g c i,j &lt; 0.2 as negative, and ensure the ratio between positive and negative points nearly 1:1. Training Objective. We train BMN in the form of a multitask loss function, including TEM loss, PEM loss and L2 regularization term:</p><formula xml:id="formula_6">L = L LEM + λ 1 · L GEM + λ 2 · L 2 (Θ),<label>(6)</label></formula><p>where weight term λ 1 and λ 2 are set to 1 and 0.0001 separately to ensure different modules are trained evenly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Inference of BMN</head><p>During inference, we use BMN to generate boundary probability sequences G S , G E and BM confidence map M CC , M CR . To get final results, we need to (1) generate candidate proposals using boundary probabilities, (2) fuse boundary probability and confidence score to generate final confidence score, (3) and suppress redundant proposals based on final confidence scores. Candidate Proposals Generation. Following BSN <ref type="bibr" target="#b16">[17]</ref>, we generate candidate proposals via combining temporal locations with high boundary probabilities. First, to locate high starting probability locations, we record all temporal locations t n with starting p s tn (1) higher than 0.5 · max(p) or (2) being a probability peak, where max(p s ) is the maximum starting probability of this video. These candidate starting locations are grouped as B S = {t s,i } N S i=1 . We can generate ending locations set B E in the same way.</p><p>Then we match each starting location t s in B S and ending location t e in B E as a proposal, if its duration is smaller than a pre-defined maximum duration D. The generated proposal ϕ is denoted as ϕ = (t s , t e , p s ts , p e te , p cc , p cr ), where p s ts , p e te are starting and ending probabilities in t s and t e separately, and p cc , p cr are classification confidence score and regression confidence score from [t e −t s , t s ] point of BM confidence map M CC and M CR separately. Thus we can get candidate proposals set Ψ = {ϕ i } Np i=1 , where N p is the number of candidate proposals. Score Fusion. To generate more reliable confidence scores, for each proposal ϕ, we fuse its boundary probabilities and confidence scores by multiplication to generate the final confidence score p f :</p><formula xml:id="formula_7">p f = p s ts · p e te · √ p cc · p cr .<label>(7)</label></formula><p>Thus, we can get candidate proposals set Ψ p =</p><formula xml:id="formula_8">{ϕ i = (t s , t e , p f )} Np i=1</formula><p>, where p f is used for proposals retrieving during redundant proposals suppression. Redundant Proposals Suppression. After generating candidate proposals, we need to remove redundant proposals to achieve higher recall with fewer proposals, where Nonmaximum suppression (NMS) algorithm is widely used for this purpose. In BMN, we mainly adopt Soft-NMS algorithm <ref type="bibr" target="#b0">[1]</ref>, since it has proven its effectiveness in proposal generation task <ref type="bibr" target="#b16">[17]</ref>. Soft-NMS algorithm suppresses redundant results via decaying their confidence scores. Soft-NMS generates suppressed final proposals set Ψ p = ϕ n = (t s , t e , p f )</p><formula xml:id="formula_9">N p n=1</formula><p>, where N p is the final proposals number. During experiment, we also try normal Greedy-NMS for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Setup</head><p>Dataset. We conduct experiments on two challenging datasets: THUMOS-14 <ref type="bibr" target="#b13">[14]</ref> dataset contains 413 temporal annotated untrimmed videos with 20 action categories; ActivityNet-1.3 <ref type="bibr" target="#b4">[5]</ref> is a large-scale action understanding dataset, containing action recognition, temporal detection, proposal generation and dense captioning tasks. ActivityNet-1.3 dataset contains 19994 temporal annotated untrimmed videos with 200 action categories, which are divided into training, validation and testing sets by ratio 2:1:1. Implementation Details. For feature encoding, following previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12]</ref>, we adopt two-stream network <ref type="bibr" target="#b32">[33]</ref>  pre-trained on training set of ActivityNet-1.3, where spatial and temporal sub-networks adopt ResNet and BN-Inception network separately. The frame interval σ is set to 5 and 16 on THUMOS-14 and ActivityNet-1.3 separately.</p><p>On THUMOS-14, we set the length of observation window l ω to 128 and the maximum duration length D to 64, which can cover length of 98% action instances. On Activi-tyNet, following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, we rescale each feature sequence to the length of the observation window l ω = 100 using linear interpolation, and the duration of corresponding annotations to range [0,1]. The maximum duration length D is set to 100, which can cover length of all action instances. To train BMN from scratch, we set learning rate to 0.001, batch size to 16 and epoch number to 10 for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Temporal Action Proposal Generation</head><p>The goal of proposal generation task is to generate high quality proposals to cover action instances with high recall and high temporal overlap.   Comparison with State-of-the-art Methods. <ref type="table" target="#tab_1">Table 2</ref> demonstrates the proposal generation performance comparison on validation and testing set of ActivityNet-1.3, where our method significantly outperforms other proposal generation methods. Especially, our method significantly improves AUC of validation set from 66.17% to 67.10% by 0.93%, which demonstrates that our method can achieve overall performance promotion. <ref type="table">Table 3</ref> demonstrates the proposal generation performance comparison on testing set of THUMOS-14. Since different feature encoding methods and redundant proposal suppression methods can affect performance largely, following BSN <ref type="bibr" target="#b16">[17]</ref>, we adopt both C3D and two-stream feature, both normal Greedy-NMS and Soft-NMS for fair comparison. Experiment results suggest that (1) based on either C3D or two-stream feature, our method outperforms other methods significantly when proposal number varies from 10 to 1000; (2) no matter Greedy-NMS or Soft-NMS is adopted, our method outperforms other methods significantly; (3) Soft-NMS can improve average recall performance especially under small proposal number, which is helpful for temporal action proposal generation task. These results together suggest the effectiveness of our method and its effectiveness mainly due to its own architecture. Qualitative results are shown in <ref type="figure" target="#fig_6">Fig 6.</ref> Ablation Comparison with BSN. To confirm the effect of the BM mechanism, we conduct more detailed ablation study and comparison of effectiveness and efficiency between BSN <ref type="bibr" target="#b16">[17]</ref> and BMN. To achieve this, we evaluate the proposal quality and speed of BSN and BMN under multiple ablation configuration. The experiment results are shown in <ref type="table" target="#tab_2">Table 4</ref> and <ref type="figure" target="#fig_5">Fig 5,</ref> which demonstrate that:</p><p>1. Under similar network architecture and training objective, TEMs of BSN and BMN achieve similar proposal quality and inference speed, which provides a reliable comparison baseline; 2. Adding separately trained PEM, both BSN and BMN obtain significant performance promotion, suggesting that PEM plays an important role in the "local to global" proposal generation framework; 3. Jointly trained BMN achieves higher recall and faster speed than separately trained BMN, suggesting the effectiveness and efficiency of overall optimization; 4. Adding separately trained PEM, BMN achieves significant faster speed than BSN, since BM mechanism can directly generate confidence scores for all proposals simultaneously, rather than one-by-one respectively in BSN. Thus, PEM based on BM mechanism is more efficient than original PEM. Combining TEM and PEM jointly can further improve the efficiency.</p><p>Thus, these ablation comparison experiments suggest the effectiveness and efficiency of our proposed Boundary-Matching mechanism and unified BMN network, which can generate reliable confidence scores for all proposals simultaneously in fast speed. Generalizability of Proposals. As a proposal generation method, an important property is the ability of generating high quality proposals for unseen action categories. To evaluate this property, following BSN <ref type="bibr" target="#b16">[17]</ref>, two un-overlapped action subsets: "Sports, Exercise, and Recreation" and "Socializing, Relaxing, and Leisure" of ActivityNet-1.3 are chosen as seen and unseen subsets separately. There are <ref type="table">Table 6</ref>. Action detection results on validation and testing set of ActivityNet-1.3, where our proposals are combined with videolevel classification results generated by <ref type="bibr" target="#b35">[36]</ref>.  <ref type="table" target="#tab_3">Table 5</ref> demonstrate that the performance drop is very slight in unseen categories, suggesting that BMN achieves great generalizability to generate high quality proposals for unseen actions, and can learn a general concept of when an action may occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Action Detection with Our Proposals</head><p>Another important aspect of evaluating the proposal quality is to put proposals in temporal action detection framework and evaluate its detection performance. Mean Average Precision (mAP) is adopted as the evaluation metric of temporal action detection task, where we calculate Average Precision (AP) on each action category respectively. mAP with IoU thresholds {0.5, 0.75, 0.95} and average mAP with IoU thresholds [0.5 : 0.05 : 0.95] are used on ActivityNet-1.3, while mAP with IoU thresholds {0.3, 0.4, 0.5, 0.6, 0.7} are used on THUMOS-14.</p><p>To achieve this, we adopt the two-stage "detection by classifying proposals" temporal action detection framework to combine BMN proposals with state-of-the-art action classifiers. Following BSN <ref type="bibr" target="#b16">[17]</ref>, on ActivityNet-1.3, we adopt top-1 video-level classification results generated by method <ref type="bibr" target="#b35">[36]</ref> and use confidence scores of BMN propos- als for detection results retrieving. On THUMOS-14, we use both top-2 video-level classification results generated by UntrimmedNet <ref type="bibr" target="#b30">[31]</ref>, and proposal-level SCNN-classifier to generate classification result for each proposal. For ActivityNet-1.3 and THUMOS-14 datasets, we use first 100 and 200 temporal proposals per video separately.</p><p>The experiment results on ActivityNet-1.3 are shown in <ref type="table">Table 6</ref>, which demonstrate that BMN proposals based detection framework significantly outperform other state-ofthe-art temporal action detection methods. The experiment results on THUMOS-14 are shown in <ref type="table">Table 7</ref>, which suggest that: (1) no matter video-level or proposal-level action classifier is used, our method achieves better detection performance than other state-of-the-art proposal generation methods; (2) using BMN proposals, video-level classifier <ref type="bibr" target="#b30">[31]</ref> achieves significant better performance than proposallevel classifier <ref type="bibr" target="#b22">[23]</ref>, indicating that BMN can generate confidence scores reliable enough for retrieving results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced the Boundary-Matching mechanism for evaluating confidence scores of densely distributed proposals, which is achieved via denoting proposal as BM pair and combining all proposals as BM confidence map. Meanwhile, we proposed the Boundary-Matching Network (BMN) for effective and efficient temporal action proposal generation, where BMN generates proposals with precise boundaries and flexible duration via combining high probability boundaries, and simultaneously generates reliable confidence scores for all proposals based on BM mechanism. Extensive experiments demonstrate that BMN outperforms other state-of-the-art proposal generation methods in both proposal generation and temporal action detection tasks, with remarkable efficiency and generalizability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of our method. Given an untrimmed video, BMN can simultaneously generate (1) boundary probabilities sequence to construct proposals and (2) Boundary-Matching confidence map to densely evaluate confidence of all proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of BM confidence map. Proposals in the same row have the same temporal duration, and proposals in the same column have the same starting time. The ending boundaries of proposals at right-bottom corner exceed the range of video, thus these proposals are not considered during training and inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of BM layer. For each proposal, we conduct dot product at T dimension between sampling weight and temporal feature sequence, to generate BM feature of shape C × N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The framework of Boundary-Matching Network. After feature extraction, we use BMN to simultaneously generate temporal boundary probability sequence and BM confidence map, and then construct proposals based on boundary probabilities and get corresponding confidence score from BM confidence map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>To evaluate proposal quality, Average Recall (AR) under multiple IoU thresholds are calculated. Following conventions, IoU thresholds [0.5 : 0.05 : 0.95] and [0.5 : 0.05 : 1.0] are used for ActivityNet-1.3 and THUMOS-14 separately. We calculate AR under different Average Number of proposals (AN) as AR@AN, and calculate the Area under the AR vs. AN curve (AUC) as metrics on ActivityNet-1.3, where AN is varied from 0 to 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Ablation comparison between BSN and BMN in terms of relative AR improvement (%) vs AN curve on validation set of ActivityNet-1.3, where relative AR improvement is calculated based on BSN-TEM results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Visualization examples of proposals and BM map generated by BMN on THUMOS-14 and ActivityNet-1.3 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The detailed architecture of BMN, where the output feature sequence of base module is shared by temporal evaluation and proposal evaluation modules. T and D are length of input feature sequence and maximum proposal duration separately.</figDesc><table><row><cell>layer</cell><cell cols="4">kernel stride dim act</cell><cell>output size</cell></row><row><cell></cell><cell></cell><cell cols="3">Base Module</cell></row><row><cell>conv1d1</cell><cell>3</cell><cell>1</cell><cell cols="2">256 relu</cell><cell>256×T</cell></row><row><cell>conv1d2</cell><cell>3</cell><cell>1</cell><cell cols="2">128 relu</cell><cell>128×T</cell></row><row><cell></cell><cell cols="4">Temporal Evaluation Module</cell></row><row><cell>conv1d3</cell><cell>3</cell><cell>1</cell><cell cols="2">256 relu</cell><cell>256×T</cell></row><row><cell>conv1d4</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell cols="2">sigmoid 2×T</cell></row><row><cell></cell><cell></cell><cell cols="3">Proposal Evaluation Module</cell></row><row><cell>BM layer</cell><cell></cell><cell></cell><cell>N -32</cell><cell></cell><cell>128×32×D×T</cell></row><row><cell>conv3d1</cell><cell cols="4">32,1,1 32,0,0 512 relu</cell><cell>512×1×D×T</cell></row><row><cell>squeeze</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>512×D×T</cell></row><row><cell>conv2d1</cell><cell>1,1</cell><cell>0,0</cell><cell cols="2">128 relu</cell><cell>128×D×T</cell></row><row><cell>conv2d2</cell><cell>3,3</cell><cell>1,1</cell><cell cols="2">128 relu</cell><cell>128×D×T</cell></row><row><cell>conv2d3</cell><cell>1,1</cell><cell>0,0</cell><cell>2</cell><cell cols="2">sigmoid 2×D×T</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison between our method and other state-of-theart temporal action proposal generation methods on validation set of ActivityNet-1.3 dataset in terms of AR@AN and AUC. 43.55 52.23 61.35 65.10 2Stream BSN+SNMS 37.46 46.06 53.21 60.64 64.52 2Stream BMN+NMS 37.15 46.75 54.84 62.19 65.22 2Stream BMN+SNMS 39.36 47.72 54.70 62.07 65.49</figDesc><table><row><cell>Method</cell><cell></cell><cell>[7]</cell><cell cols="2">[13] [19] [10] [17] BMN</cell></row><row><cell cols="2">AR@100 (val)</cell><cell>-</cell><cell>-</cell><cell>73.01 73.17 74.16 75.01</cell></row><row><cell>AUC (val)</cell><cell></cell><cell cols="3">59.58 63.12 64.40 65.72 66.17 67.10</cell></row><row><cell cols="2">AUC (test)</cell><cell cols="3">61.56 64.18 64.80</cell><cell>-</cell><cell>66.26 67.19</cell></row><row><cell cols="5">Table 3. Comparison between our method with state-of-the-art</cell></row><row><cell cols="5">proposal generation methods SCNN [23], SST [3], TURN [12],</cell></row><row><cell cols="5">TAG [35], CTAP [10], BSN [17] on THUMOS-14 dataset in terms</cell></row><row><cell cols="5">of AR@AN, where SNMS stands for Soft-NMS.</cell></row><row><cell cols="3">Feature Method</cell><cell cols="2">@50 @100 @200 @500 @1000</cell></row><row><cell>C3D</cell><cell cols="2">SCNN-prop</cell><cell cols="2">17.22 26.17 37.01 51.57 58.20</cell></row><row><cell>C3D</cell><cell>SST</cell><cell></cell><cell cols="2">19.90 28.36 37.90 51.58 60.27</cell></row><row><cell>C3D</cell><cell>TURN</cell><cell></cell><cell cols="2">19.63 27.96 38.34 53.52 60.75</cell></row><row><cell>C3D</cell><cell cols="2">BSN+NMS</cell><cell cols="2">27.19 35.38 43.61 53.77 59.50</cell></row><row><cell>C3D</cell><cell cols="4">BSN+SNMS 29.58 37.38 45.55 54.67 59.48</cell></row><row><cell>C3D</cell><cell cols="4">BMN+NMS 29.04 37.72 46.79 56.07 60.96</cell></row><row><cell>C3D</cell><cell cols="4">BMN+SNMS 32.73 40.68 47.86 56.42 60.44</cell></row><row><cell cols="2">2Stream TAG</cell><cell></cell><cell cols="2">18.55 29.00 39.61</cell><cell>-</cell><cell>-</cell></row><row><cell>Flow</cell><cell>TURN</cell><cell></cell><cell cols="2">21.86 31.89 43.02 57.63 64.17</cell></row><row><cell cols="2">2Stream CTAP</cell><cell></cell><cell cols="2">32.49 42.61 51.97</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">2Stream BSN+NMS</cell><cell>35.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation comparison between BSN<ref type="bibr" target="#b16">[17]</ref> and BMN in validation set of ActivityNet-1.3 in terms of AR@AN, AUC and inference speed. Inference speed here is the second (s) cost for processing a 3-minute video using a Nvidia 1080-Ti graphic card, including network inference time T inf , proposal generation and proposal-feature generation (for BSN) time Tpro and the total inference time Tsum = T inf + Tpro. e2e here means modules of network are trained jointly.</figDesc><table><row><cell cols="2">Method Module</cell><cell cols="2">e2e @100 AUC T inf Tpro Tsum</cell></row><row><cell>BSN</cell><cell>TEM</cell><cell>-</cell><cell>73.57 64.80 0.002 0.034 0.036</cell></row><row><cell>BSN</cell><cell cols="3">TEM+PEM × 74.16 66.17 0.005 0.624 0.629</cell></row><row><cell cols="2">BMN TEM</cell><cell>-</cell><cell>73.72 65.17 0.003 0.032 0.035</cell></row><row><cell cols="4">BMN TEM+PEM × 74.36 66.43 0.007 0.062 0.069</cell></row><row><cell cols="2">BMN TEM+PEM</cell><cell></cell><cell>75.01 67.10 0.005 0.047 0.052</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Generalizability evaluation of BMN on validation set of ActivityNet-1.3 in terms of AR@AN and AUC.</figDesc><table><row><cell></cell><cell>Seen</cell><cell></cell><cell cols="2">Unseen</cell></row><row><cell>Training Data</cell><cell cols="2">AR@100 AUC</cell><cell cols="2">AR@100 AUC</cell></row><row><cell>Seen+Unseen</cell><cell>72.96</cell><cell>65.02</cell><cell>72.68</cell><cell>65.05</cell></row><row><cell>Seen</cell><cell>72.47</cell><cell>64.37</cell><cell>72.46</cell><cell>64.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>87 and 38 action categories, 4455 and 1903 training videos, 2198 and 896 validation videos on seen and unseen subsets separately. And we adopt C3D network<ref type="bibr" target="#b27">[28]</ref> pre-trained on Sports-1M dataset<ref type="bibr" target="#b14">[15]</ref> for feature extraction, to guarantee the validity of experiments. We train BMN with seen and seen+unseen training videos separately, and evaluate both BMN models on seen and unseen validation videos separately. Results in</figDesc><table><row><cell></cell><cell></cell><cell cols="2">validation</cell><cell></cell><cell></cell><cell>testing</cell></row><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell cols="4">0.95 Average Average</cell></row><row><cell>CDC [22]</cell><cell cols="3">43.83 25.88 0.21</cell><cell>22.77</cell><cell></cell><cell>22.90</cell></row><row><cell>SSN [34]</cell><cell cols="3">39.12 23.48 5.49</cell><cell>23.98</cell><cell></cell><cell>28.28</cell></row><row><cell>Lin et al. [19]</cell><cell cols="3">44.39 29.65 7.09</cell><cell>29.17</cell><cell></cell><cell>32.26</cell></row><row><cell cols="4">BSN [17] + [36] 46.45 29.96 8.02</cell><cell>30.03</cell><cell></cell><cell>32.87</cell></row><row><cell>Ours + [36]</cell><cell cols="3">50.07 34.78 8.29</cell><cell>33.85</cell><cell></cell><cell>36.42</cell></row><row><cell cols="7">Table 7. Action detection results on testing set of THUMOS14,</cell></row><row><cell cols="7">where video-level classifier UntrimmedNet [31] and proposal-</cell></row><row><cell cols="7">level classifier SCNN-Classifier [23] are combined with proposals.</cell></row><row><cell>Method</cell><cell>classifier</cell><cell>0.7</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell>SST [3]</cell><cell>SCNN-cls</cell><cell>-</cell><cell>-</cell><cell>23.0</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">TURN[12] SCNN-cls</cell><cell>7.7</cell><cell cols="4">14.6 25.6 33.2 44.1</cell></row><row><cell>BSN [17]</cell><cell>SCNN-cls</cell><cell cols="5">15.0 22.4 29.4 36.6 43.1</cell></row><row><cell>Ours</cell><cell>SCNN-cls</cell><cell cols="5">17.0 24.5 32.2 40.2 45.7</cell></row><row><cell>SST [3]</cell><cell>UNet</cell><cell>4.7</cell><cell cols="4">10.9 20.0 31.5 41.2</cell></row><row><cell cols="2">TURN[12] UNet</cell><cell>6.3</cell><cell cols="4">14.1 24.5 35.3 46.3</cell></row><row><cell>BSN [17]</cell><cell>UNet</cell><cell cols="5">20.0 28.4 36.9 45.0 53.5</cell></row><row><cell>Ours</cell><cell>UNet</cell><cell cols="5">20.5 29.7 38.8 47.4 56.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Softnmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5727" to="5736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khrisna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<title level="m">Activitynet challenge 2017 summary. CVPR ActivityNet Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning deep correspondence through prior and posterior feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01039</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haisheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Temporal convolution based action proposal: Submission to activitynet 2017</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">CVPR Ac-tivityNet Workshop</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1417" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR ActivityNet Workshop</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Edgestereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05196</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<title level="m">Convnet architecture search for spatiotemporal feature learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR ActivityNet Workshop</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A pursuit of temporal accuracy in general activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1703.02716</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08011</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
