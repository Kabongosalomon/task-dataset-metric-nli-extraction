<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cassandra: Detecting Trojaned Networks from Adversarial Perturbations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
							<email>x.zhang@knights.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
							<email>ajmal.mian@uwa.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Gupta</surname></persName>
							<email>rohitg@knights.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Rahnavard</surname></persName>
							<email>nazanin@eecs.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
						</author>
						<title level="a" type="main">Cassandra: Detecting Trojaned Networks from Adversarial Perturbations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks are being widely deployed for many critical tasks due to their high classification accuracy. In many cases, pre-trained models are sourced from vendors who may have disrupted the training pipeline to insert Trojan behaviors into the models. These malicious behaviors can be triggered at the adversary's will and hence, cause a serious threat to the widespread deployment of deep models. We propose a method to verify if a pre-trained model is Trojaned or benign. Our method captures fingerprints of neural networks in the form of adversarial perturbations learned from the network gradients. Inserting backdoors into a network alters its decision boundaries which are effectively encoded in their adversarial perturbations. We train a two stream network for Trojan detection from its global (L ∞ and L 2 bounded) perturbations and the localized region of high energy within each perturbation. The former encodes decision boundaries of the network and latter encodes the unknown trigger shape. We also propose an anomaly detection method to identify the target class in a Trojaned network. Our methods are invariant to the trigger type, trigger size, training data and network architecture. We evaluate our methods on MNIST, NIST-Round0 and NIST-Round1 datasets, with up to 1,000 pre-trained models making this the largest study to date on Trojaned network detection, and achieve over 92% detection accuracy to set the new state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) are the main driving force behind the current success of Artificial Intelligence. However, training DNN models requires enormous amounts of data and computational resources. Hence, many users prefer to source and deploy pre-trained models in their, often security critical, applications such as drug discovery <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, facial recognition <ref type="bibr" target="#b2">[3]</ref>, autonomous driving <ref type="bibr" target="#b3">[4]</ref>, surveillance <ref type="bibr" target="#b4">[5]</ref>. It is well known that DNNs easily learn any bias that is present in the training data. Vendors of DNN models, with malicious intentions, can exploit this vulnerability of DNNs and intentionally inject Trojan behavior into the network during the training process. This is generally achieved by inserting a trigger into some of the samples and then training the DNN to exhibit malicious behavior for data that contains the trigger and normal behavior for data without the trigger. With full control over the DNN training process, the adversary is able to choose any trigger shape. Triggers are chosen such that they do not appear suspicious to the human observer e.g. a yellow rectangular sticker on a stop sign can be used to trigger a DNN to classify it as a speed limit sign. Since only the adversaries have knowledge of the trigger, they can initiate malicious behaviour at will and with no knowledge of the trigger, users of pre-trained models may not even suspect the presence of backdoors. This causes a serious threat to the widespread deployment of pre-trained models. Note that attacking Trojaned DNNs is much easier and different than adversarial attacks on clean DNNs, since the former has access to the DNN training process itself, while the latter only exploits intrinsic vulnerabilities of neural networks <ref type="bibr" target="#b5">[6]</ref>.</p><p>Given that noise-based adversarial attacks are inherent to CNN models, it is no surprise that triggerbased Trojan attacks also exist <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Trojans are generally inserted into the deep model during training or transfer learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. A backdoor is typically inserted into a network <ref type="bibr" target="#b12">[13]</ref> to make the CNNs mis-classify some specific class or classes. Instead of training a model with a dataset poisoned with triggers, another possible way the adversary can Trojan a network is by modifying the weights of selected neurons so that the model responds maliciously to a specific trigger <ref type="bibr" target="#b9">[10]</ref>.</p><p>Current challenges for Trojan (backdoor) detection in practice are: 1) Lack of a deep learning-based model trained on a large-scale dataset for Trojan detection; 2) Unavailability of trigger information for a suspected Trojan infected model; usually only limited training data of clean samples is available; 3) Very limited information which can be obtained from the query model predictions since the test accuracy for Trojaned DNNs is normal for clean inputs, and 4) The target class in the infected model is unknown, and it is computationally expensive to search all possible targeted attacks when the output labels are in the hundreds.</p><p>To address these challenges, we propose the first deep learning based Trojan Detection Network (TDN). Our method has two stages, the first one is a two stream neural network that outputs the probability of a model containing a Trojan, and the second stage predicts the target class in a Trojaned model. Our contributions are summarized as follows. First, we propose a deep neural network for Trojan detection from only a few clean samples. To the best of our knowledge, we are the first to use a DNN classifier, trained on a large scale dataset of benign and Trojaned models, for Trojan detection. Second, we propose a method for target class prediction in a Trojaned model. We introduce a new variable (γ) that quantifies the difficulty of attacking a model. This variable is a critical indicator for the target class of a Trojan infected model.</p><p>Theoretical Justification: Inserting Trojan behaviour into a network essentially puts an additional constraint on the model optimization during the training process. The model must learn to exhibit normal behavior and achieve an expected high classification accuracy on clean training/validation samples but exhibit the chosen malicious behaviour on samples containing a trigger, a localized pattern. This has two important consequences. Firstly, the decision boundaries of the model must adjust to allow such a behavior. Secondly, the model must become more responsive to local patterns (the trigger). Our hypothesis is that if we can encode these two aspects, we will be able to detect Trojaned models accurately. For the former, we use universal adversarial perturbations <ref type="bibr" target="#b13">[14]</ref> which, being image agnostic, reasonably capture a fingerprint of the decision boundaries. For the latter, we look for a localized region of high energy in the adversarial perturbation. Thirdly, we also hypothesize that Trojaned models are easier to fool with minimal universal perturbation energy compared to clean models. Our proposed method basically capitalizes on these three factors to detect Trojaned networks and the target class of such networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adversarial attacks on CNNs have focused on the phenomenon of noise-based adversarial examples <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, which are visually almost indistinct from the original images, but can mislead DNN classifiers into making incorrect predictions. Even universal adversarial perturbations <ref type="bibr" target="#b13">[14]</ref> have been discovered that are image agnostic and when added to any image of any class, can cause the DNN to mis-classify them. By computing singular vectors of the Jacobian matrices of hidden layers, universal perturbations can be constructed with very few images <ref type="bibr" target="#b16">[17]</ref>. Adversarial attacks generally do not assume access to the training process of deep models. A comprehensive survey of such method is reported in <ref type="bibr" target="#b5">[6]</ref>. In this paper, we focus on defending against Trojan attacks where the attacker disrupts the training pipeline of the DNN to insert a backdoor.</p><p>The risk of Trojan models arises when the training process of a DNN is outsourced or a pre-trained model from an untrusted source is deployed. This security risk was first investigated in Badnets <ref type="bibr" target="#b12">[13]</ref>. It was shown that backdoors in networks infected with Trojans can remain a threat even after transfer learning. Chen et al. <ref type="bibr" target="#b11">[12]</ref> proposed a backdoor attack algorithm that uses poisoned data to contaminate the CNN model. Trojaning attack <ref type="bibr" target="#b9">[10]</ref> introduced a way to generate triggers and maximize the activation of some specific neurons to insert a backdoor. The embedded backdoors are stealthy and the unexpected malicious behavior is activated only by triggers, making them extremely challenging to detect with only clean data samples.</p><p>Defense methods were first developed to detect adversarial images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Metzen et al. <ref type="bibr" target="#b22">[23]</ref> detect adversarial perturbations with a target classification network. Feinman et al. <ref type="bibr" target="#b23">[24]</ref> also use a binary classifier to detect adversarial perturbations. Magnet <ref type="bibr" target="#b24">[25]</ref> trains a classifier on manifolds of normal examples to discriminate adversarial perturbations without any prior knowledge of the attack. Safetynet <ref type="bibr" target="#b25">[26]</ref> is designed to detect adversarial-noise based attacks and exploits the different adversarial perturbations produced to train a SVM classifier.</p><p>Methods for detecting and defending against Trojan attacks have also been proposed. Liu et al. <ref type="bibr" target="#b26">[27]</ref> proposed a pruning and fine-tuning procedure to suppress backdoor attacks. Chen et al. <ref type="bibr" target="#b27">[28]</ref> proposed Activation Clustering methodology for detecting and removing backdoors from DNNs. SentiNet <ref type="bibr" target="#b28">[29]</ref> uses the behavior of adversarial misclassification of poisoned networks to detect an attack. However, all these methods fail in the realistic settings where access to poisoned data is not available. Neural Cleanse <ref type="bibr" target="#b29">[30]</ref> was the first method to detect Trojan infected models with clean samples by reverse engineering the trigger. They employ the Median Absolute Deviation (MAD) technique to compute the anomaly in the L 1 norm of the reversed triggers to detect Trojaned models. However, the trigger must be reverse engineered for each class, which is not scalable in practice for DNNs with hundreds and thousands of classes. DeepInspect <ref type="bibr" target="#b30">[31]</ref> uses conditional GAN to reconstruct trigger patterns for Trojan detection. NeuronInspect <ref type="bibr" target="#b31">[32]</ref> detects backdoor from the output features, such as sparsity, smoothness, and persistence of saliency maps obtained from back-propagation of the confidence scores. Tabor <ref type="bibr">[33]</ref> propose metrics to measure the quality of reversed triggers and achieve improved performance than Neural Cleanse by introducing several regularization terms to refine the generated triggers.</p><p>The above methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> are sub-optimal because they are not learning-based and employ the MAD technique and manually tuned anomaly thresholds to detect the outliers of reverse engineered triggers. More importantly, none of these techniques report results on large scale data of benign/Trojaned models and none of them can predict the target class of a Trojaned model. To address these challenges, we propose Cassandra, a Trojan detection method that exploits universal adversarial perturbations <ref type="bibr" target="#b16">[17]</ref> generated from a very limited number of clean samples. Given their image-agnostic nature, we compute universal adversarial perturbations for a batch of clean samples, where the batches could be as few as 5. Note that this holds even if the number of classes is in thousands, unlike prior work such as Neural Cleanse, where one perturbation per class is necessary. Our method also provides the target class of a Trojan infected model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Detecting Trojan Infected Models</head><p>During training, a neural network simultaneously learns feature representation and decision boundaries that partition the feature vector space into the respective classes. When an adversary inserts a backdoor into a network, the decision boundaries are altered. Our hypothesis is that Trojan infected networks exhibit decision boundaries that are different from typical, benign classification networks. Our approach exploits this fact by retrieving the fingerprints of the decision boundaries of a network, and subsequently trains a classifier on these fingerprints to classify a query network as benign or Trojan infected. We use adversarial perturbations to retrieve fingerprints of the decision boundaries of the query network. In contrast to image specific perturbations, universal perturbations <ref type="bibr" target="#b13">[14]</ref> are image agnostic, such that the generated perturbations when added to any input image sends it across the decision boundary to change its label. The success of universal perturbations is measured by its fooling rate, the proportion of images that are successfully mis-classified after the perturbation is added. Since universal adversarial perturbations capture the geometry of the decision boundaries <ref type="bibr" target="#b13">[14]</ref>, the perturbations for benign and Trojan models are expected to be significantly different in character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fingerprinting Decision Boundaries with Adversarial Perturbations</head><p>We formulate Trojan detection as a classification problem. For a query neural network model, f , we define a Trojan detection classifier F as</p><formula xml:id="formula_0">F ( x(f ), E( x(f )), η(f )) = 0 for benign model 1 for Trojan infected model.<label>(1)</label></formula><p>Here f outputs a prediction y for each input image x, drawn from the distribution µ of images in R d , η(f ) is fooling rate and E( x(f )) is the perturbation energy. Similarly,f andŷ denote the Trojaned classifier and corresponding prediction for x. For a desired threshold δ, we obtain universal perturbations x and x for classifiers f andf , respectively, such that the following holds:</p><formula xml:id="formula_1">P x∼µ {f (x + x) = y} ≥ 1 − δ and P x∼µ {f (x + x) =ŷ} ≥ 1 − δ ,<label>(2)</label></formula><p>Note that the observed fooling rate η can go much higher than 1 − δ during the generation of universal adversarial perturbations. We define the perturbation energy E as:</p><formula xml:id="formula_2">E = x 1 = h(x) 1 ,<label>(3)</label></formula><p>where h is parameterized by the process that generates the perturbations. Let E BA denote the perturbation energy cost to transform all data samples from class B to class A across the decision boundary for a benign model, and vice versa for E AB . Similarly, E A B and E B A denote the same for a Trojan infected model. In an infected model the decision boundary is changed such that some backdoors are created close to other classes. Due to these changes in the decision boundary, E A B &lt; E AB and E B A &lt; E BA for a given fooling rate (see <ref type="figure" target="#fig_0">Fig. 1a</ref>,b), where AB is proportional to E AB and so on.</p><p>We define the notion of attack difficulty for both universal perturbations and targeted attacks as</p><formula xml:id="formula_3">γ = E/S,<label>(4)</label></formula><p>where S is the fooling rate (η) for universal perturbations and attack success rate for targeted attacks. Universal adversarial perturbations of clean and Trojan infected models are distinguishable above a given fooling rate η, both visually and in terms of energy, as shown in <ref type="figure" target="#fig_0">Figure 1c</ref>.</p><p>4 Trojan Detector   Perturbation Generator: Since the target class of the (potentially Trojan infected) query network is unknown, the universal adversarial perturbations (Eq. 2) <ref type="bibr" target="#b13">[14]</ref> are computed that cause mis-classification of any input image. The DeepFool [34] kernel is used for perturbation generation. A batch of training images are passed to the query network, the direction of the nearest decision boundary is computed which is back-propagated to compute a small L ∞ or L 2 bounded perturbation for the input. By iteratively refining the perturbation over different mini-batches, a universal (image agnostic) adversarial perturbation is obtained. The generated perturbations are sent to their respective feature extractor stream for further processing. In our case, we stop the iterations when a certain threshold 1 − δ is achieved by the universal perturbation or a maximum number of iterations are reached. To fully capture properties of the complex decision boundaries, we divide the training data into 10 batches and obtain 10 probabilities for each query model. The final score is computed as the mean value of these 10 probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Target Class Prediction</head><p>We propose targeted attack difficulty, as a metric for outlier class prediction in a Torjan infected model. An outlier class is the one which is easy to launch a targeted attack against, compared to the other classes, and hence most likely to be the target class of the Trojan infected model.</p><p>Attack difficulty is defined as γ = E/S, where E is the perturbation energy (Eq. 3) and S is the attack success rate for the targeted attack i.e. the proportion of images whose predictions change to target labels. Attack difficulty (or its reciprocal attack efficiency) measures the perturbation energy normalized by the success rate of the attack. Given a query model, we use the Fast Gradient Sign Method (FGSM) [36], given its fast execution time, to compute adversarial perturbations for each class. For example, NIST-Round0 data contains five class labels and the Trojan models classify triggered images of any class to class 0. In this case, class 0 is the target class and the attack is called an "any-to-one" targeted attack. <ref type="figure" target="#fig_4">Fig. 3</ref> shows that the proposed attack difficulty is able to correctly detect the target class of the Trojan attack as outlier, but L 1 norm used for Trojaned model detection in Neural Cleanse <ref type="bibr" target="#b29">[30]</ref> fails. We finalize our target class prediction with a two stage method. The first stage is our Trojan detection network which outputs the probability of the model being infected with a Trojan, and the second stage is outlier detection based on Median Absolute Deviation (MAD) [37, 30] for predicting the target class. The anomaly index for outlier detection is defined as the absolute deviation of the data points from their median and then normalized by the median, to measure the dispersion of the data distribution. For Trojaned models, the second stage selects the label with anomaly index value above a threshold as the predicted target class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>For all experiments, we perform 5-fold cross validation and report average results. In Eqn. 2, δ is set to 0.2 to quantify the desired fooling rate. We use the Adam optimizer with a learning rate of 0.001.The constant estimator for the MAD outlier detector is 1.4826, so that any data sample with anomaly index larger than 2 has &gt; 95% probability of being an outlier. We employ anomaly index threshold of 2, such that the class labels with anomaly index larger than 2 are considered the target class. For training we use a server with 6 Nvidia RTX 2080 Ti GPUs. Perturbation generation and training for NIST-Round1 data takes around 12 hours. Inference for each model takes about 560s with Nvidia RTX 2080 Ti, and inference for 200 models finishes within 24 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We evaluate our proposed approach on a dataset of trigger infected models for classifying images from MNIST and the public NIST-Round0 and NIST-Round1 datasets. We will refer to the dataset of trigger infected MNIST classification models as Triggered MNIST dataset throughout. Code to generate the triggered MNIST dataset was used from the TrojAI GitHub repo <ref type="bibr" target="#b3">4</ref> . NIST datasets were obtained from the TrojAI challenge website 5 .</p><p>Triggered MNIST Dataset: Two types of triggers, Type I and II (see <ref type="figure" target="#fig_6">Fig. 4</ref>) are inserted into each image of clean MNIST dataset to generate Triggered data. A total of 900 models of 3 architectures (ModdedBadNet, BadNet and ModdedLeNet5) are generated. Out of these, 300 were benign models, 300 were trained for any-to-any attack, and 300 were trained for any-to-one targeted attack. Details of the models and their performance on clean and triggered data are given in the supplementary material.</p><p>NIST Datasets: The NIST datasets consist of traffic sign classification models (half benign and half Trojaned) with 3 possible architectures (Inception-v3, DenseNet-121, and ResNet50). The models were trained on synthetically created images of artificial traffic signs superimposed on road background scenes. The Trojan infected models are poisoned with an unknown embedded trigger.</p><p>NIST-Round0 and NIST-Round1 datasets are both from the same distribution, the main difference is that Round0 consists of 200 models, while Round1 dataset has 1,000 models. Details of the models in the NIST datasets including their accuracy and attack success rates are provided in the supplementary material. Clean data samples used to train the NIST models can be seen in <ref type="figure">Figure 5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class 0 Class 2 Class 1</head><p>Class 3 Class 4 <ref type="figure">Figure 5</ref>: Clean image samples of 5 classes from the NIST datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we report the classification accuracy for a variety of training and test set model configurations for the Triggered MNIST dataset. The classification accuracy is consistently high for both Type I (93.3%) and Type II (91.7%) triggers. Even when training and test models are infected by different types of triggers, the algorithm still has a high classification accuracy of 91.7% (or 90%) which shows that our method is independent of trigger types. We achieve 94.4% performance for the configuration where both trigger types are present, in equal proportions, in the training and test sets (Table 1 last row). NIST datasets are more challenging compared to Triggered MNIST, not only in terms of trigger types, color and size of the data used to train the infected models, but also due to the fact that the NIST models are much deeper. Our method obtains high classification accuracy of 92.5% for NIST-Round0 and 92.0% for NIST-Round1 datasets. <ref type="table" target="#tab_2">Table 2</ref> shows results of our method on the Triggered MNIST, NIST Round0 and NIST Round1 datasets and compares them to Neural Cleanse <ref type="bibr" target="#b29">[30]</ref>. Our proposed Trojan Detection Network outperforms Neural Cleanse on all three datasets with large margins of 17.8%, 25% and 18% respectively. This can be attributed to two reasons. Firstly, it is difficult for Neural Cleanse to find an optimal anomaly index threshold. Secondly, reverse engineering the trigger does not perform well when the triggers are complex.   <ref type="table" target="#tab_3">Table 3</ref> shows our target class prediction results. The proposed two stage prediction algorithm based on the attack difficulty and predicted P (T rojan) improves the classification accuracy significantly over the baseline (without P (T rojan)) from 76.1%, 72.5% and 70.0% to 90.0%, 94.7% and 88.1% on Triggered MNIST, NIST-Round0 and NIST-Round1 datasets respectively. Using Ground truth P(Trojan) further improves classification accuracy which demonstrates attack difficulty is a critical indicator of target class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Study Trojan Detector Network Modules:</head><p>In <ref type="table">Table 4</ref>, we explore different network architectures and the functionality of individual modules of our method. Using only universal perturbations computed from the complete training data of NIST-Round0, we achieved 77.5% classification accuracy. After dividing the training data into 10 batches (these are different from the training mini-batches), we generate 10 perturbations for each model. With these 10 perturbations, the accuracy improves to 85%. Adding the attack difficulty further improves the classification accuracy in all cases. Finally, with multi-batch and two stream architecture we achieve 92.5% classification accuracy. <ref type="table">Table 4</ref>: Effects of using multiple perturbations and attack difficulty. Trojan detection accuracy on the NIST-Round0 validation data improves significantly after using multiple perturbations (n=10) calculated from different batches of training data. Using L ∞ and L 2 perturbations in a two stream architecture combined with attack difficulty (γ) further improves the accuracy.</p><p>Classification Accuracy Input for classifier without γ with γ L ∞ perturbation from all training data 77.5 ± 2.1 82.5 ± 1.4 multi-batch L ∞ perturbations 85.0 ± 1.8 90.0 ± 1.8 multi-batch + two stream (L ∞ &amp; L 2 ) perturbations 85.0 ± 1.4 92.5 ± 1.1 Universal Perturbation Generator Hyper-parameters: The choice of hyper-parameters may impact on the effectiveness of the generated universal adversarial perturbations for various tasks. However, our experiments show that the proposed method is robust to these parameters. We compare the mean classification accuracy when using different number of iterations and magnitudes for L 2 and L ∞ bounded universal adversarial perturbations, and find that the Trojan detection accuracy varies only slightly as shown in <ref type="table" target="#tab_4">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed the first deep learning based method, that is trained on a large scale dataset of Trojaned and clean models, for detecting Trojan infected models. We exploit the universal adversarial perturbations to retrieve the fingerprints of Trojans in the DNNs and train our proposed TDN based on the features of the perturbations and attack difficulty to discriminate benign and Trojaned models. We also proposed simple variable, coined attack difficulty γ, to measure the energy needed to achieve an average unit fooling rate. Based on the attack difficulty, we proposed a two stage target class prediction method that can predict the target class of a Trojaned model in addition to the Trojan probability. This provides further information on the type of malicious behaviour embedded in a Trojan infected model e.g. which identity is being impersonated in a Trojaned face recognition model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>TrojAI Leaderboard Results on NIST-Round0 Dataset <ref type="figure">Figure 6</ref> shows a snapshot of the TrojAI Leaderboard for NIST Round0 (see Section 7 for dataset description). These results are compiled by the NIST server using a held-out test set not available publicly. The snapshot was taken at 21:30 hours on 10 June 2020 and adjusted to fit on this page without interfering with the results. We can see that Cassandra outperforms all other competitors by a significant margin. The best results in terms of Cross-Entropy Loss and ROC-AUC for each method are repeated in <ref type="table" target="#tab_6">Table 6</ref>. Notice that we have the lowest loss and the highest ROC-AUC. <ref type="figure">Figure 6</ref>: A snapshot of the TrojAI Leaderboard shows our method on top. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST Model Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clean Model Generation</head><p>The data is split into training set : 60,000 images (6,000 images per class), and test set: 10,000 images (1,000 from each class). The clean data are used for training 300 benign/clean models with three architecture types (ModdedBadnet, Badnet and ModdedLenet5net), each with 100 models (see <ref type="table" target="#tab_7">Table 7</ref>) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trojaned Model Generation</head><p>Clean Data: The MNIST dataset has 10 classes with 70,000 clean images (without triggers).</p><p>Triggered Data: Two types of triggers, Type I and Type II (see Figure in main paper) were inserted into images of MNIST dataset. The Triggered MNIST data was combined with clean data to generate Trojaned models. The following three data splits were used in our experiments: Data split 1: training: 60,000 (triggered data: 10%), testing:10,000 (triggered data: 10%). Data split 2: training: 60,000 (triggered data: 15%), testing:10,000 (triggered data: 15%). Data split 3: training: 60,000 (triggered data: 20%), testing:10,000 (triggered data: 20%).</p><p>Models: In addition to the 300 benign models, another 600 Trojaned models of the same three architectures (ModdedBadnet, Badnet and ModdedLenet5net) were generated. Trojaned models were trained by the Triggered MNIST data and clean data where the proportion of triggered data varied as 10%, 15% and 20%. <ref type="table" target="#tab_7">Table 7</ref> shows the details of both clean and infected models trained for any-to-any Trojan attack. Any-to-one attack models were generated similar to any-to-any models. 300 Trojaned models were trained by any-to-any targeted attack, and another 300 were trained for any-to-one targeted attack.</p><p>Evaluations of ModdedBadnet, Badnet and ModdedLeNet5 models are shown in <ref type="table" target="#tab_8">Table 8</ref> for anyto-any attack and in <ref type="table" target="#tab_9">Table 9</ref> for any-to-one targeted attack. The clean models and Trojaned models both have high classification accuracy when the test data is clean. The clean models also have high classification accuracy when the test data is triggered. Since there is no Trojan in the clean model, the triggered image samples are correctly classified. However, for the Trojaned models, the classification accuracy (100 − Attack Success Rate) for triggered data is low since the triggered images are misclassified. The tables show Attack Success Rates only for the triggered data which is very high. These results imply that the Trojan (backdoor) was successfully inserted into the models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIST Round0 and NIST Round1 Datasets</head><p>The NIST datasets consist of CNN classification models for traffic sign signals. Half of the models are benign models and half are Trojaned models. The models have three architectures namely, Inception-v3, DenseNet-121, and ResNet50. The models were trained on synthetically created image data of artificial traffic signs superimposed on road background scenes. The Trojaned models have been poisoned with triggers of different color, size and shape. Round0 dataset consists of 200 models, while Round1 dataset has 1,000 models. NIST also holds a sequestered test dataset to evaluate models. For that, models must be uploaded to the TrojAI Leaderboard website. Section 7 and <ref type="table" target="#tab_6">Table 6</ref> discuss our results on the TrojAI Leaderboard. <ref type="table" target="#tab_1">Table 10</ref> and <ref type="table" target="#tab_1">Table 11</ref> show the model details and the performance of the three architecture types present in the NIST Round0 and Round1 datasets. Notice that the Trojan infected models have accuracy at par with the clean models and yet they have a very high attack success rate on the triggered data.  Target Class Detection Algorithm</p><p>The procedure for target class prediction is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data: Query model</head><p>Result: P (T rojan) and Target Class Stage One: Use Trojan Detection network to get P (T rojan); if P (T rojan) &gt;= 0.5 then for C i ← 0 to C do use FGSM to calculate adversarial perturbations with C i as the target class; compute attack difficulty (σ i = L1N orm F oolingRate ) for perturbation; end T argetClass = perform outlier detection over the attack difficulties σ i s; output P (T rojan) and target class prediction; else output P (T rojan) and target class(None); end Algorithm 1: Two-stage method to detect a Trojan infected model and predict its target class using only clean image samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) and (b) illustrate how decision boundaries can change after inserting a Trojan in a network. The Trojaned model (b) has a complicated decision boundary after being compromised. Introducing triggers in the training data changes the decision boundary of model (b) to accommodate the poisoned samples. This makes it easier to perturb the class label of a sample from class A to B (shown by arrows) since the distance across decision boundary is smaller compared to in (a). (c) Shows universal adversarial perturbations computed using L ∞ (top row) and L 2 norm (bottom row) for benign and Trojaned models. Models from left to right: Trojaned Inception-v3, Trojaned DenseNet-121, benign ResNet50, and benign DenseNet121.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>shows the schematic overview of our proposed Trojan detector, referred to as Cassandra. The query network, along with the clean labelled training data, are used to generate two types of universal perturbations i.e. those bounded by L ∞ and L 2 norms. Note that we do not assume the presence of triggered images in the training data, since triggers are unknown in a realistic scenario. The L ∞ universal perturbations are fed to one stream of the network together with their corresponding attack difficulty, γ ∞ . Similarly, the L 2 norm bounded universal perturbations and their attack difficulty, γ 2 , are fed to the second stream of the trojan detection network. The feature extractor inFig. 2extracts distinguish features from the bounded perturbations of each stream. The two feature extractors have identical architectures, but have different sets of weights. Outputs from the two streams are concatenated and used to train the Trojan classifier with binary cross-entropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Trojan Detection: Features extracted from universal adversarial perturbations are used to fingerprint the query model. A two stream architecture is used with L 2 and L ∞ norm bounded perturbations. The two feature extractor modules have identical architectures, but don't share weights. Three sets of features characterizing the query model are extracted: features to represent the maximum energy window in the perturbation extracted by a 5 layer MLP, CNN embedding from the perturbation image, using a MobileNetv3 network and the attack difficulty (γ) of the generated perturbation. Features from the two streams are concatenated and passed to the fully connected classification layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Feature</head><label></label><figDesc>Extractor contains two parallel modules. The first one (top right in Fig. 2) is a Multi Layer Perceptron (MLP), which crops a 50 × 50 window from the perturbation image (after conversion to grayscale) and outputs a 256 dimensional feature vector. The MLP layers have 2500, 3126, 2048, 1024, 512 and 256 neurons, respectively. A sliding window is moved over the grayscale perturbation and the location which has the maximum L 1 norm is selected as input to the MLP. The second module (bottom right in Fig. 2) comprises of a MobilenetV3-Large CNN [35] pre-trained on ImageNet classification. The 1280-dimensional embedding output from the penultimate layer is used as a feature for the Trojan classifier. The network has a total of 5.5M trainable parameters. Trojan Classifier: The output of each feature extractor module is a concatenation of the MLP features (256-D), CNN features (1280-D) and the attack difficulty(1-D). This totals to a 1537-D vector. The outputs of the two-streams (L ∞ and L 2 perturbations) are concatenated to form a 3,074-D vector that is fed to the Trojan classifier, which is a simple fully connected layer. The probability of the query model being Trojan infected is obtained by applying the sigmoid activation to the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Target class prediction: The L 1 norm (left) and attack difficulty (L 1 Norm/Success Rate) (right) of FGSM adversarial perturbations computed per label for benign and Trojan infected networks. The true target class is 0, detected outlier in each case is encircled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(b) Triggered MNIST (Trigger type II) (a) Triggered MNIST (Trigger type I)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Triggered MNIST dataset samples containing Type I triggers (a) and Type II triggers (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Trojan Detection results for the Triggered MNIST dataset of models. The proposed method achieves good results even when trained on a trigger, which is different from the one seen at test time.</figDesc><table><row><cell cols="4"># Train Models Train Trigger # Test Models Test Trigger</cell><cell>Accuracy</cell></row><row><cell>240</cell><cell>I</cell><cell>60</cell><cell>I</cell><cell>93.3 ± 1.6</cell></row><row><cell>240</cell><cell>II</cell><cell>60</cell><cell>II</cell><cell>91.7 ± 1.7</cell></row><row><cell>240</cell><cell>I</cell><cell>60</cell><cell>II</cell><cell>90.0 ± 1.2</cell></row><row><cell>240</cell><cell>II</cell><cell>60</cell><cell>I</cell><cell>91.7 ± 1.1</cell></row><row><cell>480</cell><cell>I, II</cell><cell>120</cell><cell>I, II</cell><cell>94.4 ± 1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Trojan detection accuracy on Triggered MNIST, NIST-Round0 &amp; Round1 datasets.</figDesc><table><row><cell>Dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Target Class Prediction Accuracy: Availability of P (T rojan) significantly increases the target class prediction accuracy. All models are attacked any-to-one i.e. one target class per model.</figDesc><table><row><cell></cell><cell cols="3">Triggered MNIST NIST-Round0 NIST-Round1</cell></row><row><cell>without P (T rojan)</cell><cell>76.1 ± 1.5</cell><cell>72.5 ± 2.5</cell><cell>70.0 ± 1.0</cell></row><row><cell>with predicted P (T rojan)</cell><cell>90.0 ± 1.0</cell><cell>94.7 ± 1.7</cell><cell>88.1 ± 0.7</cell></row><row><cell>with ground truth P (T rojan)</cell><cell>95.0 ± 1.3</cell><cell>98.8 ± 1.4</cell><cell>91.7 ± 1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effect of perturbation generator hyper-parameters on Trojan detection accuracy</figDesc><table><row><cell></cell><cell></cell><cell cols="3">L ∞ Perturbation Magnitude (ξ ∞ /255)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">ξ 2 /255 = 10, # L 2 iterations = 10</cell><cell></cell></row><row><cell># of Iterations</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.8</cell><cell>1</cell></row><row><cell>5</cell><cell cols="5">87.5 ± 2.7 87.5 ± 1.4 90.0 ± 1.6 90.0 ± 1.4 90.0 ± 2.1</cell></row><row><cell>10</cell><cell cols="5">87.5 ± 1.7 90.5 ± 1.1 92.5 ± 1.4 92.5 ± 1.2 92.5 ± 1.1</cell></row><row><cell>15</cell><cell cols="5">90.0 ± 2.1 90.5 ± 1.3 90.0 ± 1.3 92.5 ± 2.1 92.5 ± 1.2</cell></row><row><cell></cell><cell></cell><cell cols="3">L 2 Perturbation Magnitude (ξ 2 /255)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">ξ ∞ /255 = 1, # L ∞ iterations = 10</cell><cell></cell></row><row><cell># of Iterations</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell></row><row><cell>5</cell><cell cols="5">87.5 ± 1.8 90.0 ± 1.4 90.0 ± 1.8 90.0 ± 1.2 90.0 ± 1.4</cell></row><row><cell>10</cell><cell cols="5">90.0 ± 1.4 92.5 ± 1.1 90.0 ± 1.4 92.0 ± 1.8 92.5 ± 1.1</cell></row><row><cell>15</cell><cell cols="5">90.0 ± 1.2 92.5 ± 1.1 92.5 ± 1.2 90.0 ± 1.3 92.5 ± 1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>[33] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems. arXiv preprint arXiv:1908.01763, 2019.[34] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2574-2582, 2016.</figDesc><table><row><cell>[35] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,</cell></row><row><cell>Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3.</cell></row><row><cell>In Proceedings of the IEEE International Conference on Computer Vision, pages 1314-1324,</cell></row><row><cell>2019.</cell></row><row><cell>[36] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing ad-</cell></row><row><cell>versarial examples. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference</cell></row><row><cell>on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference</cell></row><row><cell>Track Proceedings, 2015.</cell></row><row><cell>[37] Frank R Hampel. The influence curve and its role in robust estimation. Journal of the american</cell></row><row><cell>statistical association, 69(346):383-393, 1974.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>TrojAI Leaderboard results on NIST-Round0 dataset sorted by ROC (AUC). We have included the best results in terms of Loss and ROC for our competitors. Best results in each column are in bold. Note that we cannot access the F1 score, Precision and Recall for other methods.</figDesc><table><row><cell>Team</cell><cell cols="2">Loss (Cross-Entropy) ROC (AUC)</cell><cell>F1</cell><cell cols="2">Precision Recall</cell></row><row><cell>Cassandra</cell><cell>0.1601</cell><cell>0.9833</cell><cell>0.9746</cell><cell>0.9897</cell><cell>0.9600</cell></row><row><cell>UCF-XR</cell><cell>0.3149</cell><cell>0.9575</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UCF-XR</cell><cell>0.2882</cell><cell>0.9563</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IceTorch</cell><cell>0.2891</cell><cell>0.9320</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IceTorch</cell><cell>0.2917</cell><cell>0.9275</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Three models trained on Triggered MNIST dataset. Half the models are for any-to-any attack and half are for any-to-one attack. For the latter case each model only has one target class.</figDesc><table><row><cell>Model name</cell><cell>Model Architecture</cell><cell>Trigger</cell><cell>Triggered data</cell><cell>#</cell></row><row><cell cols="2">ModdedBadNet 2 Conv + 1 Dense</cell><cell cols="3">Type I, II 10%, 15% and 20% 100+100</cell></row><row><cell>BadNet</cell><cell>2 Conv + 2 Dense</cell><cell cols="3">Type I, II 10%, 15% and 20% 100+100</cell></row><row><cell cols="2">ModdedLeNet5 3 Conv + 2 Dense</cell><cell cols="3">Type I, II 10%, 15% and 20% 100+100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Attack success rate and classification accuracy for three types of trojaned models (any-to-any attack) for Triggered MNIST dataset. Success rate is the proportion of images for which predictions by the Trojaned model is changed to an incorrect label.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Trojaned Model</cell><cell></cell><cell>Clean Model</cell></row><row><cell></cell><cell cols="2">Attack Success Rate</cell><cell></cell><cell cols="2">Classification Accuracy</cell></row><row><cell>Model Type</cell><cell cols="4">Trigger I Trigger II Trigger I Trigger II</cell><cell></cell></row><row><cell>BadNet</cell><cell>98.7</cell><cell>98.7</cell><cell>98.9</cell><cell>99.0</cell><cell>99.1</cell></row><row><cell>ModdedBadNet</cell><cell>97.3</cell><cell>97.6</cell><cell>97.2</cell><cell>96.5</cell><cell>98.8</cell></row><row><cell>ModdedLeNet5net</cell><cell>97.8</cell><cell>98.6</cell><cell>98.0</cell><cell>97.3</cell><cell>98.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Attack success rate and classification accuracy for three types of trojaned models (any-to-one targeted attack) on the Triggered MNIST dataset. Success rate is the proportion of images that changed label to the target class for Trojaned model.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Trojaned Model</cell><cell></cell></row><row><cell></cell><cell cols="2">Attack Success Rate</cell><cell cols="2">Classification Accuracy</cell></row><row><cell>Model Type</cell><cell cols="3">Trigger I Trigger II Trigger I</cell><cell>Trigger II</cell></row><row><cell>Badnet</cell><cell>99.1</cell><cell>99.0</cell><cell>98.8</cell><cell>98.9</cell></row><row><cell>ModdedBadnet</cell><cell>98.5</cell><cell>98.3</cell><cell>97.6</cell><cell>97.4</cell></row><row><cell>ModdedLenet5net</cell><cell>98.8</cell><cell>98.4</cell><cell>98.0</cell><cell>97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Attack success rate (for Trojan trigger infused data) and top-1 classification accuracy (for clean data) for NIST-Round0 dataset. Success rate is the proportion of images for which the prediction changes to the target label in Trojaned models.</figDesc><table><row><cell></cell><cell cols="2">Trojan Infected Model</cell><cell>Clean Model</cell><cell></cell></row><row><cell>Model Type</cell><cell cols="4">Attack Success Rate Classification Accuracy # models</cell></row><row><cell>DenseNet-121</cell><cell>99.82</cell><cell>99.76</cell><cell>99.90</cell><cell>63</cell></row><row><cell>Inception-v3</cell><cell>99.87</cell><cell>99.69</cell><cell>99.75</cell><cell>69</cell></row><row><cell>ResNet50</cell><cell>99.80</cell><cell>99.68</cell><cell>99.76</cell><cell>68</cell></row><row><cell># models</cell><cell>100</cell><cell></cell><cell>100</cell><cell>200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Attack success rate (for Trojan trigger infused data) and top-1 classification accuracy (for clean data) for three types of Trojaned and clean models from the NIST-Round1 dataset. Success rate is the proportion of images for which the prediction changes to the target label in Trojaned models.</figDesc><table><row><cell></cell><cell cols="2">Trojan Infected Model</cell><cell>Clean Model</cell><cell></cell></row><row><cell>Model Type</cell><cell cols="4">Attack Success Rate Classification Accuracy # models</cell></row><row><cell>DenseNet-121</cell><cell>99.88</cell><cell>99.81</cell><cell>99.88</cell><cell>313</cell></row><row><cell>Inception-v3</cell><cell>99.84</cell><cell>99.85</cell><cell>99.89</cell><cell>250</cell></row><row><cell>ResNet50</cell><cell>99.58</cell><cell>99.81</cell><cell>99.83</cell><cell>437</cell></row><row><cell># models</cell><cell>500</cell><cell></cell><cell>500</cell><cell>1000</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/trojai 5 https://pages.NIST.gov/trojai/docs/data.html#download-links</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part under ARC Discovery Grant DP190102443. Xiaoyu Zhang and Rohit Gupta were supported by University of Central Floria ORC fellowships.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The rise of deep learning in drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ola</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Blaschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug discovery today</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1241" to="1250" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seq3seq fingerprint: towards end-to-end semi-supervised deep drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
		<meeting>the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deepid3: Face recognition with very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tracking and object classification for automated surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="343" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="14410" to="14430" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial examples: Attacks and defenses for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qile</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2805" to="2824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on neural trojans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zuzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Quality Electronics Design</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trojaning attack on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25nd Annual Network and Distributed System Security Symposium, NDSS 2018</title>
		<meeting><address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Internet Society</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1625" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05526</idno>
		<title level="m">Targeted backdoor attacks on deep learning systems using data poisoning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Badnets: Evaluating backdooring attacks on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="47230" to="47244" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1765" to="1773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Defense against universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3389" to="3398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Art of singular vectors and universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Defense against universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature denoising for improving adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the (statistical) detection of adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathrin</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06280</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00530</idno>
		<title level="m">Early methods for detecting adversarial images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ryan R Curtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew B</forename><surname>Shintre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00410</idno>
		<title level="m">Detecting adversarial samples from artifacts</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Magnet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Safetynet: Detecting and rejecting adversarial examples robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theerasit</forename><surname>Issaranon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="446" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-pruning: Defending against backdooring attacks on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Research in Attacks, Intrusions, and Defenses</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="273" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryant</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilka</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biplav</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03728</idno>
		<title level="m">Detecting backdoor attacks on deep neural networks by activation clustering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sentinet: Detecting physical attacks against deep learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Pellegrino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00292</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural cleanse: Identifying and mitigating backdoor attacks in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanshun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bimal</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="707" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farinaz</forename><surname>Koushanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4658" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07399</idno>
		<title level="m">Neuroninspect: Detecting backdoors in neural networks via output explanations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
