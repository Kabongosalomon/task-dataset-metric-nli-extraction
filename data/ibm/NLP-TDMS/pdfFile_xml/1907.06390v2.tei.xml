<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence Level Semantics Aggregation for Video Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
							<email>haiping.wu2@mail.mcgill.cachenyuntao2016</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">TuSimple</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
							<email>zhaoxiang.zhang@ia.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence Level Semantics Aggregation for Video Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-theart results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed fast progress in object detection using deep convolutional networks. Renewed detection paradigms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b10">11]</ref>, strong backbone <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref> and large scale datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref> jointly push forward the limit of object detection.</p><p>Video Object Detection (VID) has now emerged as a new challenge beyond object detection in still images. Thanks to the fast progress in still image object detection, detectors' performance on slow-moving objects in video object detection has somewhat saturated <ref type="bibr" target="#b35">[36]</ref>. The main challenge now lies in the scenario where objects or cameras are under fast </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>motion.</head><p>Fast motion brings up image degradation unseen in the still image setting like motion blur, camera defocus and large pose variation as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Still image detectors often fail in these cases. On the other hand, a video provides far richer visual information than a still image. When the appearance of an object deteriorates in a frame, it is natural to include information from the video (e.g. nearby frames) to mitigate this degradation. The second and third columns in <ref type="figure" target="#fig_0">Figure 1</ref> show various difficult sequences in VID. Though in these hard cases, there are still some frames more salient than the others. A good video object detector should be able to identify the salient views to refine its beliefs on those degraded views if they are (semantically) similar, either to support its beliefs or deny them. Note that useful information is not necessarily from temporal nearby frames, any objects share high similarity with the object of interest in any frames (even within the same frame) could contribute.</p><p>Post-processing methods try to incorporate video-level information by designing sophisticated rule set for linking bounding boxes generated by still image detectors. These two-stage methods are not jointly optimized and may lead to sub-optimal results. Instead, end-to-end feature aggregation utilizes motion information estimated from optical flow <ref type="bibr" target="#b36">[37]</ref> or instance tracking <ref type="bibr" target="#b28">[29]</ref> for object feature calibration. Feature calibration methods heavily rely on accurate motion estimation, which is somewhat contradictory. In the circumstance of fast motion, the appearance of objects degrades drastically. Thus the results of optical flow are usually unsatisfactory in such cases, which makes it less helpful for VID task.</p><p>To lift this limitation in a principled way, we need to take a deeper look at the video itself. Existing works generally take video as sequential frames, and thus mainly utilize the temporal information to enhance the performance of a detector. For example, Flow Guided Feature Aggregation (FGFA) <ref type="bibr" target="#b35">[36]</ref> uses at most 21 frames during training and testing, which is less than 5% of average video length. Instead of taking a consecutive viewpoint, we propose to treat video as a bag of unordered frames and try to learn an invariant representation of each class on the full sequence level. This reinterprets video object detection from a sequential detection task to a multi-shot detection task.</p><p>In the multi-shot view, a video consists of clusters of objects, with each cluster containing hundreds even thousands of shots. The appearance degradation of an object is the manifestation of large intra-class feature variance. Thus reducing the feature variance lies in the core of addressing appearance changes. As mentioned before, temporal feature aggregation is a well-established way for feature variance reduction. However, it fails to utilize the rich information beyond a fixed time window.</p><p>We take a further step by clustering and enhancing features in the entire sequence level. In this work, we present Sequence Level Semantics Aggregation (SELSA) method. We introduce SELSA module which is inspired by spectral clustering. Features of Region of Interests (ROI) are extracted from frames sampled from the whole video, and then go through our clustering module and transformation module. The enhanced features are handed to the detection head to get final detection results. Our method is thoroughly tested on the large scale ImageNet VID and EPIC KITCHEN datasets. We also design ablation experiments to demonstrate the effectiveness of proposed methods. We achieve 82.7 mAP with Faster-RCNN detector and ResNet-101 backbone and 84.3 mAP with ResNeXt-101 backbone, improving the state-of-the-art results by a large margin. Additional experiments on EPIC KITCHENS <ref type="bibr" target="#b3">[4]</ref> dataset show that our method generalize to more complex scenes.</p><p>In summary, our contributions are three folds:</p><p>1. We first treat video detection as a sequence level multishot detection problem and then introduce a global clustering viewpoint of VID task for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>To incorporate such view into current deep object detection pipeline, we introduce a simple but effective Sequence Level Semantics Aggregation (SELSA) module to fully utilize video information.</p><p>3. We test our proposed method on the large scale Ima-geNet VID and EPIC KITCHEN datasets and demonstrate significant improvement over previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review several works that are closely related to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Detection in Still Images</head><p>Thanks to the success of deep neural networks, state-ofthe-art detection systems <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3]</ref> are based on deep convolution neural networks (CNNs). The typical two-stage detector R-CNN [9] first extracts regional features from backbone networks based on deep CNNs, and then classifies and refines the corresponding bounding boxes. Fast R-CNN <ref type="bibr" target="#b7">[8]</ref> proposed RoIPooling operation to speed up the regional feature extraction process. Traditionally, region proposals are generated through selective search <ref type="bibr" target="#b27">[28]</ref>. The Regional Proposal Network (RPN) was proposed in Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> to generate region proposals through deep CNNs, using backbone networks shared with Fast R-CNN. R-FCN <ref type="bibr" target="#b2">[3]</ref> introduced position-sensitive RoIPooling operation, improving the detection efficiency by sharing the computation of regional features.</p><p>On the other hand, one-stage object detector directly predicts the bounding box of interest based on the extracted feature map from CNN. Without the extra stage, one-stage detector is usually faster than the two-stage counterpart. Representative works include YOLO <ref type="bibr" target="#b21">[22]</ref> and its variants <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, SSD <ref type="bibr" target="#b18">[19]</ref> and its variants <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. Nevertheless, one-stage detector can hardly extend to more complicated tasks such as key point detection and instance segmentation. Similarly in our work, it can hardly be extended to extract proposal-level object semantic features. Thus we choose Faster R-CNN as our basic still image detector.</p><p>Recently, high-level relations among objects in object detection has been studied in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref>. These works model the appearance and geometry relations among object proposals within a single image. This enables joint reasoning of objects and improves the accuracy. It could also be used as a duplicate removal step instead of NMS since the geometry relations is embedded. Similarly, our work also captures relations among objects. However, we especially capture the relation measured by semantic similarity (objects of the same class across the video) instead of high-level interaction between objects (e.g person v.s glove in <ref type="bibr" target="#b12">[13]</ref>). We use these similarities to guide our feature aggregation and alleviate problems introduced by videos (fast motion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection in Videos</head><p>For object detection in videos, the main challenge lies in how to utilize the rich information of videos (e.g. temporal continuity) to improve the accuracy as well as the speed upon still image detectors.</p><p>Several previous works devised various post-processing techniques applied to the results of still image detectors by leveraging temporal information: Kang et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> proposed to suppress false positive detections via multicontext suppression (MCS) and propagate predicted bounding boxes across frames using the motion calculated by optical flow. Then a temporal convolution neural network is trained to rescore the tubelets generated using visual tracking. Feichtenhofer et al. <ref type="bibr" target="#b5">[6]</ref> performed single-frame object detection and object movements regression across frames (tracking) in a multi-task fashion. Then it links the detections across frames to object tubelets using the predicted movements, and re-weights detection scores in tubelets. Han et al. <ref type="bibr" target="#b9">[10]</ref> proposed Seq-NMS to form high score linkages using bounding box IoU across frames and then rescore the boxes associated with each linkage to the average or maximum scores of the linkage. These methods perform box-level post-processing upon still image detections, which could be sub-optimal since they are not optimized jointly. In contrast, our method manages to leverage video-level information at proposal-level by end-to-end optimization without post-processing steps.</p><p>Another line of work <ref type="bibr" target="#b13">[14]</ref> focuses on utilizing optical flow to extract motion information to facilitate object detection. However, such pre-computed optical flow is neither efficient nor task related. Deep Feature Flow (DFF) <ref type="bibr" target="#b36">[37]</ref> is the first work that adopts in-network fine-tuned optical flow computation. It utilizes the optical flow generated by FlowNet <ref type="bibr" target="#b4">[5]</ref> to propagate and align the features of selected keyframes to nearby non-keyframes, thus reducing redundant calculation and speeding up the system. FGFA <ref type="bibr" target="#b35">[36]</ref> is built on DFF <ref type="bibr" target="#b36">[37]</ref>. However, its objective is to improve the accuracy by aligning and aggregating features from keyframes using optical flow. Based on DFF and FGFA, MANet <ref type="bibr" target="#b28">[29]</ref> adds an instance-level feature calibration and aggregation module besides the pixel-level one in FGFA, and then it combines these two levels through a motion pattern reasoning module. Furthermore, <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b0">[1]</ref> design more advanced feature propagation and keyframe selection mechanisms to improve the accuracy as well as the speed.</p><p>Using optical flow to calibrate features across frames could be error-prone since object location, appearance and pose could change dramatically, where optical flow estimation becomes unreliable. Unlike these methods, our method does not intend to align features across frames by temporal information. We aggregate features on the proposal level, which makes our method more robust and superior.</p><p>Tripathi et al. <ref type="bibr" target="#b26">[27]</ref> trained a recurrent neural network to refine its initial detection results. Lu et al. <ref type="bibr" target="#b19">[20]</ref> used association LSTM to address the object association between consecutive frames. STMN <ref type="bibr" target="#b32">[33]</ref> used a Spatial-Temporal Memory module as the recurrent operation to pass the information through a video. Unlike <ref type="bibr" target="#b32">[33]</ref>, our method does not need to pass information using memory modules in temporal order. We form clusters and aggregate features in a multi-shot view to capture the rich information of videos instead. Also, our clustering and feature aggregation are performed on instance-level features, where redundant pixellevel calculation is unnecessary. Moreover, it focuses more on subjects of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first describe the motivation of our Sequence Level Semantics Aggregation (SELSA) method in Sec. 3.1. We then elaborate the details of our SELSA module in Sec. 3.2. We further interpret our method from the clustering view in Sec. 3.3. Finally, we discuss the relation between our method and existing works in Sec.3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>Feature aggregation is an effective way to mitigate the appearance degradation in video detection. The vital part of this method is to choose proper features for aggregation. Previous methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref> generally utilize features from a short temporal window. But appearance deterioration could span a wide time window and thus makes temporal-based methods less effective. Moreover, the frames may be highly redundant in a short time window and consequently weaken the advantage of feature aggregation. To address this problem, we propose to aggregate feature from the semantic neighborhood, which is not susceptible to the appearance degradation lasting in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sequence Level Semantics Aggregation</head><p>The ideal way for feature aggregation is to aggregate within the ground truth tracklet. But the golden association for proposals across frames is not available during test phase. Inspired by the ReID-based association which is popular in multi-object tracking system <ref type="bibr" target="#b31">[32]</ref>, we propose to link proposals across space-time with their semantic similarities. This semantic feature based association approach is well known for its robustness to appearance change.</p><formula xml:id="formula_0">Semantic Guidance For each frame f , let X f = {x f</formula><p>1 , x f 2 , · · · } be the proposals generated by the RPN network from Faster-RCNN. For a specific pair of proposals (x k i , x l j ), we measure the semantic similarity between them with the generalized cosine similarity: where φ(·) and ψ(·) are some general transformation functions. Higher similarity indicates a higher chance of proposals being in the same category.</p><formula xml:id="formula_1">w kl ij = φ(x k i ) T ψ(x l j ),<label>(1)</label></formula><p>Feature Aggregation After defining the similarity between proposals, the semantic similarity now serves as guidence for the reference proposal to aggregate features from other proposals. By aggregating across multiple proposals, the new proposal feature contains much richer information and should be robust against appearance variation like pose change, motion blur, and object deformation. Moreover, since the similarities are built on the proposal level, they are more robust compared with the optical flow which is computed on each position in feature maps. In order to preserve the magnitude of the features after aggregation, we normalize the similarities with softmax function across all proposals. Formally, suppose that we are aggregating from randomly picked F frames in the video with N proposals produced in each frame, the aggregated feature for reference proposal is defined as:</p><formula xml:id="formula_2">x k i = l∈Ω N j=1 w kl ij x l j ,<label>(2)</label></formula><p>where Ω is the set of frame indexes randomly selected for the aggregation. The SELSA module is fully differentiable and can be optimized end-to-end with standard SGD. After the aggregation, the enhanced proposal features are further fed into the detection header network for classification and bounding box regression. <ref type="figure" target="#fig_1">Figure 2</ref> shows how the proposed SELSA module work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A Spectral Clustering Viewpoint</head><p>Besides the simple and intuitive formulation of our method, we further reveal its close connection with the clas-sic spectral clustering algorithm. This sheds light on how SELSA work from an intra-class variance reduction viewpoint.</p><p>With proposals X as nodes and similarity W as edges, we can define a semantic similarity graph G = (X, W) on the proposals. From a probabilistic viewpoint, the random walk on graph G is controlled by the stochastic matrix T which is obtained by normalizing each row in W to sum 1. T ij describes the transition probability from proposal i to proposal j during a random walk. Proposals belong to the same class should form a subgraph A ⊂ X. For feature aggregation, we are especially interested in minimizing the risk of incorrectly aggregating the features of a proposal which does not belong to the reference class. This risk can be measured by the transition probability PĀ A from the subgraphĀ = X − A to the subgraph A.</p><p>The transition probability between subgraphs is formally defined as,</p><formula xml:id="formula_3">PĀ A = i∈Ā,j∈A π i T ij i∈Ā π i ,<label>(3)</label></formula><p>where π i = k W jk / j,k W jk denotes the stationary distribution of the graph. π i represents the connection strength between a proposal and the rest proposals in a graph.</p><p>As proved in <ref type="bibr" target="#b20">[21]</ref>, the transition probability is equivalent to the normalized minimum cut,</p><formula xml:id="formula_4">NCut(A,Ā) = P AĀ + PĀ A .<label>(4)</label></formula><p>From the traditional spectral clustering view, the stochastic matrix T is fixed, and the transition probability is minimized by finding the optimal partition A,Ā. However, from the supervised deep learning view, the stochastic matrix T derived from proposal features is the variable to optimize, and the optimal partition A,Ā is given. The optimization of T is further propagated to the proposal features and backbone network for discriminative feature learning. Furthermore, <ref type="bibr" target="#b20">[21]</ref> gives the desired form of T, a blockwise diagonal matrix w.r.t A,Ā, which is exactly the desired guide for proposal feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Connection to Graph Convolution Network</head><p>Recently, Wang et al. <ref type="bibr" target="#b30">[31]</ref> have applied GCN for video classification task. They built a space-time graph with a similar affinity measurement to us. In their work, they took the edges of a graph as a general relation in space-time and mainly focus on modeling the high order interaction of objects in a video. However, in our work, we design the SELSA module to refine the features of a reference proposal by the relationship between them, which leads to a different motivation and optimization objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on ImageNet VID</head><p>In this section, we first introduce the datasets and evaluation metrics used for VID in Sec. 4.1, then followed by the implementation details of our method in Sec. 4.2. We next justify the design choice of our SELSA module in Sec. 4.3 by ablation studies. We also investigate the effects of existing post-processing techniques on our method. Finally, we compare our method with other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Setup</head><p>We train our model with a mixture of ImageNet VID and DET datasets with the split provided in FGFA <ref type="bibr" target="#b35">[36]</ref>. We evaluate our proposed method on ImageNet VID dataset <ref type="bibr" target="#b25">[26]</ref>. We report the mAP@IoU=0.5 and motionspecific mAP on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Feature Network We use ResNet-101 <ref type="bibr" target="#b11">[12]</ref> as the backbone network for ablation studies. ResNeXt-101-32 × 4d <ref type="bibr" target="#b33">[34]</ref> is also used for the final results. The total stride of conv5 block is changed from 32 to 16 with dilated convolutions. Detection Network RPN is applied on the output of conv4. Anchors of 3 scales and 3 aspect ratios are used. Then Fast R-CNN is applied on the output of conv5. We apply two fully connected (FC) layers upon the RoI pooled features followed by classification and bounding box regression. SELSA Module We insert two SELSA modules into our network. Each one is inserted after one fully-connected layer in Faster R-CNN (FC → SELSA → FC → SELSA). The general transformation functions in Eq. 1 are instantiated as one fully-connected layer. Training and Testing Details The backbone networks are initialized with ImageNet pre-trained weights. A total of 220k iterations of SGD training is performed with a total batch size of 4 on 4 GPUs. The initial learning rate is 2.5 × 10 −4 and is divided by 10 at the 110k and the 165k iterations. For training, one training frame is sampled along with two random frames from the same video (identical frames for the DET dataset). For inference, K frames from the same video are sampled along with the inference frame. In both training and inference, the images are resized to a shorter side of 600 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>In this subsection, we study the impact of each design choice and parameter settings. <ref type="table">Table 1</ref> compares our proposed methods with the single-frame baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of SELSA</head><p>Column (a) shows the results of our single-frame baseline. It uses ResNet-101 as the backbone and achieves a reasonable mAP of 73.62 as in <ref type="bibr" target="#b35">[36]</ref>.</p><p>Column (b) performs semantics aggregation (SA) within a single frame, a degenerated variant of SELSA. More specifically, only proposals obtained from the same frame are considered as possible semantic neighbors for aggregation. This leads to a gain of 1.64 mAP compared with the baseline. When multiple objects with the same semantics or multiple proposals corresponding to the same object appear in the same frame, the semantically aggregated proposal features are hence enhanced with contextual information like in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref>, thus leading to the performance improvement. Note that for objects under fast motion, the mAP (fast) receives no improvement over baseline. This indicates that appearance degradation induced by fast motion could not be remedied by the contextual or object interaction information.</p><p>Column (c) is the proposed SELSA method. It utilizes the SELSA module to enhance proposal features by sampling semantic neighbors from the full video sequence. It gives an mAP of 80.25, a large 6.63 mAP improvement compared with the baseline method. Note that it enhances the motion-specific performance in fast motion to 61.38 mAP, which is a huge improvement of 9.95 mAP compared with the baseline. Compared with column (b) and (c), it is easy to see that our method directly harvests high-quality features from aggregating sequence level features other than high order interaction information on the graph, as previously stated in Sec. 3.4.</p><p>Sampling Strategies for Feature Aggregation Frame sampling strategy matters for video detection. As previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref> pointed out, using more frames in feature aggregation during testing yields better results. Besides, <ref type="bibr" target="#b32">[33]</ref> samples frames with a uniform stride during testing to improve the performance.</p><p>We examine the influence of the number of frames used and sampling strides when testing our method. Specifically, by using a sampling stride of S, one frame in every S frames is used for testing instead of consecutive frames.</p><p>First, we use sampling stride one and vary the number of frames used in aggregation. As seen in <ref type="figure" target="#fig_2">Figure 3</ref>(a), with more frames used for testing, the performance increases consistently. For example, using 21 frames for aggregation instead of 5 contributes a 1.04 mAP improvement.</p><p>We then fix the number of frames for aggregation to 21 and examine the impact of sampling stride. <ref type="figure" target="#fig_2">Figure 3(b)</ref> shows the performance with different sample strides. Increasing the sampling stride from 1 to 10 further improves the performance from 77.02 to 79.36 mAP (a gain of 2.34 mAP). Notice that the sampling stride demonstrates a larger influence on the performance than the number of testing frames in general, which coincides with our assumption that our sequence level method could benefit more from sample diversity. Other feature aggregation methods which use optical flow or RNN may not benefit from the larger stride since it violates the temporal continuity assumption of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantics Aggregation in Sequence Level</head><p>As discussed earlier, good features for aggregation in VID should be more diverse in terms of appearance and poses. This ob-servation motivates the use of semantic neighbors instead of temporal neighbors. Thus, taking a step further, we sample semantic neighbors uniformly from the full video sequence regardless of the temporal orders (shuffled test setting). This is feasible since our method does not rely on any temporal information (e.g. optical flow), and also no feature alignment operation across frames is performed. Our method is exempt from possible inaccurate predictions of temporal information (e.g. optical flow estimation <ref type="bibr" target="#b35">[36]</ref>, bounding box shifting prediction <ref type="bibr" target="#b5">[6]</ref>) and feature alignment process <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b28">29]</ref>, which is important when the motion is large. In fact, performance drops have been shown in optical flow based method <ref type="bibr" target="#b28">[29]</ref> as the number of frames increase when exceeding a certain threshold (12 frames in <ref type="bibr" target="#b28">[29]</ref>). Our method, on the contrary, shows its power of performing feature aggregation in the whole video sequence level in <ref type="figure" target="#fig_2">Figure 3(c)</ref>. As we have seen, using only 5 frames in shuffled test already achieves the same level of performance as 21 frames in strided testing. And using 21 frames along with shuffled testing gives an mAP of 80.25. This introduces an improvement of 0.89 mAP against to the strong result of 79.36 mAP where a sampling stride of 10 and in total 21 frames are used. This gain comes from sampling more diverse features in semantic neighbors rather than temporal neighbors, which further shows the effectiveness of SELSA for capturing the full sequence level information for feature aggregation. This is the default test setting in the following experiments.</p><p>Data augmentation Existing VID datasets usually suffer from lacking of semantic diversity. Frames in a video are high similar to each other and thus lead to potential overfitting. Thus we adopt data augmentation to alleviate this problem. Photometric distortion, random expand and random crop as in <ref type="bibr" target="#b18">[19]</ref> are used besides the original random flipping operation. This gives us an improvement of 2.44 mAP, leading to 82.69 mAP when using ResNet-101 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-101</head><p>ResNeXt-101</p><p>Seq-NMS mAP (%) 82.69 82.48 ↓0.21 84.30 83.73 ↓0.57 <ref type="table">Table 2</ref>. The effects of post-processing on our method. The absolute gains compared with the method without Seq-NMS are shown in the subscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Video-level post-processing techniques</head><p>One advantage of our method is that it does not rely on post-processing methods (e.g Seq-NMS) to incorporate the full-sequence level information. Nearly all the state-of-theart video detection systems <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b32">33]</ref> adopted postprocessing methods which gives huge gains in performance. To illustrate that our method has already captured the fullsequence level information, we further apply the Seq-NMS post-processing upon our method. <ref type="table">Table 2</ref> shows the results of how Seq-NMS affects our methods when using different backbone networks. As easily seen, adding Seq-NMS only has a minor impact on the results. In particular, adding Seq-NMS to ResNet-101/ResNext-101 backbone network yields 0.21/0.57 mAP drop.</p><p>Referring to <ref type="table" target="#tab_1">Table 3</ref>, post processing methods have introduced large performance improvement upon existing state-of-the-art methods: 2.1 mAP for FGFA <ref type="bibr" target="#b35">[36]</ref> and 2.2 mAP for MANet <ref type="bibr" target="#b28">[29]</ref> with Seq-NMS and 4 mAP for D (&amp; T loss) <ref type="bibr" target="#b5">[6]</ref> with tubelet rescore. In contrast, almost no gain from Seq-NMS upon our method with ResNet-101 as backbone network shows that our method has already largely captured the full-video level information through our SELSA module without any post-processing techniques. Moreover, different from post-processing methods like Seq-NMS which involves two separate stages, our method could be trained end-to-end with sequence level information. As the backbone feature network becomes stronger, our method could even better utilize such sequence level information, thus shows a better result than that with Seq-NMS, in which the separate post-processing steps might lead to sub-optimal results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the state-of-the-art methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Backbone mAP (%)</p><p>FGFA <ref type="bibr" target="#b35">[36]</ref> ResNet-101 <ref type="bibr" target="#b5">[6]</ref> 75.8 MANet <ref type="bibr" target="#b28">[29]</ref> 78.1 Ours 80.25</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="76.3">D (&amp; T loss)</head><p>FGFA* <ref type="bibr" target="#b35">[36]</ref> ResNet-101 78.4 MANet* <ref type="bibr" target="#b28">[29]</ref> 80.3 ST-Lattice* <ref type="bibr" target="#b0">[1]</ref> 79.6 D&amp;T* <ref type="bibr" target="#b5">[6]</ref> 79.8 STMN*+ <ref type="bibr" target="#b32">[33]</ref> 80  The middle part of <ref type="table" target="#tab_1">Table 3</ref> shows the comparison with methods that utilize sequence-level post-processing techniques. FGFA*, MANet* and STMN*+ <ref type="bibr" target="#b32">[33]</ref> use Seq-NMS, while D&amp;T* <ref type="bibr" target="#b5">[6]</ref>, ST-Lattice* [1] utilize tubelet rescoring. Our method, by using Seq-NMS as the post-processing method, achieves 80.54 mAP, which is slightly better than the previous state-of-the-art method STMN*+.</p><p>Furthermore, by plugging in the stornger ResNeXt-101, our method achieves performance of 83.11 mAP without any post-processing techniques (e.g Seq-NMS), which surpasses the D&amp;T with the same backbone and tubulet rescoring by a large margin (1.15 mAP). Our method benefits from the stronger representation power introduced by better backbone networks. When equipped with training data augmentation, our methods show a significant gain of 2.44/1.19 mAP for ResNet-101/ResNeXt-101. This indicates SELSA can benefit from the diversity of proposal features during aggregation. These results reveal the potential of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Additional Experiments on Epic Kitchen</head><p>ImageNet VID dataset falls short in the density and diversity of objects. Here we evaluate SELSA on the EPIC KITCHENS dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset and Evaluation Setup</head><p>EPIC KITCHENS <ref type="bibr" target="#b3">[4]</ref> is a large scale egocentric dataset, capturing daily activities happened in the kitchens. In EPIC KITCHENS dataset, each frame contains avg/max 1.7/9 ob- jects, which is far more complex and challenging. The video object detection task consists of 32 different kitchens with 454,255 object bounding boxes spanning 290 classes. 272 video sequences captured in 28 kitchens are used for training. 106 sequences collected in the same 28 kitchens (S1) and 54 sequences collected in other 4 unseen kitchens (S2) are used for evaluation. Videos are annotated in 1s interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Mostly, we adopt the same network setting as on Ima-geNet VID dataset. No data augmentation except random horizontal flip is used. A total of 600k iterations of SGD training is performed on 4 GPUs. The initial learning rate is 2.5 × 10 −4 and is divided by 10 at the 300k iterations. For both training and inference, we sample frames within a ±10s window for the SELSA module. Here we present some preliminary results on the EPIC KITCHENS dataset. As shown in <ref type="table" target="#tab_4">Table 4</ref>, SELSA improves over Faster R-CNN baseline by 1.4/2.94 mAP for Seen/Unseen splits. Although the training scheme and the hyper parameter selection are far from optimal, our method still achieves promising results. This shows that SELSA is applicable to more complex video detection tasks. <ref type="figure" target="#fig_3">Figure 4</ref> shows some results of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have proposed a novel view of VID problem by taking the full-sequence level feature aggregation. Instead of using methods such as optical flow or RNN, we propose a simple yet effective SELSA module for aggregating semantic features across frames. Since the aggregation is conducted on the proposal level rather than feature map or even pixel level, our method is more robust to motion blur and large pose variation. Furthermore, we have derived the connection between our method and the classic spectral clustering method, providing a novel clustering view of our method. Extensive ablation analyses demonstrate the effectiveness of the proposed SELSA module. When compared with previous methods, our method achieves superior performance without sophisticated postprocessing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Challenges in video object detection. Motion blur, camera defocus and pose variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of the proposed model. We first extract proposals in different frames from the video, then the semantic similarities of proposals are computed across frames. At last, we aggregate the features from other proposals based on these similarities to obtain a more discriminative and robust features for object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Ablation analyses of different test settings. (a) The effect of different number of frames on sequential test performance. (b) The effect of different sampling stride on sequential test performance. (c) The effect of different number of frames on shuffled test performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual results of our method on EPIC KITCHENS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>↓0.10 61.38 ↑9.85 Table 1. Detection results on the ImageNet VID validation set. For sequence-level methods, 21 frames are used when testing. No post-processing techniques are used. The absolute gains compared with the baseline are shown in the subscript.</figDesc><table><row><cell>Component</cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell></row><row><cell>Semantics Aggregation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sequence-level Info</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP (%)</cell><cell cols="3">73.62 75.26 ↑1.64 80.25 ↑6.63</cell></row><row><cell>mAP (%) (slow)</cell><cell cols="3">82.12 83.59 ↑1.47 86.91 ↑4.79</cell></row><row><cell>mAP (%) (medium)</cell><cell cols="3">70.96 72.88 ↑1.92 78.94 ↑7.98</cell></row><row><cell>mAP (%) (fast)</cell><cell cols="2">51.53 51.43</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 summarizes</head><label>3</label><figDesc>the performance of our methods and other state-of-the-art methods on the ImageNet VID validation set. Our method achieves the best performance among various testing settings.With no video-level post-processing techniques, compared with FGFA<ref type="bibr" target="#b35">[36]</ref> (76.3 mAP) and MANet<ref type="bibr" target="#b28">[29]</ref> (78.1 mAP) which are both built on flow-based feature aggregation, our method is remarkably better (80.25 mAP), outperforming these two methods by 3.95 and 2.15 mAP, respectively. It also outperforms D (&amp; T loss)<ref type="bibr" target="#b5">[6]</ref> by a large margin of 4.45 mAP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison with state-of-the-art systems on the ImageNet VID validation set. * indicates use of video-level post-processing methods (e.g Seq-NMS, tubelet rescoring). + indicates use of model emsembling. indicates using data augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison on EPIC KITCHENS test set. S1 and S2 indicate Seen and Unseen splits.</figDesc><table><row><cell></cell><cell></cell><cell>S1</cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">mAP@.05 mAP@.5 mAP@.75</cell></row><row><cell>EPIC [4]</cell><cell>45.99</cell><cell>34.18</cell><cell>8.49</cell></row><row><cell>Faster R-CNN</cell><cell>53.12</cell><cell>36.57</cell><cell>9.97</cell></row><row><cell>Ours</cell><cell>54.67</cell><cell>37.97</cell><cell>9.81</cell></row><row><cell></cell><cell></cell><cell>S2</cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">mAP@.05 mAP@.5 mAP@.75</cell></row><row><cell>EPIC [4]</cell><cell>44.95</cell><cell>32.01</cell><cell>7.87</cell></row><row><cell>Faster R-CNN</cell><cell>48.91</cell><cell>31.86</cell><cell>7.36</cell></row><row><cell>Ours</cell><cell>50.25</cell><cell>34.80</cell><cell>8.10</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing video object detection via a scale-time lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context refinement for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun. R-Fcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">DSSD: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08465</idno>
		<title level="m">Seq-NMS for video object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Tubelets with convolutional neural networks for object detection from videos. TCSVT</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online video object detection using association LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A random walks view of spectral segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Context matters: Refining object detection in video with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truong</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04648</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully motion-aware network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video object detection with an aligned spatial-temporal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yichen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
