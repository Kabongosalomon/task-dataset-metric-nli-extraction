<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MASKED LABEL PREDICTION: UNIFIED MESSAGE PASSING MODEL FOR SEMI-SUPERVISED CLASSIFICA- TION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
							<email>shiyunsheng01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
							<email>huangzhengjie@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
							<email>wangwenjin02@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhong</surname></persName>
							<email>zhonghui03@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
							<email>fengshikun01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MASKED LABEL PREDICTION: UNIFIED MESSAGE PASSING MODEL FOR SEMI-SUPERVISED CLASSIFICA- TION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural network (GNN) and label propagation algorithm (LPA) are both message passing algorithms, which have achieved superior performance in semisupervised classification. GNN performs feature propagation by a neural network to make predictions, while LPA uses label propagation across graph adjacency matrix to get results. However, there is still no good way to combine these two kinds of algorithms. In this paper, we proposed a new Unified Message Passaging Model (UniMP) that can incorporate feature propagation and label propagation with a shared message passing network, providing a better performance in semi-supervised classification. First, we adopt a Graph Transformer jointly label embedding to propagate both the feature and label information. Second, to train UniMP without overfitting in self-loop label information, we propose a masked label prediction strategy, in which some percentage of training labels are simply masked at random, and then predicted. UniMP conceptually unifies feature propagation and label propagation and be empirically powerful. It obtains new state-of-the-art semi-supervised classification results in Open Graph Benchmark (OGB). Our implementation is available online https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>There are various scenarios in the world, e.g., recommending related news and products, discovering new drugs, or predicting social relations, which can be described as graph structures. Many methods have been proposed to optimize these graph-based problems and achieved significant success in many related domains such as predicting the properties of nodes <ref type="bibr" target="#b12">Kipf &amp; Welling, 2016)</ref>, links <ref type="bibr" target="#b8">(Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b0">Battaglia et al., 2018)</ref>, and graphs <ref type="bibr" target="#b5">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b21">Niepert et al., 2016;</ref><ref type="bibr" target="#b1">Bojchevski et al., 2018)</ref>.</p><p>In the task of semi-supervised node classification, we are required to learn with labeled examples and then make predictions for those unlabeled ones. To better classify the nodes' labels in the graph, based on the Laplacian smoothing assumption <ref type="bibr" target="#b29">Xu et al., 2018b)</ref>, the message passing models were proposed to aggregate the information from its connected neighbors in the graph, acquiring enough facts to produce a more robust prediction for unlabeled nodes. Generally, there are two kinds of practical methods to implement message passing model, the Graph Neural Networks (GNNs) <ref type="bibr" target="#b12">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b9">Hamilton et al., 2017;</ref><ref type="bibr" target="#b29">Xu et al., 2018b;</ref><ref type="bibr" target="#b17">Liao et al., 2019;</ref><ref type="bibr" target="#b28">Xu et al., 2018a;</ref><ref type="bibr" target="#b22">Qu et al., 2019)</ref> and the Label Propagation Algorithms (LPAs) <ref type="bibr" target="#b35">(Zhu, 2005;</ref><ref type="bibr" target="#b34">Zhu et al., 2003;</ref><ref type="bibr" target="#b33">Zhang &amp; Lee, 2007;</ref><ref type="bibr" target="#b25">Wang &amp; Zhang, 2007;</ref><ref type="bibr" target="#b11">Karasuyama &amp; Mamitsuka, 2013;</ref><ref type="bibr" target="#b7">Gong et al., 2016;</ref>. GNNs combine graph structures by propagating and aggregating nodes features through several neural layers, which get predictions from feature propagation. While LPAs make predictions for unlabeled instances by label propagation iteratively.</p><p>Since GNN and LPA are based on the same assumption, making semi-supervised classifications by information propagation, there is an intuition that incorporating them together for boosting perfor-mance. Some superior studies have proposed their graph models based on it. For example, APPNP <ref type="bibr" target="#b13">(Klicpera et al., 2019)</ref> and TPN  integrate GNN and LPA by concatenating them together, and GCN-LPA <ref type="bibr" target="#b27">(Wang &amp; Leskovec, 2019)</ref> uses LPA to regularize their GCN model. However, as shown in Tabel 1, aforementioned methods still can not incorporate GNN and LPA within a message passing model, propagating feature and label in both training and prediction procedure. To unify the feature and label propagation, there are mainly two issues needed to be addressed:</p><p>Aggregating feature and label information. Since node feature is represented by embeddings, while node label is a one-hot vector. They are not in the same vector space. In addition, there are different between their message passing ways, GNNs can propagate the information by diverse neural structures likes GraphSAGE <ref type="bibr" target="#b9">(Hamilton et al., 2017)</ref>, <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref> and GAT <ref type="bibr" target="#b24">(Veličković et al., 2017)</ref>. But LPAs can only pass the label message by graph adjacency matrix.</p><p>Supervised training. Supervised training a model with feature and label propagation will overfit in self-loop label information inevitably, which makes the label leakage in training time and causes poor performance in prediction.</p><p>In this work, inspired by several advantages developments <ref type="bibr" target="#b23">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b26">Wang et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2018)</ref> in Natural Language Processing (NLP), we propose a new Unified Message Passing model (UniMP) with masked label prediction that can settle the aforementioned issues. UniMP is a multi-layer Graph Transformer, jointly using label embedding to transform nodes labels into the same vector space as nodes features. It propagates nodes features like the previous attention based GNNs <ref type="bibr" target="#b24">(Veličković et al., 2017;</ref>. Meanwhile, its multi-head attentions are used as the transition matrix for propagating labels vectors. Therefore, each node can aggregate both features and labels information from its neighbors. To supervised training UniMP without overfitting in self-loop label information, we draw lessons from masked word prediction in BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref> and propose a masked label prediction strategy, which randomly masks some training instances' label embedding vectors and then predicts them. This training method perfectly simulates the procedure of transducing labels information from labeled to unlabeled examples in the graph.</p><p>We conduct experiments on three semi-supervised classification datasets in the Open Graph Benchmark (OGB), where our new methods achieve novel state-of-the-art results in all tasks, gaining 82.56% ACC in ogbn-products, 86.42% ROC-AUC in ogbn-proteins and 73.11% ACC in ogbnarxiv. We also conduct the ablation studies for the models with different inputs to prove the effectiveness of our unified method. In addition, we make the most thorough analysis of how the label propagation boosts our model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>We first introduce our notation about graph. We denote a graph as G = (V, E), where V denotes the nodes in the graph with |V | = n and E denotes edges with |E| = m. The nodes are described by the feature matrix X ∈ R n×f , which usually are dense vectors with f dimension, and the target class matrix Y ∈ R n×c , with the number of classes c. The adjacency matrix A = [a i,j ] ∈ R n×n is used to describe graph G, and the diagonal degree matrix is denoted by D = diag(d 1 , d 2 , ..., d n ) , where d i = j a ij is the degree of node i. A normalized adjacency matrix is defined as D −1 A or D − 1 2 AD − 1 2 , and we adopt the first definition in this paper.      </p><formula xml:id="formula_0">L q / k + a X f Z 1 t E = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i R S 0 G X B j c s K 9 i F N K J P p p B 0 6 k 4 S Z S a G E f I G / 4 F b 3 7 s S t X + H W L 3 H S Z q G t B y 4 c z r m X e z h B w p n S j v N l V T Y 2 t 7 Z 3 q r u 1 v f 2 D w y P 7 + K S r 4 l Q S 2 i E x j 2 U / w I p y F t G O Z p r T f i I p F g G n v W B 6 W / i 9 G Z W K x d G D n i f U F 3 g c s Z A R r I 0 0 t G 1 P Y D 0 J w s y b Y J 0 9 5 v n Q r j s N Z w G 0 T t y S 1 K F E e 2 h / e 6 O Y p I J G m n C s 1 M B 1 E u 1 n W G p G O M 1 r X q p o g s k U j + n A 0 A g L q v x s k T x H F 0 Y Z o T C W Z i K N F u r v i w w L p e Y i M J t F T r X q F e K / X i B W P u v w x s 9 Y l K S a R m T 5 O E w 5 0 j E q a k E j J i n R f G 4 I J p K Z 7 I h M s M R E m / J q p h R 3 t Y J 1 0 r 1 q u E 7 D v W / W W</formula><formula xml:id="formula_1">L q / k + a X f Z 1 t E = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i R S 0 G X B j c s K 9 i F N K J P p p B 0 6 k 4 S Z S a G E f I G / 4 F b 3 7 s S t X + H W L 3 H S Z q G t B y 4 c z r m X e z h B w p n S j v N l V T Y 2 t 7 Z 3 q r u 1 v f 2 D w y P 7 + K S r 4 l Q S 2 i E x j 2 U / w I p y F t G O Z p r T f i I p F g G n v W B 6 W / i 9 G Z W K x d G D n i f U F 3 g c s Z A R r I 0 0 t G 1 P Y D 0 J w s y b Y J 0 9 5 v n Q r j s N Z w G 0 T t y S 1 K F E e 2 h / e 6 O Y p I J G m n C s 1 M B 1 E u 1 n W G p G O M 1 r X q p o g s k U j + n A 0 A g L q v x s k T x H F 0 Y Z o T C W Z i K N F u r v i w w L p e Y i M J t F T r X q F e K / X i B W P u v w x s 9 Y l K S a R m T 5 O E w 5 0 j E q a k E j J i n R f G 4 I J p K Z 7 I h M s M R E m / J q p h R 3 t Y J 1 0 r 1 q u E 7 D v W / W W</formula><formula xml:id="formula_2">L q / k + a X f Z 1 t E = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i R S 0 G X B j c s K 9 i F N K J P p p B 0 6 k 4 S Z S a G E f I G / 4 F b 3 7 s S t X + H W L 3 H S Z q G t B y 4 c z r m X e z h B w p n S j v N l V T Y 2 t 7 Z 3 q r u 1 v f 2 D w y P 7 + K S r 4 l Q S 2 i E x j 2 U / w I p y F t G O Z p r T f i I p F g G n v W B 6 W / i 9 G Z W K x d G D n i f U F 3 g c s Z A R r I 0 0 t G 1 P Y D 0 J w s y b Y J 0 9 5 v n Q r j s N Z w G 0 T t y S 1 K F E e 2 h / e 6 O Y p I J G m n C s 1 M B 1 E u 1 n W G p G O M 1 r X q p o g s k U j + n A 0 A g L q v x s k T x H F 0 Y Z o T C W Z i K N F u r v i w w L p e Y i M J t F T r X q F e K / X i B W P u v w x s 9 Y l K S a R m T 5 O E w 5 0 j E q a k E j J i n R f G 4 I J p K Z 7 I h M s M R E m / J q p h R 3 t Y J 1 0 r 1 q u E 7 D v W / W W</formula><formula xml:id="formula_3">L q / k + a X f Z 1 t E = " &gt; A A A C C X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i R S 0 G X B j c s K 9 i F N K J P p p B 0 6 k 4 S Z S a G E f I G / 4 F b 3 7 s S t X + H W L 3 H S Z q G t B y 4 c z r m X e z h B w p n S j v N l V T Y 2 t 7 Z 3 q r u 1 v f 2 D w y P 7 + K S r 4 l Q S 2 i E x j 2 U / w I p y F t G O Z p r T f i I p F g G n v W B 6 W / i 9 G Z W K x d G D n i f U F 3 g c s Z A R r I 0 0 t G 1 P Y D 0 J w s y b Y J 0 9 5 v n Q r j s N Z w G 0 T t y S 1 K F E e 2 h / e 6 O Y p I J G m n C s 1 M B 1 E u 1 n W G p G O M 1 r X q p o g s k U j + n A 0 A g L q v x s k T x H F 0 Y Z o T C W Z i K N F u r v i w w L p e Y i M J t F T r X q F e K / X i B W P u v w x s 9 Y l K S a R m T 5 O E w 5 0 j E q a k E j J i n R f G 4 I J p K Z 7 I h M s M R E m / J q p h R 3 t Y J 1 0 r 1 q u E 7 D v W / W W 8 2 y n i q c w T l c g g v X 0 I I 7 a E M H C M z g G V 7 g 1 X q y 3 q x 3 6 2 O 5 W r H K m 1 P 4 A + v z B 8 D q m p M = &lt; / l a t e x i t &gt; A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O K o O 0 2 j 3 M s n U g R v E P k P 0 d l 2 n v a w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 b X z G + V i 1 t B o N g F X Y l o G X E x j K C e W C y h N n J b D J k Z n a Z m R X C k s p f s N X e T m z 9 E l u / x N l k C 0 0 8 c O F w z r 3 c w w k T z r T x v C + n t L a + s b l V 3 q 7 s 7 O 7 t H 7 i H R 2 0 d p 4 r Q F o l 5 r L o h 1 p Q z S V u G G U 6 7 i a J Y h J x 2 w s l N 7 n c e q d I s l v d m m t B A 4 J F k E S P Y W O m h L 7 A Z h 1 F 2 P R u 4 V a / m z Y F W i V + Q K h R o D t z v / j A m</formula><formula xml:id="formula_4">U g R v E P k P 0 d l 2 n v a w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 b X z G + V i 1 t B o N g F X Y l o G X E x j K C e W C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">FEATURE PROPAGATION AND LABEL PROPAGATION</head><p>In semi-supervised node classification, based on the Laplacian smoothing assumption, the GNN transforms and propagates nodes features X across the graph by several layers, including linear layers and nonlinear activation to build the approximation of the mapping: X → Y . The feature propagation scheme of GNN in layer l is:</p><formula xml:id="formula_5">H (l+1) = σ(D −1 AH (l) W (l) ) Y = f out (H (L) )<label>(1)</label></formula><p>where the σ is an activation function, W (l) is the trainable weight in the l-th layer, and the H (l) is the l-th layer representations of nodes. H (0) is equal to node input features X. Finally, a f out output layer is applied on the final representation to make prediction for Y .</p><p>As for LPA, it also assumes the labels between connected nodes are smoothing and propagates the labels iteratively across the graph. Given an initial label matrixŶ <ref type="bibr">(0)</ref> , which consists of one-hot label indicator vectorsŷ 0 i for the labeled nodes or zeros vectors for the unlabeled. A simple iteration equation of LPA is formulated as following:</p><formula xml:id="formula_6">Y (l+1) = D −1 AŶ (l)<label>(2)</label></formula><p>Labels are propagated from each other nodes through a normalized adjacency matrix D −1 A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">UNIFIED MESSAGE PASSING MODEL</head><p>As shown in <ref type="figure" target="#fig_5">Figure 1</ref>, we employ a Graph Transformer, jointly using label embedding to construct our unified message passing model for combining the aforementioned feature and label propagation together.</p><p>Graph Transformer. Since Transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> has been proved being powerful in NLP, we employ its vanilla multi-head attention into graph learning with taking into account the case of edge features. Specifically, given nodes features</p><formula xml:id="formula_7">H (l) = {h (l) 1 , h (l) 2 , ..., h<label>(l)</label></formula><p>n }, we calculate multi-head attention for each edge from j to i as following:</p><formula xml:id="formula_8">q (l) c,i = W (l) c,q h (l) i + b (l) c,q k (l) c,j = W (l) c,k h (l) j + b (l) c,k e c,ij = W c,e e ij + b c,e α (l) c,ij = q (l) c,i , k (l) c,j + e c,ij u∈N (i) q (l) c,i , k (l) c,u + e c,iu<label>(3)</label></formula><p>where q, k = exp( q T k √ d ) is exponential scale dot-product function and d is the hidden size of each head. For the c-th head attention, we firstly transform the source feature h </p><formula xml:id="formula_9">(l) c,q , W (l) c,k , b (l) c,q , b (l)</formula><p>c,k . The provided edge features e ij will be encoded and added into key vector as additional information for each layer.</p><p>After getting the graph multi-head attention, we make a message aggregation from the distant j to the source i: v</p><formula xml:id="formula_10">(l) c,j = W (l) c,v h (l) j + b (l) c,v h (l) i = C c=1 j∈N (i) α (l) c,ij (v (l) c,j + e c,ij ) r (l) i = W (l) r h (l) i + b (l) r β (l) i = sigmoid(W (l) g [ĥ (l) i ; r (l) i ;ĥ (l) i − r (l) i ]) h (l+1) i = ReLU(LayerNorm((1 − β (l) i )ĥ (l) i + β (l) i r (l) i )) (4)</formula><p>where the is the concatenation operation for C head attention. Comparing with the Equation 1, multi-head attention matrix replaces the original normalized adjacency matrix as transition matrix for message passing. The distant feature h j is transformed to v c,j ∈ R d for weighted sum. In addition, inspired by <ref type="bibr" target="#b2">Chen et al., 2020)</ref> to prevent oversmoothing, we propose a gated residual connections between layers by r i ∈ R d and β (l) i ∈ R 1 . Specially, similar to GAT, if we apply the Graph Transformer on the output layer, we will employ averaging for multi-head output as following:</p><formula xml:id="formula_11">h (l) i = 1 C C c=1 j∈N (i) α (l) c,ij (v (l) c,j + e (l) c,ij ) h (l+1) i = (1 − β (l) i )ĥ (l) i + β (l) i r (l) i<label>(5)</label></formula><p>Label Embedding and Propagation. We propose to embed the partially observed labels information into the same space as nodes features:Ŷ ∈ R n×c →Ŷ e ∈ R n×f , which consist of the label embedding vector for labeled nodes and zeros vectors for the unlabeled. And then, we combine the label propagation into Graph Transformer by simply adding the nodes features and labels features together as propagation features (H 0 = X +Ŷ e ) ∈ R n×f . We can prove that by mapping partially-labeledŶ and nodes features X into the same space and adding them up, our model is unifying both label propagation and feature propagation within a shared message passing framework. Let's takeŶ e =Ŷ W e and A * to be normalized adjacency matrix D −1 A or the attention matrix from our Graph Transformer like Equation 3. Then we can find that:</p><formula xml:id="formula_12">H (0) = X +Ŷ W c H (l+1) = σ(((1 − β)A * + βI)H (l) W (l) )<label>(6)</label></formula><p>where β can be the gated function like Equation 4 or a pre-defined hyper-parameters like APPNP <ref type="bibr" target="#b13">(Klicpera et al., 2019)</ref>. For simplification, we let σ function as identity function, then we can get:</p><formula xml:id="formula_13">H (l) = ((1 − β)A * + βI) l (X +Ŷ W c )W (1) W (2) . . . W (l) = ((1 − β)A * + βI) l XW + ((1 − β)A * + βI) lŶ W c W<label>(7)</label></formula><p>where W = W (1) W (2) . . . W <ref type="bibr">(l)</ref> . Then we can find that our model can be approximately decomposed into feature propagation ((1 − β)A * + βI) l XW and label propagation ((1 − β)A * + βI) lŶ W c W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MASKED LABEL PREDICTION</head><p>Previous works on GNNs seldom consider using the partially observed labelsŶ in both training and inference stages. They only take those labels information as ground truth target to supervised train their model's parameters θ with given X and A:</p><formula xml:id="formula_14">arg max θ log p θ (Ŷ |X, A) =V i=1 log p θ (ŷ i |X, A)<label>(8)</label></formula><p>whereV represents the partial nodes with labels. However, our UniMP model propagates nodes features and labels to make prediction: p(y|X,Ŷ , A). Simply using above objective for our model will make the label leakage in the training stage, causing poor performance in inference. Learning from BERT, which masks input words and makes prediction for them to pretrain their model (masked word prediction), we propose a masked label prediction strategy to train our model. During training, at each step, we corrupt theŶ intoỸ by randomly masking a portion of node labels to zeros and keep the others remain, which is controlled by a hyper-parameter called label rate. Let those masked labels beȲ , our objective function is to predictȲ with given X,Ỹ and A:</p><formula xml:id="formula_15">arg max θ log p θ (Ȳ |X,Ỹ , A) =V i=1 log p θ (ȳ i |X,Ỹ , A)<label>(9)</label></formula><p>whereV represents those nodes with masked labels. In this way, we can train our model without the leakage of self-loop labels information. And during inference, we will employ allŶ as input labels to predict the remaining unlabeled nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We propose a Unified Message Passing Model (UniMP) for semi-supervised node classification, which incorporates the feature and label propagation jointly by a Graph Transformer and employ a masked label prediction strategy to optimize it. We conduct the experiments on the Node Property Prediction of Open Graph Benchmark (OGBN), which includes several various challenging and large-scale datasets for semi-supervised classification, splitted in the procedure that closely matches the real-world application <ref type="bibr" target="#b10">Hu et al. (2020)</ref>. To verified our models effectiveness, we compare our model with others State-Of-The-Art models (SOTAs) in ogbn-products, ogbn-proteins and ogbnarxiv three OGBN datasets. We also provide more experiments and comprehensive studies to show our motivation more intuitively, and how LPA improves our model to achieve better results. Datasets. Most of the frequently-used graph datasets are extremely small compared to graphs found in real applications. And the performance of GNNs on these datasets is often unstable due to several issues including their small-scale nature, non-negligible duplication or leakage rates, unrealistic data splits <ref type="bibr" target="#b6">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b10">Hu et al., 2020)</ref>. Consequently, we conduct our experiments on the recently released datasets of Open Graph Benchmark (OGB) <ref type="bibr" target="#b10">(Hu et al., 2020)</ref>, which overcome the main drawbacks of commonly used datasets and thus are much more realistic and challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS AND EXPERIMENTAL SETTINGS</head><p>OGB datasets cover a variety of real-world applications and span several important domains ranging from social and information networks to biological networks, molecular graphs, and knowledge graphs. They also span a variety of predictions tasks at the level of nodes, graphs, and links/edges. As shown in table 2, in this work, we performed our experiments on the three OGBN datasets with different sizes and tasks for getting credible result, including ogbn-products about 47 products categories classification with given 100-dimensional nodes features , ogbn-proteins about 112 kinds of proteins function prediction with given 8-dimensional edges features and ogbn-arxiv about 40class topics classification with given 128 dimension nodes features. More details about these datasets are provided in Appendix.</p><p>Implementation Details. As mentioned above, these datasets are different from each other in sizes or tasks. So we evaluate our model on them with different sampling methods like previous studies , getting credible comparison results. In ogbn-products dataset, we use Neighbor-Sampling with size = 10 for each layer to sample the subgraph during training and use full-batch for inference. In ogbn-proteins dataset, we use Random Partition to split the dense graph into subgraph to train and test our model. The number of partitions is 9 for training and 5 for test. As for small-size ogbn-arxiv dataset, we just apply full batch for both training and test. We set the hyper-parameter of our model for each dataset in <ref type="table" target="#tab_2">Table 3</ref>, and the label rate means the percentage of labels we preserve during applying masked label prediction strategy. We use Adam optimizer with lr = 0.001 to train our model. Specially, we set weight decay to 0.0005 for our model in small-size ogbn-arxiv dataset to prevent overfitting. More details about the tuned hyper-parameters are provided in Appendix. We implement all our models by PGL 1 and PaddlePaddle 2 and run all experiments on a single NVIDIA V100 32GB.  <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref> 0.7897 ± 0.0036 0.9212 ± 0.0009 206, 895 GAT-Cluster 0.7923 ± 0.0078 0.8985 ± 0.0022 1, 540, 848 GAT-NeighborSampling 0.7945 ± 0.0059 -1, 751, 574 GraphSAINT <ref type="bibr" target="#b31">(Zeng et al., 2019)</ref> 0.8027 ± 0.0026 -331, 661 DeeperGCN  0.8090 ± 0.0020 0.9238 ± 0.0009 253, 743 UniMP 0.8256 ± 0.0031 0.9308 ± 0.0017 1, 475, 605   0.7803 ± 0.0073 --GeniePath-BS <ref type="bibr" target="#b20">(Liu et al., 2020b)</ref> 0.7825 ± 0.0035 -316, 754 MWE-DGCN 0.8436 ± 0.0065 0.8973 ± 0.0057 538, 544 DeepGCN  0.8496 ± 0.0028 0.8921 ± 0.0011 2, 374, 456 DeeperGCN  0.8580 ± 0.0017 0.9106 ± 0.0016 2, 374, 568 UniMP 0.8642 ± 0.0008 0.9175 ± 0.0007 1, 909, 104   0.7192 ± 0.0016 0.7262 ± 0.0014 1, 471, 506 GaAN  0.7197 ± 0.0024 -1, 471, 506 DAGNN <ref type="bibr" target="#b18">(Liu et al., 2020a)</ref> 0.7209 ± 0.0025 -1, 751, 574 JKNet <ref type="bibr" target="#b29">(Xu et al., 2018b)</ref> 0.7219 ± 0.0021 0.7335 ± 0.0007 331, 661 GCNII <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> 0.7274 ± 0.0016 -2, 148, 648 UniMP 0.7311 ± 0.0021 0.7450 ± 0.0005 <ref type="bibr">473,</ref><ref type="bibr">489</ref> Baseline and other comparative SOTAs are provided by OGB leaderboard. Some of the including results are conducted officially by authors from original papers, while the others are re-implemented by communities. And all these results are guaranteed to be reproducible with open source codes. Following the requirement of OGB, we run our experimental results for each dataset 10 times and report the mean and standard deviation. As shown in Tabel 4, Tabel 5, and Tabel 6, our unified model outperform all other comparative models in three OGBN datasets. Since most of the compared models only consider optimizing their models for the features propagation, these results demonstrate that incorporating label propagation into GNN models can bring significant improvements. Specifically, we gain 82.56% ACC in ogbn-products, 86.42% ROC-AUC in ogbn-proteins, which achieves about 0.6-1.6% absolute improvements compared to the newly SOTA methods like DeeperGCN . In ogbn-arxiv, our method gains 73.11% ACC, achieve 0.37% absolute improvements compared to GCNII <ref type="bibr" target="#b2">(Chen et al., 2020)</ref>, whose parameters are four times larger than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARISON WITH SOTAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We first propose a unified message passing model, UniMP, which jointly performs feature propagation and label propagation within a Graph Transformer to make the semi-supervised classification. Furthermore, we propose a masked label prediction method to supervised training our model, preventing it from overfitting in self-loop label information. Experimental results show that UniMP outperforms the previous state-of-the-art models on three main OGBN datasets: ogbn-products, ogbn-proteins and ogbn-arxiv by a large margin, and ablation studies demonstrate the effectiveness of unifying feature propagation and label propagation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " X 9 G 1 z c o G m B W 2 u z Z z Z G o m a / 5 D N x w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 T X z G + o p Y 2 g 0 G w C r s i a B m w s Y x g H p g s Y X Y y m w y Z m V 1 m Z o W w p P I X b L W 3 E 1 u / x N Y v c T b Z Q h M P X D i c c y / 3 c M J E c G M 9 7 w u V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d v E q a a s R W M R 6 2 5 I D B N c s Z b l V r B u o h m R o W C d c H K T + 5 1 H p g 2 P 1 b 2 d J i y Q Z K R 4 x C m x T n r o S 2 L H Y Z R 1 Z 4 N q z a t 7 c + B V 4 h e k B g W a g + p 3 f x j T V D J l q S D G 9 H w v s U F G t O V U s F m l n x q W E D o h I 9 Z z V B H J T J D N E 8 / w m V O G O I q 1 G 2 X x X P 1 9 k R F p z F S G b j N P a J a 9 X P z X C + X S Z x t d B x l X S W q Z o o v H U S q w j X F e B x 5 y z a g V U 0 c I 1 d x l x 3 R M N K H W l V Z x p f j L F a y S 9 k X d 9 + r + 3 W W t c V n U U 4 Y T O I V z 8 O E K G n A L T W g B B Q X P 8 A K v 6 A m 9 o X f 0 s V g t o e L m G P 4 A f f 4 A O 4 q X l A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 9 G 1 z c o G m B W 2 u z Z z Z G o m a / 5 D N xw = " &gt; A A A C A X i c b V C 7 S g N B F L 0 T X z G + o p Y 2 g 0 G w C r s i a B m w s Y x g H p g s Y X Y y m w y Z m V 1 m Z o W w p P I X b L W 3 E 1 u / x N Y v c T b Z Q h M P X D i c c y / 3 c M J E c G M 9 7 w u V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d v E q a a s R W M R 6 2 5 I D B N c s Z b l V r B u o h m R o W C d c H K T + 5 1 H p g 2 P 1 b 2 d J i y Q Z K R 4 x C m x T n r o S 2 L H Y Z R 1 Z 4 N q z a t 7 c + B V 4 h e k B g W a g + p 3 f x j T V D J l q S D G 9 H w v s U F G t O V U s F m l n x q W E D o h I 9 Z z V B H J T J D N E 8 / w m V O G O I q 1 G 2 X x X P 1 9 k R F p z F S G b j N P aJ a 9 X P z X C + X S Z x t d B x l X S W q Z o o v H U S q w j X F e B x 5 y z a g V U 0 c I 1 d x l x 3 R M N K H W l V Z x p f j L F a y S 9 k X d 9 + r + 3 W W t c V n U U 4 Y T O I V z 8 O E K G n A L T W g B B Q X P 8 A K v 6 A m 9 o X f 0 s V g t o e L m G P 4 A f f 4 A O 4 q X l A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 9 G 1 z c o G m B W 2 u z Z z Z G o m a / 5 D N x w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 T X z G + o p Y 2 g 0 G w C r s i a B m w s Y x g H p g s Y X Y y m w y Z m V 1 m Z o W w p P I X b L W 3 E 1 u / x N Y v c T b Z Q h M P X D i c c y / 3 c M J E c G M 9 7 w u V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d v E q a a s R W M R 6 2 5 I D B N c s Z b l V r B u o h m R o W C d c H K T + 5 1 H p g 2 P 1 b 2 d J i y Q Z K R 4 x C m x T n r o S 2 L H Y Z R 1 Z 4 N q z a t 7 c + B V 4 h e k B g W a g + p 3 f x j T V D J l q S D G 9 H w v s U F G t O V U s F m l n x q W E D o h I 9 Z z V B H J T J D N E 8 / w m V O G O I q 1 G 2 X x X P 1 9 k R F p z F S G b j N P a J a 9 X P z X C + X S Z x t d B x l X S W q Z o o v H U S q w j X F e B x 5 y z a g V U 0 c I 1 d x l x 3 R M N K H W l V Z x p f j L F a y S 9 k X d 9 + r + 3 W W t c V n U U 4 Y T O I V z 8 O E K G n A L T W g B B Q X P 8 A K v 6 A m 9 o X f 0 s V g t o e L m G P 4 A f f 4 A O 4 q X l A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 9 G 1 z c o G m B W 2 u z Z z Z G o m a / 5 D N x w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 T X z G + o p Y 2 g 0 G w C r s i a B m w s Y x g H p g s Y X Y y m w y Z m V 1 m Z o W w p P I X b L W 3 E 1 u / x N Y v c T b Z Q h M P X D i c c y / 3 c M J E c G M 9 7 w u V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d v E q a a s R W M R 6 2 5 I D B N c s Z b l V r B u o h m R o W C d c H K T + 5 1 H p g 2 P 1 b 2 d J i y Q Z K R 4 x C m x T n r o S 2 L H Y Z R 1 Z 4 N q z a t 7 c + B V 4 h e k B g W a g + p 3 f x j T V D J l q S D G 9 H w v s U F G t O V U s F m l n x q W E D o h I 9 Z z V B H J T J D N E 8 / w m V O G O I q 1 G 2 X x X P 1 9 k R F p z F S G b j N P a J a 9 X P z X C + X S Z x t d B x l X S W q Z o o v H U S q w j X F e B x 5 y z a g V U 0 c I 1 d x l x 3 R M N K H W l V Z x p f j L F a y S 9 k X d 9 + r + 3 W W t c V n U U 4 Y T O I V z 8 O E K G n A L T W g B B Q X P 8 A K v 6 A m 9 o X f 0 s V g t o e L m G P 4 A f f 4 A O 4 q X l A = = &lt; / l a t e x i t &gt;Ŷ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J a 7 8 6 7 P k X J a H H /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>8 2 y n i q c w T l c g g v X 0 I I 7 a E M H C M z g G V 7 g 1 X q y 3 q x 3 6 2 O 5 W r H K m 1 P 4 A + v z B 8 D q m p M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J a 7 8 6 7 P k X J a H H /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>8 2 y n i q c w T l c g g v X 0 I I 7 a E M H C M z g G V 7 g 1 X q y 3 q x 3 6 2 O 5 W r H K m 1 P 4 A + v z B 8 D q m p M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J a 7 8 6 7 P k X J a H H /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>8 2 y n i q c w T l c g g v X 0 I I 7 a E M H C M z g G V 7 g 1 X q y 3 q x 3 6 2 O 5 W r H K m 1 P 4 A + v z B 8 D q m p M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X J a 7 8 6 7 P k X J a H H /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>q a D S E I 6 1 7 v l e Y o I M K 8 M I p 7 N K P 9 U 0 w W S C R 7 R n q c S C 6 i C b J 5 6 h M 6 s M U R Q r O 9 K g u f r 7 I s N C 6 6 k I 7 W a e U C 9 7 u f i v F 4 q l z y a 6 C j I m k 9 R Q S R a P o 5 Q j E 6 O 8 D j R k i h L D p 5 Z g o p j N j s g Y K 0 y M L a 1 i S / G X K 1 g l 7 Y u a 7 9 X 8 u 3 q 1 U S / q K c M J n M I 5 + H A J D b i F J r S A g I R n e I F X 5 8 l 5 c 9 6 d j 8 V q y S l u j u E P n M 8 f F y e X f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O K o O 0 2 j 3 M s n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>y h N n J b D J k Z n a Z m R X C k s p f s N X e T m z 9 E l u / x N l k C 0 0 8 c O F w z r 3 c w w k T z r T x v C + n t L a + s b l V 3 q 7 s 7 O 7 t H 7 i H R 2 0 d p 4 r Q F o l 5 r L o h 1 p Q z S V u G G U 6 7 i a J Y h J x 2 w s l N 7 n c e q d I s l v d m m t B A 4 J F k E S P Y W O m h L 7 A Z h 1 F 2 P R u 4 V a / m z Y F W i V + Q K h R o D t z v / j A m q a D S E I 6 1 7 v l e Y o I M K 8 M I p 7 N K P 9 U 0 w W S C R 7 R n q c S C 6 i C b J 5 6 h M 6 s M U R Q r O 9 K g u f r 7 I s N C 6 6k I 7 W a e U C 9 7 u f i v F 4 q l z y a 6 C j I m k 9 R Q S R a P o 5 Q j E 6 O 8 D j R k i h L D p 5 Z g o p j N j s g Y K 0 y M L a 1 i S / G X K 1 g l 7 Y u a 7 9 X 8 u 3 q 1 U S / q K c M J n M I 5 + H A J D b i F J r S A g I R n e I F X 5 8 l 5 c 9 6 d j 8 V q y S l u j u E P n M 8 f F y e X f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O K o O 0 2 j 3 M s n U g R v E P k P 0 d l 2 n v a w = "&gt; A A A C A X i c b V C 7 S g N B F L 0 b X z G + V i 1 t B o N g F X Y l o G X E x j K C e W Cy h N n J b D J k Z n a Z m R X C k s p f s N X e T m z 9 E l u / x N l k C 0 0 8 c O F w z r 3 c w w k T z r T x v C + n t L a + s b l V 3 q 7 s 7 O 7 t H 7 i H R 2 0 d p 4 r Q F o l 5 r L o h 1 p Q z S V u G G U 6 7 i a J Y h J x 2 w s l N 7 n c e q d I s l v d m m t B A 4 J F k E S P Y W O m h L 7 A Z h 1 F 2 P R u 4 V a / m z Y F W i V + Q K h R o D t z v / j A m q a D S E I 6 1 7 v l e Y o I M K 8 M I p 7 N K P 9 U 0 w W S C R 7 R n q c S C 6 i C b J 5 6 h M 6 s M U R Q r O 9 K g u f r 7 I s N C 6 6 k I 7 W a e U C 9 7 u f i v F 4 q l z y a 6 C j I m k 9 R Q S R a P o 5 Q j E 6 O 8 D j R k i h L D p 5 Z g o p j N j s g Y K 0 y M L a 1 i S / G X K 1 g l 7 Y u a 7 9 X 8 u 3 q 1 U S / q K c M J n M I 5 + H A J D b i F J r S A g I R n e I F X 5 8 l 5 c 9 6 d j 8 V q y S l u j u E P n M 8 f F y e X f Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "O K o O 0 2 j 3 M s n U g R v E P k P 0 d l 2 n v a w = " &gt; A A A C A X i c b V C 7 S g N B F L 0 b X z G + V i 1 t B o N g F X Y l o G X E x j K C e W C y h N n J b D J k Z n a Z m R X C k s p f s N X e T m z 9 E l u / x N l k C 0 0 8 c O F w z r 3 c w w k T z r T x v C + n t L a + s b l V 3 q 7 s 7 O 7 t H 7 i H R 2 0 d p 4 r Q F o l 5 r L o h 1 p Q z S V u G G U 6 7 i a J Y h J x 2 w s l N 7 n c e q d I s l v d m m t B A 4 J F k E S P Y W O m h L 7 A Z h 1 F 2 P R u 4 V a / m z Y F W i V + Q K h R o D t z v / j A mq a D S E I 6 1 7 v l e Y o I M K 8 M I p 7 N K P 9 U 0 w W S C R 7 R n q c S C 6 i C b J 5 6 h M 6 s M U R Q r O 9 K g u f r 7 I s N C 6 6 k I 7 W a e U C 9 7 u f i v F 4 q l z y a 6 C j I m k 9 R Q S R a P o 5 Q j E 6 O 8 D j R k i h L D p 5 Z g o p j N j s g Y K 0 y M L a 1 i S / G X K 1 g l 7 Y u a 7 9 X 8 u 3 q 1 U S / q K c M J n M I 5 + H A J D b i F J r S A g I R n e I F X 5 8 l 5 c 9 6 d j 8 V q y S l u j u E P n M 8 f F y e X f Q = = &lt; / l a t e x i t &gt; V U |X,Ŷ, A) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F 7 O O S D M A h L Y G H U l L r C z S t 2 c a w 7 Q = " &gt; A A A C N X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 W o o C W R g i 5 c V N y 4 r G D a S h P K Z D p p h 0 4 e z E y E E v M z / o S / 4 F Z 3 L t y I u P U X n L Q p 1 N Y D A 4 d z z u X e O W 7 E q J C G 8 a 4 V l p Z X V t e K 6 6 W N z a 3 t H X 1 3 r y n C m G N i 4 Z C F v O 0 i Q R g N i C W p Z K Q d c Y J 8 l 5 G W O 7 z O / N Y D 4 Y K G w Z 0 c R c T x U T + g H s V I K q m r X z Y q t o / k w P W S + 7 S b N E + t 9 H E q t N M T O O X 2 A E m V m F G u 0 u O u X j a q x h h w k Z g 5 K Y M c j a 7 + a f d C H P s k k J g h I T q m E U k n Q V x S z E h a s m N B I o S H q E 8 6 i g b I J 8 J J x r 9 M 4 Z F S e t A L u X q B h G N 1 d i J B v h A j 3 1 X J 7 E Q x 7 2 X i v 5 7 r z 2 2 W 3 o W T 0 C C K J Q n w Z L E X M y h D m F U I e 5 Q T L N l I E Y Q 5 V b d D P E A c Y a m K L q l S z P k K F k n z r G o a V f O 2 V q 7 X 8 n q K 4 A A c g g o w w T m o g x v Q A B b A 4 A m 8 g F f w p j 1 r H 9 q X 9 j 2 J F r R 8 Z h / 8 g f b z C y E 1 r H M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F 7 O O S D M A h L Y G H U l L r C z S t 2 c a w 7 Q = " &gt; A A A C N X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 W o o C W R g i 5 c V N y 4 r G D a S h P K Z D p p h 0 4 e z E y E E v M z / o S / 4 F Z 3 L t y I u P U X n L Q p 1 N Y D A 4 d z z u X e O W 7 E q J C G 8 a 4 V l p Z X V t e K 6 6 W N z a 3 t H X 1 3 r y n C m G N i 4 Z C F v O 0 i Q R g N i C W p Z K Q d c Y J 8 l 5 G W O 7 z O / N Y D 4 Y K G w Z 0 c R c T x U T + g H s V I K q m r X z Y q t o / k w P W S + 7 S b N E + t 9 H E q t N M T O O X 2 A E m V m F G u 0 u O u X j a q x h h w k Z g 5 K Y M c j a 7 + a f d C H P s k k J g h I T q m E U k n Q V x S z E h a s m N B I o S H q E 8 6 i g b I J 8 J J x r 9 M 4 Z F S e t A L u X q B h G N 1 d i J B v h A j 3 1 X J 7 E Q x 7 2 X i v 5 7 r z 2 2 W 3 o W T 0 C C K J Q n w Z L E X M y h D m F U I e 5 Q T L N l I E Y Q 5 V b d D P E A c Y a m K L q l S z P k K F k n z r G o a V f O 2 V q 7 X 8 n q K 4 A A c g g o w w T m o g x v Q A B b A 4 A m 8 g F f w p j 1 r H 9 q X 9 j 2 J F r R 8 Z h / 8 g f b z C y E 1 r H M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F 7 O O S D M A h L Y G H U l L r C z S t 2 c a w 7 Q = " &gt; A A A C N X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 W o o C W R g i 5 c V N y 4 r G D a S h P K Z D p p h 0 4 e z E y E E v M z / o S / 4 F Z 3 L t y I u P U X n L Q p 1 N Y D A 4 d z z u X e O W 7 E q J C G 8 a 4 V l p Z X V t e K 6 6 W N z a 3 t H X 1 3 r y n C m G N i 4 Z C F v O 0 i Q R g N i C W p Z K Q d c Y J 8 l 5 G W O 7 z O / N Y D 4 Y K G w Z 0 c R c T x U T + g H s V I K q m r X z Y q t o / k w P W S + 7 S b N E + t 9 H E q t N M T O O X 2 A E m V m F G u 0 u O u X j a q x h h w k Z g 5 K Y M c j a 7 + a f d C H P s k k J g h I T q m E U k n Q V x S z E h a s m N B I o S H q E 8 6 i g b I J 8 J J x r 9 M 4 Z F S e t A L u X q B h G N 1 d i J B v h A j 3 1 X J 7 E Q x 7 2 X i v 5 7 r z 2 2 W 3 o W T 0 C C K J Q n w Z L E X M y h D m F U I e 5 Q T L N l I E Y Q 5 V b d D P E A c Y a m K L q l S z P k K F k n z r G o a V f O 2 V q 7 X 8 n q K 4 A A c g g o w w T m o g x v Q A B b A 4 A m 8 g F f w p j 1 r H 9 q X 9 j 2 J F r R 8 Z h / 8 g f b z C y E 1 r H M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F 7 O O S D M A h L Y G H U l L r C z S t 2 c a w 7 Q = " &gt; A A A C N X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 W o o C W R g i 5 c V N y 4 r G D a S h P K Z D p p h 0 4 e z E y E E v M z / o S / 4 F Z 3 L t y I u P U X n L Q p 1 N Y D A 4 d z z u X e O W 7 E q J C G 8 a 4 V l p Z X V t e K 6 6 W N z a 3 t H X 1 3 r y n C m G N i 4 Z C F v O 0 i Q R g N i C W p Z K Q d c Y J 8 l 5 G W O 7 z O / N Y D 4 Y K G w Z 0 c R c T x U T + g H s V I K q m r X z Y q t o / k w P W S + 7 S b N E + t 9 H E q t N M T O O X 2 A E m V m F G u 0 u O u X j a q x h h w k Z g 5 K Y M c j a 7 + a f d C H P s k k J g h I T q m E U k n Q V x S z E h a s m N B I o S H q E 8 6 i g b I J 8 J J x r 9 M 4 Z F S e t A L u X q B h G N 1 d i J B v h A j 3 1 X J 7 E Q x 7 2 X i v 5 7 r z 2 2 W 3 o W T 0 C C K J Q n w Z L E X M y h D m F U I e 5 Q T L N l I E Y Q 5 V b d D P E A c Y a m K L q l S z P k K F k n z r G o a V f O 2 V q 7 X 8 n q K 4 A A c g g o w w T m o g x v Q A B b A 4 A m 8 g F f w p j 1 r H 9 q X 9 j 2 J F r R 8 Z h / 8 g f b z C y E 1 r H M = &lt; / l a t e x i t &gt; The architecture of our UniMP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>∈ R d and key vector k (l) c,j ∈ R d respectively using different trainable parameters W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparision between message passing models</figDesc><table><row><cell>Training</cell><cell>Prediction</cell></row><row><cell cols="2">Feature Label Feature Label</cell></row><row><cell>LPA</cell><cell></cell></row><row><cell>GCN</cell><cell></cell></row><row><cell>APPNP</cell><cell></cell></row><row><cell>GCN-LPA</cell><cell></cell></row><row><cell>UniMP (Ours)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics of OGB node property prediction</figDesc><table><row><cell>Name</cell><cell>Node</cell><cell>Edges</cell><cell cols="3">Tasks Split Rate Split Type</cell><cell>Task Type</cell><cell>Metric</cell></row><row><cell cols="3">ogbn-products 2, 449, 029 61, 859, 140</cell><cell>1</cell><cell cols="4">8\02\88 Sales rank Multi-class class Accuracy</cell></row><row><cell>ogbn-proteins</cell><cell cols="5">132, 534 39, 561, 252 112 65\16\19 Species</cell><cell>Binary class</cell><cell>ROC-AUC</cell></row><row><cell>ogbn-arxiv</cell><cell>169, 343</cell><cell>1, 166, 243</cell><cell>1</cell><cell>78\08\14</cell><cell>Time</cell><cell cols="2">Multi-class class Accuracy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The hyper-paramerter setting of our model</figDesc><table><row><cell></cell><cell>ogbn-products</cell><cell>ogbn-proteins</cell><cell>ogbn-arxiv</cell></row><row><cell cols="4">sampling method NeighborSampling Random Partition Full-batch</cell></row><row><cell>num layers</cell><cell>3</cell><cell>7</cell><cell>3</cell></row><row><cell>hidden size</cell><cell>128</cell><cell>64</cell><cell>128</cell></row><row><cell>num heads</cell><cell>4</cell><cell>4</cell><cell>2</cell></row><row><cell>dropout</cell><cell>0.3</cell><cell>0.1</cell><cell>0.3</cell></row><row><cell>lr</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>weight decay label rate</cell><cell>*  0.625</cell><cell>*  0.5</cell><cell>0.0005 0.625</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results for ogbn-products</figDesc><table><row><cell>Model</cell><cell>Test Accuracy</cell><cell>Validation Accuracy</cell><cell>Params</cell></row><row><cell>GCN-Cluster</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results for ogbn-proteins</figDesc><table><row><cell>Model</cell><cell>Test ROC-AUC</cell><cell>Validation ROC-AUC</cell><cell>Params</cell></row><row><cell>GaAN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results for ogbn-arxiv</figDesc><table><row><cell>Model</cell><cell>Test Accuracy</cell><cell>Validation Accuracy</cell><cell>Params</cell></row><row><cell>DeeperGCN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/PaddlePaddle/PGL 2 https://github.com/PaddlePaddle/Paddle</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">Günnemann</forename><surname>Netgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00816</idno>
		<title level="m">Generating graphs via random walks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02133</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Label propagation via teaching-tolearn and learning-to-teach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1452" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Manifold-based similarity adaptation for label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1547" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predict then propagate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gunnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph neural networks meet personalized pagerank. arXiv: Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepergcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<title level="m">All you need to train deeper gcns</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="3538" to="3545" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanczosnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01484</idno>
		<title level="m">Multi-scale deep graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to propagate labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transductive propagation network for few-shot learning. arXiv: Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05806</idno>
		<title level="m">Bandit samplers for training graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gmnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06214</idno>
		<title level="m">Graph markov neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Joint embedding of words and labels for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04174</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Unifying graph convolutional neural networks and label propagation. arXiv: Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hyperparameter learning for graph based semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1585" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
