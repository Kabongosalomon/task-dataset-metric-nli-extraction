<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Multiple Hypotheses for 3D Human Pose Estimation with Mixture Density Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim</forename><forename type="middle">Hee</forename><surname>Lee</surname></persName>
							<email>gimhee.lee@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Multiple Hypotheses for 3D Human Pose Estimation with Mixture Density Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D human pose estimation from a monocular image or 2D joints is an ill-posed problem because of depth ambiguity and occluded joints. We argue that 3D human pose estimation from a monocular input is an inverse problem where multiple feasible solutions can exist. In this paper, we propose a novel approach to generate multiple feasible hypotheses of the 3D pose from 2D joints. In contrast to existing deep learning approaches which minimize a mean square error based on an unimodal Gaussian distribution, our method is able to generate multiple feasible hypotheses of 3D pose based on a multimodal mixture density networks. Our experiments show that the 3D poses estimated by our approach from an input of 2D joints are consistent in 2D reprojections, which supports our argument that multiple solutions exist for the 2D-to-3D inverse problem. Furthermore, we show state-of-the-art performance on the Hu-man3.6M dataset in both best hypothesis and multi-view settings, and we demonstrate the generalization capacity of our model by testing on the MPII and MPI-INF-3DHP datasets. Our code is available at the project website 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation from a single RGB image is an extensively studied problem in computer vision because of many potential useful real world applications such as forensic science, sports analysis and surveillance etc. Significant progress in 3D human pose estimation has been made with deep learning in the recent years. One of the commonly used and effective deep learning based methods for 3D human pose estimation is the two-stage approach, where the 2D joints are first detected from the image input <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> followed by the 3D joint estimations from the detected 2D joints <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17]</ref>. The advantage of the two-stage approach is that it decouples the harder problem of 3D depth estimation from the easier 2D pose estimation. In particular, variations in background scene, lighting, clothing shape, skin color etc. are removed before the 3D joint estimation stage. Furthermore, the model can be trained on different domains, e.g. indoor and outdoor, with 2D annotations that are readily available.</p><p>Despite the significant progress with deep learning, 3D human pose estimation remains as a very challenging task due to the ambiguity in recovering 3D information from a single RGB image. More specifically, recovering 3D information from a single RGB image or 2D joint locations is an inverse problem <ref type="bibr" target="#b2">[3]</ref> where multiple solutions may exist for the depth of a 3D joint along the light ray that reprojects onto the same 2D joint location, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The problem is further aggravated by the nonrigidity of the human pose and joint occlusions on the 2D image. Consequently, there could be many solutions of the 3D pose that satisfy the same 2D pose on an image, even after eliminating the infeasible 3D pose solutions by en-forcing various geometric constraints, e.g. joint limits <ref type="bibr" target="#b0">[1]</ref> and bone ratio <ref type="bibr" target="#b26">[27]</ref> etc. In view of the inherent ambiguity of the 3D human pose estimation problem, we argue that it is more reasonable to design a model that generates multiple hypotheses of geometrically feasible 3D human pose that are consistent with the detected 2D joints from a single RGB image. In contrast, the widely adopted single estimate for the inverse problem with inherent ambiguity could lead to overfitting the model to the training data, and might not generalize well. This idea of generating multiple 3D pose hypotheses was first suggested very recently by Jahangiri and Yuille in <ref type="bibr" target="#b11">[12]</ref>.</p><p>To this end, we introduce the mixture density networks (MDN) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> to the 3D joint estimation module of the two-stage approach. Contrary to most existing works that generate a single 3D pose by minimizing the negative loglikelihood of an unimodal Gaussian, i.e. a mean squared error, we propose to estimate multiple hypotheses of the 3D pose by minimizing the negative log-likelihood of a multimodal mixture-of-Gaussians. The outputs of our mixture model is a set of mixing coefficients and parameters of the Gaussian kernels, i.e. means and variances. The set of 3D pose hypotheses are given by the means of the Gaussian kernels, and the mixing coefficient and variances represent the uncertainties of each 3D pose hypothesis. Specifically our network consists of a feature extractor to lift the 2D joints into a feature space, and a hypotheses generator to generate multiple hypotheses. The whole network is a simple network made up of several linear layers with different non-linear activation units.</p><p>We show that our network achieves state-of-the-art results on the Human3.6M dataset <ref type="bibr" target="#b10">[11]</ref> in both best hypothesis and multi-view settings. We also report results of our network on the outdoor MPII <ref type="bibr" target="#b1">[2]</ref> dataset and the MPI-INF-3DHP <ref type="bibr" target="#b15">[16]</ref> dataset, where 3D pose labels are not used for training the network. Furthermore, we show the robustness of our network by applying it to scenarios where one or two limb joints are occluded/missing. Our main contributions are as follows:</p><p>• We explore the idea of generating multiple 3D pose hypotheses to alleviate the ambiguity problem that has not received much attention in the literature.</p><p>• To the best of our knowledge, we are the first to introduce the mixture density model into 3D human pose estimation, which is more powerful than the single Gaussian distribution.</p><p>• Our network achieves state-of-the art results on Hu-man3.6M dataset in both best hypothesis and multiview settings, and in cases where one or two limb joints are occluded/missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Existing human 3D pose estimation approaches fall into two categories according to their training techniques. The first category is to train deep convolutional neural networks (CNNs) end-to-end to estimate 3D human poses directly from the input images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. Zhou et al. <ref type="bibr" target="#b27">[28]</ref> use a sparse representation for 3D poses and predict the 3D pose with an expectation-maximization (EM) algorithm. The 2D poses are regarded as a hidden variable in the EM algorithm to remove the need of synchronized 2D-3D data. Park et al. <ref type="bibr" target="#b18">[19]</ref> improve conventional CNNs by concatenating 2D pose estimation as well as information on relative positions with respect to multiple joints. Pavlakos et al. <ref type="bibr" target="#b19">[20]</ref> use volumetric representation to represent 3D poses and adopt the stacked hourglass network <ref type="bibr" target="#b17">[18]</ref>, which is originally designed for 2D pose estimation, to predict 3D volumetric heatmaps. Mehta et al. <ref type="bibr" target="#b15">[16]</ref> use transfer learning to transfer the knowledge learned for 2D pose estimation to the task of 3D pose estimation. Similarly, Zhou et al. <ref type="bibr" target="#b26">[27]</ref> propose a weakly-supervised transfer learning method that uses mixed 2D and 3D labels. The 2D pose estimation sub-network and 3D depth regression sub-network share the same features such that the 3D pose labels for indoor environments can be transferred to in-the-wild images. The direct approach benefits from the rich information contained in images, e.g. the front-back orientation of limbs. However, it will also be affected by a number of factors such as background, lighting, clothing etc. A network trained on one dataset can not be generalized well to the other datasets with different environment, for example from indoor and outdoor environment.</p><p>The second category <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17]</ref> decouples 3D pose estimation into the well-studied 2D joint detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> and 3D pose estimation from the detected 2D joints. Akhter et al. <ref type="bibr" target="#b0">[1]</ref> propose a multi-stage approach to estimate the 3D pose from 2D joints using an over-complete dictionary of poses. Bogo et al. <ref type="bibr" target="#b3">[4]</ref> estimate 3D pose by first fitting a statistical body shape model to the 2D joints, and then minimizing the error between the reprojected 3D model and detected 2D joints. Chen <ref type="bibr" target="#b5">[6]</ref> and Yasin <ref type="bibr" target="#b24">[25]</ref> regard 3D pose estimation as a matching between the estimated 2D pose and the 3D pose from a large pose library. Martinez et al. <ref type="bibr" target="#b14">[15]</ref> design a simple fully connected residual network to regress 3D pose from 2D joint detections. The decoupled approach can make use of both indoor and in-the-wild images to train the 2D pose estimators. More importantly, this approach is domain invariant since the input of the second stage is the 2D joints. However, estimating 3D pose from 2D joints is more challenging because 2D pose data contains less information than images, thus there are more ambiguities.</p><p>To solve the ill-posed problem of estimating 3D pose from 2D joints, Jahangiri and Yuille <ref type="bibr" target="#b11">[12]</ref> first proposed to generate multiple diverse pose hypotheses. They first learned a 3D Gaussian mixture model (GMM) model <ref type="bibr" target="#b21">[22]</ref> from a uniformly sampled set of Human3.6M poses, and then use conditional sampling to get samples of the 3D poses with reprojected joints errors that are within a threshold. Inspired by their work, we solve the ambiguity problem by generating multiple hypotheses. Instead of using the traditional GMM approach, we introduce the MDN which was first proposed in <ref type="bibr" target="#b2">[3]</ref>. The MDN can represent arbitrary conditional distributions by combining a conventional neural network with a mixture density model. Ye et al. <ref type="bibr" target="#b25">[26]</ref> used a hierarchical MDN to solve the occlusion problem in hand pose estimation. Inspired by the work of Ye et al., we use the MDN to solve the depth ambiguity and occlusion problem in 3D human pose estimation. <ref type="figure" target="#fig_1">Figure 2</ref> shows the illustration of our deep network to generate multiple hypotheses for 3D human pose estimation. Our network follows the commonly used two-stage approach that first estimates the 2D joints from the input images followed by the 3D pose estimation from the estimated 2D joints. We adopt the state-of-the-art stacked hourglass <ref type="bibr" target="#b17">[18]</ref> network as the 2D joint estimation module, and use our MDN which consists of a feature extractor and a hypotheses generator to generate the multiple 3D pose hypotheses. Given the 2D joint detections x ∈ R 2N , where N is the number of joints in one pose, our goal is to learn a function f : x → Θ which maps x into a set of output parameters Θ = {µ, σ, α} for our mixture model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Mixture Density Network</head><formula xml:id="formula_0">µ = {µ 1 , ..., µ M | µ i ∈ R 3N }, σ = {σ 1 , ..., σ M | σ i ∈ R} and α = {α 1 , ..., α M | 0 ≤ α i ≤ 1, i α i = 1}</formula><p>are the means, variances and mixing coefficients of the mixture model. M is the number of Gaussian kernels. The mean of each Guassian kernel µ i ∈ µ represents one 3D pose hypothesis, and the number of Gaussian kennels M decides the number of hypotheses generated by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Representation</head><p>The probability density of the 3D pose y ∈ R 3N given the 2D joints x ∈ R 2N is represented as a linear combination of Gaussian kernel functions</p><formula xml:id="formula_1">p(y | x) = M i=1 α i (x)φ i (y | x),<label>(1)</label></formula><p>where M is the number of Gaussian kernels, i.e. the number of hypotheses. α i (x) is the mixing coefficients, which can be regarded as a prior probability of a 3D pose data y being generated from the i th Gaussian kernel given the input 2D joints x. Here α i (x) must satisfy the constraint</p><formula xml:id="formula_2">0 ≤ α i (x) ≤ 1, M i=1 α i (x) = 1. (2) φ i (y | x)</formula><p>is the conditional density of the 3D pose y for the i th kernel, which can be expressed as a Gaussian distribution</p><formula xml:id="formula_3">φ i (y | x) = 1 (2π) d/2 σ i (x) d exp − y − µ i (x) 2 2σ i (x) 2 .<label>(3)</label></formula><p>µ i (x) and σ i (x) denote the mean and variance of the i th kernel, respectively. d is the dimension of the output 3D pose y. All the parameters of the mixture model, including the mixing coefficients α i (x), the mean µ i (x) and the variance σ i (x) are functions of the input 2D pose x.</p><p>Note that the mixture model degenerates to a single Gaussion distribution when the means and variances of all Gaussian kernels are similar, i.e.</p><formula xml:id="formula_4">µ i (x) ≈ µ(x), σ i (x) ≈ σ(x) for i = 1, ..., M . Hence, p(y | x) = M i=1 α i (x)φ i (y | x) ≈ M i=1 α i (x)N (µ(x), σ(x))<label>(4)</label></formula><p>= N (µ(x), σ(x)).</p><p>Specifically in our case, the 3D pose hypotheses generated by the MDN will collapse into approximately a single Gaussian when the given 2D pose is simple and less ambiguous, e.g. no occlusions and/or missing joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>From Eqn. <ref type="formula" target="#formula_1">(1)</ref>, <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref>, we can see that all parameters Θ(x) = {µ(x), σ(x), α(x)} of the Gaussian mixture distribution of y are functional form of x. Hence, we learn this function f : x → Θ using a deep network which can be expressed as</p><formula xml:id="formula_5">Θ = f (x; w),<label>(5)</label></formula><p>where w is the set of learnable weights in the deep network. The probability density in Eqn. <ref type="formula" target="#formula_1">(1)</ref> can be rewritten to include the learnable weights w of the deep network, i.e.,</p><formula xml:id="formula_6">p(y | x, w) = M i=1 α i (x, w)φ i (y | x, w),<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">φ i (y | x, w) = 1 (2π) d/2 σ i (x, w) d exp − y − µ i (x, w) 2 2σ i (x, w) 2 . (7) The parameters Θ(x, w) = {µ(x, w), σ(x, w), α(x, w)} are now dependent on the learnable weights w of the deep net- work f (x; w).</formula><p>We modify the 3D pose estimation module in <ref type="bibr" target="#b14">[15]</ref> to form our deep network f (x; w). More specifically, our approach is a simple multilayer neural network. Given an input of 2D joints x ∈ R 2N , we use one linear layer to map the input into an 1024 dimensional feature space, followed by two residual blocks which respectively consists of a linear layer, batch normalization , dropout , and Rectified Linear Units. And there are residual connections between the input and output of each residual block. Different from <ref type="bibr" target="#b14">[15]</ref> which adds another linear layer to directly regress the 3D pose y ∈ R 3N from the feature space, our network estimates the parameters Θ of the mixture model. In particular, we use different activation functions to satisfy the constraints of the three parameters Θ(</p><formula xml:id="formula_8">x, w) = {µ(x, w), σ(x, w), α(x, w)}.</formula><p>Specifically, we use a normal linear layer for parameter µ(x, w), a softmax function for the mixture coefficient α(x, w) so that it lies in the range of [0, 1] and sums up to 1, and a modified ELU function <ref type="bibr" target="#b6">[7]</ref> defined as:</p><formula xml:id="formula_9">h(t) = t + 1, if t ≥ 0 γ(exp(t) − 1) + 1, otherwise<label>(8)</label></formula><p>for the variance σ(x, w) to keep it positive. Here, γ is a scale for negative factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>Given a training dataset with K pairs of ground truth labels for the corresponding 2D joints X and 3D poses Y, i.e. {X, Y} = {{x j , y j } | j = 1, ..., K}, the objective is to find the maximum a posterior of the set of learnable weights w. More formally, assuming that each training data is independent and identically distributed (i.i.d), the posterior distribution of w is given by</p><formula xml:id="formula_10">p(w | X, Y, Ψ) ∝ p(Y | X, w)p(w | X, Ψ) (9) = p(w | X, Ψ) K j=1 p(y j | x j , w) = p(w | X, Ψ) K j=1 M i=1 α i (x j , w)φ i (y j | x j , w),</formula><p>where Ψ is the hyperparameter of the prior over the learnable weights w. Hence, the optimal weight w * can be obtained from the minimization of the negative log-posterior</p><formula xml:id="formula_11">w * = argmin w − ln p(w | X, Y, Ψ) (10) = argmin w − K j=1 ln p(y j | x j , w) − ln p(w | X, Ψ) L ,</formula><p>where L is taken to be the loss function for training our deep network f (x; w). More specifically,</p><formula xml:id="formula_12">L = − K j=1 ln p(y j | x j , w) − ln p(w | X, Ψ) (11) = − K j=1 ln M i=1 α i (x j , w)φ i (y j | x j , w) − ln p(w | X, Ψ) = L 3D + L prior .</formula><p>The prior loss L prior can be further evaluated into:</p><formula xml:id="formula_13">L prior = − ln p(w | X, Ψ) (12) = − ln p(w, X | Ψ) + ln p(X | Ψ) ∝ − ln p(Θ(w, X) | Ψ) = − ln p(α(w, X) | Ψ) − ln p(µ(w, X), σ(w, X) | Ψ),</formula><p>where the term ln p(X | Ψ) can be dropped in the loss function since it is independent of w, and we write the random variables {w, X} in its functional form Θ(w, X) given by the deep network. We further assume a uniform prior over µ(w, X) and σ(w, X), and a Dirichlet conjugate prior over the mixing coefficients α(w, X) that follows a Categorical distribution, we get</p><formula xml:id="formula_14">L prior = − ln p(α(w, X) | Λ) (13) = − K j=1 ln p(α 1 (w, x j ), ..., α M (w, x j ) | Λ), where p(α 1 , ..., α M | Λ) = Dir [α1,...,α M ] [λ 1 , ..., λ M ]<label>(14)</label></formula><formula xml:id="formula_15">= Γ[ M i=1 λ i ] M i=1 Γ[λ i ] M i=1 α i (w, x j ) λi−1 . Γ[.]</formula><p>is the Gamma function, and Λ = {λ 1 , ..., λ M } are the hyperparameters of the Dirichlet distribution, where λ i &gt; 0 for 1 ≤ i ≤ M . The total loss function to train our deep network is given by</p><formula xml:id="formula_16">L = L 3D + L prior , where L 3D = − K j=1 ln M i=1 α i (x j , w)φ i (y j | x j , w)<label>(15)</label></formula><formula xml:id="formula_17">L prior = − K j=1 M i=1 (λ i − 1) ln α i (w, x j ).</formula><p>Note that we drop</p><formula xml:id="formula_18">Γ[ M i=1 λi] M i=1 Γ[λi]</formula><p>in L prior because it is independent of w.</p><p>Remarks: The term L prior regularizes the mixing coefficients of our mixture model. Setting λ i = 1 for i = 1, .., M implies that we have no prior knowledge over the mixing coefficients. In our experiments, we set λ 1 = ... = λ M = C, where C &gt; 1 is a constant scalar value to prevent overfitting of a single Gaussian kernel in the MDN to the training data, i.e. a single mixing coefficient α i ≈ 1 and α j =i ≈ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our model is implemented in Tensorflow, and we use the ADAM <ref type="bibr" target="#b12">[13]</ref> optimizer with an initial learning rate of 0.001 and exponential decay. The batch size is set to 64 and we initialize the weights of linear layers with the Kaiming initialization <ref type="bibr" target="#b8">[9]</ref>. The number of Gaussian kernels is set to 5 and the hyperparameters {λ 1 , ..., λ M } in Eqn. <ref type="bibr" target="#b13">(14)</ref> are set to 2. We train our network for 200 epoches with a dropout rate of 0.5. We also apply max-norm constraint on the weight of each layer so that it is in range [0, 1]. Moreover, we clip the value of α i (x) to [1e−8, 1] and σ i (x) to [1e−15, 1e15] to prevent the training loss from becoming NaN. We also use the log-sum-exp trick as previous work <ref type="bibr" target="#b4">[5]</ref> to avoid the underflow problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Protocols</head><p>We show numerical results for the Human3.6M dataset <ref type="bibr" target="#b10">[11]</ref> and compare with other state-of-the-art approaches. We also apply our approach to other datasets including MPII <ref type="bibr" target="#b1">[2]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b15">[16]</ref> datasets to test the generalization capacity of our network.</p><p>Human3.6M dataset: This is currently the largest available video pose dataset, which provides accurate 3D body joint locations recorded by a Vicon motion capture system. There are 15 activity scenarios in total such as "Walking", "Eating", "Sitting" and "Discussion", each action is performed by 7 professional actors. Accurate 2D joint locations , 3D pose annotations and camera parameters are provided. Following <ref type="bibr" target="#b14">[15]</ref>, we apply standard normalization to the 2D inputs and 3D outputs by subtracting the mean and dividing by the standard deviation of the training data. We also zero-center the 3D poses around the hip joint since we do not predict the global position of the 3D pose.</p><p>MPII dataset: This is a standard dataset for 2D human pose estimation, which contains 25K unconstrained images collected from YouTube videos. This is the most challenging in-the-wild dataset and we use it to test the generalization of our approach. We report qualitative result for this datset because 3D pose information is not provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP dataset:</head><p>It is a newly released 3D human pose dataset which is captured by a Mocap system in both indoor and outdoor scenes. We only use the test split of this dataset that includes 2935 frames from six subjects performing seven actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D detections:</head><p>We use the state-of-the-art stacked hourglass network <ref type="bibr" target="#b17">[18]</ref> to get the 2D joint detections. The stacked hourglass network is pretrained on the MPII dataset and then fine-tuned on the Human3.6M dataset.</p><p>Evaluation protocols: For the Human3.6M dataset, we follow the standard protocol of using S1, S5, S6, S7 and S8 for training, and S9 and S11 for testing. The evaluation metric is the Mean Per Joint Position Error (MPJPE) in millimeter between the ground truth and the estimated 3D pose. Since our network generating multiple hypotheses for each 2D detection, we follow <ref type="bibr" target="#b11">[12]</ref> to compute the MPJPE between the ground truth and the best 3D hypothesis generated by our network. The 3D Percentate of Correct Keypoints (3DPCK) <ref type="bibr" target="#b15">[16]</ref> is adopted as the metric for the MPI-INF-3DHP dataset .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Human3.6M dataset</head><p>We first report our results on the Human3.6 dataset and compare with other state-of-the-art approaches. From the results shown in <ref type="table" target="#tab_0">Table 1</ref>, we can see that our method outperforms the others in most cases. Our approach achieves an  <ref type="figure">Figure 3</ref>: Qualitative results on the MPII test set. The first and second columns are the input images and output 2D joint detections of the stacked hourglass network, the last column is the 3D pose generated by our network.  <ref type="bibr" target="#b14">[15]</ref>. This indicates the efficiency of our approach by generating multiple hypotheses. Moreover, our network outperforms <ref type="bibr" target="#b11">[12]</ref> which also generates multiple hypotheses by 22.5%. Following previous work, We show our result under Protocol #2 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> where the estimated pose has been further aligned with the ground truth via a rigid transfermation. The MPJPE error in <ref type="table" target="#tab_0">Table 1</ref> shows that our approach consistently outperforms other approaches. It is difficult to disambiguate the multiple 3D pose hy-potheses generated by our model in a monocular view because most of them are feasible solutions to the inverse 2D-to-3D problem. Hence, we utilize the multi-view images from the set of calibrated cameras provided by the Hu-man3.6M dataset to disambiguate and verify the correctness of the multiple 3D pose hypotheses generated by our network. Specifically, we transform the same pose under different cameras into the global world coordinates, and then we choose the pose which is most consistent with the poses from other camera coordinates. Finally, we get our estimated pose by averaging all poses from different camera coordinates. We list our result in <ref type="table" target="#tab_1">Table 2</ref> and compare with other state-of-the-art approaches based on multi-view <ref type="bibr" target="#b20">[21]</ref> (spatial constraint) or video (temporal constraint) information <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10]</ref>. Note that it is however not a fair comparison    with other results listed in <ref type="table" target="#tab_0">Table 1</ref> because they did not use any multi-view or video information. The results show that our approach has the best performance among both spatial and temporal constraints based methods, indicating the advantage of our approach by generating multiple hypotheses. In realistic scenarios, it is common that some joints are occluded and cannot be detected. In order to show that our model can handle with missing joints, we ran experiments with different number of missing joints selected randomly from the limb joints including l/r elbow, l/r wrist, l/r knee, l/r ankle. We show our results in <ref type="table" target="#tab_2">Table 3</ref> and compared with the baseline 2D-to-3D estimator <ref type="bibr" target="#b14">[15]</ref> and the GMM based methods <ref type="bibr" target="#b11">[12]</ref> which also focus on generating multiple hypotheses. The baseline outperforms GMM based methods by a large margin, which indicates the advantage of using deep networks. Moreover, our method improves the baseline for all actions with average error decreased by 10.4mm for both cases, further showing the robustness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer to MPII and MPI-INF-3DHP datasets</head><p>We test our method on the MPII and MPI-INF-3DHP datasets to validate the generalization capacity. Note that we train the feature extractor and hypotheses generator on the Human3.6 dataset which contains data from only the indoor environment. The validation set of MPI-INF-3DHP dataset includes images recorded under three different scenes: 1143 images in studio with green screen background (Studio GS), 1064 images in studio without green screen background (Studio no GS) and 728 images in outdoor environment (Outdoor). We use the 2D joints provided by the dataset as input and compute the 3DPCK. The results in <ref type="table" target="#tab_3">Table 4</ref> show that the 3DPCK of our approach is slightly lower than <ref type="bibr" target="#b15">[16]</ref> even though we did not train on   <ref type="bibr" target="#b15">[16]</ref>. This further suggests the domain-invariant capability of the two-stage approach that we adopted. We only give qualitative results for the MPII dataset in <ref type="figure">Figure 3</ref> since the ground truth 3D pose data is not provided. We can see that our network can be generalized well to outdoor unseen scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Different number of kernels Our hypotheses generator is based on MDN where each of the M Gaussian kernels in Eqn.(1) yields different result. We note that our network cannot fit the data completely if M is too small, while larger M requires more computation resource. We thus train three different models with M setting to 3, 5, 8, respectively. We show the average MPJPE on the Human3.6M dataset in <ref type="table" target="#tab_4">Table 5</ref> and compare them with the baseline method, which is based on single Gaussian distribution. The results suggest that our MDN has better performance than single Gaussion based method. Moreover, the performance does not improve much when M is larger than five. Consequently, we set M to five in our experiments in view of the trade-off between accuracy and computational complexity.</p><p>Dirichlet prior We add a Dirichlet conjugate prior to the distribution of the mixture coefficients α(x) to prevent overfitting of a single Gaussion kernel to the training data. In order to explore the role of the Dirichlet prior, we compare the performance of our model with and without L prior . The results are shown in <ref type="table" target="#tab_5">Table 6</ref>, it can be seen that the performance improves by adding the Dirichlet conjugate prior, especially for the difficult poses in actions "Sitting" and "SittingDown". The reason is that most of the poses in the Human3.6 dataset are in a standing position, resulting in a worse performance on the "Sitting" and "SittingDown" actions. This further indicates that the Dirichlet conjugate prior can prevent overffiting effectively.</p><p>What is generated by each kernel? In order to explore the relation between different hypotheses, we reproject all five pose hypotheses into the image plane and compute the difference between projections and the 2D input joints. We adopt the PCKh@0.5 score <ref type="bibr" target="#b17">[18]</ref> which is the standard metric for 2D pose estimation to measure the difference. The high PCKh@0.5 score in <ref type="table" target="#tab_6">Table 7</ref> suggests that all the five hypotheses have almost the same 2D reprojections which are consistent with the 2D input. Note that we do not add any constraint as <ref type="bibr" target="#b11">[12]</ref> did to force all hypotheses to be consistent in the 2D reprojections. We give several visualization results in <ref type="figure" target="#fig_2">Figure 4</ref> to further illustrate the relations between all pose hypotheses. As described by Eqn. (4), each Gaussian kernel can be seen to generate the same hypotheses for simple pose with less ambiguity, e.g. standing (first row). This means that single Gaussian distribution is sufficient for simple poses. In comparison, our network can be seen to generate different hypotheses for challenging poses like "GettingDown" or "SittingDown" (second and third rows) due to two reasons. Firstly, our network receives lesser information on this type of poses since most of the poses in the Human3.6 dataset are the "standing" poses. Secondly, there are more ambiguities and occlusions for the "GettingDown" or "Sitting-Down" poses. As a result, our network generates multiple pose hypotheses to mitigate the increase of the uncertainty. We also visualize the 2D reprojections of all hypotheses in the last column. We indicate the corresponding 2D reprojection and 3D pose with the same color. The overlaps between the 2D reprojections further validate that our network generates hypotheses that are consistent in the 2D image coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduce the use of a mixture density network to generate multiple feasible hypotheses for the inverse problem of 3D human pose estimation from 2D inputs. Experimental results show that our network achieves state-of-the-art results in both best hypothesis and multiview settings. Furthermore, the 3D pose hypotheses generated by our network are consistent in the 2D reprojections suggests that the hypotheses model the ambiguity along the depth of the joints. Results on the MPII and MPI-INF-3DHP datasets further show the generalization capacity of our network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 :</head><label>1</label><figDesc>https://github.com/chaneyddtt/Generating-Multiple-Hypotheses-for-3D-Human-Pose-Estimation-with-Mixture-Density-Network An example of multiple feasible 3D pose hypotheses generated from our network reprojecting into similar 2D joint locations. (Best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our network consists of a feature extractor and a 3D pose hypotheses generator, it generates multiple pose hypotheses from the 2D joints detected by 2D pose estimator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>3D Pose hypotheses generated by our network. The first column is the input of our network, i.e. the 2D joints estimated by the stacked hourglass network. The second column is the ground truth 3D pose, and the third to seventh columns are the hypotheses generated by our network. The last column is the 2D reprojections of all five hypotheses. The corresponding 2D projection and 3D pose are drawn in the same color. (Best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results of MPJPE in millimeter on Human3.6M under protocol # 1 and # 2. (Best result in bold) Protocol #1 Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg. LinKDE et al. [11] 132.7 183.6 132.3 164.4 162.1 205.9 150.6 171.3 151.6 243.0 162.1 170.7 177.1 96.6 127.9 162.1 Du et al. [8] 85.1 112.7 104.9 122.1 139.1 135.9 105.9 166.2 117.5 226.9 120.0 117.7 137.4 99.3 106.5 126.5 Zhou et al. [28] 87.4 109.3 87.1 103.2 116.2 143.3 106.9 99.8 124.5 199.2 107.4 118.1 114.2 79.4 97.7 113.0 Pavlakos et al. [20] 67.4 71.9 66.7 69.1 72.0 77.0 65.0 68.3 83.7 96.5 71.7 65.8 74.9 59.1 63.2 71.9 Jahangiri et al. [12] 63.1 55.9 58.1 64.5 68.7 61.3 55.6 86.1 117.</figDesc><table><row><cell>6 71.0</cell><cell>71.2 66.3 57.1 62.5 61.0 68.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results by using multi-view information</figDesc><table><row><cell cols="5">Methods Lee[14] Hossain[10] Pavlakos [21] Ours</cell></row><row><cell>Avg.</cell><cell>52.8</cell><cell>51.9</cell><cell>56.9</cell><cell>49.6</cell></row><row><cell cols="5">improvement of 5.5% compared to the previous best result</cell></row><row><cell cols="5">55.8 mm [14] and 16.2 % compared to the baseline archi-</cell></row><row><cell>tecture</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results with one (the first three rows) or two (the last three rows) missing joints Direct. Discuss Eating Greet Phone Smoke Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg. Jahangiri et al. [12] 108.6 105.9 105.6 109.0 105.5 109.9 102.0 111.3 119.6 107.8 107.1 111.3 108.4 107.0 110.3 108.6 Martinez et al. [15] 57.4 61.6 64.3 65.6 73.3 85.5 61.0 62.1 84.0 101.1 68.2 66.7 70.8 55.6 59.6 69.1 Ours 48.9 53.9 54.5 55.5 62.6 70.4 51.3 52.0 69.7 83.9 60.7 57.2 62.4 48.3 50.8 58.8 Jahangiri et al. [12] 125.0 121.8 115.1 124.1 116.9 123.8 116.4 119.6 130.8 120.6 118.4 127.1 125.9 121.6 127.6 122.3 Martinez et al. [15] 62.9 66.9 69.9 71.4 80.2 93.8 66.3 65.9 90.</figDesc><table><row><cell>6 109.7</cell><cell>74.2 72.1 75.5 61.7 65.7 75.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results on MPI-INF-3DHP dataset</figDesc><table><row><cell></cell><cell cols="4">Studio GS Studio no GS Outdoor All PCK</cell></row><row><cell>Mehta et al. [16]</cell><cell>84.1</cell><cell>68.9</cell><cell>59.6</cell><cell>72.5</cell></row><row><cell>Ours</cell><cell>70.1</cell><cell>68.2</cell><cell>66.6</cell><cell>67.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison between different number of kernels</figDesc><table><row><cell>Number of kernels</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>8</cell></row><row><cell>Avg. MPJPE</cell><cell>62.9</cell><cell>55.2</cell><cell>52.7</cell><cell>52.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of our network with and without Dirichlet prior Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg. Wo prior 44.4 49.6 50.0 51.0 57.3 63.0 46.0 49.2 64.1 78.7 55.4 51.4 56.8 43.1 44.9 53.7 W prior 43.8 48.6 49.1 49.8 57.6 61.5 45.9 48.3 62.0 73.4 54.8 50.6 56.0 43.4 45.5 52.7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>The similarity of the 2D reprojections of all five pose hypotheses Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.</figDesc><table><row><cell>PCKh@0.5 99.6</cell><cell>99.5</cell><cell>99.6 94.9 99.5 99.7 99.9 98.8 99.0</cell><cell>87.6</cell><cell>99.6 94.6 99.1 99.2 99.5 98.1</cell></row><row><cell cols="3">their dataset, indicating the generalization of our network.</cell><cell></cell><cell></cell></row><row><cell cols="3">Moreover, our results for different scenes do not change too</cell><cell></cell><cell></cell></row><row><cell cols="3">much compared to the results of</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mixture density networks for distribution and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brando Guillaumes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Universitat Politècnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision, International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Marker-less 3d human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision,International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6988" to="6997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tracking loose-limbed people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="421" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Occlusion-aware hand pose estimation using hierarchical mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10872</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
