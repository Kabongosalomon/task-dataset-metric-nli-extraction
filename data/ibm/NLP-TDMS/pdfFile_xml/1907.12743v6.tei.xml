<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekwon</forename><surname>Yoo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sony Interactive Entertainment LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sony Interactive Entertainment LLC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Binghamton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two largescale video DA datasets with much larger domain discrepancy: UCF-HMDB f ull and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA 3 N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over "Source only" from 73.9% to 81.8% on "HMDB → UCF", and 10.3% gain on "Kinetics → Gameplay"). The code and data are released at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Domain adaptation (DA) <ref type="bibr" target="#b31">[32]</ref> has been studied extensively in recent years <ref type="bibr" target="#b4">[5]</ref> to address the domain shift problem <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b33">34]</ref>, which means the models trained on source labeled dataset do not generalize well to target datasets and tasks. DA is categorized in terms of the availability of annotations in the target domain. In this paper, we focus on the harder unsupervised DA problem, which requires training models that can generalize to target samples without access to any target labels. While many unsupervised DA approaches are able to diminish the distribution gap between source and target domains while learning discriminative deep features <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>, most methods have been developed only for images and not videos.</p><p>Furthermore, unlike image-based DA work, there do not exist well-organized datasets to evaluate and benchmark the performance of DA algorithms for videos. The most common datasets are UCF-Olympic and UCF-HMDB small <ref type="bibr">[</ref> In addition to spatial discrepancy between frame images, videos also suffer from temporal discrepancy between sets of time-ordered frames that contain multiple local temporal dynamics with different contributions to the overall domain shift, as indicated by the thickness of green dashed arrows. Therefore, we propose to focus on aligning the temporal dynamics which have higher domain discrepancy using a learned attention mechanism to effectively align the temporal-embedded feature space for videos. Here we use the action basketball as the example. <ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17]</ref>, which have only a few overlapping categories between source and target domains. This introduces limited domain discrepancy so that a deep CNN architecture can achieve nearly perfect performance even without any DA method (details in Section 5.2 and <ref type="table" target="#tab_3">Table 2</ref>). Therefore, we propose two larger-scale datasets to investigate video DA: 1) UCF-HMDB f ull : We collect 12 overlapping categories between UCF101 <ref type="bibr" target="#b42">[43]</ref> and HMDB51 <ref type="bibr" target="#b20">[21]</ref>, which is around three times larger than both UCF-Olympic and UCF-HMDB small , and contains larger domain discrepancy (details in Section 5.2 and <ref type="table" target="#tab_5">Tables 3 and 4)</ref>.</p><p>2) Kinetics-Gameplay: We collect from several currently popular video games with 30 overlapping categories with Kinetics-600 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. This dataset is much more challenging than UCF-HMDB f ull due to the significant domain shift between the distributions of virtual and real data. Videos can suffer from domain discrepancy along both the spatial and temporal directions, bringing the need of alignment for embedded feature spaces along both directions, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. However, most DA approaches have not explicitly addressed the domain shift problem in the temporal direction. Therefore, we first investigate different DA integration methods for video classification and show that: 1) aligning the features that encode temporal dynamics outperforms aligning only spatial features. 2) to effectively align domains spatio-temporally, which features to align is more important than what DA approaches to use. To support our claims, we then propose Temporal Adversarial Adaptation Network (TA 2 N), which simultaneously aligns and learns temporal dynamics, outperforming other approaches which naively apply more sophisticated imagebased DA methods for videos.</p><p>The temporal dynamics in videos can be represented as a combination of multiple local temporal features corresponding to different motion characteristics. Not all of the local temporal features equally contribute to the overall domain shift. We want to focus more on aligning those which have high contribution to the overall domain shift, such as the local temporal features connected by thicker green arrows shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Therefore, we propose Temporal Attentive Adversarial Adaptation Network (TA 3 N) to explicitly attend to the temporal dynamics by taking into account the domain distribution discrepancy. In this way, the temporal dynamics which contribute more to the overall domain shift will be focused on, leading to more effective temporal alignment. TA 3 N achieves state-of-the-art performance on all four investigated video DA datasets.</p><p>In summary, our contributions are three-fold: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Video Classification. With the rise of deep convolutional neural networks (CNNs), recent work for video classification mainly aims to learn compact spatio-temporal representations by leveraging CNNs for spatial information and designing various architectures to exploit temporal dynamics <ref type="bibr" target="#b17">[18]</ref>. In addition to separating spatial and temporal learning, some works propose different architectures to encode spatio-temporal representations with consideration of the trade-off between performance and computational cost <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47]</ref>. Another branch of work utilizes optical flow to compensate for the lack of temporal information in raw RGB frames <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29]</ref>. Moreover, some works extract temporal dependencies between frames for video tasks by utilizing recurrent neural networks (RNNs) <ref type="bibr" target="#b5">[6]</ref>, attention <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref> and relation modules <ref type="bibr" target="#b56">[57]</ref>. Note that we focus on attending to the temporal dynamics to effectively align domains and we consider other modalities, e.g. optical flow, to be complementary to our method.</p><p>Domain Adaptation. Most recent DA approaches are based on deep learning architectures designed for addressing the domain shift problems given the fact that the deep CNN features without any DA method outperform traditional DA methods using hand-crafted features <ref type="bibr" target="#b6">[7]</ref>. Most DA approaches follow the two-branch (source and target) architecture, and aim to find a common feature space between the source and target domains. The models are therefore optimized with a combination of classification and domain losses <ref type="bibr" target="#b4">[5]</ref>.</p><p>One of the main classes of methods used is Discrepancybased DA, whose metrics are designed to measure the distance between source and target feature distributions, including variations of maximum mean discrepancy (MMD) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b26">27]</ref> and the CORAL function <ref type="bibr" target="#b44">[45]</ref>. By diminishing the distance of distributions, discrepancybased DA methods reduce the gap across domains. Another common method, Adversarial-based DA, adopts a similar concept as GANs <ref type="bibr" target="#b12">[13]</ref> by integrating domain discriminators into the architectures. Through the adversarial objectives, the discriminators are optimized to classify different domains, while the feature extractors are optimized in the opposite direction. ADDA <ref type="bibr" target="#b47">[48]</ref> uses an inverted label GAN loss to split the optimization into two parts: one for the discriminator and the other for the generator. In contrast, the gradient reversal layer (GRL) is used in some work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b54">55]</ref> to invert the gradients so that the discriminator and generator are optimized simultaneously. Additionally, Normalization-based DA <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref> adapts batch nor-malization <ref type="bibr" target="#b15">[16]</ref> to DA problems by calculating two separate statistics, representing source and target, for normalization. Furthermore, Ensemble-based DA <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref> builds a target branch ensemble by incorporating multiple target branches. Recently, TADA <ref type="bibr" target="#b50">[51]</ref> adopts the attention mechanism to adapt the transferable regions. We extend these concepts to spatio-temporal domains, aiming to attend to the important parts of temporal dynamics for alignment.</p><p>Video Domain Adaptation. Unlike image-based DA, video-based DA is still an under-explored area. Only a few works focus on small-scale video DA with only a few overlapping categories <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17]</ref>. <ref type="bibr" target="#b43">[44]</ref> improves the domain generalizability by decreasing the effect of the background.</p><p>[52] maps source and target features to a common feature space using shallow neural networks. AMLS <ref type="bibr" target="#b16">[17]</ref> adapts pre-extracted C3D <ref type="bibr" target="#b45">[46]</ref> features on a Grassmann manifold obtained using PCA. However, the datasets used in the above works are too small to have enough domain shift to evaluate DA performance. Therefore, we propose two larger cross-domain datasets UCF-HMDB f ull and Kinetics-Gameplay, and provide benchmarks with different baseline approaches. Recently, TSRNet <ref type="bibr" target="#b55">[56]</ref> transfers knowledge for action localization using MMD, but only aligns the videolevel features. Instead, our TA 3 N simultaneously attends, aligns, and encodes temporal dynamics into video features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach</head><p>We first introduce our baseline model which simply extends image-base DA for videos using the temporal pooling mechanism (Section 3.1). And then we investigate better ways to incorporate temporal dynamics for video DA (Section 3.2), and describe our final proposed method with the domain attention mechanism (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline Model</head><p>Given the recent success of large-scale video classification using CNNs <ref type="bibr" target="#b17">[18]</ref>, we build our baseline on such architectures, as shown in the lower part of <ref type="figure" target="#fig_2">Figure 2</ref>.  We first feed the input video X i = {x 1 i , x 2 i , ..., x K i } extracted from ResNet <ref type="bibr" target="#b13">[14]</ref> pre-trained on ImageNet into our model, where x j i is the jth frame-level feature representation of the ith video. The model can be divided into two parts: 1) Spatial module G sf (.; θ sf ), which consists of multilayer perceptrons (MLP) that aims to convert the general-purpose feature vectors into task-driven feature vectors, where the task is video classification in this paper; 2) Temporal module G tf (.; θ tf ) aggregates the frame-level feature vectors to form a single video-level feature vector for each video. In our baseline architecture, we conduct mean-pooling along the temporal direction to generate video-level feature vectors, and note it as TemPooling. Finally, another fully-connected layer G y (.; θ y ) converts the video-level features into the final predictions, which are used to calculate the class prediction loss L y .</p><p>Similar to image-based DA problems, the baseline approach is not able to generalize to data from different domains due to domain shift. Therefore, we integrate TemPooling with the unsupervised DA method inspired by one of the most popular adversarial-based approaches, DANN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. The main idea is to add additional domain classifiers G d (.; θ d ), to discriminate whether the data is from the source or target domain. Before back-propagating the gradients to the main model, a gradient reversal layer (GRL) is inserted between G d and the main model to invert the gradient, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. During adversarial training, the parameters θ sf are learned by maximizing the domain discrimination loss L d , and parameters θ d are learned by minimizing L d with the domain label d. Therefore, the feature generator G f will be optimized to gradually align the feature distributions between the two domains.</p><p>In this paper, we note the Adversarial DiscriminatorĜ d as the combination of a gradient reversal layer (GRL) and a domain classifier, and insertĜ d into TemPooling in two ways: 1)Ĝ sd : show how directly applying image-based DA approaches can benefit video DA; 2)Ĝ td : indicate how DA on temporal-dynamics-encoded features benefits video DA.</p><p>The prediction loss L y , spatial domain loss L sd and temporal domain loss L td can be expressed as follows (ignoring all the parameter symbols through the paper to save space):</p><formula xml:id="formula_0">L i y = Ly(Gy(G tf (G sf (Xi))), yi)<label>(1)</label></formula><formula xml:id="formula_1">L i sd = 1 K K j=1 L d (G sd (G sf (x j i )), di)<label>(2)</label></formula><formula xml:id="formula_2">L i td = L d (G td (G tf (G sf (Xi))), di)<label>(3)</label></formula><p>where K is the number of frames sampled from each video. L is the cross entropy loss function. The overall loss can be expressed as follows:</p><formula xml:id="formula_3">L = 1 NS N S i=1 L i y − 1 NS∪T N S∪T i=1 (λsL i sd + λtL i td )<label>(4)</label></formula><p>where N S equals the number of source data, and N S∪T equals the number of all data. λ s and λ t is the trade-off weighting for spatial and temporal domain loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integration of Temporal Dynamics with DA</head><p>One main drawback of directly integrating image-based DA approaches into our baseline architecture is that the feature representations learned in the model are mainly from the spatial features. Although we implicitly encode the temporal information by the temporal pooling mechanism, the relation between frames is still missing. Therefore, we would like to address two questions: 1) Does the video DA problem benefit from encoding temporal dynamics into features? 2) Instead of only modifying feature encoding methods, how can DA be further integrated while encoding temporal dynamics into features?</p><p>To answer the first question, given the fact that humans can recognize actions by reasoning the observations across time, we propose the TemRelation architecture by replacing the temporal pooling mechanism with the Temporal Relation module, which is modified from <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b56">57]</ref>, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>The n-frame temporal relation is defined by the function:</p><formula xml:id="formula_4">Rn(Vi) = m g φ (n) ((V n i )m) (5) where (V n i ) m = {v a i , v b i , .</formula><p>..} m is the mth set of frame-level representations from n temporal-ordered sampled frames. a and b are the frame indices. We fuse the feature vectors that are time-ordered with the function g φ (n) , which is an MLP with parameters φ (n) . To capture temporal relations at multiple time scales, we sum up all the n-frame relation features into the final video representation. In this way, the temporal dynamics are explicitly encoded into features. We then insertĜ d into TemRelation as we did for TemPooling.</p><p>Although aligning temporal-dynamic-encoded features benefits video DA, feature encoding and DA are still two separate processes, leading to sub-optimal DA performance. Therefore, we address the second question by proposing Temporal Adversarial Adaptation Network (TA 2 N), which explicitly integratesĜ d inside the Temporal module to align the model across domains while learning temporal dynamics. Specifically, we integrate each nframe relation with a corresponding relation discriminator G n rd because different n-frame relations represent different temporal characteristics, which correspond to different parts of actions. The relation domain loss L rd can be expressed as follows:</p><formula xml:id="formula_5">L i rd = 1 K − 1 K n=2 L d (G n rd (Rn(G sf (Xi))), di)<label>(6)</label></formula><p>The experimental results show that our integration strategy can effectively align domains spatio-temporally for videos, and outperform those which are extended from sophisticated DA approaches although TA 2 N is adopted from a simpler DA method (DANN) (see details in <ref type="table" target="#tab_5">Tables 3 to 5</ref>).  <ref type="figure">Figure 3</ref>: The domain attention mechanism in TA 3 N. Thicker arrows correspond to larger attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Attentive Alignment for Videos</head><p>The final video representation of TA 2 N is generated by aggregating multiple local temporal features. Although aligning temporal features across domains benefits video DA, not all the features are equally important to align. In order to effectively align overall temporal dynamics, we want to focus more on aligning the local temporal features which have larger domain discrepancy. Therefore, we represent the final video representation as a combination of local temporal features with different attention weighting, as shown in <ref type="figure">Figure 3</ref>, and aim to attend to features of interest that are domain discriminative so that the DA mechanism can focus on aligning those features. The main question becomes: How to incorporate domain discrepancy for attention?</p><p>To address this, we propose Temporal Attentive Adversarial Adaptation Network (TA 3 N), as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, by introducing the domain attention mechanism, which utilize the entropy criterion to generate the domain attention value for each n-frame relation feature as below:</p><formula xml:id="formula_6">w n i = 1 − H(d n i )<label>(7)</label></formula><p>whered n i is the output of G n rd for the ith video. H(p) = − k p k · log(p k ) is the entropy function to measure uncertainty. w n i increases when H(d n i ) decreases, which means the domains can be distinguished well. We also add a residual connection for more stable optimization. Therefore, the final video feature representation h i generated from attended local temporal features, which are learned by local temporal modules G (n) tf , can be expressed as:</p><formula xml:id="formula_7">hi = K n=2 (w n i + 1) · G (n) tf (G sf (Xi))<label>(8)</label></formula><p>Finally, we add the minimum entropy regularization to refine the classifier adaptation. However, we only want to minimize the entropy for the videos that are similar across domains. Therefore, we attend to the videos which have low domain discrepancy, so that we can focus more on minimizing the entropy for these videos. The attentive entropy loss L ae can be expressed as follows:</p><formula xml:id="formula_8">L i ae = (1 + H(di)) · H(ŷi)<label>(9)</label></formula><p>whered i andŷ i is the output of G td and G y , respectively. We also adopt the residual connection for stability. By combining Equations (1) to <ref type="formula" target="#formula_2">(3)</ref>, <ref type="formula" target="#formula_5">(6)</ref> and <ref type="formula" target="#formula_8">(9)</ref>, and replacing G sf and G tf with h i by Equation <ref type="formula" target="#formula_7">(8)</ref>, the overall loss of TA 3 N can be expressed as follows:</p><formula xml:id="formula_9">L = 1 NS N S i=1 L i y + 1 NS∪T N S∪T i=1 γL i ae − 1 NS∪T N S∪T i=1 (λ s L i sd + λ r L i rd + λ t L i td )<label>(10)</label></formula><p>where λ s , λ r and λ t is the trade-off weighting for each domain loss. γ is the weighting for the attentive entropy loss. All the weightings are chosen via grid search. Our proposed TA 3 N and TADA <ref type="bibr" target="#b50">[51]</ref> both utilize entropy functions for attention but with different perspectives. TADA aims to focus on the foreground objects for image DA, while TA 3 N aims to find important and discriminative parts of temporal dynamics to align for video DA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>There are very few benchmark datasets for video DA, and only small-scale datasets have been widely used <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17]</ref>. Therefore, we specifically create two cross-domain datasets to evaluate the proposed approaches for the video DA problem, as shown in <ref type="table">Table 1</ref>. For more details about the datasets, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">UCF-HMDB f ull</head><p>We extend UCF-HMDB small <ref type="bibr" target="#b43">[44]</ref>, which only selects 5 visually highly similar categories, by collecting all of the relevant and overlapping categories between UCF101 <ref type="bibr" target="#b42">[43]</ref> and HMDB51 <ref type="bibr" target="#b20">[21]</ref>, which results in 12 categories. We follow the official split method to separate training and validation sets. This dataset, UCF-HMDB f ull , includes more than 3000 video clips, which is around 3 times larger than UCF-HMDB small and UCF-Olympic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Kinetics-Gameplay</head><p>In addition to real-world videos, we are also interested in virtual-world videos for DA. While there are more than ten real-world video datasets, there is a limited number of virtual-world datasets for video classification. It is mainly because rendering realistic human actions using game engines requires gaming graphics expertise which is timeconsuming. Therefore, we create the Gameplay dataset by collecting gameplay videos from currently popular video games, Detroit: Become Human and Fortnite, to build our own video dataset for the virtual domain. For the real domain, we use one of the largest public video datasets Kinetics-600 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. We follow the closed-set DA setting <ref type="bibr" target="#b33">[34]</ref> to select 30 overlapping categories between the UCF-HMDB small UCF-Olympic UCF-HMDB f ull Kinetics-Gameplay length (sec.) 1 - <ref type="table" target="#tab_3">21  1 -39  1 -33  1 -10  class #  5  6  12  30  video #  1171  1145  3209  49998   Table 1</ref>: The comparison of the cross-domain video datasets.</p><p>Kinetics-600 and Gameplay datasets to build the Kinetics-Gameplay dataset with both domains, including around 50K video clips. See the supplementary material for the complete statistics and example snapshots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We therefore evaluate DA approaches on four datasets: UCF-Olympic, UCF-HMDB small , UCF-HMDB f ull and Kinetics-Gameplay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>UCF-Olympic and UCF-HMDB small .</p><p>First, we evaluate our approaches on UCF-Olympic and UCF-HMDB small , and compare with all other works that also evaluate on these two datasets <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17]</ref>. We follow the default settings, but the method to split the UCF video clips into training and validations sets is not specified in these papers, so we follow the official split method from UCF101 <ref type="bibr" target="#b42">[43]</ref>.</p><p>UCF-HMDB f ull and Kinetics-Gameplay. For the self-collected datasets, we follow the common experimental protocol of unsupervised DA <ref type="bibr" target="#b33">[34]</ref>: the training data consists of labeled data from the source domain and unlabeled data from the target domain, and the validation data is all from the target domain. However, unlike most of the image DA settings, our training and validation data in both domains are separate to avoid potentially overfitting while aligning different domains. To compare with image-based DA approaches, we extend several state-of-the-art methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> for video DA with our TemPooling and TemRelation architectures, as shown in Tables 3 to 5. The difference between the "Target only" and "Source only" settings is the domain used for training. The "Target only" setting can be regarded as the upper bound without domain shift while the "Source only" setting shows the lower bound which directly applies the model trained with source data to the target domain without modification. See supplementary materials for full implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Results</head><p>UCF-Olympic and UCF-HMDB small . In these two datasets, our approach outperforms all the previous methods by at least 6.5% absolute difference (98.15% -91.60%) on the "U → O" setting, and 9% difference (99.33% -90.25%) on the "U → H" setting, as shown in <ref type="table" target="#tab_3">Table 2</ref>.  These results also show that the performance on these datasets is saturated. With a strong CNN as the backbone architecture, even our baseline architecture TemPooling can achieve high accuracy without any DA method (e.g. 96.3% for "U → O"). This suggests that these two datasets are not enough to evaluate more sophisticated DA approaches, so larger-scale datasets for video DA are needed.</p><formula xml:id="formula_10">Source → Target U → O O → U U → H H → U W.</formula><p>UCF-HMDB f ull . We then evaluate our approaches and compare with other image-based DA approaches on the UCF-HMDB f ull dataset, as shown in Tables 3 and 4. The accuracy difference between "Target only" and "Source only" indicates the domain gap. The gaps for the HMDB dataset are 11.11% for TemRelation and 10.28% for Tem-Pooling (see <ref type="table" target="#tab_5">Table 3</ref>), and the gaps for the UCF dataset are 21.01% for TemRelation and 17.16% for TemPooling (see <ref type="table" target="#tab_7">Table 4</ref>). It is worth noting that the "Source only" accuracy of our baseline architecture (TemPooling) on UCF-HMDB f ull is much lower than UCF-HMDB small (e.g. 28.39 lower for "U → H"), which implies that UCF-HMDB f ull contains much larger domain discrepancy than UCF-HMDB small . The value "Gain" is the difference from the "Source only" accuracy, which directly indicates the effectiveness of the DA approaches. We now answer the two questions for video DA in Section 3.2 (see <ref type="table" target="#tab_5">Tables 3 and 4):</ref> 1. Does the video DA problem benefit from encoding temporal dynamics into features?</p><p>From <ref type="table" target="#tab_5">Tables 3 and 4</ref>, we see that for the same DA method, TemRelation outperforms TemPooling in  most cases, especially for the gain value. For example, "TemPooling+DANN" reaches 0.83% absolute accuracy gain on the "U → H" setting and 0.17% gain on the "H → U" setting while "TemRelation+DANN" reaches 3.61% gain on "U → H" and 2.45% gain on "H → U". This means that applying DA approaches to the video representations which encode the temporal dynamics improves the overall performance for crossdomain video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">How to further integrate DA while encoding temporal dynamics into features?</head><p>Although integrating TemRelation with image-based DA approaches generally has better alignment performance than the baseline (TemPooling), feature encoding and DA are still two separate processes. The alignment happens only before and after the temporal dynamics are encoded in features. In order to explicitly force alignment of the temporal dynamics across domains, we propose TA 2 N, which reaches 77.22% (5.55% gain) on "U → H" and 80.56% (6.66% gain) on "H → U". <ref type="table" target="#tab_5">Tables 3 and 4</ref> show that although TA 2 N is adopted from a simple DA method (DANN), it still outperforms other approaches which are extended from more sophisticated DA methods but do not follow our strategy.</p><p>Finally, with the domain attention mechanism, our proposed TA 3 N reaches 78.33% (6.66% gain) on "U → H" and 81.79% (7.88% gain) on "H → U", achieving state-of-theart performance on UCF-HMDB f ull in terms of accuracy and gain, as shown in <ref type="table" target="#tab_5">Tables 3 and 4</ref>.</p><p>Kinetics-Gameplay. Kinetics-Gameplay is much more challenging than UCF-HMDB f ull because the data is from real and virtual domains, which have more severe domain shifts. Here we only utilize TemRelation as our backbone architecture since it is proved to outperform TemPooling on   UCF-HMDB f ull . <ref type="table" target="#tab_8">Table 5</ref> shows that the accuracy gap between "Source only" and "Target only" is 47.27%, which is more than twice the number in UCF-HMDB f ull . In this dataset, TA 3 N also outperforms all the other DA approaches by increasing the "Source only " accuracy from 17.22% to 27.50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study and Analysis</head><p>Integration ofĜ d . We use UCF-HMDB f ull to investigate the performance for integratingĜ d in different positions. There are three ways to insert the adversarial discriminator into our architectures, where each corresponds to different feature representations, leading to three types of discriminatorsĜ sd ,Ĝ td andĜ rd , which are shown in <ref type="figure" target="#fig_3">Figure 4</ref> and the full experimental results are shown in <ref type="table" target="#tab_10">Table 6</ref>. For the TemRelation architecture, the accuracy of utilizinĝ G td shows better performance than utilizingĜ sd (averagely 0.58% absolute gain improvement across two tasks), while the accuracies are the same for TemPooling. This means that the temporal relation module can encode temporal dynamics that help the video DA problem, but temporal pooling cannot. Utilizing the relation discriminatorĜ rd can further improve the performance (0.92% improvement) since we simultaneously align and learn the temporal dynamics across domains. Finally, by combining all three discriminators, TA 2 N improves even more (4.20% improvement).    Attention mechanism. In addition to TemRelation, we also apply the domain attention mechanism to TemPooling by attending to the raw frame features instead of relation features, and improve the performance as well, as shown in <ref type="table" target="#tab_11">Table 7</ref>. This implies that video DA can benefit from the domain attention even if the backbone architecture does not encode temporal dynamics. We also compare the domain attention module with the general attention module, which calculates the attention weights via the FC-Tanh-FC-Softmax architecture. However, it performs worse since the weights are computed within one domain, lacking of the consideration of domain discrepancy, as shown in <ref type="table" target="#tab_12">Table 8</ref>.</p><p>Visualization of distribution. To investigate how our approaches bridge the gap between source and target domains, we visualize the distribution of both domains using t-SNE <ref type="bibr" target="#b30">[31]</ref>. <ref type="figure">Figure 5</ref> shows that TA 3 N can group source data (blue dots) into denser clusters and generalize the distribution into the target domains (orange dots) as well.</p><p>Domain discrepancy measure. To measure the alignment between different domains, we use Maximum Mean Discrepancy (MMD) and domain loss, which are calculated using the final video representations. Lower MMD values and higher domain loss both imply smaller domain gap. TA 3 N reaches lower discrepancy loss (0.0842) compared to (a) TemPooling + DANN <ref type="bibr" target="#b11">[12]</ref> (b) TA 3 N <ref type="figure">Figure 5</ref>:  <ref type="table">Table 9</ref>: The discrepancy loss (MMD), domain loss and validation accuracy of our baselines and proposed approaches.</p><p>the TemPooling baseline (0.184), and shows great improvement in terms of the domain loss (from 1.116 to 1.9286), as shown in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we present two large-scale datasets for video domain adaptation, UCF-HMDB f ull and Kinetics-Gameplay, including both real and virtual domains. We use these datasets to investigate the domain shift problem across videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment without the need for sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA 3 N) to simultaneously attend, align and learn temporal dynamics across domains, achieving state-of-theart performance on all of the cross-domain video datasets investigated. The code and data are released here.</p><p>The ultimate goal of our research is to solve real-world problems. Therefore, in addition to integrating more DA approaches into our video DA pipelines, there are two main directions we would like to pursue for future work: 1) apply TA 3 N to different cross-domain video tasks, including video captioning, segmentation, and detection; 2) we would like to extend these methods to the open-set setting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15]</ref>, which has different categories between source and target domains. The open-set setting is much more challenging but closer to real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary</head><p>In the supplementary material, we would like to show more detailed ablation studies, more implementation details, and a complete introduction of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Visualization of distribution</head><p>We visualize the distribution of both domains using t-SNE <ref type="bibr" target="#b30">[31]</ref> to investigate how our approaches bridge the gap between the source and target domains. <ref type="figure" target="#fig_4">Figures 6a  and 6b</ref> show that the models using the TemPooling architecture poorly align the distribution between different domains, even with the integration of image-based DA approaches. <ref type="figure" target="#fig_4">Figure 6c</ref> shows the temporal relation module helps to group source data (blue) into denser clusters but is still not able to generalize the distribution into the target domains (orange). Finally, with TA 3 N, data from both domains are clustered and aligned with each other <ref type="figure" target="#fig_4">(Figure 6d</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Domain Attention Mechanism</head><p>We also apply the domain attention mechanism to Tem-Pooling by attending to the raw frame features, as shown in <ref type="figure" target="#fig_6">Figure 7</ref>. <ref type="table" target="#tab_15">Tables 10 and 11</ref> show that the domain attention mechanism improves the performance for both TemPooling and TemRelation architectures, including all types of adversarial discriminators. This implies that video DA can benefit from domain attention even if the backbone architecture does not encode temporal dynamics.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Detailed architectures</head><p>The architecture with detailed notations for the baseline is shown in <ref type="figure" target="#fig_7">Figure 8</ref>. For our proposed TA 3 N, after generating the n-frame relation features R n by the temporal relation module, we calculate the domain attention value w n using the domain predictiond from the relation discriminator G n rd , and then attend to R n using w n with a residual connection. To calculate the attentive entropy loss L ae , since the videos with low domain discrepancy are what we only want to focus on, we attend to the class entropy loss H(ŷ) using the domain entropy H(d) as the attention value with a residual connection, as shown in <ref type="figure" target="#fig_9">Figure 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Optimization</head><p>Our implementation is based on the PyTorch <ref type="bibr" target="#b32">[33]</ref> framework. We utilize the ResNet-101 model pre-trained on Im-ageNet as the frame-level feature extractor. We sample a fixed number K of frame-level feature vectors with equal spacing in the temporal direction for each video (K is equal to 5 in our setting to limit computational resource requirements). For optimization, the initial learning rate is 0.03, and we follow one of the commonly used learning-ratedecreasing strategies shown in DANN <ref type="bibr" target="#b11">[12]</ref>. We use stochastic gradient descent (SGD) as the optimizer with the momentum and weight decay as 0.9 and 1×10 −4 , respectively. The ratio between the source and target batch size is proportional to the scale between the source and target datasets. The source batch size depends on the scale of the dataset, which is 32 for UCF-Olympic and UCF-HMDB small , 128 for UCF-HMDB f ull and 512 for Kinetics-Gameplay. The optimized values of λ s , λ r and λ t are found using the coarse-to-fine grid-search approach. We first search using a coarse-grid with the geometric sequence [0, 10 −3 , 10 −2 , ..., 10 0 , 10 1 ]. After finding the optimized range of values, [0, 1], we search again using a fine-grid with the arithmetic sequence [0, 0.25, ..., 1]. The final values are 0.75 for λ s , 0.5 for λ r and 0.75 for λ t , respectively. We search γ only by a coarse-grid, and the best value is 0.3. For future work, we plan to adopt adaptive weighting techniques used for multitask learning, such as uncertainty weighting <ref type="bibr" target="#b19">[20]</ref> and Grad-Norm <ref type="bibr" target="#b3">[4]</ref>, to replace the manual grid-search method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Comparison with other work</head><p>As mentioned in the experimental setup, we compare our proposed TA 3 N with other approaches by extending several state-of-the-art image-based DA methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> for video DA with our TemPooling and TemRelation architectures, which are shown as follows:</p><p>1. DANN <ref type="bibr" target="#b11">[12]</ref>: we add one adversarial discriminatorĜ sd right after the spatial module and add another oneĜ td right after the temporal module. We do not add one more discriminator for relation features for the fair comparison between TemPooling and TemRelation.</p><p>2. JAN <ref type="bibr" target="#b26">[27]</ref>: we add Joint Maximum Mean Discrepancy (JMMD) to the final video representation and the class prediction.</p><p>3. AdaBN <ref type="bibr" target="#b22">[23]</ref>: we integrate an adaptive batchnormalization layer into the feature generator G sf . In the adaptive batch-normalization layer, the statistics (mean and variance) for both source and target domains are calculated, but only the target statistics are used for validating the target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MCD [39]</head><p>: we add another classifier G y and follow the adversarial training procedure of Maximum Classifier Discrepancy to iteratively optimize the generators (G sf and G tf ) and the classifier (G y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Datasets</head><p>The full summary of all four datasets investigated in this paper is shown in <ref type="table" target="#tab_3">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">UCF-HMDB f ull</head><p>We collect all of the relevant and overlapping categories between UCF101 <ref type="bibr" target="#b42">[43]</ref> and HMDB51 <ref type="bibr" target="#b20">[21]</ref>, which results in 12 categories: climb, fencing, golf, kick ball, pullup, punch, pushup, ride bike, ride horse, shoot ball, shoot bow, and walk. Each category may correspond to multiple categories in the original UCF101 or HMDB51 dataset, as shown in <ref type="table" target="#tab_5">Table 13</ref>. This dataset, UCF-HMDB f ull , includes 1438 training videos and 571 validation videos from UCF, and 840 training videos and 360 validation videos from HMDB, as shown in <ref type="table" target="#tab_3">Table 12</ref>. Most videos in UCF are from certain scenarios or similar environments, while videos in HMDB are in unconstrained environments and different camera angles, as shown in <ref type="figure" target="#fig_0">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Kinetics-Gameplay</head><p>We create the Gameplay dataset by first collecting gameplay videos from two video games, Detroit: Become Human and Fortnite, to build our own action dataset for the virtual domain. The total length of the videos is 5 hours and 41 minutes. We segment all of the raw, untrimmed videos into video clips according to human annotations, which results in 91 categories:       <ref type="table" target="#tab_3">Table 12</ref>. Kinetics-Gameplay is much more challenging than UCF-HMDB f ull due to the significant domain shift between the distributions of virtual and real data. Furthermore, The alignment between imbalanced-scaled source and target data is also another   <ref type="bibr" target="#b26">[27]</ref> does not perform well on Kinetics-Gameplay compared to the performance on UCF-HMDB f ull . The main reason is the imbalanced size between the source and target data in Kinetics-Gameplay. The discrepancy loss MMD is calculated using the same number of source and target data (not the case for other types of DA approaches). Therefore, in each iteration, MMD is calculated using parts of the source batch and the whole target batch. This means that the domain discrepancy is reduced only between part of source data and target data during training, so the learned model is still overfitted to the source domain. The discrepancy loss MMD works well when the source and target data are balanced, which is the case for most image DA datasets and UCF-HMDB f ull , but not for Kinetics-Gameplay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2">Comparison with AMLS [17]</head><p>When evaluating on UCF-HMDB small , AMLS <ref type="bibr" target="#b16">[17]</ref> finetunes their networks using UCF and HMDB, respectively, before applying their DA approach. Here we only show their results which are fine-tuned with source data, because the target labels should be unseen during training in unsupervised DA settings. For example, we don't compare their results which test on HMDB data using the models finetuned with HMDB data since it is not unsupervised DA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.3">Other baselines</head><p>3D ConvNets <ref type="bibr" target="#b45">[46]</ref> have also been used for extracting videolevel feature representations. However, 3D ConvNets consume a great deal of GPU memory, and <ref type="bibr" target="#b46">[47]</ref> also shows that 3D ConvNets are limited by efficiency and effectiveness issues when extracting temporal information.</p><p>Optical-flow extracts the motion characteristics between neighbor frames to compensate for the lack of temporal information in raw RGB frames. In this paper, we focus on attending to the temporal dynamics to effectively align domains even with only RGB frames. We consider opticalflow to be complementary to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.4">Comparison with literature in other fields</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cycle-consistency.</head><p>Some papers related to cycleconsistency <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b7">8]</ref> introduce self-supervised methods for learning visual correspondence between images or videos from unlabeled videos. They use cycle-consistency as free supervision to learn video representations. The main difference from our approach is that we explicitly align the feature spaces between source and target domains, while these self-supervised methods aim to learn general representations using only the source domain. We see cycleconsistency as a complementary method that can be integrated into our approach to achieve more effective domain alignment. Robotics. In Robotics, it is a common trend to transfer the models trained in simulation to real world. One of the effective method to bridge the domain gap is randomizing the dynamics of the simulator during training to improve the robustness for different environments <ref type="bibr" target="#b34">[35]</ref>. The setting is different from our task because we focus on feature learning rather than policy learning, and we see domain randomization as a complementary technique that can extend our approach to a more generalized version.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kinetics-Gameplay</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.5">Failure cases for TemRelation</head><p>TemRelation shows limited improvement over TemPooling for some categories with consistency across time. For example, with the same DA method (DANN), TemRelation has the same accuracy with TemPooling for ride bike (97%), and has lower accuracy for ride horse (93% and 97%). The possible reason is that temporal pooling can already model temporally consistent actions well, and it may be redundant to model these actions with multiple timescales like TemRelation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>44, * Work partially done as a SIE intern An overview of proposed TA 3 N for video DA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Baseline architecture (TemPooling) with the adversarial discriminatorsĜ sd andĜ td . L y is the class prediction loss, and L sd and L td are the domain losses. See the detailed architecture in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The overall architecture of the proposed Temporal Attentive Adversarial Adaptation Network (TA 3 N). In the temporal relation module, time-ordered frames are used to generate K-1 relation feature representations R = {R 2 , ..., R K }, where R n corresponds to the n-frame relation (the numbers in this figure are examples of time indices). After attending with the domain predictions from relation discriminators G n rd , the relation features are summed up to the final video representation. The attentive entropy loss L ae , which is calculated by domain entropy H(d) and class entropy H(ŷ), aims to enhance the certainty of those videos that are more similar across domains. See the detailed architecture in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The comparison of t-SNE visualization with source (blue) and target (orange) distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Baseline architecture (TemPooling) equipped with the domain attention mechanism (ignoring the input feature parts to save space).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>The detailed baseline architecture (TemPooling) with the adversarial discriminatorsĜ sd andĜ td .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>The detailed architecture of the proposed TA 3 N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Snapshots of some example categories on UCF-HMDB f ull . For each category, the snapshots from UCF are shown in the upper row, and the snapshots from HMDB are shown in the lower row.challenge. Some example snapshots are shown inFigure 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Some example screenshots from YouTube videos in Kinetics-Gameplay (left two: Gameplay, right two: Kinetics) 7.5. More Details 7.5.1 JAN on Kinetics-GameplayJAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and attends to representations with domain distribution discrepancy. TA 3 N achieves state-of-the-art performance on both small-and largescale cross-domain video datasets.</figDesc><table /><note>1. Video DA Dataset Collection: We collect two large-scale video DA datasets, UCF-HMDB f ull and Kinetics-Gameplay, to investigate the domain dis- crepancy problem across videos, which is an under- explored research problem. To our knowledge, they are by far the largest datasets for video DA problems.2. Feature Alignment Exploration for Video DA: We investigate different DA integration approaches for videos and provide a strategy to effectively align do- mains spatio-temporally for videos by aligning tempo- ral relation features. We propose this simple but effec- tive approach, TA 2 N, to demonstrate the importance of determining what to align over the DA method to use.3. Temporal Attentive Adversarial Adaptation Net- work (TA 3 N): We propose TA 3 N, which simultane- ously aligns domains, encodes temporal dynamics into video representations,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The accuracy (%) for the state-of-the-art work on UCF-Olympic and UCF-HMDB small (U: UCF, O: Olympic, H: HMDB). †We only show their results which are fine-tuned with source data for fair comparison. Please refer to the supplementary material for more details. ‡[17] did not test DAAA on UCF-HMDB small .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The comparison of accuracy (%) with other approaches on UCF-HMDB f ull (U → H). Gain represents the absolute difference from the "Source only" accuracy. TA 2 N and TA 3 N are based on the TemRelation architecture, so they are not applicable to TemPooling.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The comparison of accuracy (%) with other approaches on UCF-HMDB f ull (H → U).</figDesc><table><row><cell></cell><cell>Acc.</cell><cell>Gain</cell></row><row><cell>Target only</cell><cell>64.49</cell><cell>-</cell></row><row><cell cols="2">Source only 17.22</cell><cell>-</cell></row><row><cell cols="3">DANN [12] 20.56 3.34</cell></row><row><cell>JAN [27]</cell><cell cols="2">18.16 0.94</cell></row><row><cell cols="3">AdaBN [23] 20.29 3.07</cell></row><row><cell>MCD [39]</cell><cell cols="2">19.76 2.54</cell></row><row><cell cols="3">Ours (TA 2 N) 24.30 7.08</cell></row><row><cell cols="3">Ours (TA 3 N) 27.50 10.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The comparison of accuracy (%) with other approaches on Kinetics-Gameplay.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>The full evaluation of accuracy (%) for integratinĝ G d in different positions without the attention mechanism. Gain values are in ().</figDesc><table><row><cell>S → T</cell><cell cols="2">UCF → HMDB</cell><cell cols="2">HMDB → UCF</cell></row><row><cell>Temporal Module</cell><cell>TemPooling</cell><cell>TemRelation</cell><cell>TemPooling</cell><cell>TemRelation</cell></row><row><cell>Target only</cell><cell>80.56 (-)</cell><cell>82.78 (-)</cell><cell>92.12 (-)</cell><cell>94.92 (-)</cell></row><row><cell>Source only</cell><cell>70.28 (-)</cell><cell>71.67 (-)</cell><cell>74.96 (-)</cell><cell>73.91 (-)</cell></row><row><cell>AllĜ d</cell><cell>71.11 (0.83)</cell><cell>77.22 (5.55)</cell><cell>75.13 (0.17)</cell><cell>80.56 (6.66)</cell></row><row><cell>AllĜ d +Domain Attn.</cell><cell>73.06 (2.78)</cell><cell>78.33 (6.66)</cell><cell>78.46 (3.50)</cell><cell>81.79 (7.88)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>The affect of the domain attention mechanism.</figDesc><table><row><cell>S → T</cell><cell cols="2">UCF → HMDB HMDB → UCF</cell></row><row><cell>Target only</cell><cell>82.78 (-)</cell><cell>94.92 (-)</cell></row><row><cell>Source only</cell><cell>71.67 (-)</cell><cell>73.91 (-)</cell></row><row><cell>No Attention</cell><cell>77.22 (5.55)</cell><cell>80.56 (6.66)</cell></row><row><cell>General Attention</cell><cell>77.22 (5.55)</cell><cell>80.91 (7.00)</cell></row><row><cell>Domain Attention</cell><cell>78.33 (6.66)</cell><cell>81.79 (7.88)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>The comparison of different attention methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>The comparison of t-SNE visualization. The blue dots represent source data while the orange dots represent target data. See the supplementary for more comparison.</figDesc><table><row><cell></cell><cell cols="3">Discrepancy Domain Validation</cell></row><row><cell></cell><cell>loss</cell><cell>loss</cell><cell>accuracy</cell></row><row><cell>TemPooling</cell><cell>0.1840</cell><cell>1.1163</cell><cell>70.28</cell></row><row><cell>TemPooling + DANN [12]</cell><cell>0.1604</cell><cell>1.2023</cell><cell>71.11</cell></row><row><cell>TemRelation</cell><cell>0.2626</cell><cell>1.7588</cell><cell>71.67</cell></row><row><cell>TA 3 N</cell><cell>0.0842</cell><cell>1.9286</cell><cell>78.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>The evaluation of accuracy (%) for integratingĜ d in different positions on "U → H" . Gain values are in ().</figDesc><table><row><cell>Temporal Module</cell><cell>TemPooling</cell><cell>TemPooling + Attn.</cell><cell>TemRelation</cell><cell>TemRelation + Attn.</cell></row><row><cell>Target only</cell><cell cols="2">92.12 (-)</cell><cell cols="2">94.92 (-)</cell></row><row><cell>Source only</cell><cell cols="2">74.96 (-)</cell><cell cols="2">73.91 (-)</cell></row><row><cell>G sd</cell><cell>75.13 (0.17)</cell><cell>77.58 (2.62)</cell><cell>74.44 (1.05)</cell><cell>78.63 (4.72)</cell></row><row><cell>G td</cell><cell>75.13 (0.17)</cell><cell>78.46 (3.50)</cell><cell>75.83 (1.93)</cell><cell>81.44 (7.53)</cell></row><row><cell>G rd</cell><cell>-(-)</cell><cell>-(-)</cell><cell>75.13 (1.23)</cell><cell>78.98 (5.07)</cell></row><row><cell>AllĜ d</cell><cell>75.13 (0.17)</cell><cell>78.46 (3.50)</cell><cell>80.56 (6.66)</cell><cell>81.79 (7.88)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>The evaluation of accuracy (%) for integratingĜ</figDesc><table /><note>d in different positions on "H → U" . Gain values are in ().</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>argue, arrange object, assemble object, break, bump, carry, carve, chop wood, clap, climb, close door, close others, crawl, cross arm, crouch, crumple, cry, cut, dance, draw, drink, drive, eat, fall down, fight, fix hair, fly helicopter, get off, grab, haircut, hit, hit break, hold, hug, juggle coin, jump, kick, kiss, kneel, knock, lick, lie down, lift, light up, listen, make bed, mop floor, news anchor,</figDesc><table><row><cell></cell><cell>Video model</cell></row><row><cell cols="2">Temporal module</cell></row><row><cell cols="2">Temporal Relation module</cell></row><row><cell>1 3</cell><cell>(2)</cell></row><row><cell>2 4</cell><cell>(2)</cell></row><row><cell></cell><cell>…</cell></row><row><cell></cell><cell>(3)</cell></row><row><cell>Spatial</cell><cell>(3)</cell></row><row><cell>module</cell><cell></cell></row><row><cell></cell><cell>2</cell></row><row><cell></cell><cell>……</cell><cell>ℒ</cell></row><row><cell></cell><cell>……</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>The summary of the cross-domain video datasets.</figDesc><table><row><cell>UCF-HMDB f ull</cell><cell>UCF</cell><cell>HMDB</cell></row><row><cell>climb</cell><cell>RockClimbingIndoor,</cell><cell>climb</cell></row><row><cell></cell><cell>RopeClimbing</cell><cell></cell></row><row><cell>fencing</cell><cell>Fencing</cell><cell>fencing</cell></row><row><cell>golf</cell><cell>GolfSwing</cell><cell>golf</cell></row><row><cell>kick ball</cell><cell>SoccerPenalty</cell><cell>kick ball</cell></row><row><cell>pullup</cell><cell>PullUps</cell><cell>pullup</cell></row><row><cell>punch</cell><cell>Punch,</cell><cell>punch</cell></row><row><cell></cell><cell>BoxingPunchingBag,</cell><cell></cell></row><row><cell></cell><cell>BoxingSpeedBag</cell><cell></cell></row><row><cell>pushup</cell><cell>PushUps</cell><cell>pushup</cell></row><row><cell>ride bike</cell><cell>Biking</cell><cell>ride bike</cell></row><row><cell>ride horse</cell><cell>HorseRiding</cell><cell>ride horse</cell></row><row><cell>shoot ball</cell><cell>Basketball</cell><cell>shoot ball</cell></row><row><cell>shoot bow</cell><cell>Archery</cell><cell>shoot bow</cell></row><row><cell>walk</cell><cell>WalkingWithDog</cell><cell>walk</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>The lists of all collected categories in UCF and HMDB.open door, open others, paint brush, pass object, pet, poke, pour, press, pull, punch, push, push object, put object, raise hand, read, row boat, run, shake hand, shiver, shoot gun, sit, sit down, slap, sleep, slide, smile, stand, stand up, stare, strangle, swim, switch, take off, talk, talk phone, think, throw, touch, walk, wash dishes, water plant, wave hand, and weld. The maximum length for each video clip is 10 seconds, and the minimum is 1 second. We also split the dataset into training, validation, and testing sets by randomly selecting videos in each category with the ratio 7:2:1. We build the Kinetics-Gameplay dataset by selecting 30 overlapping categories between Gameplay and one of the largest public video datasets Kinetics-600<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>: break, carry, clean floor, climb, crawl, crouch, cry, dance, drink, drive, fall down, fight, hug, jump, kick, light up, news anchor, open door, paint brush, paraglide, pour, push, read, run, shoot gun, stare, talk, throw, walk, and wash dishes. Each category may also correspond to multiple categories in both datasets, as shown inTable 14. Kinetics-Gameplay includes 43378 training videos and 3246 validation videos from Kinetics, and 2625 training videos and 749 validation videos from Gameplay, as shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 14 :</head><label>14</label><figDesc>The lists of all collected categories in Kinetics and Gameplay.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.5.6</head><p>Testing time for TA 3 N Different from TA 2 N, TA 3 N passes data to all the domain discriminators during testing. However, since all our domain discriminators are shallow, the testing time is similar. In our experiment, TA 3 N only computes 10% more time than TA 2 N.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">A short note about kinetics-600. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comprehensive survey on domain adaptation for visual applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal cycleconsistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selfensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to cluster in order to transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep domain adaptation in action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshad</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Deodhare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Mohammad Haris Baig, and Daniel Ulbricht</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<publisher>ICLRW</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ts-lstm and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition. Signal Processing: Image Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attend and interact: Higher-order object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<idno>2017. 10</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop</title>
		<imprint>
			<publisher>NeurIPSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Syn2real: A new benchmark for synthetic-to-real visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09755</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sim-to-real transfer of robotic control with dynamics randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dataset Shift in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Human action recognition across datasets by foreground-weighted histogram decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop (ECCVW)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dual many-to-one-encoder-based transfer learning for crossdataset human action recognition. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning transferable selfattentive representations for action recognition in untrimmed videos with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
