<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
							<email>zongweiz@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85259</postCode>
									<settlement>Scottsdale</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vatsal</forename><surname>Sodha</surname></persName>
							<email>vasodha@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85259</postCode>
									<settlement>Scottsdale</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md Mahfuzur</roleName><forename type="first">Rahman</forename><surname>Siddiquee</surname></persName>
							<email>mrahmans@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85259</postCode>
									<settlement>Scottsdale</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85259</postCode>
									<settlement>Scottsdale</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
							<email>ntajbakh@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85259</postCode>
									<settlement>Scottsdale</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
							<email>gotway.michael@mayo.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>85259</postCode>
									<settlement>Scottsdale</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Liang</surname></persName>
							<email>jianming.liang@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85259</postCode>
									<settlement>Scottsdale</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning from natural image to medical image has established as one of the most practical paradigms in deep learning for medical image analysis. However, to fit this paradigm, 3D imaging tasks in the most prominent imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing rich 3D anatomical information and inevitably compromising the performance. To overcome this limitation, we have built a set of models, called Generic Autodidactic Models, nicknamed Models Genesis, because they are created ex nihilo (with no manual labeling), self-taught (learned by self-supervision), and generic (served as source models for generating application-specific target models). Our extensive experiments demonstrate that our Models Genesis significantly outperform learning from scratch in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but our Models Genesis consistently top any 2D approaches including fine-tuning the models pre-trained from ImageNet as well as fine-tuning the 2D versions of our Models Genesis, confirming the importance of 3D anatomical information and significance of our Models Genesis for 3D medical imaging. This performance is attributed to our unified self-supervised learning framework, built on a simple yet powerful observation: the sophisticated yet recurrent anatomy in medical images can serve as strong supervision signals for deep models to learn common anatomical representation automatically via selfsupervision. As open science, all pre-trained Models Genesis are available at https://github.com/MrGiovanni/ModelsGenesis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Transfer learning from natural image to medical image has established as one of the most practical paradigms in deep learning for medical image analysis. However, to fit this paradigm, 3D imaging tasks in the most prominent imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing rich 3D anatomical information and inevitably compromising the performance. To overcome this limitation, we have built a set of models, called Generic Autodidactic Models, nicknamed Models Genesis, because they are created ex nihilo (with no manual labeling), self-taught (learned by self-supervision), and generic (served as source models for generating application-specific target models). Our extensive experiments demonstrate that our Models Genesis significantly outperform learning from scratch in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but our Models Genesis consistently top any 2D approaches including fine-tuning the models pre-trained from ImageNet as well as fine-tuning the 2D versions of our Models Genesis, confirming the importance of 3D anatomical information and significance of our Models Genesis for 3D medical imaging. This performance is attributed to our unified self-supervised learning framework, built on a simple yet powerful observation: the sophisticated yet recurrent anatomy in medical images can serve as strong supervision signals for deep models to learn common anatomical representation automatically via selfsupervision. As open science, all pre-trained Models Genesis are available at https://github.com/MrGiovanni/ModelsGenesis. critical organs, which are prone to a number of diseases that result in substantial morbidity and mortality and thus are associated with significant health-care costs. In this research, we focus on Chest CT, because of its prominent role in diagnosing lung diseases, and our research community has accumulated several Chest CT image databases, for instance, LIDC-IDRI 1 and NLST 2 , containing a large number of Chest CT images. Therefore, we seek to answer the following question: Can we utilize the large number of available Chest CT images without systematic annotation to train source models that can yield high-performance target models via transfer learning?</p><p>To answer this question, we have developed a framework that trains generic, source models for 3D imaging. We call the models trained with our framework Generic Autodidactic Models, nicknamed Models Genesis, and refer to the model trained using Chest CT scans as Genesis Chest CT. As ablation studies, we have also trained a downgraded 2D version using 2D Chest CT slices, called Genesis Chest CT 2D. To demonstrate the effectiveness of Models Genesis in 2D applications, we have trained a 2D model based on ChestX-ray8 3 , named as Genesis Chest X-ray.</p><p>Our extensive experiments detailed in Sec. 3 demonstrate that Models Genesis, including Genesis Chest CT, Genesis Chest CT 2D, and Genesis Chest X-ray, significantly outperform learning from scratch in all seven target tasks (see <ref type="table">Table 1</ref>). As revealed in <ref type="table" target="#tab_2">Table 4</ref>, learning from scratch simply in 3D may not necessarily yield performance better than fine-tuning state-of-the-art Ima-geNet models, but our Genesis Chest CT consistently top any 2D approaches including fine-tuning ImageNet models as well as fine-tuning our Genesis Chest X-ray and Genesis Chest CT 2D, confirming the importance of 3D anatomical information in Chest CT and significance of our self-supervised learning method in 3D medical image analysis.</p><p>This performance is attributable to the following key observation: medical imaging protocols typically focus on particular parts of the body for specific clinical purposes, resulting in images of similar anatomy. The sophisticated yet recurrent anatomy offers consistent patterns for self-supervised learning to discover common representation of a particular body part (the lungs in our case). <ref type="figure" target="#fig_0">Fig. 1</ref>: Our unified self-supervised learning framework consolidates four novel transformations: I) non-linear, II) local-shuffling, III) out-painting, and IV) in-painting into a single image restoration task. Specifically, each arbitrarily-size patch X cropped at random location from an unlabeled image can undergo at most three of above transformations, resulting in a transformed patchX (see I-V). Note that out-painting and in-painting are mutually exclusive. For simplicity and clarity, we illustrate our idea on a 2D CT slice, but our Genesis Chest CT is trained using 3D images directly. A Model Genesis, an encoder-decoder architecture, is trained to learn a common visual representation by restoring the original patch X (as ground truth) from the transformed onẽ X (as input), aiming to yield high-performance target models.</p><p>The fundamental idea behind our unified self-supervised learning method as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> is to recover anatomical patterns from images transformed via various ways in a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models Genesis</head><p>Models Genesis learn from scratch on unlabeled images, with an objective to yield a common visual representation that is generalizable and transferable across diseases, organs, and modalities. In Models Genesis, an encoder-decoder, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, is trained using a series of self-supervised schemes. Once trained, the encoder alone can be fine-tuned for target classification tasks; while the encoder and decoder together can be for target segmentation tasks. For clarity, we formally define a training scheme as the process that transforms patches with any of the transformations, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, and trains a model to restore the original patches from the transformed counterparts. In the following, we first explain each of our self-supervised learning schemes with its learning objectives and perspectives, followed by a summary of the four unique properties of our Models Genesis. Along the way, we also contrast Models Genesis with existing approaches to show our innovations and novelties.</p><p>‚ Learning appearance via non-linear transformation. Absolute or relative intensity values in medical images convey important information about the imaged structures and organs. For instance, the Hounsfield Units in CT scans correspond to specific substances of the human body. As such, intensity information can be used as a strong source of pixel-wise supervision.</p><p>To preserve relative intensity information of anatomies during image transformation, we use Bézier Curve, a smooth and monotonous transformation function, which assigns every pixel a unique value, ensuring a one-to-one mapping. Restoring image patches distorted with non-linear transformation focuses Models Genesis on learning organ appearance (shape and intensity distribution). ‚ Learning texture via local pixel shuffling. Given an original patch, local pixel shuffling consists of sampling a random window from the patch followed by shuffling the order of contained pixels resulting in a transformed patch. The size of the local window determines the task difficulty, but we keep it smaller than the model's receptive field, and also small enough to prevent changing the global content of the image. Note that our method is quite different from PatchShuffling <ref type="bibr">[5]</ref>, which is a regularization technique to avoid over-fitting. To recover from local pixel shuffling, Models Genesis must memorize local boundaries and texture. Examples of local-shuffling are illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>-II. We include the underlying mathematics and implementation details in Appendix 4 Sec. C.</p><p>‚ Learning context via out-painting and in-painting. To realize the self-supervised learning via out-painting, we generate an arbitrary number of windows of various sizes and aspect ratios, and superimpose them on top of each other, resulting in a single window of a complex shape. We then assign a random value to all pixels outside the window while retaining the original intensities for the pixels within. As for in-painting, we retain the original intensities outside the window and replace the intensity values of the inner pixels with a constant value. Unlike <ref type="bibr">[6]</ref>, where in-painting is proposed as a proxy task by restoring only the patch central region, we restore the entire patch in the output. Out-painting compels Models Genesis to learn global geometry and spatial layout of organs via extrapolating, while inpainting requires Models Genesis to appreciate local continuities of organs via interpolating. Examples of out-painting and in-painting are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>-III and <ref type="figure" target="#fig_0">Fig. 1</ref>-IV, respectively. More visualizations can be found in Appendix 4 Secs. D-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models Genesis have the following four unique properties:</head><p>1) Autodidactic-requiring no manual labeling. Models Genesis are trained in a self-supervised manner with abundant unlabeled image datasets, demanding zero expert annotation effort. Consequently, Models Genesis are very different from traditional supervised transfer learning from ImageNet <ref type="bibr">[7,</ref><ref type="bibr">9]</ref>, which offers modest benefit to 3D medical imaging applications as well as that from the pretrained models of NiftyNet 5 , which is ineffective (see Sec. 3 and Appendix 4 Sec. I) due to the small datasets and specific applications (e.g., brain parcellation and organ segmentation) these models are trained for.</p><p>2) Eclectic-learning from multiple perspectives. Our unified approach trains Models Genesis from multiple perspectives (appearance, texture, context, etc.), leading to more robust models across all target tasks, as evidenced in <ref type="table">Table 3</ref>, where our unified approach is compared with our individual schemes. This eclectic approach, incorporating multiple tasks into a single image restoration task, empowers Models Genesis to learn more comprehensive representation.</p><p>3) Scalable-eliminating proxy-task-specific heads. Consolidated into a single image restoration task, our novel self-supervised schemes share the same encoder and decoder during training. Had each task required its own decoder, due to limited memory on GPUs, our framework would have failed to accommodate a large number of self-supervised tasks. By unifying all tasks as a single image restoration task, any favorable transformation can be easily amended into our framework, overcoming the scalability issue associated with multi-task learning <ref type="bibr">[2]</ref>, where the network heads are subject to the specific proxy tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Generic-yielding diverse applications. Models Genesis learn a generalpurpose image representation that can be leveraged for a wide range of target tasks. Specifically, Models Genesis can be utilized to initialize the encoder for the target classification tasks and to initialize the encoder-decoder for the target segmentation tasks, while the existing self-supervised approaches are largely focused on providing encoder models only <ref type="bibr">[4]</ref>. As shown in <ref type="table">Table 2</ref>, Models Genesis can be generalized across diseases (e.g., nodule, embolism, tumor), organs (e.g., lung, liver, brain), and modalities (e.g., CT, X-ray, MRI), a generic behavior that sets us apart from all previous works in the literature where the representation is learned via a specific self-supervised task; and thus lack generality. Such specific schemes include predicting the distance and 3D coordinates of two patches randomly sampled from a same brain <ref type="bibr">[8]</ref>, identifying whether two scans belong to the same person, predicting the level of vertebral bodies <ref type="bibr">[3]</ref>, and finally the systematic study by Tajbakhsh et al. <ref type="bibr">[10]</ref> where individualized self-supervised schemes are studied for a set of target tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Experiment protocol. Our Genesis CT and Genesis X-ray are self-supervised pre-trained from 534 CT scans in LIDC-IDRI 1 and 77,074 X-rays in ChestX-ray8 3 , respectively. The reason that we decided not to use all images in LIDC-IDRI and in ChestX-ray8 for training Models Genesis is to avoid test-image leaks between proxy and target tasks, so that we can confidently use the rest images solely for testing Models Genesis as well as the target models, although Models <ref type="table">Table 2</ref>: Fine-tuning models from our Genesis Chest CT (3D) significantly outperforms learning from scratch in the five 3D target tasks (p ă 0.05). The cells checked by denote the properties that are different between the proxy and target datasets. Our results show that our Genesis Chest CT generalizes across organs, diseases, datasets, and modalities. Footnotes show state-of-the-art performance for each target task. Genesis are trained from only unlabeled images, involving no annotation shipped with the datasets. We evaluate Models Genesis in seven medical imaging applications including 3D and 2D image classification and segmentation tasks (codified as detailed in <ref type="table">Table 1</ref>). For 3D applications in CT and MRI, we investigate the capability of both 2D slice-based solutions and 3D volume-based solutions; for 2D applications in X-ray and Ultrasound, we compare Models Genesis with random initialization and fine-tuning from ImageNet. 3D U-Net architecture 6 is used in five 3D applications; U-Net architecture with ResNet-18 encoder 7 is used in seven 2D applications. We utilize the L1-norm distance as the loss function in the image restoration tasks. Performances of target image classification and segmentation tasks are measured by the AUC (Area Under the Curve) and IoU (Intersection over Union), respectively, through at least 10 trials. We report the performance metrics with mean and standard deviation and further present statistical analysis based on the independent two-sample t-test.</p><p>Models Genesis outperform 3D models trained from scratch. We evaluate the effectiveness of Genesis Chest CT in five distinct 3D medical target tasks. These target tasks are selected such that they show varying levels of semantic distance to the proxy task, as shown in <ref type="table">Table 2</ref>, allowing us to investigate the transferability of Genesis Chest CT with respect to the domain distance. <ref type="table">Table 2</ref> demonstrates that models fine-tuned from Genesis Chest CT consistently outperform their counterparts trained from scratch. Our statistical analysis show that the performance gain is significant for all the target tasks under study. Specifically, for NCC and NCS where the target and proxy tasks are in the same domain, initialization with Genesis Chest CT achieves 4 and 3 points increase in the AUC and IoU score, respectively, compared with training from scratch. For ECC, the target and proxy tasks are different in both the disease affecting the organ and the dataset itself; yet, Genesis Chest CT achieves a remarkable improvement over training from scratch, increasing the AUC by 8 points. Genesis Chest CT continues to yield significant IoU gain for LCS and BMS even though <ref type="table">Table 3</ref>: Comparison between our unified framework and each of the suggested selfsupervised schemes on five 3D target tasks. The statistical analyses is conducted between the top-2 models in each column highlighted in red. While there is no clear winner, our unified framework is more robust across all target tasks, yielding either the best result or comparable performance to the best model (p ą 0.05). their domain distances with the proxy task are the widest. To our knowledge, we are the first to investigate cross-domain self-supervised learning in medical imaging. Given the fact that Genesis Chest CT is pre-trained on Check CT only, it is remarkable that our model can generalize to different diseases, organs, datasets, and even modalities.</p><formula xml:id="formula_0">Approach NCC (%) NCS (%) ECC (%) LCS (%) BMS (%)</formula><p>Models Genesis consistently top any 2D approaches. A common technique to handle limited data in medical imaging is to reformat 3D data into a 2D image representation followed by fine-tuning pre-trained ImageNet models <ref type="bibr">[7,</ref><ref type="bibr">9]</ref>. This approach increases the training examples by an order of magnitude, but it scarifies the 3D context. It is interesting to compare how Genesis Chest CT compares to this de facto standard in 2D. For this purpose, we adopt the trained 2D models from an ImageNet pre-trained model 7 for the tasks of NCC, NCS, and ECC. The 2D representation is obtained by extracting axial slices from volumetric datasets. <ref type="table" target="#tab_2">Table 4</ref> compares the results for 2D and 3D models. Note that the results for 3D models are identical to those reported in <ref type="table">Table 2</ref>. As evidenced by our statistical analyses, the 3D models trained from Genesis Chest CT significantly outperform the 2D models trained from ImageNet, achieving higher average performance and lower standard deviation (see <ref type="table" target="#tab_2">Table 4</ref> and Appendix 4 Sec. H). However, the same conclusion does not apply to the models trained from scratch-3D scratch models outperform 2D scratch models in only two out of the three target tasks and also exhibit undesirably larger standard deviation. We attribute the mixed results of 3D scratch models to the larger number of model parameters and limited sample size in the target tasks, which together impede the full utilization of 3D context. In fact, the undesirable performance of the 3D scratch models highlights the effectiveness of Genesis Chest CT, which unlocks the power of 3D models for medical imaging.</p><p>Models Genesis (2D) offer equivalent performances to supervised pretrained models. To compare our self-supervised approaches with those supervised pre-training from ImageNet <ref type="bibr">[1]</ref>, we deliberately downgrade our Models Genesis to 2D versions: Genesis Chest CT 2D and Genesis Chest X-ray (2D) (see visualization of Genesis 2D in Appendix 4 Secs. F-G). The statistical analysis in <ref type="figure" target="#fig_1">Fig. 2</ref> suggests that the downgraded Models Genesis 2D offer equivalent performance to state-of-the-art fine-tuning from ImageNet within modality, outperforming random initialization by a large margin, which is a significant achievement because ours comes at zero annotation cost. Meanwhile, the  downgraded Models Genesis 2D are fairly robust in cross-domain transfer learning, although they tend to underperform when domain distance is large, which suggests same-domain transfer learning should be preferred where possible in medical imaging. For 3D applications, we also examine the effectiveness of finetuning from NiftyNet 5 , which is not designed for transfer learning but is the only available supervised pre-trained 3D model. Compared with training from scratch, fine-tuning NiftyNet suffers 3.37, 0.18, and 0.03 points decrease for NCS, LCS, and BMS tasks, respectively (detailed in Appendix 4 Sec. I), suggesting that strong supervision with limited annotated data cannot guarantee good transferability like ImageNet. Conversely, Models Genesis benefit from both large scale unlabeled datasets and dedicated proxy tasks which are essential for learning general-purpose visual representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>A key contribution of ours is a collection of generic source models, nicknamed Models Genesis, built directly from unlabeled 3D image data with our novel unified self-supervised method, for generating powerful application-specific target models through transfer learning. While our empirical results are strong, surpassing state-of-the-art performances in most of the applications, an important future work is to extend our Models Genesis to modality-oriented models, such as Genesis MRI and Genesis Ultrasound, as well as organ-oriented models, such as Genesis Brain and Genesis Heart. In fact, we envision that Models Genesis may serve as a primary source of transfer learning for 3D medical imaging applications, in particular, with limited annotated data. To benefit the research community, we make the development of Models Genesis open science, releasing our codes and models to the public, and inviting researchers around the world to contribute to this effort. We hope that our collective efforts will lead to the Holy Grail of Models Genesis, effective across diseases, organs, and modalities.  Given an image, we first extract patches X of arbitrary sizes from random locations and then apply the transformations on them as mentioned in <ref type="figure" target="#fig_3">Fig. 4</ref>. Models Genesis learns the visual representation by restoring the original patches X from the transformed onesX. We have provided examples of the transformed images for Genesis Chest CT (left) and Genesis Chest X-ray (right). For simplicity and clarity, we illustrate our idea on a 2D CT slice and a 2D X-ray image, but our Genesis Chest CT is trained using 3D Check CT images directly. Each transformation is independently applied to a patch with a predefined probability, while out-painting and in-painting are considered mutually exclusive. Therefore, in addition to the four original individual transformations, this process yields eight more transformations framed in red, including one identity mapping (i.e., V: none, meaning none of the four individual transformations is selected) and seven combined transformations as indicated under each patch framed in red. For clarity, we further define a training scheme as the process that transforms patches with any of the twelve aforementioned transformations and trains a model to restore the original patches from the transformed ones. For convenience, we refer to an individual training scheme as the scheme using one particular individual transformation. Finally, our unified learning framework utilizes all possible transformations randomly with pre-defined probabilities and trains a model to restore the original patches from the ones undergone any possible transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Models Genesis</head><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, our proposed self-supervised learning framework consists of two components: image transformation (illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>) and image restoration, where Models Genesis, taking an encoder-decoder architecture, are trained by restoring original patch X from transformed patchX, aiming to learn common visual representation that is transferable and generalizable across diseases, organs and, modalities and thus yield high-performance target models. From this work, we have concluded:</p><p>1. Models Genesis significantly outperform learning from scratch in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but Models Genesis consistently top any 2D approaches including fine-tuning from ImageNet <ref type="bibr" target="#b20">[11]</ref> as well as fine-tuning our 2D Models Genesis, confirming the importance of 3D anatomical information and significance of our Models Genesis for 3D medical imaging. 2. Despite the outstanding performance of Models Genesis, a large, strongly annotated dataset for medical image analysis like ImageNet <ref type="bibr">[4]</ref> for computer vision is still highly demanded. In computer vision, at the time this paper is written, no self-supervised learning method outperforms fine-tuning models pre-trained from ImageNet <ref type="bibr">[10,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b21">12]</ref>. One of our goals of developing Models Genesis is to help create such a large, strongly annotated dataset for medical image analysis, because based on a small set of expert annotations, models fine-tuned from Models Genesis will be able to help quickly generate initial rough annotations of unlabeled images for expert review, thus reducing the annotation efforts and accelerating the creation of a large, strongly annotated, medical ImageNet. In summary, Models Genesis are not designed to replace such a large, strongly annotated dataset for medical image analysis like ImageNet for computer vision, but rather helping create one. 3. Same-domain transfer learning is always preferred whenever possible. Samedomain transfer learning strikes as a preferred choice in terms of performance; therefore, as our future work, we continue training modality-oriented models, including Genesis CT, Genesis MRI, Genesis X-ray, and Genesis Ultrasound, as well as organ-oriented models, including Genesis Brain, Genesis Lung, Genesis Heart, and Genesis Liver. 4. Cross-domain transfer learning is the Holy Grail. Retrieving a large number of unlabeled images from a PACS system requires an IRB approval, often a long process; the retrieved images must be de-identified; organizing the deidentified images in a way suitable for deep learning is tedious and laborious. Therefore, large quantities of unlabeled datasets may not be readily available to many target domains. Evidenced by our results in <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref>, Models Genesis have a great potential for cross-domain transfer learning; particularly, distortion-based approaches take advantage of relative intensity values (in all modalities) to learn shapes and appearances of various organs. Therefore, as our future work, we will be focusing on methods that generalize well in cross-domain transfer learning. Building the Holy Grail of Models Genesis, effective across diseases, organs, and modalities, takes a village. As a result, we make the development of Models Genesis open science, inviting researchers around the world to join this effort. All pre-trained Models Genesis will be made public at https://github.com/MrGiovanni/ModelsGenesis.  <ref type="figure" target="#fig_2">3</ref>) and X-ray (Rows 4-5) images is provided. We utilize four control points (P0-P3) in Eq. 1 to modify the shape of the transformation function (Row 1). Notice that, when P0 " P1 and P2 " P3 the transformation function is a linear function (shown in Examples 1-2). Besides, we set P0 " p0, 0q and P3 " p1, 1q to get an increasing function (shown in Examples 1, 3, and 5) and the opposite to get a decreasing function (shown in Examples 2, 4, and 6). The control points are randomly generated for more variances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Non-linear Intensity Transformation Visualization</head><p>We propose a novel self-supervised training scheme based on non-linear translation, with which the model learns to restore the intensity values of the input image transformed with a set of non-linear functions. The rationale is that the absolute intensity values (i.e., Hounsfield Units) in CT scans or relative intensity values in other imaging modalities convey important information about the underlying structures and organs <ref type="bibr">[2,</ref><ref type="bibr">5]</ref>. Hence, this training scheme enables the model to learn the appearance of the anatomic structures present in the images. In order to keep the appearance of the anatomic structures perceivable, we keep the non-linear intensity transformation function monotonic, allowing pixels of different values to be assigned with new distinct values. To realize this idea, we use Bézier Curve <ref type="bibr" target="#b23">[14]</ref>, a smooth and monotonous transformation function, which is generated from two end points (P 0 and P 3 ) and two control points (P 1 and P 2 ), defined as:</p><formula xml:id="formula_1">Bptq " p1´tq 3 P 0`3 p1´tq 2 tP 1`3 p1´tqt 2 P 2`t 3 P 3 , t P r0, 1s,<label>(1)</label></formula><p>where t is a fractional value along the length of the line. In <ref type="figure" target="#fig_4">Fig. 5</ref>, we illustrate the original patches (the left-most column) and the transformed patches of 2D CT and X-rays based on different transformation functions. The corresponding transformation functions are shown in the top row. In order to apply the transformation functions on CT images, we first clip the HU values to a range of r´1000, 1000s and then normalize to r0, 1s for each of the CT image slices.</p><p>In contrast, the X-ray images are directly normalized to r0, 1s without intensity clipping.</p><p>C Local Pixel Shuffling Visualization <ref type="figure">Fig. 6</ref>: We adopt local pixel shuffling as a new training scheme for self-supervised learning, which allows the model to learn the global geometry and spatial layout of organs as well as the local shape and texture of organs. Illustration of local pixel shuffling using multiple window sizes (Columns 2-7) applied on CT (Rows 1-2) and X-ray (Rows 3-4) images is provided. When 5ˆ5 window is applied, the shapes are largely maintained; while the ribs are mostly invisible for window size equal to 20ˆ20. Besides, various aspect ratios of windows also impose more local variances in different directions. Taking the restored X-ray patches in the last two columns as examples, a window size with h ! w (Column 6) distorts the boundary of the spine while preserving the overall presence of the ribs. On the other hand, when h " w (Column 7), the ribs are hardly visible but the width of spine and heart is barely changed.</p><p>We propose local pixel shuffling to enrich local variations of a patch without dramatically compromising its global structures, which encourages the model to learn the shapes and boundaries of the objects as well as the relative layout of different parts of the objects. To be specific, for each input patch, we randomly select 1,000 windows from the patch and then shuffle the pixels inside each window sequentially. Mathematically, let us consider a small window W with the size of mˆn. The local-shuffling acts on each window and can be formulated asW</p><formula xml:id="formula_2">" PˆWˆP 1 ,<label>(2)</label></formula><p>whereW is the transformed window, P and P 1 denote permutation metrics with the size of mˆm and nˆn, respectively. Pre-multiplying W with P permutes the rows of the window W, whereas post-multiplying W with P 1 results in the permutation of the columns of the window W. In practice, we keep the window sizes smaller than the receptive field of the network, so that the network can learn a more robust visual representation by "resetting" the original pixel positions. To facilitate the understanding, we have explored the local-shuffling transformation of varying window sizes and illustrated them along with the original patches. The window sizes can control the degree of distortion. As shown in <ref type="figure">Fig. 6</ref>, local-shuffling within an extent keeps the objects perceivable, it will benefit the deep neural network in learning invariant visual representations by restoring the original patches. Unlike de-noising <ref type="bibr" target="#b25">[16]</ref> and in-painting <ref type="bibr" target="#b24">[15,</ref><ref type="bibr">9]</ref>, our local-shuffling transformation does not intend to replace the pixel values with noise, which therefore preserves the identical global distributions to the original patch.</p><p>D Out-painting Visualization <ref type="figure">Fig. 7</ref>: We adopt out-painting as a new training scheme for self-supervised learning, which allows the model to learn the global geometry and spatial layout of organs. Illustration of the transformation for out-painting using various window sizes in CT (Rows 1-3) and X-ray (Rows 4-6) images is provided. The first and last columns denote the original patches and the final transformed patches, respectively. From Column 2 to Column 6, we generate a new window (red framed) and merge it with the existing ones. Moreover, to prevent the task to be too difficult or even unsolvable, we limit the masked surrounding region less than 1{4 of the whole patch.</p><p>We devise out-painting as a new training scheme for self-supervised learning, which allows the network to learn global geometry and spatial layout of organs in medical images by extrapolation. To realize it, we generate an arbitrary number (ď 10) of windows with various sizes and aspect ratios, and superimpose them on top of each other, resulting in a single window of a complex shape. When applying this merged window to a patch, we leave the patch region inside the window exposed and mask its surrounding with a random number. We have illustrated this process step by step in <ref type="figure">Fig. 7</ref>.</p><p>E In-painting Visualization <ref type="figure">Fig. 8</ref>: We adopt in-painting as a new training scheme for self-supervised learning, which allows the model to learn local shape and texture of organs in medical images via interpolation. The final transformed patches (Column 7) are obtained by iteratively superimposing a window of random size and aspect ratio, filled with a random number, to the original patches (Column 1). Columns 2-6 illustrate this process step by step. Similar to out-painting, the masked areas are also limited to be less than 1{4 of the whole patch, in order to keep the task reasonably difficult. <ref type="figure">Fig. 9</ref>: Qualitative assessment of CT image restoration quality using Models Genesis trained with different training schemes, including the unified framework and four individual training schemes. LIDC-IDRI <ref type="bibr">[1]</ref> is used for both training and testing. We test these models with transformed patches that have undergone four individual transformations as well as the identity mapping (i.e., no transformation). First of all, it can be seen the models trained with single-transformation-based schemes fail to handle other transformations. Taking non-linear transformation (Row 2) as an example, any individual training scheme besides non-linear transformation itself cannot invert the pixel intensity from transformed whitish to the original blackish. As expected, the model trained with the unified framework successfully restores original images from various transformations. Second, the model trained with the unified framework shows its superior to other models even if they are trained with and tested on the same transformation. For example, in local-shuffling case (Row 3), the patch recovered from the local-shuffling pre-trained model (Column 4) is noisy and lacks texture. However, the model trained with the unified framework (Column 7) generates a patch with more underlying structures, which demonstrates that learning with augmented tasks can even improve the performance on each individual tasks. These observations suggest that the model trained with the proposed unified self-supervised learning framework can successfully learn general anatomical structures and yield promising transferability on different target tasks.  <ref type="figure">Fig. 9</ref>. To further test our models, we show the restoration results on CT slices undergone seven different combined transformations. As expected, the model trained with our unified self-supervised framework significantly outperforms models trained with individual training schemes, further demonstrating the effectiveness of the proposed unified training framework as well as the pre-trained Models Genesis. <ref type="figure" target="#fig_0">Fig. 11</ref>: Qualitative assessment of image restoration quality by Genesis Chest CT across dataset, organ, and modality is visualized. Genesis Chest CT is trained on LIDC-IDRI (CT) <ref type="bibr">[1]</ref> via our unified self-supervised training framework. For testing, we use the pre-trained model to directly restore images from LIDC-IDRI (CT), ChestX-ray8 (Xray) <ref type="bibr" target="#b26">[17]</ref>, CIMT (Ultrasound) <ref type="bibr">[8,</ref><ref type="bibr" target="#b27">18]</ref>, and BraTS (MRI) <ref type="bibr" target="#b22">[13]</ref>. Though the model is only trained on CT data, it can largely maintain the texture and structures during restoration not only in the same modality (CT), but also in different modalities including X-ray, Ultrasound, and MRI, suggesting that Genesis Chest CT is transferable across datasets, organs, and modalities. Besides, we notice that the restoration quality is also consistent with the results of Genesis Chest CT on target tasks (see <ref type="figure" target="#fig_1">Fig. 2</ref>). For example, compared to cross-modality performance, Genesis Chest CT yields better performance in CT for both restoration and target tasks (i.e., NCC and NCS). Moreover, the relative lower restoration quality of ultrasound images may explain the relative lower target performance of Genesis Chest CT on IUC (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Finally, by comparing the performance of Genesis Chest CT in various modalities, we find out that a model pre-trained in the same domain is still preferred whenever possible. Thereby, we will continue developing modality-oriented models including Genesis MRI and Genesis Ultrasound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Genesis Chest CT</head><p>G Genesis Chest X-ray <ref type="figure" target="#fig_0">Fig. 12</ref>: Qualitative comparisons of Genesis Chest X-ray trained with unified selfsupervised framework and four individual training schemes. We train and test all five models on ChestX-ray8 <ref type="bibr" target="#b26">[17]</ref> where transformed patches (Column 1) undergo one of the four transformations (Rows 2-5) as well as an identity mapping (Row 1). It is clear from the figure that the models trained with a single transformation fail to handle other transformations. For example, considering the training scheme based on in-painting (Row 5), models trained on individual training schemes fail to in-paint the masked region except for the in-painting-trained model (Column 6). However, the model trained with the unified framework (Column 7) handles all of the transformations and generates patches fairly close to the ground truths. These observations suggest that Models Genesis trained with proposed unified self-supervised learning framework learns general anatomical structures better, yielding high-performance target models through transfer learning.  Qualitative results of image restoration from Genesis Chest X-ray across dataset, organ, and modality are visualized. Genesis Chest X-ray is trained on Chest X-ray8 (X-ray) <ref type="bibr" target="#b26">[17]</ref> via our unified training framework, and tested to restore images from Chest X-ray8 (X-ray), LIDC-IDRI (CT) <ref type="bibr">[1]</ref>, CIMT (Ultrasound) <ref type="bibr">[8,</ref><ref type="bibr" target="#b27">18]</ref>, and BraTS (MRI) <ref type="bibr" target="#b22">[13]</ref>. Similar to <ref type="figure" target="#fig_0">Fig. 11</ref>, we observe that the performance of restoration and target tasks in various modalities may be positively correlated. For instance, while Genesis Chest X-ray restores ultrasound images reasonably, it injects unintended artifacts in the restored CT slices. As a result, Genesis Chest X-ray achieves better performance on IUC task compared with Genesis Chest CT, but it fails on NCC task (see <ref type="figure" target="#fig_1">Fig. 2</ref>). The analysis further confirms our claims provided in <ref type="figure" target="#fig_0">Fig. 11</ref>. 0.0848 0.0520 0.2102 0.4249 0.4276 :: These p-values are calculated between the top-2 models in each column highlighted in red. <ref type="figure" target="#fig_0">Fig. 15</ref>: Comparison of Models Genesis and Models ImageNet. In the top three subfigures, the 3D volume-based solutions and 2D slice-based solutions are denoted with square and diamond markers, respectively. The horizontal and vertical error bars indicate 95% confidence intervals of training from scratch and fine-tuning, respectively. The shorter the vertical bar, the more consistent and stable the model is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Models Genesis vs. Models ImageNet</head><p>The comparisons of our Models Genesis and Models ImageNet (i.e., models pre-trained on ImageNet) are summarized in three figures and two tables in <ref type="figure" target="#fig_0">Fig. 15</ref>. Training 3D models simply from scratch does not necessarily outperform the 2D counterparts (see NCC), however, fine-tuning the same 3D models from Genesis Chest CT significantly outperforms (p ă 0.05) the slice-based 2D models including fine-tuning from Models ImageNet. As seen, Models Genesis enjoys a higher stability on the target tasks. Moreover, comparing our unified framework with individual training schemes demonstrates that the former is more robust across all target tasks, yielding either the best result or comparable performance to the best model (p ă 0.05). This superiority of our Models Genesis is attributable to consolidating multiple self-supervised training schemes, which enables the model to learn a stronger image representation. Thus, fine-tuning Models Genesis leads to powerful and stable application-specific target models, confirming the importance of Models Genesis in 3D medical imaging.  The table in <ref type="figure" target="#fig_0">Fig. 16</ref> compares fine-tuning the pre-trained NiftyNet with training from scratch on three target tasks: (1) lung nodule segmentation (NCS) in CT images, (2) liver segmentation (LCS) in CT images, and (3) brain tumor segmentation (BMS) in MRI images using dice-coefficient (mean˘s.d.) as the evaluation metric, demonstrating that fine-tuning NiftyNet's 3D supervised pretrained weights has no benefit over random initialization (p ą 0.05). It is further corroborated by the learning curves on validation dataset provided at the bottom in <ref type="figure" target="#fig_0">Fig. 16</ref>. However, Models Genesis significantly improve performance over random initialization (see <ref type="table">Table 2</ref> in the main paper) and perform consistently better than NiftyNet models on the three same target tasks. Note that the pretrained NiftyNet model was trained using strong supervision, whereas Models Genesis learns representations using the proposed self-supervised paradigm. In contrast to pre-trained weights of NiftyNet's model zoo, the pre-trained weights from our proposed self-supervised method are found to be more robust across diseases, organs, and imaging modalities, thanks to the ability of our approach to learn representations from a large-scale unannotated dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I The NiftyNet Transfer Learning Capability</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 -</head><label>1</label><figDesc>I shows examples of the transformed images. Due to limited space, we provide the implementation details in Appendix 4 Sec. B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Comparison of 2D solutions on four 2D target tasks. To investigate the same-and cross-domain transferability of Models Genesis, we have trained Genesis Chest CT 2D using 2D axial slices from LUNA dataset (left panel), and Genesis Chest X-ray (2D) trained using radiographs from ChestX-ray8 dataset (right panel). In same-domain target tasks (NCC and NCS in the left panel and DXC in the right panel), Models Genesis 2D outperform training from scratch and offer equivalent performance to fine-tuning from ImageNet. While in cross-domain target tasks (DXC and IUC in the left panel; NCS and IUC in the right panel), Models Genesis 2D also produce fairly robust performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of our unified self-supervised learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>[Better viewed on-line in color and zoomed in for details] Our novel unified selfsupervised learning framework aims to learn general-purpose visual representation by recovering original image patches from their transformed ones. We have designed four individual transformations: I) non-linear (see Sec. B), II) local-shuffling (see Sec. C), III) out-painting (see Sec. D), and IV) in-painting (see Sec. E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>We adopt non-linear intensity transformation as a new training scheme for selfsupervised learning, which allows the model to learn the absolute or relative appearance of structures. Illustration of various non-linear intensity transformations (Examples 1-6) for CT (Rows 2-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Continued from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 13 :</head><label>13</label><figDesc>Continued fromFig. 12. We further test our five models on seven combinations of transformations. The models train with individual training scheme can only handle a single transformation (Columns 3-6) and fail to restore the patches completely, while Models Genesis trained via proposed unified self-supervised learning framework (Column 7) fairly handle seven augmented transformations and restores the patches close to the original patch. Taking a combination of out-painting and non-linear transformation (Row 1) as an example, the model trained on non-linear-based scheme (Column 3) recovers the original intensity values, but fails to out-paint the image; however, the model trained with a unified framework not only recovers the original intensity values but also out-paints the image. This observation demonstrates the superiority of Models Genesis trained with unified self-supervised learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Qualitative results of image restoration from Genesis Chest X-ray across dataset, organ, and modality are visualized. Genesis Chest X-ray is trained on Chest X-ray8 (X-ray) [17] via our unified training framework, and tested to restore images from Chest X-ray8 (X-ray), LIDC-IDRI (CT) [1], CIMT (Ultrasound) [8,18], and BraTS (MRI) [13]. Similar to Fig. 11, we observe that the performance of restoration and target tasks in various modalities may be positively correlated. For instance, while Genesis Chest X-ray restores ultrasound images reasonably, it injects unintended artifacts in the restored CT slices. As a result, Genesis Chest X-ray achieves better performance on IUC task compared with Genesis Chest CT, but it fails on NCC task (see Fig. 2). The analysis further confirms our claims provided in Fig. 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 16 :</head><label>16</label><figDesc>Fine-tuning the pre-trained NiftyNet vs. training it from scratch. The results reported in the table statistically infer that fine-tuning the pre-trained NiftyNet offers no benefit over training it from scratch. This fact is further supported by the learning curve comparison provided in the figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>LUNA winner holds an official score of 0.968 vs. 0.971 (ours) 2 Wu et al. holds a Dice of 74.05% vs. 75.86%˘0.90% (ours) 3 Zhou et al. holds an AUC of 87.06% vs. 88.04%˘1.40% (ours) 4 LiTS winner w/ postprocessing (PP) holds a Dice of 96.60% vs. 91.13%˘1.51% (ours w/o PP)</figDesc><table><row><cell cols="2">Task Metric Disease Organ Dataset Modality</cell><cell>Scratch (%)</cell><cell>Genesis (%)</cell><cell>p-value</cell></row><row><cell>NCC 1</cell><cell>AUC</cell><cell>94.25˘5.07</cell><cell>98.20˘0.51</cell><cell>0.0180</cell></row><row><cell>NCS 2</cell><cell>IoU</cell><cell>74.05˘1.97</cell><cell>77.62˘0.64</cell><cell>1.04e-4</cell></row><row><cell>ECC 3</cell><cell>AUC</cell><cell>79.99˘8.06</cell><cell>88.04˘1.40</cell><cell>0.0058</cell></row><row><cell>LCS 4</cell><cell>IoU</cell><cell>74.60˘4.57</cell><cell>79.52˘4.77</cell><cell>0.0361</cell></row><row><cell>BMS 5</cell><cell>IoU</cell><cell>90.16˘0.41</cell><cell>90.60˘0.20</cell><cell>0.0041</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>5 BraTS winner w/ ensembling holds a Dice of 91.00% vs. 92.58%˘0.30% (ours w/o ensembling)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Comparison between 3D solutions and 2D slice-based solutions on three 3D target tasks. Training 3D models from scratch does not necessarily outperform the 2D counterparts (see NCC). However, training the same 3D models from Genesis Check CT outperforms (p ă 0.05) all 2D solutions, demonstrating the effectiveness of Genesis Chest CT in unlocking the power of 3D models. These p-values are calculated between our Models Genesis vs. the fine-tuning from ImageNet, which always offers the best performance (highlighted in red) for all three tasks in 2D.</figDesc><table><row><cell>Task</cell><cell>Scratch</cell><cell>2D (%) ImageNet</cell><cell>Genesis</cell><cell>Scratch</cell><cell>3D (%) ImageNet</cell><cell>Genesis</cell><cell>p-value :</cell></row><row><cell>NCC</cell><cell cols="3">96.03˘0.86 97.79˘0.71 97.45˘0.61</cell><cell>94.25˘5.07</cell><cell>N/A</cell><cell cols="2">98.20˘0.51 0.0213</cell></row><row><cell>NCS</cell><cell cols="3">70.48˘1.07 72.39˘0.77 72.20˘0.67</cell><cell>74.05˘1.97</cell><cell>N/A</cell><cell>77.62˘0.64</cell><cell>ă1e-8</cell></row><row><cell>ECC</cell><cell cols="3">71.27˘4.64 78.61˘3.73 78.58˘3.67</cell><cell>79.99˘8.06</cell><cell>N/A</cell><cell cols="2">88.04˘1.40 5.50e-4</cell></row></table><note>:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis Zongwei Zhou 1 , Vatsal Sodha 1 , Md Mahfuzur Rahman Siddiquee 1 , Ruibin Feng 1 , Nima Tajbakhsh 1 , Michael B. Gotway 2 , and Jianming Liang 1 Arizona State University, Scottsdale, AZ 85259 USA {zongweiz,vasodha,mrahmans,rfeng12,ntajbakh,jianming.liang}@asu.edu</figDesc><table /><note>12 Mayo Clinic, Scottsdale, AZ 85259 USA Gotway.Michael@mayo.edu Abstract. This document provides supplementary material for the pa- per entitled "Models Genesis: Generic Autodidactic Models for 3D Medi- cal Image Analysis". The supplementary material is organized as follows. In Sec. A, we begin with a brief overview of Models Genesis. Secs. B-E describe at length the detailed implementation and illustration of four in- dividual transformations. Secs. F-G contain a qualitative visualization on the pre-trained Genesis CT and Genesis X-ray for both same-and cross-domain image restoration. Secs. H-I present the transfer learning results of Models ImageNet, NiftyNet, and our Models Genesis in various target tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>These p-values are calculated between our Models Genesis vs. the fine-tuning from ImageNet, which always offers the best performance (highlighted in red) for all three tasks in 2D.</figDesc><table><row><cell>Task</cell><cell>Scratch</cell><cell>2D (%) ImageNet</cell><cell>Genesis</cell><cell>Scratch</cell><cell cols="2">3D (%) ImageNet</cell><cell>Genesis</cell><cell>p-value :</cell></row><row><cell>NCC</cell><cell cols="3">96.03˘0.86 97.79˘0.71 97.45˘0.61</cell><cell>94.25˘5.07</cell><cell></cell><cell>N/A</cell><cell cols="2">98.20˘0.51 0.0213</cell></row><row><cell>NCS</cell><cell cols="3">70.48˘1.07 72.39˘0.77 72.20˘0.67</cell><cell>74.05˘1.97</cell><cell></cell><cell>N/A</cell><cell cols="2">77.62˘0.64</cell><cell>ă1e-8</cell></row><row><cell>ECC</cell><cell cols="3">71.27˘4.64 78.61˘3.73 78.58˘3.67</cell><cell>79.99˘8.06</cell><cell></cell><cell>N/A</cell><cell cols="2">88.04˘1.40 5.50e-4</cell></row><row><cell cols="2">Approach</cell><cell>NCC (%)</cell><cell>NCS (%)</cell><cell>ECC (%)</cell><cell></cell><cell cols="2">LCS (%)</cell><cell>BMS (%)</cell></row><row><cell>Scratch</cell><cell></cell><cell>94.25˘5.07</cell><cell>74.05˘1.97</cell><cell cols="2">79.99˘8.06</cell><cell cols="2">74.60˘4.57</cell><cell>90.16˘0.41</cell></row><row><cell cols="2">Distortion (ours)</cell><cell>96.46˘1.03</cell><cell>77.08˘0.68</cell><cell cols="2">88.04˘1.40</cell><cell cols="2">79.08˘4.26</cell><cell>90.60˘0.20</cell></row><row><cell cols="2">Painting (ours)</cell><cell>98.20˘0.51</cell><cell>77.02˘0.58</cell><cell cols="2">87.18˘2.72</cell><cell cols="2">78.62˘4.05</cell><cell>90.46˘0.21</cell></row><row><cell cols="2">Unified (ours)</cell><cell>97.90˘0.57</cell><cell>77.62˘0.64</cell><cell cols="2">87.20˘2.87</cell><cell cols="2">79.52˘4.77</cell><cell>90.59˘0.21</cell></row><row><cell cols="2">p-value ::</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>These p-values are calculated between NiftyNet scratch and NiftyNet model zoo.</figDesc><table><row><cell>Initialization</cell><cell>NCS (Dice %)</cell><cell>LCS (Dice %)</cell><cell>BMS (Dice %)</cell></row><row><cell>Models Genesis (ours)</cell><cell>75.86˘0.90</cell><cell>91.13˘1.51</cell><cell>92.58˘0.30</cell></row><row><cell>NiftyNet scratch [7]</cell><cell>69.65˘2.56</cell><cell>91.09˘0.76</cell><cell>90.68˘0.24</cell></row><row><cell>NiftyNet model zoo [6]</cell><cell>69.24˘1.77</cell><cell>90.84˘0.63</cell><cell>90.65˘0.54</cell></row><row><cell>p-value :</cell><cell>0.3433</cell><cell>0.2214</cell><cell>0.4301</cell></row></table><note>:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI 2 https://biometry.nci.nih.gov/cdas/nlst/ 3 https://nihcc.app.box.com/v/ChestXray-NIHCC</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Appendix can be found in the full version at tinyurl.com/ModelsGenesisFullVersion</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">NiftyNet Model Zoo: https://github.com/NifTK/NiftyNetModelZoo</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">with the datasets. We evaluate</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">3D U-Net Convolution Neural Network: https://github.com/ellisdg/3DUnetCNN 7 Segmentation Models: https://github.com/qubvel/segmentation models</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This research has been supported partially by ASU and Mayo Clinic through a Seed Grant and an Innovation Grant, and partially by NIH under Award Number R01HL128785. The content is solely the responsibility of the authors and does not necessarily represent the official views of NIH.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-supervised learning for spinal MRIs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>DLMIA</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06162</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07103</idno>
		<title level="m">Patchshuffle regularization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving cytoarchitectonic segmentation of human brain areas with self-supervised siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Spitzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for medical image analysis: Full training or fine tuning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Surrogate supervision for medical image analysis: Effective deep learning from limited quantities of labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ISBI</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Buzug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Handbook of Medical Technology</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Self-Supervised GANs via Auxiliary Rotation Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Human body composition: growth, aging, nutrition, and activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Forbes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic multi-organ segmentation on abdominal ct with dense v-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Niftynet: a deep-learning platform for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S016926071731182326" />
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incidence of subclinical atherosclerosis as a marker of cardiovascular risk in retired professional football players</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Hurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American journal of cardiology</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06162</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. In: NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1920" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>TMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mathematics for computer graphics applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Mortenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Industrial Press Inc</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integrating active learning and transfer learning for carotid intimamedia thickness video interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
