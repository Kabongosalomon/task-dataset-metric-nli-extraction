<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sawyer</forename><surname>Birnbaum</surname></persName>
							<email>sawyerb@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
							<email>kuleshov@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zayd Enam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Pang</roleName><forename type="first">Wei</forename><surname>Koh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Afresh Technologies</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning representations that accurately capture long-range dependencies in sequential inputs -including text, audio, and genomic data -is a key problem in deep learning. Feed-forward convolutional models capture only feature interactions within finite receptive fields while recurrent architectures can be slow and difficult to train due to vanishing gradients. Here, we propose Temporal Feature-Wise Linear Modulation (TFiLM) -a novel architectural component inspired by adaptive batch normalization and its extensions -that uses a recurrent neural network to alter the activations of a convolutional model. This approach expands the receptive field of convolutional sequence models with minimal computational overhead. Empirically, we find that TFiLM significantly improves the learning speed and accuracy of feed-forward neural networks on a range of generative and discriminative learning tasks, including text classification and audio super-resolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many application domains of deep learning -including speech <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref>, genomics <ref type="bibr" target="#b0">[1]</ref>, and natural language <ref type="bibr" target="#b47">[48]</ref> -data takes the form of long, high-dimensional sequences. The prevalence and importance of sequential inputs has motivated a range of deep architectures specifically designed for this data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>One of the challenges in processing sequential data is accurately capturing long-range input dependencies -interactions between symbols that are far apart in the sequence. For example, in speech recognition, data occurring at the beginning of a recording may influence the conversion of words enunciated much later.</p><p>Sequential inputs in deep learning are often processed using recurrent neural networks (RNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>. However, training RNNs is often difficult, mainly due to the vanishing gradient problem <ref type="bibr" target="#b1">[2]</ref>. Feed-forward convolutional approaches are highly effective at processing both images <ref type="bibr" target="#b31">[32]</ref> and sequential data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b14">15]</ref> and are easier to train. However, convolutional models only account for feature interactions within finite receptive fields and are not ideally suited to capture long-term dependencies.</p><p>In this paper, we introduce Temporal Feature-Wise Linear Modulation (TFiLM), a neural network component that captures long-range input dependencies in sequential inputs by combining elements of convolutional and recurrent approaches. Our component modulates the activations of a convolutional model based on long-range information captured by a recurrent neural network. More specifically, TFiLM parametrizes the rescaling parameters of a batch normalization layer as in earlier work on image stylization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref> and visual question answering <ref type="bibr" target="#b42">[43]</ref>. <ref type="table" target="#tab_0">(Table 1</ref> outlines recent work applying feature-wise linear modulation.) We demonstrate real-world applications of TFiLM in both discriminative and generative tasks involving sequential data. We define and study a family of signal processing problems called time series superresolution, which consists of reconstructing a high-resolution signal from low-resolution measurements. For example, in audio super-resolution, we reconstruct high-quality audio from a low-quality input containing a fraction (15-50%) of the original time-domain samples. Likewise, in genomic superresolution, we recover high-quality measurements from experimental assays using a limited number of lowerquality measurements.</p><p>We observe that TFiLM significantly improves the performance of deep neural networks on a wide range of discriminative classification tasks as well as on complex high-dimensional time series super-resolution problems. Interestingly, our model is domain-agnostic, yet outperforms more specialized approaches that use domain-specific features.</p><p>Contributions. This work introduces a new architectural component for deep neural networks that combines elements of convolutional and recurrent approaches to better account for long-range dependencies in sequence data. We demonstrate the component's effectiveness on the discriminative task of text classification and on the generative task of time series super-resolution, which we define. Our architecture outperforms strong baselines in multiple domains and could, inter alia, improve speech compression, reduce the cost of functional genomics experiments, and improve the accuracy of text classification systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Batch Normalization and Feature-Wise Linear Modulation. Batch normalization (batch norm; <ref type="bibr" target="#b21">[22]</ref>) is a widely used technique for stabilizing the training of deep neural networks. In this setting, batch norm takes as input a tensor of activations F ∈ R T ×C from a 1D convolution layer -where T and C are, respectively, the 1D spatial dimension and the number of channels -and performs two operations: rescaling F and applying an affine transformation. Formally, this produces tensorsF , F ∈ R T ×C whose (t, c)-th elements are the following:</p><formula xml:id="formula_0">F t,c = F t,c − µ c σ c + F t,c = γ cFt,c + β c<label>(1)</label></formula><p>Here, µ c , σ c are estimates of the mean and standard deviation for the c-th channel, and γ, β ∈ R C are trainable parameters that define an affine transformation.</p><p>Feature-Wise Linear Modulation (FiLM) <ref type="bibr" target="#b10">[11]</ref> extends this idea by allowing γ, β to be functions γ, β : Z → R C of an auxiliary input z ∈ Z. For example, in feed-forward image style transfer <ref type="bibr" target="#b10">[11]</ref>, z is an image defining a new style; by using different γ(z), β(z) for each z, the same feed-forward network (using the same weights) can apply different styles to a target image. <ref type="bibr" target="#b9">[10]</ref> provides a summary of applications of FiLM layers.  <ref type="bibr" target="#b42">[43]</ref> Visual QA Images Text RNN/ and de Vries et al. <ref type="bibr" target="#b5">[6]</ref> MLP Dumoulin et al. <ref type="bibr" target="#b10">[11]</ref> Style Transfer Images Images CNN Kim et al. <ref type="bibr" target="#b28">[29]</ref> Speech Audio Self (Sequence) Feedforward Hu et al. <ref type="bibr" target="#b19">[20]</ref> Image Classification </p><formula xml:id="formula_1">F pool b,c = Pool(F blk b,:,c ) 3. Compute sequence of normalizers γ b , β b ∈ R C for b = 1, 2, ..., B using an RNN applied to pooled blocks: (γ b , β b ), h b = RNN(F pool b,· ; h b−1 ) starting with h 0 = 0. 4. Compute normalized block tensor F norm ∈ R B×T /B×C as F norm b,t,c = γ b,c · F block b,t,c + β b,c . 5. Reshape F norm into output F ∈ R T ×C as F ,c = F norm t/B ,t mod B,c .</formula><p>Recurrent and Convolutional Sequence Models. Sequential data is often modeled using RNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref> in a sequence-to-sequence (seq2seq) framework <ref type="bibr" target="#b47">[48]</ref>. RNNs are effective on language processing tasks over medium-sized sequences; however, on longer time series RNNs may become difficult to train and computationally impractical <ref type="bibr" target="#b49">[50]</ref>.</p><p>An alternative approach to modeling sequences is to use one-dimensional (1D) convolutions. While convolutional networks are faster and easier to train than RNNs, convolutions have a limited receptive field, and a subsequence of the output depends on only a finite subsequence of the input. This paper introduces a new layer that addresses these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Temporal Feature-Wise Linear Modulation</head><p>In this section, we describe a new neural network component called Temporal Feature-Wise Linear Modulation (TFiLM) that effectively captures long-range input dependencies in sequential inputs by combining elements of convolutional and recurrent approaches. At a high level, TFiLM modulates the activations of a convolutional model using long-range information captured by a recurrent neural network.</p><p>Specifically, let F ∈ R T ×C be a tensor of activations from a 1D convolutional layer (at one datapoint, i.e. N = 1 and we drop the batch size dimension N for simplicity); a TFiLM layer takes as input F and applies the following series of transformations. First, F is split along the time axis into blocks of length B to produce F blk ∈ R B×T /B×C . Intuitively, blocks correspond to regions along the spatial dimension in which the activations are closely correlated; for example, when processing audio, blocks could be chosen to correspond to audio samples that define the same phoneme. Next, we compute for each block b an affine transformation parameters γ b , β b ∈ R C using an RNN:</p><formula xml:id="formula_2">F pool b,c = Pool(F blk b,:,c ) ∈ R B×C (γ b , β b ), h b = RNN(F pool b,: ; h b−1 ) for b = 1, 2, ..., B</formula><p>starting with h 0 = 0, where h b denotes the hidden state, F pool ∈ R B×C is a tensor obtained by pooling along the the second dimension of F blk , and the notation F blk b,:,c refers to a slice of F blk along the second dimension. In all our experiments, we use an LSTM and max pooling.</p><p>Finally, activations in each block b are normalized by γ b , β b to produce a tensor F norm defined as</p><formula xml:id="formula_3">F norm b,t,c = γ b,c · F block b,t,c + β b,c .</formula><p>Notice that each γ b , β b is a function of both the current and all the past blocks. Hence, activations can be modulated using long-range signals captured by the RNN. In the audio example, the super resolution of a phoneme could depend on previous phonemes beyond the receptive field of the convolution; the RNN enables us to use this long-range information.</p><p>Although TFiLM relies on an RNN, it remains computationally tractable because each RNN is small (the dimensionality of its output is only O(C)) and because the RNN is invoked only a small number of times. Consider again the speech example, where blocks are chosen to match phonemes: a 5 second recording contains ≈ 50 0.1 second phonemes, yielding only about 50 RNN invocations for 80,000 audio samples at 16KHz. Despite being small, this RNN also carries useful long-range information, as our experiments demonstrate. In order to demonstrate the real-world applications of TFiLM, we define and study a new generative modeling task called time series superresolution, which consists of reconstructing a high-resolution signal y = (y 1 , y 2 , ..., y T ) from low-resolution measurements x = (x 1 , x 2 , ..., x T ); x, y denote the source and target time series, respectively. For example, y may be a high-quality speech signal while x is a noisy phone recording.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Time Series Super-Resolution</head><p>This task is closely inspired by image super resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>, which involves reconstructing a high-resolution image from a lowresolution version. As in image super-resolution, we assume that lowand high-resolution time series x, y have a natural alignment, which can arise, for example, from applying a low-pass filter to y to obtain x. Below, we provide two examples of time series super-resolution problems.</p><p>Audio Super-Resolution. Audio super-resolution (also known as bandwidth extension; <ref type="bibr" target="#b11">[12]</ref>) involves predicting a high-quality audio signal from a fraction (15-50%) of its time-domain samples. Note that this is equivalent to predicting the signal's high frequencies from its low frequencies. Formally, given a low-resolution signal x = (x 1/R1 , ..., x R1T /R1 ) sampled at a rate R 1 /T (e.g., lowquality telephone call), our goal is to reconstruct a high-resolution version y = (y 1/R2 , ..., y R2T /R2 ) of x that has a sampling rate R 2 &gt; R 1 . We use r = R 2 /R 1 to denote the upsampling ratio of the two signals. Thus, we expect that y rt/R2 ≈ x t/R1 for t = 1, 2, ..., T R 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Super-Resolution of Genomics Experiments.</head><p>Many genomics experiments can be seen as taking a real-valued measurement at every position of the genome; experimental results can therefore be represented by a time series. Measurements are generally obtained using a large set of probes (e.g., sequencing reads) that each randomly examine a different position in the genome; the genomic time series is an aggregate of the measurements taken by these probes. In this setting, super-resolution corresponds to reconstructing high-quality experimental measurements taken using a large set of probes from noisy measurements taken using a small set of probes. This process can significantly reduce the cost of scientific experiments. This paper focuses on a particular genomic experiment called chromatin immunoprecipitation sequencing (ChIP-seq) <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Deep Neural Network Architecture for Time Series-Super Resolution</head><p>The TFiLM layer is a key part of our deep neural network architecture shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Other notable features of the architecture include the following: (1) a sequence of downsampling blocks that halve the spatial dimension and double the feature size and of upsampling blocks that do the reverse;</p><p>(2) max pooling to reduce the size of LSTM inputs; (3) skip connections between corresponding downsamping and upsampling blocks; and (4) subpixel shuffling <ref type="bibr" target="#b45">[46]</ref> to increase the time dimension during upscaling and avoid artifacts <ref type="bibr" target="#b39">[40]</ref>. For more details, see the Appendix. We train the model on a dataset D = {x i , y i } n i=1 of source/target time series pairs. As in image super-resolution, we take the x i , y i to be small patches sampled from the full time series. We train the model using a mean squared error objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we show that TFiLM layers improve the performance of convolutional models on both discriminative and generative tasks. We analyze TFiLM on sentiment analysis, a text classification task, as well as on several generative time series super-resolution problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text Classification</head><p>Datasets. We use the Yelp-2 and Yelp-5 datasets [54], which are standard datasets for sentiment analysis. In the Yelp-2 dataset, reviews are classified as positive or negative based on the number of stars given by the reviewer. Reviews with 1 or 2 stars are classified as negative, and reviews with 4 or 5 stars are classified as positive. In the Yelp-5 dataset, reviews are labeled with their star rating (i.e., 1-5). The Yelp-2 dataset includes about 600,00 reviews and the Yelp-5 dataset includes 700,00 reviews. For both tasks, there are an equal number of examples with each possible label. With zero-padding and truncation, we set the length of Yelp reviews to 256 tokens.</p><p>Methods. We tokenize the reviews using Keras's built-in tokenizer and use 100-dimensional GLoVe word embeddings <ref type="bibr" target="#b41">[42]</ref> to encode the tokens.</p><p>We compare our method against a variety of recent work on the Yelp-2 and Yelp-5 datasets, and to a CNN model based on the architecture of Johnson and Zhang's Deep Pyramid Convolutional Neural Network <ref type="bibr" target="#b23">[24]</ref>. This "SmallCNN" model copies the DPCNN architecture, but reduces the number of convolutional layers to 3 and does not include region embeddings. Our full model inserts TFiLM layers after the convolutional layers in this architecture. We normalize the number of parameters between the basic SmallCNN model and the full model that includes TFiLM layers. To do so, we adjust the number of filters in the convolutional layers.</p><p>We train for 20 epochs using the ADAM optimizer with a learning rate of 10 −3 and a batch size of 128.</p><p>Evaluation. <ref type="table" target="#tab_2">Table 2</ref> presents the results of our experiments. On both datasets, TFiLM layers improve the performance of the SmallCNN architecture; the resulting TFiLM model performs at or near the level of state-of-the-art methods from the literature. The models that outperform the TFiLM architecture use several times as many parameters (VDCNN and DenseCNN) or run unsupervised pre-training on external data (DPCNN, BERT, and XLNet). Additionally, the TFiLM model trains several times faster than the SmallCNN model or a comparably-sized LSTM. (See the Appendix for details.) Overall, these results indicate that TFiLM layers can provide performance and efficiency benefits on discriminative sequence classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Audio Super-Resolution</head><p>Datasets. We use the VCTK dataset <ref type="bibr" target="#b51">[52]</ref> -which contains 44 hours of data from 108 speakers -and a Piano dataset -10 hours of Beethoven sonatas <ref type="bibr" target="#b38">[39]</ref>. We generate low-resolution audio signal from the 16 KHz originals by applying an order 8 Chebyshev type I low-pass filter before subsampling the signal by the desired scaling ratio. The SINGLESPEAKER task trains the model on the first 223 recordings of VCTK Speaker 1 (about 30 minutes) and tests on the last 8 recordings.</p><p>In the MULTISPEAKER task, we train on the first 99 VCTK speakers and test on the 8 remaining ones. Lastly, the PIANO task extends audio super resolution to non-vocal data; we use the standard 88%-6%-6% data split. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Yelp-2 Yelp-5 Param FastText <ref type="bibr" target="#b24">[25]</ref> 95.7% 63.9% Linear LSTM <ref type="bibr" target="#b53">[55]</ref> 92.6% 59.6% -Self-Attention <ref type="bibr" target="#b35">[36]</ref> 93.5% 63.4% -CNN <ref type="bibr" target="#b29">[30]</ref> 93.5% 61.0% -CharCNN <ref type="bibr" target="#b56">[58]</ref> 94.6% 62.0% -VDCNN <ref type="bibr" target="#b4">[5]</ref> 95.4% 64.7% &gt;5M DenseCNN <ref type="bibr" target="#b50">[51]</ref> 96 We instantiate our model with K = 4 blocks and train it for 50 epochs on patches of length 8192 (in the high-resolution space) using the ADAM optimizer with a learning rate of 3×10 −4 . To ensure source/target series are of the same length, the source input is pre-processed with cubic upscaling. We adjust the TFiLM block length B so that T /B (the number of blocks) is always 32. We use a pooling stride and spatial extent of 2. To increase the receptive field of our convolutional layers, we use dilated convolutions with a dilation factor of 2 <ref type="bibr" target="#b54">[56]</ref>.</p><p>Including TFiLM layers significantly increases the number of parameters per layer compared with the DNN baseline and the basic CNN architecture. Accordingly, we adjust the number of filters per layer to normalize the parameter counts between the models.</p><p>Metrics. Given a reference signal y and an approximation x, the signal to noise ratio (SNR) is defined as SNR(x, y) = 10 log . The SNR is a standard metric used in the signal processing literature. The log-spectral distance (LSD) <ref type="bibr" target="#b16">[17]</ref> measures the reconstruction quality of individual frequencies as follows LSD(x, y) = 1 T T t=1</p><formula xml:id="formula_4">1 K K k=1 X(t, k) −X(t, k) 2</formula><p>, where X andX are the log-spectral power magnitudes of y and x, respectively. These are defined as X = log |S| 2 , where S is the short-time Fourier transform (STFT) of the signal. We use t and k index frames and frequencies, respectively; we used frames of length 8092.</p><p>Evaluation. The results of our experiments are summarized in <ref type="table" target="#tab_4">Table 3</ref>. According to our SNR metric, our basic convolutional architecture shows an average improvement of 0.3 dB over the DNN and Spline baselines, with the strongest improvements on the SINGLESPEAKER task. Based on the LSD metric, the convolutional architecture also shows an average improvement of 0.5 over the DNN baseline and 1.6 over the Spline baseline. <ref type="bibr" target="#b1">2</ref> The convolutional architecture appears to use our modeling capacity more efficiently than a dense neural network; we expect such architectures will soon be more widely used in audio generation tasks. <ref type="bibr" target="#b2">3</ref> Including the TFiLM layers improves performance by an additional 1.0 dB on average in terms of SNR and 0.2 on average in terms of LSD. The TFiLM layers prove particularly beneficial on the MULTISPEAKER task, perhaps because this is the most complex task and therefore the one which benefits most from additional long-term temporal connections in the model. # Params. N/A 6.72e7 7.09e7 6.82e7 N/A 6.72e7 7.09e7 6.82e7 N/A 6.72e7 7.09e7 6.82e7</p><p>Finally, to confirm our results, we ran a study in which human raters assessed the quality of the interpolated audio samples. Our method ranked the best among the upscaling techniques; details are in the appendix.  <ref type="bibr" target="#b30">[31]</ref>, which introduused a simpler convolutional neural network architecture. The Koh results are the state of the art for this task; we use them as a baseline in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Chromatin Immunoprecipitation Sequencing</head><p>Formally, given an input noisy ChIP-seq signal X ∈ R k×T , where k is the number of distinct histone marks, and T is the length of the genome, we aim to reconstruct a high-quality ChIP-seq signal Y ∈ R T . We use the k low-quality signals as input and train a separate model for each high quality target mark. We use B = 2 and training windows of length 1000; all other hyper-parameters are as in the audio-super resolution task.</p><p>To evaluate our results, we measure Pearson correlation between our model output and the true, high-quality ChIP-seq signal; this is a standard comparison metric in the field (e.g., <ref type="bibr" target="#b13">[14]</ref>). As shown in <ref type="table" target="#tab_5">Table 4</ref>, our method significantly improves the quality of the input signal over the Koh results, and on all but one histone mark outperforms the specialized CNN baseline. Across all of the histone marks, the model output from an input of 1M sequencing reads is equivalent in quality to signal derived from 10M to 20M reads, constituting a significant efficiency gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Additional Analyses</head><p>Model Visualization. We examined the internals of the TFiLM layer by visualizing the adaptive normalizer parameters in the audio super-resolution and sentiment analysis experiments. On the former, we observed that the parameters tend to cluster by gender, suggesting that the layer learns useful features. Figures are in the Appendix.</p><p>Ablation Analysis. The ablation analysis in <ref type="figure" target="#fig_5">Figure 5</ref> indicates that temporal adaptive normalization significantly improves model performance. In addition, we performed an ablation analysis for the skip connections and found that they also significantly improve reconstruction accuracy. Our results are in the Appendix.</p><p>Model Generalization. We examined the extent to which the model generalizes across datasets. On the audio task, we observed a loss in performance when evaluating the model that was trained on speech on piano music (and vice versa). This highlights the need for diverse training data. Details are in the Appendix.</p><p>Missing Value Imputation. We experimented with imputing missing values from a sequence of daily grocery retail sales using various zero-out rates. TFiLM layers consistently provided performance benefits. Our full methodology and results are in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Previous Work and Discussion</head><p>Feature-Wise Linear Modulation. Previous work has applied feature-wise linear modulation to tasks including question answering, style transfer, and speech recognition (see <ref type="table" target="#tab_0">Table 1</ref>). Our approach is most similar to that of Kim et al. <ref type="bibr" target="#b29">[30]</ref>, which modulates layer normalization parameters using a feed-forward model conditioned on an input audio sequence. Conversely, our method adjusts the batch normalization parameters of a feed-forward CNN using an RNN conditioned on the entire sequence. This significantly improves the CNN's performance.</p><p>Time Series Modeling. In the machine learning literature, time series signals have most often been modeled with auto-regressive models, of which variants of recurrent networks are a special case <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Our approach generalizes conditional modeling ideas used in computer vision for tasks such as image super-resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref> or colorization <ref type="bibr" target="#b55">[57]</ref>.</p><p>Applications to Audio and Genomics. Existing learning-based approaches include Gaussian mixture models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>, linear predictive coding <ref type="bibr" target="#b2">[3]</ref>, and neural networks <ref type="bibr" target="#b34">[35]</ref>. Other recent work on audio super-resolution includes Wang et al.'s WaveNet model <ref type="bibr" target="#b49">[50]</ref> and Macartney and Weyde's Wave-U-Net model <ref type="bibr" target="#b37">[38]</ref>. Our work proposes a new architecture, which scales better with data size and outperforms recent methods. Moreover, existing techniques involve hand-crafted features <ref type="bibr" target="#b43">[44]</ref>; our approach is fully domain-agnostic. Statistical modeling of genomic data has been explored in population and functional genomics <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref>; our approach has the potential to make scientific experiments significantly more affordable.</p><p>Computational Performance. Our model is computationally efficient and can be run in real time.</p><p>Unlike sequence-to-sequence architectures, our model does not require the complete input sequence to begin generating an output sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In summary, our work introduces a temporal adaptive normalization neural network layer that integrates convolutional and recurrent layers to efficiently incorporate long-term information when processing sequential data. We demonstrate the layer's effectiveness on three diverse domains. Our results have applications in areas including text-to-speech generation and sentiment analysis and could reduce the cost of genomics experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Sentiment Analysis Experiments</head><p>Additional Comparisons. To measure the memory and run-time efficiency of the TFiLM model, we compare the TFiLM model against the basic SmallCNN architecture and a one-layer LSTM network. We run two experiments, one in which the number of parameters between the models is normalized to about 1 million, and one in which we increase the size of each model so that it uses almost all of the memory of the GPU (a NVIDIA Tesla P100). Note that in the latter experiment the number of parameters varies depending which layer acts as the memory bottleneck.</p><p>Evaluation. <ref type="table" target="#tab_6">Table 5</ref> presents the results of our experiments. In keeping with our other findings, in each experiment, the TFiLM model preforms significantly better than the basic SmallCNN architecture. The TFiLM model performs only slightly better than the LSTM model; this is unsurprising, as the sequences are only of length 256, short enough that the pure RNN can avoid the vanishing gradient problem.</p><p>Moreover, the TFiLM model trains on average over 50% faster than the SmallCNN model and almost twice as fast as the LSTM model. <ref type="figure" target="#fig_3">Figure 3</ref> presents learning curves for the 1-million parameter Yelp-2 experiment. On this experiment, the TFiLM model trains over twice as fast as the SmallCNN model and over three times as fast as the LSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Time Series Super-Resolution Model Details</head><p>Bottleneck Convolutional Layers The core of the model is formed by K successive downsampling and upsampling layer blocks: each performs a convolution, dropout, and ReLU nonlinearity. Downsampling block k = 1, 2, ..., K contains min(2 6+k , 512) convolutional filters of length max(2 7−k + 1, 9) with a stride of 2. Upsampling block k has min(2 7+(K−k+1) , 512) filters of length max(2 7−(K−k+1) <ref type="bibr">+ 1, 9)</ref>. Thus, at a downsampling step, we halve the spatial dimension and double the filter size; during upsampling, this is reversed. This bottleneck architecture resembles a conovlutional auto-encoder and encourages the model to learn a hierarchy of features.</p><p>Max Pooling. Because we expect correlation between data at consecutive time-steps, operating the LSTM over T /B × C tensors would be inefficient, especially in the first downsampling blocks. We use max pooling to reduce the size of the LSTM inputs. Specifically, after step 1 of Algorithm 1, we apply max pooling to condense F blk n,b,t,c tensors into F blk' n,b,t,c,f,s = F n,((b×t)−f )/s,c tensors, where f is the pooling spatial extent and s is the pooling stride.</p><p>Skip Connections. When the source series x is similar to the target y, downsampling features will also be useful for upsampling <ref type="bibr" target="#b22">[23]</ref>. We thus add additional skip connections that stack the tensor of k-th downsampling features with the (K − k + 1)-th tensor of upsampling features. We also add an additive residual connection from the input to the final output: the model thus only needs to learn y − x. This speeds up training.</p><p>Subpixel Shuffling. To increase the time dimension during upscaling, we have implemented a one-dimensional version of the subpixel layer of <ref type="bibr" target="#b45">[46]</ref>, which has been shown to be less prone to produce artifacts <ref type="bibr" target="#b39">[40]</ref>. Given a N × T × C input tensor, the convolution in a U-block outputs a tensor of shape N × T × C/2. The subpixel layer reshuffles this tensor into another one of size N × 2T × C/4; these are concatenated with C/4 features from the downsampling stage, for a final output of size N × 2T × C/2. Thus, we have halved the number of filters and doubled the spatial dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Bidirectional RNN</head><p>In some applications -like real-time audio super-resolution -samples from the future may not be accessible; therefore, in our experiments we left the TFiLM RNN uni-directional for full generality. To assess the impact of using a bidirectional RNN, we reran some of our super-resolution experiments with a BiLSTM. As <ref type="table" target="#tab_7">Table 6</ref> shows, in most cases the BiLSTM provides only a minor benefit, and in some cases it even reduces performance (perhaps due to overfitting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D TSNE Embeddings</head><p>We generated t-Distributed Stochastic Neighbor Embedding (t-SNE) plots of the adaptive batch normalization parameters on the MULTISPEAKER audio super-resolution task and on the 1-million parameter Yelp review sentiment analysis task. t-SNE is a non-linear dimensionality reduction algorithm that allows one to visualize relationships between the activations on different data points. <ref type="figure" target="#fig_4">Figure 4</ref> shows that activations of the final TFiLM layer reflect high-level concepts, including the gender of the speaker and the sentiment of the review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MUSHRA Test</head><p>We confirmed our objective audio super-resolution experiments with a study in which human raters assessed the quality of super-resolution using a MUSHRA (MUltiple Stimuli with Hidden Reference  and Anchor) test. For each trial, an audio sample was upscaled using different techniques <ref type="bibr" target="#b3">4</ref> . We collected four VCTK speaker recordings of audio samples from the MULTISPEAKER testing set. For each recording, we collected the original utterance, a downsampled version at r = 4, and signals super-resolved using Splines, DNNs, and our model (six versions in total). We recruited 10 subjects and used an online survey to them to rate each sample reconstruction on a scale of 0 (extremely bad) to 100 (excellent). <ref type="table" target="#tab_8">Table 7</ref> summarizes the results. Our method ranked as the best of the three upscaling techniques.</p><p>F Additional Ablation Analysis.  the green curve removes the additive residual connection; the blue curve further removes the additive skip connection (while preserving the same total number of filters). This shows that symmetric skip connections are crucial for attaining good performance; additive connections provide an additional small, but perceptible, improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Understanding the Generalization of the Super-Resolution Model</head><p>We tested the sensitivity of our method to out-of-distribution input via an audio super-resolution experiment in which the training set did not use a low-pass filter, while the test set did, and vice versa. We focused on the PIANO task and r = 2. The output from the model was noisier than expected, indicating that generalization is an important concern. We suspect this behavior may be common in super-resolution algorithms but has not been widely documented. A potential solution might be to train on data that has been generated using multiple techniques.</p><p>In addition, we examined the ability of our model to generalize from speech to music and vice versa. We found that switching domains produced noisy output, again highlighting the specialization of the model.  <ref type="table" target="#tab_9">Table 9</ref> reports objective metrics for models trained on the MULTISPEAKER and the PIANO tasks and tested both on the same and on the other dataset.</p><p>Listening to the samples, we found that although the model predicts many high frequencies, these are often corrupted with noise. Thus, our neural networks appear to learn a dictionary that is specialized to the type of audio that they are trained on. We also considered the super-resolution task of imputing missing values in daily retail sales data. Missing values naturally occur in financial time series due to bookkeeping errors or censoring, and they occur in other domains for myriad reasons. Robustness to missing values improves the reliability of downstream machine learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Missing Data Imputation</head><p>We downloaded publicly available grocery retail sales data from Kaggle's Grocery Sales Forecasting Competition <ref type="bibr" target="#b26">[27]</ref>. From this data, we extracted sales figures for 1452 items on 1024 days. We split the data 80% / 20% into training and testing sets, and we experiment with setting 10%, 20%, and 30% of the values to zero uniformly at random. We train the model (with and without TFiLM layers) to fill in the missing values. We train for 50 epochs using the ADAM optimizer with a learning rate of 3 × 10 −4 . As in the audio super-resolution tasks, we compare our results with a cubic B-spline and a DNN. (The DNN hyper-parameters are the same as in the audio experiments.)</p><p>As <ref type="table" target="#tab_0">Table 10</ref> shows, the convolutional architecture consistently outperforms both baselines, and including TFiLM layers consistently provides an additional benefit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The TFiLM layer combines the strengths of convolutional and recurrent neural networks. Above: operation of the TFiLM layer with T = 8, C = 2, B = 2, and a pooling factor of 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Top: A deep neural network architecture for time series super-resolution that consists of K downsampling blocks followed by a bottleneck layer and K upsampling blocks; features are reused via symmetric residual skip connections. Bottom: Internal structure of downsampling and upsampling convolutional blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>||y|| 2 2 ||x−y|| 2 2</head><label>22</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Learning curves for the 1-million parameter Yelp-2 experiment. Left: validation accuracy; Right: validation loss. Note that the accuracy and loss converge several epochs slower for the LSTM model compared with the TFiLM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Left: t-SNE plot of activations after the final TFiLM layer for the r = 4 model trained on MULTI-SPEAKER recordings. The male speakers (blue) are generally separated from the female speakers (red). Right: t-SNE plot of activations after the final TFiLM layer for 1-million parameter Yelp review experiment. The positive reviews (blue) are seperated from the negative reviews (red). No additive or stacking connections No additive connection Full model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Model ablation analysis on the MULTISPEAKER audio super-resolution task with r = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5</head><label>5</label><figDesc>displays the result of a longer ablation analysis: the red line displays the validation set 2 loss of the original model over time;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Recent work applying feature-wise linear modulation.</figDesc><table><row><cell>Paper</cell><cell>Problem Area</cell><cell>Base Modality</cell><cell>Conditioning</cell><cell>Conditioning</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Modality</cell><cell>Architecture</cell></row><row><cell>Dhingra et al. [8]</cell><cell>QA</cell><cell>Text (document)</cell><cell>Text (query)</cell><cell>CNN</cell></row><row><cell>Perez et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Text classification on Yelp-2 and Yelp-5 datasets. Methods with * use unsupervised pre-training (unsupervised region embeddings or transformers) on external data and are not directly comparable. Parameter counts exclude models with lower performance. Embedding parameters are not counted.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy evaluation of audio super resolution methods on each of the three super-resolution tasks at upscaling ratios r = 2, 4, and 8. DNN and CNN are baselines from the literature. [KEE17] denotes the convolutional method of Kuleshov et al. (2017)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">SINGLESPEAKER</cell><cell></cell><cell></cell><cell cols="2">MULTISPEAKER</cell><cell></cell><cell></cell><cell>PIANO</cell><cell></cell></row><row><cell>Ratio</cell><cell cols="2">Obj. Spline</cell><cell>DNN</cell><cell cols="3">Conv Full Spline</cell><cell>DNN</cell><cell cols="3">Conv. Full Spline</cell><cell>DNN</cell><cell cols="2">Conv. Full</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">[Li et al.] [KEE17]</cell><cell>Us</cell><cell></cell><cell cols="2">[Li et al.] [KEE17]</cell><cell>Us</cell><cell></cell><cell cols="2">[Li et al.] [KEE17]</cell><cell>Us</cell></row><row><cell>r = 2</cell><cell cols="2">SNR 19.0</cell><cell>19.0</cell><cell cols="3">19.4 19.5 18.0</cell><cell>17.9</cell><cell cols="3">18.1 19.8 24.8</cell><cell>24.7</cell><cell cols="2">25.3 25.4</cell></row><row><cell></cell><cell>LSD</cell><cell>3.5</cell><cell>3.0</cell><cell>2.6</cell><cell>2.5</cell><cell>2.9</cell><cell>2.5</cell><cell>1.9</cell><cell>1.8</cell><cell>1.8</cell><cell>2.5</cell><cell>2.0</cell><cell>2.0</cell></row><row><cell>r = 4</cell><cell cols="2">SNR 15.6</cell><cell>15.6</cell><cell cols="3">16.4 16.8 13.2</cell><cell>13.3</cell><cell cols="3">13.1 15.0 18.6</cell><cell>18.6</cell><cell cols="2">18.8 19.3</cell></row><row><cell></cell><cell>LSD</cell><cell>5.6</cell><cell>4.0</cell><cell>3.7</cell><cell>3.5</cell><cell>5.2</cell><cell>3.9</cell><cell>3.1</cell><cell>2.7</cell><cell>2.8</cell><cell>3.2</cell><cell>2.3</cell><cell>2.2</cell></row><row><cell>r = 8</cell><cell cols="2">SNR 12.2</cell><cell>12.3</cell><cell cols="2">12.7 12.9</cell><cell>9.8</cell><cell>9.8</cell><cell cols="3">9.9 12.0 10.7</cell><cell>10.7</cell><cell cols="2">11.1 13.3</cell></row><row><cell></cell><cell>LSD</cell><cell>7.2</cell><cell>4.7</cell><cell>4.2</cell><cell>4.3</cell><cell>6.8</cell><cell>4.6</cell><cell>4.3</cell><cell>2.9</cell><cell>4.0</cell><cell>3.5</cell><cell>2.7</cell><cell>2.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Pearson Correlation</cell><cell></cell></row><row><cell cols="6">Histone Input Linear CNN CNN Full</cell></row><row><cell></cell><cell cols="3">[K17] [K17] [K17]</cell><cell>Us</cell><cell>Us</cell></row><row><cell>H3K4me1</cell><cell>0.37</cell><cell>0.41</cell><cell>0.59</cell><cell cols="2">0.79 0.81</cell></row><row><cell>H3K4me3</cell><cell>0.63</cell><cell>0.67</cell><cell>0.72</cell><cell cols="2">0.66 0.90</cell></row><row><cell>H3K27ac</cell><cell>0.55</cell><cell>0.61</cell><cell>0.77</cell><cell cols="2">0.85 0.89</cell></row><row><cell>H3K27me3</cell><cell>0.14</cell><cell>0.18</cell><cell>0.30</cell><cell cols="2">0.65 0.64</cell></row></table><note>Pearson correlation of the model output and the high- quality ChIP-seq signal derived from an experiment with high sequencing depth. [K17] indicates results from Koh et al. (2017); linear method performance is estimated.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Accuracy evaluation of sentiment analysis methods.</figDesc><table><row><cell>Experiment</cell><cell>Small (1M Params.)</cell><cell></cell><cell>Large</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">SmallCNN LSTM TFiLM SmallCNN LSTM TFiLM</cell></row><row><cell>Accuracy</cell><cell cols="2">78.1% 95.2% 95.6%</cell><cell cols="2">78.0% 95.2% 95.3%</cell></row><row><cell># Params.</cell><cell cols="2">1.06e6 1.03e6 1.04e6</cell><cell cols="2">1.50e6 9.61e6 2.77e7</cell></row><row><cell>Secs. per Epoch</cell><cell>896 1141</cell><cell>340</cell><cell>1069 1655</cell><cell>728</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of audio super-resolution results with a bidirectional LSTM in the TFiLM layer. Switching to a BiLSTM generally provides a minor improvement in performance.</figDesc><table><row><cell cols="2">Experiment Ratio Obj.</cell><cell>TFiLM</cell><cell cols="2">TFiLM Improvement</cell></row><row><cell></cell><cell></cell><cell cols="2">w/LSTM w/BiLSTM</cell><cell></cell></row><row><cell>SINGLESPEAKER</cell><cell>4 SNR</cell><cell>16.8</cell><cell>16.9</cell><cell>+0.1</cell></row><row><cell></cell><cell>LSD</cell><cell>3.5</cell><cell>3.6</cell><cell>-0.1</cell></row><row><cell>PIANO</cell><cell>4 SNR</cell><cell>19.3</cell><cell>20.5</cell><cell>+1.2</cell></row><row><cell></cell><cell>LSD</cell><cell>2.2</cell><cell>2.1</cell><cell>+0.2</cell></row><row><cell>MULTISPEAKER</cell><cell>2 SNR</cell><cell>19.8</cell><cell>19.6</cell><cell>-0.2</cell></row><row><cell></cell><cell>LSD</cell><cell>1.8</cell><cell>1.7</cell><cell>+0.1</cell></row><row><cell>MULTISPEAKER</cell><cell>4 SNR</cell><cell>15.0</cell><cell>15.1</cell><cell>+0.1</cell></row><row><cell></cell><cell>LSD</cell><cell>2.7</cell><cell>2.6</cell><cell>+0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>MUSHRA test user study scores. We show scores for each sample, averaged over individual users. The average across all samples is also displayed.</figDesc><table><row><cell cols="4">MULTISPEAKER Sample</cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>Average</cell></row><row><cell cols="3">Ours 69 75 64</cell><cell>37</cell><cell>61.3</cell></row><row><cell cols="3">DNN 51 55 66</cell><cell>53</cell><cell>56.3</cell></row><row><cell cols="3">Spline 31 25 38</cell><cell>47</cell><cell>35.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Out-of-distribution performance. We train models on the PIANO and MULTISPEAKER datasets at r = 4 and measure SNR and LSD (in dB) on a different testing dataset.</figDesc><table><row><cell></cell><cell cols="2">PIANO (TEST)</cell><cell cols="2">MULTISPKR (TEST)</cell></row><row><cell></cell><cell>SNR</cell><cell>LSD</cell><cell>SNR</cell><cell>LSD</cell></row><row><cell>PIANO (TRAIN)</cell><cell>23.5</cell><cell>3.6</cell><cell>9.6</cell><cell>4.1</cell></row><row><cell>MULTISPKR (TRAIN)</cell><cell>0.7</cell><cell>8.1</cell><cell>16.1</cell><cell>3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Sensitivity of the model to whether lowresolution audio was subject to a low-pass filter (LPF) in dB.</figDesc><table><row><cell></cell><cell cols="2">LPF (Test)</cell><cell cols="2">No LPF (Test)</cell></row><row><cell></cell><cell cols="4">SNR LSD SNR LSD</cell></row><row><cell>LPF (Train)</cell><cell>30.1</cell><cell>3.4</cell><cell>0.42</cell><cell>4.5</cell></row><row><cell>No LPF (Train)</cell><cell>0.43</cell><cell>4.4</cell><cell>33.2</cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Accuracy evaluation of time series imputation methods (using L2 distance) with zero-out rates of 10%, 20%, and 30%.</figDesc><table><row><cell cols="4">% Missing Spline DNN Conv. Full</cell></row><row><cell>10%</cell><cell>2.48</cell><cell>2.45</cell><cell>1.00 0.84</cell></row><row><cell>20%</cell><cell>3.55</cell><cell>3.30</cell><cell>1.39 1.22</cell></row><row><cell>30%</cell><cell>4.32</cell><cell>3.97</cell><cell>1.69 1.48</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We calculated the LSD using the natural log, so the LSD units are not dB.<ref type="bibr" target="#b2">3</ref> We also experimented with a sequence-to-sequence architecture. This model preformed very poorly, achieving SNR of about 0 dB for all upscaling ratios. As discussed above, sequence-to-sequence models generally struggle to solve problems involving extremely long time-series signals, as is the case here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We anonymously posted our set of samples to https://anonymousqwerty.github.io/audio-sr/. We will release our source code there as well.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), and AFOSR (FA9550-19-1-0024)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting the sequence specificities of dna-and rna-binding proteins by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Weirauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">831</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Linear predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bradbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Mc G. Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical recovery of wideband speech from narrowband speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><forename type="middle">Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Shaughnessy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="544" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1606.01781</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno>abs/1707.00683</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Harm de Vries, Aaron Courville, and Yoshua Bengio. Feature-wise transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
		<idno>abs/1610.07629</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bandwidth extension of audio signals by spectral band replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Per</forename><surname>Ekstrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st IEEE Benelux Workshop on Model Based Processing and Coding of Audio (MPCA&apos;02. Citeseer</title>
		<meeting>the 1st IEEE Benelux Workshop on Model Based Processing and Coding of Audio (MPCA&apos;02. Citeseer</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale imputation of epigenomic datasets for systematic annotation of diverse human tissues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Kellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="364" to="376" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno>abs/1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Applying lstm to time series predictable through time-window approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="669" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distance measures for speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustine</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Markel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="380" to="391" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1703.06868</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks. arxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Speech and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Corporación favorita grocery sales forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extensive variation in chromatin states across humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Kasowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><surname>Kyriazopoulou-Panagiotopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Grubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">B</forename><surname>Zaugg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">P</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangfeng</forename><forename type="middle">Cliff</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fouad</forename><surname>Zakharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Damek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Spacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">M</forename><surname>Olarerin-George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Steinmetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Hogenesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serafim</forename><surname>Kellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Batzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">342</biblScope>
			<biblScope unit="issue">6159</biblScope>
			<biblScope unit="page" from="750" to="752" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic layer normalization for adaptive neural acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchul</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1707.06065</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Denoising genome-wide histone chip-seq with convolutional neural networks. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">52118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Whole-genome haplotyping using long reads and statistical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Pushkarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Blauwkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kertesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="266" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dnn-based speech bandwidth expansion and its application to adding high-frequency missing features for automatic speech recognition of narrowband speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for noise reduction in robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><forename type="middle">M</forename><surname>Oneil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improved speech enhancement with the wave-u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Macartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
		</author>
		<idno>abs/1811.11307</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Samplernn: An unconditional end-to-end neural audio generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arxiv:1612.07837</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Distill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Narrowband to wideband conversion of speech using gmm based transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youl</forename><surname>Kun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung Soon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing, 2000. ICASSP&apos;00. Proceedings. 2000 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1843" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno>abs/1709.07871</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Speech bandwidth extension using gaussian mixture model-based estimation of the highband mel spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Pulakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulpu</forename><surname>Remes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalle</forename><surname>Palomäki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paavo</forename><surname>Alku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5100" to="5103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Integrative analysis of 111 reference human epigenomes</title>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7539</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Consortium Roadmap Epigenomics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">How to fine-tune BERT for text classification? CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Speech super-resolution using parallel wavenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP)</title>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="260" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Densely connected cnn with multi-scale feature attention for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI&apos;18</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence, IJCAI&apos;18</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4468" to="4474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">English multi-speaker corpus for cstr voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<ptr target="http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Generative and discriminative text classification with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Colorful image colorization. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1509.01626</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
