<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Vehicle Re-identification</term>
					<term>Image Representation</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One fundamental challenge of vehicle reidentification (re-id) is to learn robust and discriminative visual representation, given the significant intra-class vehicle variations across different camera views. As the existing vehicle datasets are limited in terms of training images and viewpoints, we propose to build a unique large-scale vehicle dataset (called VehicleNet) by harnessing four public vehicle datasets, and design a simple yet effective two-stage progressive approach to learning more robust visual representation from VehicleNet. The first stage of our approach is to learn the generic representation for all domains (i.e., source vehicle datasets) by training with the conventional classification loss. This stage relaxes the full alignment between the training and testing domains, as it is agnostic to the target vehicle domain. The second stage is to fine-tune the trained model purely based on the target vehicle set, by minimizing the distribution discrepancy between our VehicleNet and any target domain. We discuss our proposed multi-source dataset VehicleNet and evaluate the effectiveness of the two-stage progressive representation learning through extensive experiments. We achieve the state-of-art accuracy of 86.07% mAP on the private test set of AICity Challenge, and competitive results on two other public vehicle re-id datasets, i.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the learned robust representations can pave the way for vehicle re-id in the real-world environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V EHICLE re-identification (re-id) is to spot the car of interest in different cameras and is usually viewed as a sub-task of image retrieval problem <ref type="bibr" target="#b0">[1]</ref>. It could be applied to the public place for the traffic analysis, which facilitates the traffic jam management and the flow optimization <ref type="bibr" target="#b1">[2]</ref>. Yet vehicle re-id remains challenging since it inherently contains multiple intra-class variants, such as viewpoints, illumination and occlusion. Thus, vehicle re-id system demands a robust and discriminative visual representation given that the realistic scenarios are diverse and complicated. Recent years, Convolutional Neural Network (CNN) has achieved the state-ofthe-art performance in many computer vision tasks, including person re-id <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> and vehicle re-id <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>, but CNN is datahungry and prone to over-fitting small-scale datasets. Since the paucity of vehicle training images compromises the learning <ref type="bibr">Zhedong</ref>  of robust features, vehicle re-id for the small datasets turn into a challenging problem. One straightforward approach is to annotate more training data and re-train the CNN-based model on the augmented dataset. However, it is usually unaffordable due to the annotation difficulty and the time cost. Considering that many vehicle datasets collected in lab environments are publicly available, an interesting problem arises: Can we leverage the public vehicle image datasets to learn the robust vehicle representation? Given the vehicle datasets are related and vehicles share the similar structure, more data from different sources could help the model to learn the common knowledge of vehicles. Inspired by the success of the large-scale dataset, i.e., ImageNet <ref type="bibr" target="#b8">[9]</ref>, we collect a large-scale vehicle dataset, called VehicleNet, in this work.</p><p>Intuitively, we could utilize VehicleNet to learn the relevance between different vehicle re-id datasets. Then the robust features could be obtained by minimizing the objective function. However, different datasets are collected in different environments, and contains different biases. Some datasets, such as CompCar <ref type="bibr" target="#b9">[10]</ref>, are mostly collected in the car exhibitions, while other datasets, e.g., City-Flow <ref type="bibr" target="#b1">[2]</ref> and VeRi-776 <ref type="bibr" target="#b5">[6]</ref>, are collected in the real traffic scenarios. Thus, another scientific problem of how to leverage the multi-source vehicle dataset occurs. In several existing works, some researchers resort to transfer learning <ref type="bibr" target="#b10">[11]</ref>, which aims at transferring the useful knowledge from the labeled source domain to the unlabeled target domain and minimizing the discrepancy between the source domain and the target domain. Inspired by the spirit of transfer learning, in this work, we propose a simple twostage progressive learning strategy to learn from VehicleNet and adapt the trained model to the realistic environment.</p><p>In a summary, to address the above-mentioned challenges, i.e., the data limitation and the usage of multi-source dataset, we propose to build a large-scale dataset, called VehicleNet, via the public datasets and learn the common knowledge of the vehicle representation via two-stage progressive learning (see <ref type="figure" target="#fig_0">Figure 1</ref>). Specifically, instead of only using the original training dataset, we first collect free vehicle images from the web. Comparing with the training set of the CityFlow dataset, we scale up the number of training images from 26, 803 to 434, 440 as a new dataset called VehicleNet. We train the CNN-based model to identify different vehicles, and extract features. With the proposed two-stage progressive learning, the model is further fine-tuned to adapt to the target data distribution, yielding the performance boost. In the experiment, we show that it is feasible to train models with a combination of multiple datasets. When training the model arXiv:2004.06305v1 [cs.CV] 14 Apr 2020 with more samples, we observe a consistent performance boost, which is consistent with the observation in some recent works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Without explicit vehicle part matching or attribute recognition, the CNN-based model learns the viewpoint-invariant feature by "seeing" more vehicles. Albeit simple, the proposed method achieves mAP 75.60% on the private testing set of CityFlow <ref type="bibr" target="#b1">[2]</ref> without extra information. With the temporal and spatial annotation, our method further arrives the 86.07% mAP. The result surpasses the AICity Challenge champion, who also uses the temporal and spatial annotation. In a nutshell, our contributions are two-folds:</p><p>• To address the data limitation, we introduce one largescale dataset, called VehicleNet, to borrow the strength of the public vehicle datasets, which facilitate the learning of robust vehicle features. In the experiment, we verify the feasibility and effectiveness of learning from VehicleNet. • To leverage the multi-source vehicle images in Vehi-cleNet, we propose a simple yet effective learning strategy, i.e., the two-stage progressive learning approach. We discuss and analyze the effectiveness of the two-stage progressive learning approach. The proposed method has achieved competitive performance on the CityFlow benchmark as well as two public vehicle re-identification datasets, i.e., VeRi-776 <ref type="bibr" target="#b5">[6]</ref> and VehicleID <ref type="bibr" target="#b13">[14]</ref>. The rest of this paper is organized as follows. Section II reviews and discusses the related works. In Section III, we illustrate the vehicle re-id dataset and the task definition, followed by the proposed two-stage progressive learning in Section IV. Extensive experiments and ablation studies are in Section V, and the conclusion is draw in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Vehicle Re-identification</head><p>Vehicle re-identification (re-id) demands robust and discriminative image representation. The recent progress of vehicle reidentification has been due to two aspects: 1) the availability of the new vehicle datasets <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> and 2) the discriminative vehicle feature from deeply-learned models. Zapletal et al. <ref type="bibr" target="#b15">[16]</ref> first collect a large-scale dataset with vehicle pairs and extract the color histograms and oriented gradient histograms feature to discriminate different cars. With recent advance in Convolutional Neural Network (CNN), Liu et al. <ref type="bibr" target="#b16">[17]</ref> combine the CNN-based feature with the traditional hand-crafted features to obtain the robust feature. To take fully advantages of the fine-grained patterns, Wang et al. <ref type="bibr" target="#b7">[8]</ref> first explore the vehicle structure and then extract the part-based CNN features according to the location of key points. Besides, Shen et al. <ref type="bibr" target="#b17">[18]</ref> involve the temporal-spatial information into the model training as well as the inference process. Another line of works regards vehicle re-identification as a metric learning problem, and explore the objective functions to help the representation learning. Triplet loss has been widely studied in person re-id <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and also has achieved successes in the vehicle re-id <ref type="bibr" target="#b5">[6]</ref>. Zhang et al. <ref type="bibr" target="#b20">[21]</ref> further company the classification loss with triplet loss, which further improves the re-identification ability. Furthermore, Yan et al. <ref type="bibr" target="#b14">[15]</ref> propose a multi-grain ranking loss to discriminate the appearance-similar cars. Besides, some works also show the attributes, e.g., color, manufactories and wheel patterns, could help the model to learn the discriminative feature <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Augmentation</head><p>Many existing works focus on involving more samples to boost the training. One line of works leverage the generative model to synthesize more samples for training. Wu et al. <ref type="bibr" target="#b23">[24]</ref> and Yue et al. <ref type="bibr" target="#b24">[25]</ref> propose to transfer the image into different image styles, e.g., weather conditions, and learn the robust feature for semantic segmentation. In a similar spirit, Zheng et al. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b25">[26]</ref> utilize the Generative Adversarial Network (GAN) <ref type="bibr" target="#b26">[27]</ref> to obtain lots of pedestrian images, and then involve the generated samples into training as an extra regularization term. Another line of works collects the realworld data from Internet to augment the original dataset. One of the pioneering work <ref type="bibr" target="#b11">[12]</ref> is to collect large number of images via searching the keywords on the online engine, i.e., Google. After removing the noisy data, the augmented dataset facilitate the model to achieve the state-of-the-art performance on several fine-grained datasets, e.g., CUBird <ref type="bibr" target="#b27">[28]</ref>. In a similar spirit, Zheng et al. <ref type="bibr" target="#b28">[29]</ref> also exploit noisy photos of university buildings from Google, benefiting the model learning. In contrast with these existing works, we focus on leveraging the public datasets with different data biases to learn the common knowledge of vehicles given that the vehicle shares the similar structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transfer Learning</head><p>Transfer learning is to propagate the knowledge of the source domain to the target domain <ref type="bibr" target="#b10">[11]</ref>. On one hand, several recent works focus on the alignment between the source domain and the target domain, which intend to minimize the discrepancy of two domains. One of the pioneering works <ref type="bibr" target="#b29">[30]</ref> is to apply the cyclegan <ref type="bibr" target="#b30">[31]</ref> to transfer the image style to the target domain, and then train the model on the transferred data. In this way, the model could learn the similar patterns of the target data. Besides the pixel-level alignment, some works <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> focus on aligning the network activation in the middle or high layers of the neural network. The discriminator is deployed to discriminate the learned feature of source domain from that of target domain, and the main target is to minimize the feature discrepancy via adversarial learning. On the other hand, some works deploy the pseudo label learning, yielding competitive results as well <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. The main idea is to make the model more confident to the prediction, which minimizes the information entropy. The pseudo label learning usually contains two steps. The first step is to train one model from scratch on the source domain and generate the pseudo label for the unlabeled data. The second step is to fine-tune the model and make the model adapt to the target data distribution via the pseudo label. Inspired by the existing works, we propose one simple yet effective two-stage progressive learning. We first train the model on the large-scale VehicleNet dataset and then finetune the model on the target dataset. The proposed method is also close to the traditional pre-training strategy, but the proposed method could converge quickly and yield competitive performance due to the related vehicle knowledge distilled in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET COLLECTION AND TASK DEFINITION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Analysis</head><p>We involve the four public datasets, i.e., CityFlow <ref type="bibr" target="#b1">[2]</ref>, VeRi-776 <ref type="bibr" target="#b5">[6]</ref>, CompCar <ref type="bibr" target="#b9">[10]</ref> and VehicleID <ref type="bibr" target="#b13">[14]</ref> into training. It results in 434,440 training images of 31,805 classes as VehicleNet. Note that the four public datasets are collected in different places. There are no overlapping images with the validation set or the private test set. We plot the data distribution of all four datasets in <ref type="figure" target="#fig_1">Figure 2</ref>.  model made in different years may contain the color and shape difference. We, therefore, view the same car model produced in the different years as different classes, which results in 4,701 classes. • VehicleID <ref type="bibr" target="#b13">[14]</ref> consists 2211,567 images of 26,328 vehicles. The vehicle images are collected in two views, i.e., frontal and rear views. Despite the limited viewpoints, the experiment shows that VehicleID also helps the viewpoint-invariant feature learning. • Other Datasets We also review other public datasets of vehicle images in <ref type="table" target="#tab_2">Table I</ref>. Some datasets contain limited images or views, while others lack ID annotations. For example, PKU-VD1 <ref type="bibr" target="#b14">[15]</ref> only contains the front view of cars. Therefore, we do not use these datasets, which may potentially compromise the feature learning.</p><formula xml:id="formula_0">• CityFlow [2]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Task Definition</head><p>Vehicle re-identification aims to learn a projection function F , which maps the input image x to the discriminative representation f i = F (x i ). Usually, F is decided by minimizing the following optimization function on a set of training data</p><formula xml:id="formula_1">X = {x i } N i=1 with the annotated label Y = {y i } N i=1 : min N i=1 loss(W F (x i ), y i ) + αΩ(F ),<label>(1)</label></formula><p>where loss(·, ·) is the loss function, W is the weight of the classifier, Ω(F ) is the regularization term, and α is the weight of the regularization. Our goal is to leverage the augmented dataset for learning robust image representation given that the vehicle shares the common structure. The challenge is to build the vehicle representation which could fit the different data distribution among multiple datasets. Given</p><formula xml:id="formula_2">X d = {x d i } N i=1 with the annotated label Y d = {y d i } N i=1,d=1</formula><p>, the objective could be formulated as:</p><formula xml:id="formula_3">min D d=1 N i=1 loss(W F (x d i ), y d i ) + αΩ(F ),<label>(2)</label></formula><p>where D is the number of the augmented datasets. The loss demands F could be applied to not only the target dataset but , VehicleID <ref type="bibr" target="#b13">[14]</ref> , CompCar <ref type="bibr" target="#b9">[10]</ref> and VeRi-776 <ref type="bibr" target="#b5">[6]</ref>. We observe that the two largest datasets, i.e., VehicleID and CompCars, suffer from the limited images per class. Note that there are only a few classes with more than 40 training images. (b) Here we also provide the image samples of the four datasets. The four datasets contain different visual biases, such as illumination conditions, collection places and viewpoints. also other datasets, yielding the good scalability. In terms of the regularization term Ω(F ), we adopt the common practise of weight decay as the weight regularization, which prevents the value of weights from growing too large and over-fits the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>We first illustrate the model structure in the Section IV-A. In the Section IV-B, we then introduce the proposed twostage progressive learning method and discuss the advantage of the training strategy, followed by the description of the post-processing methods in the Section IV-C.</p><p>A. Model Structure Feature Extractor. Following the common practise in reidentification problems <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b39">[40]</ref>, we deploy the off-the-shelf Convolutional Neural Network (CNN) model pre-trained on the ImageNet dataset <ref type="bibr" target="#b40">[41]</ref> as the backbone. Specifically, the proposed method is scalable and could be applied to different network backbones. We have trained and evaluated the stateof-the-art structures, including ResNet-50 <ref type="bibr" target="#b41">[42]</ref>, DenseNet-121 <ref type="bibr" target="#b42">[43]</ref>, SE-ResNeXt101 <ref type="bibr" target="#b43">[44]</ref> and SENet-154 <ref type="bibr" target="#b43">[44]</ref>, in the Section V. The classification layer of the pre-trained backbone model is removed, which is dedicated for image recognition on ImageNet. The original average pooling layer is replaced with the adaptive average pooling layer, and the adaptive average pooling layer outputs the mean of the input feature map in terms of the height and width channels. We add one fully-connected layer 'fc1' of 512 dimensions and one batch normalization layer to reduce the feature dimension, followed by a fully-connected layer 'fc2' to output the final classification prediction as shown in the <ref type="figure" target="#fig_2">Figure 3</ref>. The length of the classification prediction equals to the category number of the dataset. The cross-entropy loss is to penalize the wrong vehicle category prediction. Feature Embedding. Vehicle re-identification is to spot the vehicle of interest from different cameras, which demands a robust representation to various visual variants, e.g., viewpoints, illumination and resolution. Given the input image x, we intend to obtain the feature embedding f = F (x|θ). In this work, the CNN-based model contains the projection function F and one linear classifier. Specifically, we regard the 'fc2' as the conventional linear classifier with the learnable weight W , and the module before the final classifier as F with the learned parameter θ. The output of the batch normalization layer as f (see the green box in the <ref type="figure" target="#fig_2">Figure 3</ref>). When inference, we extract the feature embedding of query images and gallery images. The ranking list is generated according to the similarity with the query image. Given the query image, we deploy the cosine similarity, which could be formulated as s(x n , x m ) = fn ||fn||2 × fm ||fm||2 . The ||.|| 2 denotes l 2 norm of the corresponding feature embedding. The large similarity value indicates that the two images are highly relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Two-stage Progressive Learning</head><p>The proposed training strategy contains two stages. During the first stage, we train the CNN-based model on the Ve-hicleNet dataset and learn the general representation of the vehicle images. In particular, we deploy the widely-adopted cross-entropy loss in the recognition tasks, and the model learns to identify the input vehicle images from different classes. The loss could be formulated as:</p><formula xml:id="formula_4">L ce = N i=1 −p i log(q i ),<label>(3)</label></formula><p>where p i is the one-hot vector of the ground-truth label y i . The one-hot vector p i (c) = 1 if the index c equals to y i , else p i (c) = 0. q i is the predicted category probability of the model, and q i = W F (x i |θ). Since we introduce the multisource dataset, the cross-entropy loss could be modified to work with the multi-source data.</p><formula xml:id="formula_5">L ce = D d=1 N i=1 −p d i log(q d i ),<label>(4)</label></formula><p>where d denotes the index of the public datasets in the proposed VehicleNet. Specifically, d = 1, 2, 3, 4 denotes the four datasets in VehicleNet, i.e., CityFlow <ref type="bibr" target="#b1">[2]</ref>, VehicleID <ref type="bibr" target="#b13">[14]</ref> , CompCar <ref type="bibr" target="#b9">[10]</ref> and VeRi-776 <ref type="bibr" target="#b5">[6]</ref>, respectively. p d i is the one-hot vector of y d i , and q d i = W F (x d i |θ). Note that we treat all the dataset equally, and demand the model with good scalability to data of different datasets in VehicleNet.</p><p>In the first stage, we optimize the Equation 4 on all the training data of VehicleNet to learn the shared representation for vehicle images. The Stage-I model is agnostic to the target environment, hence the training domain and the target domain are not fully aligned. In the second stage, we take one more step to further fine-tune the model only upon the target dataset, e.g., CityFlow <ref type="bibr" target="#b1">[2]</ref>, according to the Equation 3. In this way, the model is further optimized for the target environment. Since only one dataset is considered in the Stage-II and the number of vehicle category is decreased, in particular, the classifier is replaced with the new f c2 layer with 333 classes from CityFlow. To preserve the learned knowledge, only the classification layer of the trained model is replaced. Although the new classifier is learned from scratch, attribute to the decent initial weights in the first stage, the model could converge quickly and meets the demand for quick domain adaptation. We, therefore, could stop the training at the early epoch. To summarize, we provide the training procedure of the proposed method in Algorithm 1. Discussion: What are the advantages of the proposed twostage progressive learning? First, the learned representation is more robust. In the Stage-I, we demand the model could output the discriminative representation for all of the data in the multi-source VehicleNet. The model is forced to learn the shared knowledge among the training vehicle images, which is similar to the pre-training practise in many re-ID works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Second, the representation is also more discriminative. The first stage contains 31, 805 training classes during training. The axuiliary classes of other real vehicles could be viewed as "virtual class" as discussed in <ref type="bibr" target="#b44">[45]</ref>. Here we provide one geometric interpretation in the <ref type="figure">Figure 4</ref>. After the convergence of Stage I, the cross-entropy loss pulls the data with the same label together, and pushes the data from different labels away <ref type="figure">Fig. 4</ref>. Geometric Interpretation. Here we give a three-class sample to show our intuition. The cross-entropy loss pulls the samples with the same label together (close to either the relative weight W 1 , W 2 or W 3 ). In this way, the positive pair is closer than the negative pair, while the samples are far from the decision boundary. Stage I, therefore, leads to a decent weight initialization to be used in Stage II with a large margin from decision boundary, when we leave out the auxiliary class, i.e., the third class with W 3 , from VehicleNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training Procedure of the Proposed Method</head><p>Require: The multi-source VehicleNet dataset</p><formula xml:id="formula_6">X d = {x d i } D i=1 ; The corresponding label Y d = {y d i } D i=1</formula><p>; Require: The initialized model parameter θ; The first stage iteration number T1 and the second stage iteration number T2. 1: for iteration = 1 to T1 do 2:</p><p>Stage-I: Input x j t to F (·|θ), extract the prediction of the classifier, and calculate the cross-entropy loss according to Equation <ref type="bibr" target="#b3">4</ref>:</p><formula xml:id="formula_7">Lce = D d=1 N i=1 −p d i log(q d i ),<label>(5)</label></formula><p>where p d i is the one-hot vector of y d i , and q d i is the predict probability. q d i = W F (x d i |θ), W is the final fully-connected layer, which could be viewed as a linear classifer. We update the θ and W during the training. <ref type="bibr" target="#b2">3</ref>: end for 4: for iteration = 1 to T2 do 5:</p><p>Stage-II: We further fine-tune the trained model only on the target dataset, e.g., CityFlow. The classifier is replaced with a new one, since we have less classes. We assume that CityFlow is the first dataset (d = 1). Thus, we could update θ upon the cross-entropy loss according to <ref type="bibr">Equation 3</ref>:</p><formula xml:id="formula_8">Lce = N i=1 −p 1 i log(q 1 i ).<label>(6)</label></formula><p>where p 1 i is the one-hot vector of y 1 i of the CityFlow dataset, and q 1 i is the predict probability. q 1 i = W F (x 1 i |θ). We note that W is the new fully-connected layer, which is trained from scratch and different from W used in the Stage-I. <ref type="bibr" target="#b5">6</ref>: end for 7: return θ. from each other on the either side of the decision boundary. In this manner, as shown in the <ref type="figure">Figure 4 (right)</ref>, the first stage will provide better weight initialization for the subsequent finetuning on the target dataset. It is because the auxiliary classes expand the decision space and the data is much far from the new decision boundary, yielding discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Post-processing</head><p>Furthermore, we also could apply several widely-adopted post-processing techniques during the inference stage as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. For a fair comparison, we do not leverage such methods to comparing the results on the public datasets, but  <ref type="bibr" target="#b45">[46]</ref>, we extract features from the trained models, i.e., 8×SE-ResNeXt101 <ref type="bibr" target="#b43">[44]</ref>. We normalize and concatenate the features. Meanwhile, we extract the camera prediction from the camera-aware model, i.e., the fine-tuned DenseNet121 <ref type="bibr" target="#b42">[43]</ref>. Then query expansion and camera verification are applied. Finally, we utilize the re-ranking technique <ref type="bibr" target="#b46">[47]</ref> to retrieve more positive samples. (This is the pipeline for the submission to the private test set on AICity Challenge 2019.) apply them to the AICity Challenge Competition. Next we provide some brief illustrations of the motivation as well as the mechanism of these techniques. Cropped Images. We notice that the vehicle datasets usually provide a relatively loose bounding box, which may introduce the background noise. Therefore, we re-detect the vehicle with the state-of-the-art MaskRCNN <ref type="bibr" target="#b45">[46]</ref>. For the final result, the vehicle representation is averaged between the original images and cropped images, yielding more robust vehicle representations. Model Ensemble. We adopt a straightforward late-fusion strategy, i.e., concatenating the features <ref type="bibr" target="#b4">[5]</ref>. Given the input image x i , the embedding f j i denotes the extracted feature of x i from the j-th trained model. The final pedestrian descriptor could be represented as:</p><formula xml:id="formula_9">f i = [ f 1 i ||f 1 i ||2 , f 2 i ||f 2 i ||2 , ... f n i ||f n i ||2 ].</formula><p>The || · || 2 operator denotes l 2 -norm, and [·] denotes feature concatenation. Query Expansion &amp; Re-ranking. We adopt the unsupervised clustering method, i.e., DBSCAN <ref type="bibr" target="#b47">[48]</ref> to find the most similar samples. The query feature is updated to the mean feature of the other queries in the same cluster. Furthermore, we adopt the re-ranking method <ref type="bibr" target="#b46">[47]</ref> to refine the final result, which takes the high-confidence candidate images into consideration. In this work, our method does not modify the re-ranking procedure. Instead, the proposed method obtains discriminative vehicle features that distill the knowledge from "seeing" various cars. With better features, re-ranking is more effective. Camera Verification. We utilize the camera verification to further remove some hard-negative samples. When training, we train one extra CNN model, i.e., DenseNet121 <ref type="bibr" target="#b42">[43]</ref>, to recognize the camera from which the photo is taken. When testing, we extract the camera-aware features from the trained model and then cluster these features. We applied the assumption that the query image and the target images are taken in different cameras. Given a query image, we remove the images of the same camera cluster from candidate images (gallery). Temporal Annotation. One common assumption is that the cars that re-appear with long interval are different cars. Given the timestamp t of the query image, we filter out the image in the gallery with long interval τ . As a result, we only consider the candidate images with the timestamp in [t−τ, t+τ ], which also could filter out lots of the hard-negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT</head><p>We first illustrate the implementation details in Section V-A followed by the qualitative results in Section V-B. Furthermore, we provide the futher evaluation and discussion in Section V-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>For the two widely-adopted public datasets, i.e., VeRi-776 and VehicleID, we follow the setting in <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> to conduct a fair comparison. We adopt ResNet-50 <ref type="bibr" target="#b56">[57]</ref> as the backbone network and the input images are resized to 256 × 256. We apply the SGD optimizer with momentum of 0.9 and minibatch size of 36. The weight decay is set to 0.0001 following the setting in <ref type="bibr" target="#b56">[57]</ref>. The initial learning rate is set to 0.02 and is divided by a factor 10 at the 40-th epoch of the first stage and the 8-th epoch in the second stage. The total epochs of the first stage is 60 epochs, while the second-stage fine-tuning is trained with 12 epochs. When inference, we only apply the mean feature of the image flipped horizontally, without using other post-processing approaches.</p><p>For the new dataset, i.e., CityFlow <ref type="bibr" target="#b1">[2]</ref>, we adopt one sophisticated model, i.e., SE-ResNeXt101 <ref type="bibr" target="#b43">[44]</ref> as the backbone to conduct the ablation study and report the performance. The vehicle images are resized to 384 × 384. Similarly, the first stage is trained with 60 epochs, and the second stage contains 12 epochs. When conducting inference on the validation set, we only apply the mean feature of the image flipped horizontally, without using other post-processing approaches.</p><p>In contrast, to achieve the best results on the private test set of CityFlow, we apply all the post-processing methods mentioned in Section IV-C. To verify the effectiveness of the proposed dataset and the approach, we conduct the ablation study and report the results of the validation set in Section V-C. Evaluation Metric. Following previous works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b53">[54]</ref>, we adopt two widely-used evaluation metrics, i.e., Rank@K and mAP. Rank@K is the probability that the true-match image appears in the top-K of the ranking list. Given a ranking list, the average precision (AP) calculates the space under the recall-precision curve, while mAP is the mean of the average precision of all queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results</head><p>Effect of VehicleNet. To verify the effectiveness of the public vehicle data towards the model performance, we involve different vehicle datasets into training and report the results, respectively (see <ref type="table" target="#tab_2">Table II</ref>). There are two primary points as follows: First, the model performance has been improved by involving the training data of one certain datasets, either VeRi-776, CompCar or VehicleID. For instance, the model trained on CityFlow + CompCar has achieved 83.37% Rank@1 and 48.71% mAP, which surpasses the baseline of 73.65% Rank@1 and 37.65% mAP. It shows that more training data from other public datasets indeed helps the model learning the robust representation of vehicle images. Second, we utilize the proposed large-scale VehicleNet to train the model, which contains all the training data of four public datasets. We notice that there are +15.12% Rank@1 improvement from 73.65% Rank@1 to 88.77% Rank@1, and +19.70% mAP increment from 37.65% mAP to 57.35% mAP. It shows that the proposed VehicleNet has successfully "borrowed" the strength from Team Name Temporal Annotation mAP(%) Baidu ZeroOne <ref type="bibr" target="#b57">[58]</ref> 85.54 UWIPL <ref type="bibr" target="#b58">[59]</ref> 79.17 ANU <ref type="bibr" target="#b59">[60]</ref> 75.89 Ours × 75.60 Ours 86.07 multiple datasets and help the model learning robust and discriminative features.</p><p>Comparison with the State-of-the-art. We mainly compare the performance with other methods on the test sets of two public vehicle re-id datasets, i.e., VeRi-776 <ref type="bibr" target="#b5">[6]</ref> and Vehi-cleID <ref type="bibr" target="#b13">[14]</ref> as well as AICity Challenge <ref type="bibr" target="#b53">[54]</ref> private test set. The comparison results with other competitive methods are as follows:</p><p>• VeRi-776 &amp; VehicleID. There are two lines of competitive methods. One line of works deploy the hand-crafted features <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b48">[49]</ref> or utilize the self-designed network <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b50">[51]</ref>. In contrast, another line of works leverages the model pre-trained on ImageNet, yielding the superior performance <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>. As shown in <ref type="table" target="#tab_2">Table III</ref>, we first evaluate the proposed approach on the VeRi-776 dataset <ref type="bibr" target="#b5">[6]</ref>. We leave out the VeRi-776 test set from the VehicleNet to fairly compare the performance, and we deploy the ResNet-50 <ref type="bibr" target="#b56">[57]</ref> as backbone network, which is used by most compared methods. The proposed method has achieved 83.41% mAP and 96.78% Rank@1 accuracy, which is superior to the second best method, i.e., Part-based model <ref type="bibr" target="#b55">[56]</ref> (74.3% mAP and 94.3% Rank@1) by a large margin. Meanwhile, we observe a similar result on the VehicleID dataset <ref type="bibr" target="#b13">[14]</ref> in all three settings (Small /Medium /Large). Small, Medium and Large setting denotes different gallery sizes of 800, 1600 and 2400, respectively. The proposed method also ar-  <ref type="table" target="#tab_2">(%) ACCURACY WITH DIFFERENT STAGES  ON THE CITYFLOW PRIVATE TEST SET. WE REPORT THE RESULTS ON THE   PRIVATE TEST SET RATHER THAN VALIDATION SET, SINCE WE INVOLVE   ALL TRAINING IMAGES INTO FINE-TUNING. POST-PROCESSING METHODS   ARE LEVERAGED ON THE PRIVATE TEST</ref>   <ref type="table" target="#tab_2">Table IV</ref>). With the help of extra annotation of temporal and spatial information (the top 3 team all used), we have achieved 86.07% mAP, which surpasses the champion of the AICity Vehicle Reid Challenge 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Evaluations and Discussion</head><p>Effect of Two-stage Progressive Learning. We compare the final results of the Stage I and the Stage II on the private test set of CityFlow (see <ref type="table" target="#tab_6">Table V</ref>). We do not evaluate the performance on the validation set we splitted, since we utilize the all training images into fine-tuning. The model of Stage II has arrived 87.45% Rank@1 and 75.60% mAP accuracy, which has significantly surpassed the one of Stage I +7.39% mAP and +4.75% Rank@1. It verifies the effectiveness of the two-stage learning. In the Stage I, the target training set, i.e., CityFlow, only occupy 6% of VehicleNet. The learned model, therefore, is sub-optimal for the target environment. To further optimize the model for CityFlow, the second stage fine-tuning helps to minor the gap between VehicleNet and the target training set, yielding better performance. Besides, we also observe similar results on the other two datasets, i.e., VeRi-776 and VehicleID. As shown in the last two row of <ref type="table" target="#tab_2">Table III</ref>, the Stage-II fine-tuning could further boost the performance. For instance, the proposed method has achieved +2.50% mAP and +0.83% Rank@1 improvement on the VeRi-776 dataset. Effect of Post-processing. Here we provide the ablation study of different post-processing techniques on the validation set of CityFlow (see <ref type="table" target="#tab_2">Table VI</ref>). When applying the augmentation with cropped image, model ensemble, query expansion, camera verification and re-ranking, the performance gradually increases, which verifies the effectiveness of the post-processing methods. We also apply the similar policy to the final result on the private test set of AICity Challenge. Effect of Different Backbones. We observe that different backbones may lead to different results. As shown in <ref type="table" target="#tab_2">Table  VII</ref>, SE-ResNeXt101 <ref type="bibr" target="#b43">[44]</ref> arrives the best performance with 83.37 Rank@1 and 48.71% mAP on the validation set of the CityFlow dataset. We speculate that it is tricky to optimize some large-scale neural networks due to the problem of gradient vanishing. For instance, we do not achieve a better result (45.14% mAP) with SENet-154 <ref type="bibr" target="#b43">[44]</ref>, which preforms better than SE-ResNeXt101 <ref type="bibr" target="#b43">[44]</ref> on ImageNet <ref type="bibr" target="#b8">[9]</ref>. We hope this observation could help the further study of the model backbone selection in terms of the re-identification task. Effect of Sampling Policy. Since we introduce more training data in the first stage, the data sampling policy has a large impact on the final result. We compare two different sampling policy. The naive method is to sample every image once in <ref type="figure">Fig. 6</ref>. Qualitative image search results using the vehicle query images from the CityFlow dataset. We select the four query images from different viewpoints, i.e., the front view, the overhead view, the rear view and the side view. The results are sorted from left to right according to the similarity score. The true-matches are in green, when the false-matches are in red.  <ref type="table" target="#tab_2">Table VIII</ref>, the balanced sampling harms the result. We speculate that the long-tailed data distribution (see <ref type="figure" target="#fig_1">Figure 2</ref>) makes the balanced sampling have more chance to select the same image in the classes with fewer images. Thus the model is prone to over-fit the class with limited samples, which compromise the final performance. Therefore, we adopt the naive data sampling policy.</p><p>Visualization of Vehicle Re-id Results. As shown in <ref type="figure">Figure 6</ref>, we provide the qualitative image search results on CityFlow. We select the four query images from different viewpoints, i.e., the front view, the overhead view, the rear view and the side view. The proposed method has successfully retrieved the relevant results in the top-5 of the ranking list.</p><p>Visualization of Learned Heatmap. Following <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b60">[61]</ref>, we utilize the network activation before the pooling layer to visualize the attention of the learned model. As shown in <ref type="figure" target="#fig_4">Figure 7</ref>, the trained model has strong response values at the regions containing discriminative details, such as headlights and tire types. In particular, despite different viewpoints, the model could focus on the salient areas, yielding the viewpointinvariant feature.</p><p>Model Convergence. As shown in <ref type="figure" target="#fig_5">Figure 8 (left)</ref>, despite a large number of training classes, i.e., 31, 805 categories in VehicleNet, the model could converge within 60 epochs. Meanwhile, as discussed, the first stage provides a decent weight initialization for fine-tuning in the second stage. Therefore, the Stage-II training converges quickly within 12 epochs (see <ref type="figure" target="#fig_5">Figure 8</ref> (right)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we intend to address the two challenges in the context of vehicle re-identification, i.e., the lack of training data, and how to harness the multiple public datasets. To address the data limitation, we build a large-scale dataset called VehicleNet with free vehicle training images from public datasets. To learn the robust feature, we propose a simple yet effective approach, called two-stage progressive learning, and discuss the advantages of the learning strategy.</p><p>To verify the effectiveness of the proposed pipeline, we have evaluated the method on the private test set of CityFlow <ref type="bibr" target="#b1">[2]</ref> and achieved the competitive performance in the AICity19 Challenge. The proposed method has surpassed the champion of the challenge, yielding 86.07% mAP accuracy. Besides, the proposed method also has achieved competitive performance on two other public datasets, i.e., VeRi-776 and VehicleID.</p><p>In this paper, we show that more training data matters, and could contribute to the learning of robust visual representation. However, the data collection is still challenging. In the future, we will investigate the synthetic data generated by either GAN <ref type="bibr" target="#b26">[27]</ref> or 3D-models <ref type="bibr" target="#b61">[62]</ref>, to further explore the robust representation learning. <ref type="bibr">Zhedong</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The motivation of our vehicle re-identification method by leveraging public datasets. The common knowledge of discriminating different vehicles could be transferred to the final model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) The image distribution per class in the vehicle re-id datasets, e.g., CityFlow<ref type="bibr" target="#b1">[2]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the model structure. We remove the original classifier of the ImageNet pre-trained model, add a new classifier and replace the average pooling with the adaptive average pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The inference pipeline for AICity Challenge Competition. Given one input image and the corresponding cropped image via MaskRCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of the activation heatmap in the learned model on VehicleNet. The vehicle images in every subfigure (a)-(c) are from the same vehicle ID. Noted that there do exist strong response values at the regions containing discriminative details, such as headlights and tire types. every epoch. Another method is called balanced sampling policy. The balanced sampling is to sample the images of different class with equal possibility. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>The training losses of the two stages. Due to the large-scale data and classes, the first stage (left) takes more epochs to converge. Attribute to the trained weight of the first stage, the second stage (right) converge early.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Yi</head><label></label><figDesc>Yang received the Ph.D. degree in computer science from Zhejiang University, Hangzhou, China, in 2010. He is currently an associate professor with University of Technology Sydney, Australia. He was a Post-Doctoral Research with the School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA. His current research interest includes machine learning and its applications to multimedia content analysis and computer vision, such as multimedia indexing and retrieval, video analysis and video semantics understanding. Tao Mei is a Technical Vice President with JD.com and the Deputy Managing Director of JD AI Research, where he also serves as the Director of Computer Vision and Multimedia Lab. Prior to joining JD.com in 2018, he was a Senior Research Manager with Microsoft Research Asia in Beijing, China. He has authored or coauthored over 200 publications (with 12 best paper awards) in journals and conferences, 10 book chapters, and edited five books. He holds over 50 US and international patents (20 granted). He is or has been an Editorial Board Member of IEEE Trans. on Image Processing, IEEE Trans. on Circuits and Systems for Video Technology, IEEE Trans. on Multimedia, ACM Trans. on Multimedia, Pattern Recognition, etc. Tao received B.E. and Ph.D. degrees from the University of Science and Technology of China, Hefei, China, in 2001 and 2006, respectively. He was elected as a Fellow of IEEE (2019), a Fellow of IAPR (2016), a Distinguished Scientist of ACM (2016), and a Distinguished Industry Speaker of IEEE Signal Processing Society (2017), for his contributions to large-scale multimedia analysis and applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Zheng, Yunchao Wei and Yi Yang are with Centre for Artificial Intelligence, University of Technology Sydney, NSW, Australia. E-mail: zhedong.zheng@student.uts.edu.au, yunchao.wei@uts.edu.au, yi.yang@uts.edu.au. Tao Ruan is with the Institute of Information Science at Beijing Jiaotong University, and the Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, China. E-mail: 16112064@bjtu.edu.cn.</figDesc><table /><note>Tao Mei is with JD AI Research, Beijing, China. E-mail: tmei@live.com.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>After the split, the training set contains 26,803 images of 255 classes, and the validation query set includes 463 images of the rest 78 classes. We deploy the all original training set as the gallery of the validation set. VeRi-776 [6] contains 49,357 images of 776 vehicles from 20 cameras. The dataset is collected in the real traffic scenario, which is close to the setting of CityFlow.</figDesc><table><row><cell>is one of the largest vehicle re-id datasets.</cell></row><row><cell>There are bounding boxes of 666 vehicle identities an-</cell></row><row><cell>notated. All images are collected from 40 cameras in a</cell></row></table><note>realistic scenario at USA City. We follow the official training/test protocol, which results in 36,935 training images of 333 classes and 19,342 testing images of other 333 classes. The training set is collected from 36 cameras, and test is collected from 23 cameras. There are 19 overlapping cameras. Official protocol does not provide a validation set. We therefore further split the training set into a validation set and a small training set.•The author also provides the meta data, e.g., the collected time and the location.• CompCar [10] is designed for the fine-grained car recog- nition. It contains 136,726 images of 1,716 car models. The author provides the vehicle bounding boxes. By cropping and ignoring the invalid bounding boxes, we finally obtain 136,713 images for training. The same car</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I PUBLICLY</head><label>I</label><figDesc>AVAILABLE DATASETS FOR VEHICLE RE-IDENTIFICATION. † : WE VIEW THE VEHICLE MODEL PRODUCED IN DIFFERENT YEARS AS DIFFERENT CLASSES, WHICH LEADS TO MORE CLASSES. ‡ : THE DOWNLOADED IMAGE NUMBER IS SLIGHTLY DIFFERENT WITH THE REPORT NUMBER IN<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell>Datasets</cell><cell># Cameras</cell><cell># Images</cell><cell>#IDs</cell></row><row><cell>CityFlow [2]</cell><cell>40</cell><cell>56,277</cell><cell>666</cell></row><row><cell>VeRi-776 [6]</cell><cell>20</cell><cell>49,357</cell><cell>776</cell></row><row><cell>CompCar [10]  †</cell><cell>n/a</cell><cell>136,713</cell><cell>4,701</cell></row><row><cell>VehicleID [14]  ‡</cell><cell>2</cell><cell>221,567</cell><cell>26,328</cell></row><row><cell>PKU-VD1 [15]</cell><cell>1</cell><cell>1,097,649</cell><cell>1,232</cell></row><row><cell>PKU-VD2 [15]</cell><cell>1</cell><cell>807,260</cell><cell>1,112</cell></row><row><cell>VehicleReID [37]</cell><cell>2</cell><cell>47,123</cell><cell>n/a</cell></row><row><cell>PKU-Vehicle [38]</cell><cell>n/a</cell><cell>10,000,000</cell><cell>n/a</cell></row><row><cell>StanfordCars [39]</cell><cell>n/a</cell><cell>16,185</cell><cell>196</cell></row><row><cell>VehicleNet</cell><cell>62</cell><cell>434,440</cell><cell>31,805</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II THE</head><label>II</label><figDesc>RANK@1 (%) AND MAP (%) ACCURACY WITH DIFFERENT NUMBER OF TRAINING IMAGES. HERE WE REPORT THE RESULTS BASED ON THE VALIDATION SET WE SPLITTED.</figDesc><table><row><cell>Training Datasets</cell><cell># Training Images</cell><cell cols="2">Performance Rank@1 (%) mAP (%)</cell></row><row><cell>CityFlow [2]  †</cell><cell>26,803</cell><cell>73.65</cell><cell>37.65</cell></row><row><cell>CityFlow [2]+ VeRi-776 [6]</cell><cell>+49,357</cell><cell>79.48</cell><cell>43.47</cell></row><row><cell>CityFlow [2]+ CompCar [10]</cell><cell>+136,713</cell><cell>83.37</cell><cell>48.71</cell></row><row><cell>CityFlow [2]+ VehicleID [14]</cell><cell>+221,567</cell><cell>83.37</cell><cell>47.56</cell></row><row><cell>VehicleNet</cell><cell>434,440</cell><cell>88.77</cell><cell>57.35</cell></row></table><note>† NOTE THAT WE SPLIT A VALIDATION SET FROM THE TRAINING SET, WHICH LEADS TO LESS TRAINING DATA.WE APPLY SE-RESNEXT101 [44] AS THE BACKBONE MODEL.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III COMPARISON</head><label>III</label><figDesc>WITH THE STATE-OF-THE-ART METHODS IN TERMS OF RANK@1 (%) AND MAP (%) ACCURACY ON THE VERI-776 DATASET [6] AND THE VEHICLEID DATASET [14]. -: DENOTES THE CONVENTIONAL HAND-CRAFTED FEATURES AND *: DENOTES THAT THE APPROACH UTILIZES THE SELF-DESIGNED NETWORK STRUCTURE. THE BEST RESULTS ARE IN BOLD.</figDesc><table><row><cell>Methods</cell><cell>Backbones</cell><cell cols="8">VeRi-776 mAP (%) Rank@1 (%) Rank@1 (%) Rank@5 (%) Rank@1 (%) Rank@5 (%) Rank@1 (%) Rank@5 (%) VehicleID (Small) VehicleID (Medium) VehicleID (Large)</cell></row><row><cell>LOMO [49]</cell><cell>-</cell><cell>9.78</cell><cell>23.87</cell><cell>19.74</cell><cell>32.14</cell><cell>18.95</cell><cell>29.46</cell><cell>15.26</cell><cell>25.63</cell></row><row><cell>GoogLeNet [10]</cell><cell>GoogLeNet</cell><cell>17.81</cell><cell>52.12</cell><cell>47.90</cell><cell>67.43</cell><cell>43.45</cell><cell>63.53</cell><cell>38.24</cell><cell>59.51</cell></row><row><cell>FACT [6]</cell><cell>-</cell><cell>18.73</cell><cell>51.85</cell><cell>49.53</cell><cell>67.96</cell><cell>44.63</cell><cell>64.19</cell><cell>39.91</cell><cell>60.49</cell></row><row><cell>XVGAN [50]</cell><cell>*</cell><cell>24.65</cell><cell>60.20</cell><cell>52.89</cell><cell>80.84</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SiameseVisual [18]</cell><cell>*</cell><cell>29.48</cell><cell>41.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OIFE [8]</cell><cell>*</cell><cell>48.00</cell><cell>65.92</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67.0</cell><cell>82.9</cell></row><row><cell>VAMI [7]</cell><cell>*</cell><cell>50.13</cell><cell>77.03</cell><cell>63.12</cell><cell>83.25</cell><cell>52.87</cell><cell>75.12</cell><cell>47.34</cell><cell>70.29</cell></row><row><cell>NuFACT [51]</cell><cell>*</cell><cell>53.42</cell><cell>81.56</cell><cell>48.90</cell><cell>69.51</cell><cell>43.64</cell><cell>65.34</cell><cell>38.63</cell><cell>60.72</cell></row><row><cell>AAVER [52]</cell><cell>ResNet-50</cell><cell>58.52</cell><cell>88.68</cell><cell>72.47</cell><cell>93.22</cell><cell>66.85</cell><cell>89.39</cell><cell>60.23</cell><cell>84.85</cell></row><row><cell>VANet [53]</cell><cell>GoogLeNet</cell><cell>66.34</cell><cell>89.78</cell><cell>83.26</cell><cell>95.97</cell><cell>81.11</cell><cell>94.71</cell><cell>77.21</cell><cell>92.92</cell></row><row><cell>PAMTRI [54]</cell><cell>DenseNet-121</cell><cell>71.88</cell><cell>92.86</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [55]</cell><cell>ResNet-50</cell><cell>72.5</cell><cell>93.3</cell><cell>79.7</cell><cell>94.3</cell><cell>78.4</cell><cell>91.3</cell><cell>75.6</cell><cell>88.3</cell></row><row><cell>Part [56]</cell><cell>ResNet-50</cell><cell>74.3</cell><cell>94.3</cell><cell>78.4</cell><cell>92.3</cell><cell>75.0</cell><cell>88.3</cell><cell>74.2</cell><cell>86.4</cell></row><row><cell>Ours (Stage-I)</cell><cell>ResNet-50</cell><cell>80.91</cell><cell>95.95</cell><cell>83.26</cell><cell>96.77</cell><cell>81.13</cell><cell>93.68</cell><cell>79.06</cell><cell>91.84</cell></row><row><cell>Ours (Stage-II)</cell><cell>ResNet-50</cell><cell>83.41</cell><cell>96.78</cell><cell>83.64</cell><cell>96.86</cell><cell>81.35</cell><cell>93.61</cell><cell>79.46</cell><cell>92.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPETITION</head><label>IV</label><figDesc>RESULTS OF AICITY VEHICLE RE-ID CHALLENGE ON THE PRIVATE TEST SET. OUR RESULTS ARE IN BOLD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V THE</head><label>V</label><figDesc>RANK@1(%) AND MAP</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>SET. For AICity Challenge Competition (on the private test set of CityFlow<ref type="bibr" target="#b1">[2]</ref>), we adopt a slightly different training strategy, using the large input size as well as the model ensemble. The images are resized to 384 × 384. We adopt the mini-batch SGD with the weight decay of 5e-4 and a momentum of 0.9. In the first stage, we decay the learning rate of 0.1 at the 40-th and 55-th epoch. We trained 32 models with different batchsizes and different learning rates. In the second stage, we fine-tune the models on the original dataset. We decay the learning rate of 0.1 at the 8-th epoch and stop training at the 12-th epoch. Finally, we select 8 best models on the validation set to extract the feature.</figDesc><table><row><cell></cell><cell cols="2">Private Test Set</cell></row><row><cell></cell><cell>Rank@1(%)</cell><cell>mAP(%)</cell></row><row><cell>Stage I</cell><cell>82.70</cell><cell>68.21</cell></row><row><cell>Stage II</cell><cell>87.45</cell><cell>75.60</cell></row><row><cell></cell><cell>TABLE VI</cell><cell></cell></row><row><cell cols="3">EFFECT OF DIFFERENT POST-PROCESSING TECHNIQUES ON THE</cell></row><row><cell cols="2">CITYFLOW VALIDATION SET.</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Performance</cell></row><row><cell>with Cropped Image?</cell><cell></cell><cell></cell></row><row><cell>Model Ensemble?</cell><cell></cell><cell></cell></row><row><cell>Query Expansion?</cell><cell></cell><cell></cell></row><row><cell>Camera Verification?</cell><cell></cell><cell></cell></row><row><cell>Re-ranking?</cell><cell></cell><cell></cell></row><row><cell>mAP (%)</cell><cell cols="2">57.35 57.68 61.29 63.97 65.97 74.52</cell></row><row><cell cols="3">rives competitive results, e.g., 83.64% Rank@1, 96.86%</cell></row><row><cell cols="3">Rank@5, of the small gallery setting, 81.35% Rank@1,</cell></row><row><cell cols="3">93.61% Rank@5, of the medium gallery setting, and</cell></row><row><cell cols="3">79.46% Rank@1, 92.04% Rank@5, of the large gallery</cell></row><row><cell>setting.</cell><cell></cell><cell></cell></row><row><cell cols="3">When testing, we adopt the horizontal flipping and scale</cell></row><row><cell cols="3">jittering, which resizes the image with the scale factors</cell></row><row><cell cols="3">[1, 0.9, 0.8] to extract features. As a result, we arrive at</cell></row><row><cell cols="3">75.60% mAP on the private testing set. Without extra</cell></row><row><cell cols="3">temporal annotations, our method has already achieved</cell></row><row><cell cols="2">competitive results (see</cell><cell></cell></row></table><note>• AICity Challenge.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII THE</head><label>VII</label><figDesc>RANK@1 (%) AND MAP (%) ACCURACY WITH DIFFERENT BACKBONES ON THE CITYFLOW VALIDATION SET. THE BEST RESULTS ARE IN BOLD.</figDesc><table><row><cell>Backbones</cell><cell>ImageNet Top5(%)</cell><cell cols="2">Performance Rank@1 (%) mAP (%)</cell></row><row><cell>ResNet-50 [57]</cell><cell>92.98</cell><cell>77.97</cell><cell>43.65</cell></row><row><cell>DenseNet-121 [43]</cell><cell>92.14</cell><cell>83.15</cell><cell>47.17</cell></row><row><cell>SE-ResNeXt101 [44]</cell><cell>95.04</cell><cell>83.37</cell><cell>48.71</cell></row><row><cell>SENet-154 [44]</cell><cell>95.53</cell><cell>81.43</cell><cell>45.14</cell></row><row><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell></row><row><cell cols="4">THE RANK@1(%) AND MAP (%) ACCURACY ON THE CITYFLOW</cell></row><row><cell cols="4">VALIDATION SET WITH TWO DIFFERENT SAMPLING METHODS. HERE WE</cell></row><row><cell cols="3">USE THE RESNET-50 BACKBONE.</cell><cell></cell></row><row><cell>Backbones</cell><cell cols="3">Performance Rank@1(%) mAP(%)</cell></row><row><cell>Naive Sampling</cell><cell></cell><cell>77.97</cell><cell>43.65</cell></row><row><cell cols="2">Balanced Sampling</cell><cell>76.03</cell><cell>40.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Zheng received the B.S. degree in computer science from Fudan University, China, in 2016. He is currently a Ph.D. student with the School of Computer Science at University of Technology Sydney, Australia. His research interests include robust learning for image retrieval, generative learning for data augmentation, and unsupervised domain adaptation. Ruan received the B.E. degree in computer science and technology from Beijing Jiaotong University, Beijing, China, in 2016. He is currently a Ph.D. candidate with the Institute of Information Science, Beijing Jiaotong University, Beijing, China. His research interests include video synopsis and pixel understanding. Yunchao Wei received his Ph.D. degree from Beijing Jiaotong University, Beijing, China. He was a Postdoctoral Researcher at Beckman Institute, UIUC, from 2017 to 2019. He is currently an Assistant Professor with the Centre for Artificial Intelligence, University of Technology Sydney. He is ARC Discovery Early Career Researcher Award (DECRA) Fellow from 2019 to 2021. His current research interests include computer vision and machine learning.</figDesc><table><row><cell>Tao</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CityFlow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anastasiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep learning-based approach to progressive vehicle re-identification for urban surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aware attentive multi-view inference for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Orientation invariant feature embedding and spatial temporal regularization for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for finegrained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting multi-grain ranking constraints for precisely searching visually-similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vehicle re-identification for automatic video traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zapletal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep neural networks for vehicle re-id with visual-spatio-temporal path proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dual-path convolutional image-text embedding with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACM TOMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving triplet-wise training of convolutional neural network for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transferable joint attributeidentity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ace: Adapting to changing environments for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain randomization and pyramid consistency: Simulationto-real generalization without accessing target domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by GAN improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">University-1652: A multi-view multisource benchmark for drone-based geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12186</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ICML</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vehicle re-identification for automatic video traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zapletal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Groupsensitive triplet embedding for vehicle reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2385" to="2399" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhedong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhiding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Virtual class enhanced discriminative embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1942" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-view gan based vehicle generation for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Provid: Progressive and multimodal vehicle reidentification for large-scale urban surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="645" to="658" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A dual path modelwith adaptive attention for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorramshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03397</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Vehicle reidentification with viewpoint-aware metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8282" to="8291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pamtri: Pose-aware multi-task learning for vehicle re-identification using highly randomized synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Stripe-based and attributeaware network: A two-branch deep model for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05549</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Part-regularized near-duplicate vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3997" to="4005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-camera vehicle tracking and reidentification based on visual and spatial-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-view vehicle re-identification using temporal attention model and metadata reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="434" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Vehicle reidentification with the location and time stamp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A discriminatively learned CNN embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">TOMM</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Simulating content consistent vehicle datasets with attribute descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08855</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
