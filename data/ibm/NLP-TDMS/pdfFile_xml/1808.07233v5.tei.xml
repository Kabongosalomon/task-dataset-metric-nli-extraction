<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Architecture Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<email>chenen@ustc.edu.cn2fetia</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tie-yan.liu@microsoft.com</email>
						</author>
						<title level="a" type="main">Neural Architecture Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 1.93% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. The best discovered architectures on both tasks are successfully transferred to other tasks such as CIFAR-100 (test error rate of 14.75%), ImageNet (top-1 test error rate of 25.7%) and WikiText-2 (test perplexity of 67.0). Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 2.93%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic design of neural network architecture without human intervention has been the interests of the community from decades ago <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref> to very recent <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr">7]</ref>. The latest algorithms for automatic architecture design usually fall into two categories: reinforcement learning (RL) <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b2">3]</ref> based methods and evolutionary algorithm (EA) based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>. In RL based methods, the choice of a component of the architecture is regarded as an action. A sequence of actions defines an architecture of a neural network, whose dev set accuracy is used as the reward. In EA based method, search is performed through mutations and re-combinations of architectural components, where those architectures with better performances will be picked to continue evolution.</p><p>It can be easily observed that both RL and EA based methods essentially perform search within the discrete architecture space. This is natural since the choices of neural network architectures are typically discrete, such as the filter size in CNN and connection topology in RNN cell. However, directly searching the best architecture within discrete space is inefficient given the exponentially growing search space with the number of choices increasing. In this work, we instead propose to optimize network architecture by mapping architectures into a continuous vector space (i.e., network embeddings) and conducting optimization in this continuous space via gradient based method. On one hand, similar to the distributed representation of natural language <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b25">26]</ref>, the continuous representation of an architecture is more compact and efficient in representing its topological information; On the other hand, optimizing in a continuous space is much easier than directly searching within discrete space due to better smoothness.</p><p>We call this optimization based approach Neural Architecture Optimization (NAO), which is briefly shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The core of NAO is an encoder model responsible to map a neural network architecture into a continuous representation (the blue arrow in the left part of <ref type="figure" target="#fig_0">Fig. 1</ref>). On top of the continuous representation we build a regression model to approximate the final performance (e.g., classification accuracy on the dev set) of an architecture (the yellow surface in the middle part of <ref type="figure" target="#fig_0">Fig. 1</ref>). It is noteworthy here that the regression model is similar to the performance predictor in previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10]</ref>. What distinguishes our method is how to leverage the performance predictor: different with previous work <ref type="bibr" target="#b26">[27]</ref> that uses the performance predictor as a heuristic to select the already generated architectures to speed up searching process, we directly optimize the module to obtain the continuous representation of a better network (the green arrow in the middle and bottom part of <ref type="figure" target="#fig_0">Fig. 1</ref>) by gradient descent. The optimized representation is then leveraged to produce a new neural network architecture that is predicted to perform better. To achieve that, another key module for NAO is designed to act as the decoder recovering the discrete architecture from the continuous representation (the red arrow in the right part of <ref type="figure" target="#fig_0">Fig. 1</ref>). The decoder is an LSTM model equipped with an attention mechanism that makes the exact recovery easy. The three components (i.e., encoder, performance predictor and decoder) are jointly trained in a multi task setting which is beneficial to continuous representation: the decoder objective of recovering the architecture further improves the quality of the architecture embedding, making it more effective in predicting the performance.</p><p>We conduct thorough experiments to verify the effectiveness of NAO, on both image classification and language modeling tasks. Using the same architecture space commonly used in previous works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27]</ref>, the architecture found via NAO achieves 1.93% test set error rate (with cutout <ref type="bibr" target="#b10">[11]</ref>) on CIFAR-10. Furthermore, on PTB dataset we achieve 56.0 perplexity, also surpassing best performance found via previous methods on neural architecture search. Furthermore, we show that equipped with the recent proposed weight sharing mechanism in ENAS <ref type="bibr" target="#b36">[37]</ref> to reduce the large complexity in the parameter space of child models, we can achieve improved efficiency in discovering powerful convolutional and recurrent architectures, e.g., both take less than 10 hours on 1 GPU. Our codes and model checkpoints are available at https://github.com/renqianluo/NAO and https://github.com/renqianluo/NAO_pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently the design of neural network architectures has largely shifted from leveraging human knowledge to automatic methods, sometimes referred to as Neural Architecture Search (NAS) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>. As mentioned above, most of these methods are built upon one of the two basic algorithms: reinforcement learning (RL) <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref> and evolutionary algorithm (EA) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b27">28]</ref>. For example, <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37]</ref> use policy networks to guide the next-step architecture component. The evolution processes in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28]</ref> guide the mutation and recombination process of candidate architectures. Some recent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref> try to improve the efficiency in architecture search by exploring the search space incrementally and sequentially, typically from shallow to hard. Among them, <ref type="bibr" target="#b26">[27]</ref> additionally utilizes a performance predictor to select the promising candidates. Similar performance predictor has been specifically studied in parallel works such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref>. Although different in terms of searching algorithms, all these works target at improving the quality of discrete decision in the process of searching architectures.</p><p>The most recent work parallel to ours is DARTS <ref type="bibr" target="#b28">[29]</ref>, which relaxes the discrete architecture space to continuous one by mixture model and utilizes gradient based optimization to derive the best Then e x is optimized into e x via maximizing the output of performance predictor f using gradient ascent (the green arrow). Afterwards e x is transformed into a new architecture x using the decoder network. architecture. One one hand, both NAO and DARTS conducts continuous optimization via gradient based method; on the other hand, the continuous space in the two works are different: in DARTS it is the mixture weights and in NAO it is the embedding of neural architectures. The difference in optimization space leads to the difference in how to derive the best architecture from continuous space: DARTS simply assumes the best decision (among different choices of architectures) is the argmax of mixture weights while NAO uses a decoder to exactly recover the discrete architecture.</p><p>Another line of work with similar motivation to our research is using bayesian optimization (BO) to perform automatic architecture design <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b21">22]</ref>. Using BO, an architecture's performance is typically modeled as sample from a Gaussian process (GP). The induced posterior of GP, a.k.a. the acquisition function denoted as a : X → R + where X represents the architecture space, is tractable to minimize. However, the effectiveness of GP heavily relies on the choice of covariance functions K(x, x ) which essentially models the similarity between two architectures x and x . One need to pay more efforts in setting good K(x, x ) in the context of architecture design, bringing additional manual efforts whereas the performance might still be unsatisfactory <ref type="bibr" target="#b21">[22]</ref>. As a comparison, we do not build our method on the complicated GP setup and empirically find that our model which is simpler and more intuitive works much better in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We introduce the details of neural architecture optimization (NAO) in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture Space</head><p>Firstly we introduce the design space for neural network architectures, denoted as X . For fair comparison with previous NAS algorithms, we adopt the same architecture space commonly used in previous works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>For searching CNN architecture, we assume that the CNN architecture is hierarchical in that a cell is stacked for a certain number of times (denoted as N ) to form the final CNN architecture. The goal is to design the topology of the cell. A cell is a convolutional neural network containing B nodes. Each of the nodes contains two branches, with each branch taking the output of one of the former nodes as input and applying an operation to it. The operation set includes 11 operators listed in Appendix. The node adds the outputs of its two branches as its output. The inputs of the cell are the outputs of two previous cells, respectively denoted as node −2 and node −1. Finally, the outputs of all the nodes that are not used by any other nodes are concatenated to form the final output of the cell. Therefore, for each of the B nodes we need to: 1) decide which two previous nodes are used as the inputs to its two branches; 2) decide the operation to apply to its two branches. We set B = 5 in our experiments as in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>For searching RNN architecture, we use the same architecture space as in <ref type="bibr" target="#b36">[37]</ref>. The architecture space is imposed on the topology of an RNN cell, which computes the hidden state h t using input i t and previous hidden state h t−1 . The cell contains B nodes and we have to make two decisions for each node, similar to that in CNN cell: 1) a previous node as its input; 2) the activation function to apply. For example, if we sample node index 2 and ReLU for node 3, the output of the node will be</p><formula xml:id="formula_0">o 3 = ReLU (o 2 · W h 3</formula><p>). An exception here is for the first node, where we only decide its activation function a 1 and its output is</p><formula xml:id="formula_1">o 1 = a 1 (i t · W i + h t−1 · W h 1 )</formula><p>. Note that all W matrices are the weights related with each node. The available activation functions are: tanh, ReLU, identity and sigmoid. Finally, the output of the cell is the average of the output of all the nodes. In our experiments we set B = 12 as in <ref type="bibr" target="#b36">[37]</ref>.</p><p>We use a sequence consisting of discrete string tokens to describe a CNN or RNN architecture. Taking the description of CNN cell as an example, each branch of the node is represented via three tokens, including the node index it selected as input, the operation type and operation size. For example, the sequence "node-2 conv 3x3 node1 max-pooling 3x3 " means the two branches of one node respectively takes the output of node −2 and node 1 as inputs, and respectively apply 3 × 3 convolution and 3 × 3 max pooling. For the ease of introduction, we use the same notation x = {x 1 , · · · , x T } to denote such string sequence of an architecture x, where x t is the token at t-th position and all architectures x ∈ X share the same sequence length denoted as T . T is determined via the number of nodes B in each cell in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Components of Neural Architecture Optimization</head><p>The overall framework of NAO is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. To be concrete, there are three major parts that constitute NAO: the encoder, the performance predictor and the decoder.</p><p>Encoder. The encoder of NAO takes the string sequence describing an architecture as input, and maps it into a continuous space E. Specifically the encoder is denoted as E : X → E. For an architecture x, we have its continuous representation (a.k.a. embedding) e x = E(x). We use a single layer LSTM as the basic model of encoder and the hidden states of the LSTM are used as the continuous representation of x. Therefore we have e</p><formula xml:id="formula_2">x = {h 1 , h 2 , · · · , h T } ∈ R T ×d where h t ∈ R d is LSTM hidden state at t-th timestep with dimension d 3 .</formula><p>Performance predictor. The performance predictor f : E → R + is another important module accompanied with the encoder. It maps the continuous representation e x of an architecture x into its performance s x measured by dev set accuracy. Specifically, f first conducts mean pooling on e x = {h 1 , · · · , h T } to obtain e x = 1 T T t h t , and then maps e x to a scalar value using a feed-forward network as the predicted performance. For an architecture x and its performance s x as training data, the optimization of f aims at minimizing the least-square regression loss (s x − f (E(x))) 2 .</p><p>Considering the objective of performance prediction, an important requirement for the encoder is to guarantee the permutation invariance of architecture embedding: for two architectures x 1 and x 2 , if they are symmetric (e.g., x 2 is formed via swapping two branches within a node in x 1 ), their embeddings should be close to produce the same performance prediction scores. To achieve that, we adopt a simple data augmentation approach inspired from the data augmentation method in computer vision (e.g., image rotation and flipping): for each (x 1 , s x ), we add an additional pair (x 2 , s x ) where x 2 is symmetrical to x 1 , and use both pairs (i.e., (x 1 , s x ) and (x 2 , s x )) to train the encoder and performance predictor. Empirically we found that acting in this way brings non-trivial gain: on CIFAR-10 about 2% improvement when we measure the quality of performance predictor via the pairwise accuracy among all the architectures (and their performances).</p><p>Decoder. Similar to the decoder in the neural sequence-to-sequence model <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b8">9]</ref>, the decoder in NAO is responsible to decode out the string tokens in x, taking e x as input and in an autoregressive manner. Mathematically the decoder is denoted as D : E → x which decodes the string tokens x from its continuous representation: x = D(e x ). We set D as an LSTM model with the initial hidden state s 0 = h T (x). Furthermore, attention mechanism <ref type="bibr" target="#b1">[2]</ref> is leveraged to make decoding easier, which will output a context vector ctx r combining all encoder outputs {h t } T t=1 at each timestep r. The decoder D then induces a factorized distribution</p><formula xml:id="formula_3">P D (x|e x ) = T r=1 P D (x r |e x , x &lt;r ) on x, where the distribution on each token x r is P D (x r |e x , x &lt;r ) = exp(Wx r [sr,ctxr])</formula><p>x ∈Vr exp(W x [sr,ctxr]) . Here W is the output embedding matrix for all tokens, x &lt;r represents all the previous tokens before position r, s r is the LSTM hidden state at r-th timestep and [, ] means concatenation of two vectors. V r denotes the space of valid tokens at position r to avoid the possibility of generating invalid architectures.</p><p>The training of decoder aims at recovering the architecture x from its continuous representation e x = E(x). Specifically, we would like to maximize log P D (x|E(x)) = T r=1 log P D (x r |E(x), x &lt;r ). In this work we use the vanilla LSTM model as the decoder and find it works quite well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference</head><p>We jointly train the encoder E, performance predictor f and decoder D by minimizing the combination of performance prediction loss L pp and structure reconstruction loss L rec :</p><formula xml:id="formula_4">L = λLpp + (1 − λ)Lrec = λ x∈X (sx − f (E(x)) 2 − (1 − λ) x∈X log PD(x|E(x)),<label>(1)</label></formula><p>where X denotes all candidate architectures x (and their symmetrical counterparts) that are evaluated with the performance number s x . λ ∈ [0, 1] is the trade-off parameter. Furthermore, the performance prediction loss acts as a regularizer that forces the encoder not optimized into a trivial state to simply copy tokens in the decoder side, which is typically eschewed by adding noise in encoding x by previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>When both the encoder and decoder are optimized to convergence, the inference process for better architectures is performed in the continuous space E. Specifically, starting from an architecture x with satisfactory performance, we obtain a better continuous representation e x by moving e x = {h 1 , · · · , h T }, i.e., the embedding of x, along the gradient direction induced by f :</p><formula xml:id="formula_5">h t = ht + η ∂f ∂ht , e x = {h 1 , · · · , h T },<label>(2)</label></formula><p>where η is the step size. Such optimization step is represented via the green arrow in <ref type="figure" target="#fig_0">Fig. 1</ref>. e x corresponds to a new architecture x which is probably better than x since we have f (e x ) ≥ f (e x ), as long as η is within a reasonable range (e.g., small enough). Afterwards, we feed e x into decoder to obtain a new architecture x assumed to have better performance <ref type="bibr" target="#b3">4</ref> . We call the original architecture x as 'seed' architecture and iterate such process for several rounds, with each round containing several seed architectures with top performances. The detailed algorithm is shown in Alg. 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Neural Architecture Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combination with Weight Sharing</head><p>Recently the weight sharing trick proposed in <ref type="bibr" target="#b36">[37]</ref> significantly reduces the computational complexity of neural architecture search. Different with NAO that tries to reduce the huge computational cost brought by the search algorithm, weight sharing aims to ease the huge complexity brought by massive child models via the one-shot model setup <ref type="bibr" target="#b4">[5]</ref>. Therefore, the weight sharing method is complementary to NAO and it is possible to obtain better performance by combining NAO and weight sharing. To verify that, apart from the aforementioned algorithm 1, we try another different setting of NAO by adding the weight sharing method. Specifically we replace the RL controller in ENAS <ref type="bibr" target="#b36">[37]</ref> by NAO including encoder, performance predictor and decoder, with the other training pipeline of ENAS unchanged. The results are reported in the next section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we report the empirical performances of NAO in discovering competitive neural architectures on benchmark datasets of two tasks, the image recognition and the language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on CIFAR-10 Classification</head><p>We move some details of model training for CIFAR-10 classification to the Appendix. The architecture encoder of NAO is an LSTM model with token embedding size and hidden state size respectively set as 32 and 96. The hidden states of LSTM are normalized to have unit length, i.e., h t = ht ||ht|| 2 2 , to constitute the embedding of the architecture x: e x = {h 1 , · · · , h T }. The performance predictor f is a one layer feed-forward network taking 1 T T t=1 h t as input. The decoder is an LSTM model with an attention mechanism and the hidden state size is 96. The normalized hidden states of the encoder LSTM are used to compute the attention. The encoder, performance predictor and decoder of NAO are trained using Adam for 1000 epochs with a learning rate of 0.001. The trade-off parameters in Eqn. (1) is λ = 0.9. The step size to perform continuous optimization is η = 10. Similar to previous works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b36">37]</ref>, for all the architectures in NAO training phase (i.e., in Alg. 1), we set them to be small networks with B = 5, N = 3, F = 32 and train them for 25 epochs. After the best cell architecture is found, we build a larger network by stacking such cells 6 times (set N = 6), and enlarging the filter size (set F = 36, F = 64 and F = 128), and train it on the whole training dataset for 600 epochs.</p><p>We tried two different settings of the search process for CIFAR-10: without weight sharing and with weight sharing. For the first setting, we run the evaluation-optimization step in Alg. 1 for three times (i.e., L = 3), with initial X set as 600 randomly sampled architectures, K = 200, forming 600 + 200 + 200 = 1000 model architectures evaluated in total. We use 200 V100 GPU cards to complete all the process within 1 day. For the second setting, we exactly follow the setup of ENAS <ref type="bibr" target="#b36">[37]</ref> and the search process includes running 150 epochs on 1 V100 GPU for 7 hours.</p><p>The detailed results are shown in <ref type="table">Table 1</ref>, where we demonstrate the performances of best experts designed architectures (in top block), the networks discovered by previous NAS algorithm (in middle block) and by NAO (refer to its detailed architecture in Appendix), which we name as NAONet (in bottom block). We have several observations. (1) NAONet achieves the best test set error rate (1.93) among all architectures. (2) When accompanied with weight sharing (NAO-WS), NAO achieves 2.93% error rate with much less parameters (2.5M) within only 7 hours (0.3 day), which is more efficient than ENAS. (3) Compared with the previous strongest architecture, the AmoebaNet, within smaller search space (#op = 11), NAO not only reduces the classification error rate (3.34 → 3.18), but also needs an order of magnitude less architectures that are evaluated (20000 → 1000). (3) Compared with PNAS <ref type="bibr" target="#b26">[27]</ref>, even though the architecture space of NAO is slightly larger, NAO is more efficient (M = 1000) and significantly reduces the error rate of PNAS (3.41 → 3.18).</p><p>We furthermore conduct in-depth analysis towards the performances of NAO. In <ref type="figure" target="#fig_2">Fig. 2(a)</ref>   <ref type="table">Table 1</ref>: Performances of different CNN models on CIFAR-10 dataset. 'B' is the number of nodes within a cell introduced in subsection 3.1. 'N' is the number of times the discovered normal cell is unrolled to form the final CNN architecture. 'F' represents the filter size. '#op' is the number of different operation for one branch in the cell, which is an indicator of the scale of architecture space for automatic architecture design algorithm. 'M' is the total number of network architectures that are trained to obtain the claimed performance. '/' denotes that the criteria is meaningless for a particular algorithm. 'NAONet-WS' represents the architecture discovered by NAO and the weight sharing method as described in subsection 3.4. 'Random-WS' represents the random search baseline, conducted in the weight sharing setting of ENAS <ref type="bibr" target="#b36">[37]</ref>. of decoded architecture using D and original architecture to measure their differences. Specifically the measure is dist D = 1 |Xtest| x∈Xtest Dist(D(E(x)), x). As shown in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>, the performance predictor is able to achieve satisfactory quality (i.e., &gt; 78% pairwise accuracy) with only roughly 500 evaluated architectures. Furthermore, the decoder D is powerful in that it can almost exactly recover the network architecture from its embedding, with averaged Hamming distance between the description strings of two architectures less than 0.5, which means that on average the difference between the decoded sequence and the original one is less than 0.5 tokens (totally 60 tokens). Furthermore, we would like to inspect whether the gradient update in Eqn.</p><p>(2) really helps to generate better architecture representations that are further decoded to architectures via D. In <ref type="figure" target="#fig_2">Fig. 2(b)</ref> we show the average performances of architectures in X eval discovered via NAO at each optimization iteration. Red bar indicates the mean of real performance values 1 |X eval | x∈X eval s x while blue bar indicates the mean of predicted value 1 |X eval | x∈X eval f (E(x)). We can observe that the performances of architectures in X eval generated via NAO gradually increase with each iteration. Furthermore, the performance predictor f produces predictions well aligned with real performance, as is shown via the small gap between the paired red and blue bars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transferring the discovered architecture to CIFAR-100</head><p>To evaluate the transferability of discovered NAOnet, we apply it to CIFAR-100. We use the best architecture discovered on CIFAR-10 and exactly follow the same training setting. Meanwhile, we evaluate the performances of other automatically discovered neural networks on CIFAR-100 by strictly using the reported architectures in previous NAS papers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27]</ref>. All results are listed in <ref type="table">Table 2</ref>. NAONet gets test error rate of 14.75, better than the previous SOTA obtained with cutout <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr">(15.20)</ref>. The results show that our NAONet derived with CIFAR-10 is indeed transferable to more complicated task such as CIFAR-100.  <ref type="table">Table 2</ref>: Performances of different CNN models on CIFAR-100 dataset. 'NAONet' represents the best architecture discovered by NAO on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transferring the discovered architecture to ImageNet</head><p>Further, we transfer the discovered architecture to ImageNet. We use the best architecture discovered on CIFAR-10 and follow the same training setting as in related works <ref type="bibr" target="#b28">[29]</ref>. All results are listed in <ref type="table" target="#tab_3">Table 3</ref>. NAONet gets top-1 test error rate of 25.7, better than most previous works. The results again show that our NAONet derived from CIFAR-10 is transferable to much more complicated task such as ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results of Language Modeling on PTB</head><p>We leave the model training details of PTB to the Appendix. The encoder in NAO is an LSTM with embedding size 64 and hidden size 128. The hidden state of LSTM is further normalized to have unit length. The performance predictor is a two-layer MLP with each layer size as 200, 1. The decoder is a single layer LSTM with attention mechanism and the hidden size is 128. The trade-off parameters in Eqn. (1) is λ = 0.8. The encoder, performance predictor, and decoder are trained using Adam with a learning rate of 0.001. We perform the optimization process in Alg 1 for two iterations (i.e., L = 2). We train the sampled RNN models for shorter time (600 epochs) during the training phase of NAO, and afterwards train the best architecture discovered yet for 2000 epochs for the sake of   better result. We use 200 P100 GPU cards to complete all the process within 1.5 days. Similar to CIFAR-10, we furthermore explore the possibility of combining weight sharing with NAO and the resulting architecture is denoted as 'NAO-WS'.</p><p>We report all the results in <ref type="table" target="#tab_4">Table 4</ref>, separated into three blocks, respectively reporting the results of experts designed methods, architectures discovered via previous automatic neural architecture search methods, and our NAO. As can be observed, NAO successfully discovered an architecture that achieves quite competitive perplexity 56.0, surpassing previous NAS methods and is on par with the best performance from LSTM method with advanced manually designed techniques such as averaged weight drop <ref type="bibr" target="#b31">[32]</ref>. Furthermore, NAO combined with weight sharing (i.e., NAO-WS) again demonstrates efficiency to discover competitive architectures (e.g., achieving 56.6 perplexity via searching in 10 hours).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Transferring the discovered architecture to WikiText-2</head><p>We also apply the best discovered RNN architecture on PTB to another language modelling task based on a much larger dataset WikiText-2 (WT2 for short in the following). <ref type="table">Table 5</ref> shows the result that NAONet discovered by our method is on par with, or surpasses ENAS and DARTS.  <ref type="table">Table 5</ref>: Performance of different models and techniques on WT2 dataset. 'NAONet' represents the best architecture discovered by NAO on PTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We design a new automatic architecture design algorithm named as neural architecture optimization (NAO), which performs the optimization within continuous space (using gradient based method) rather than searching discrete decisions. The encoder, performance predictor and decoder together makes it more effective and efficient to discover better architectures and we achieve quite competitive results on both image classification task and language modeling task. For future work, first we would like to try other methods to further improve the performance of the discovered architecture, such as mixture of softmax <ref type="bibr" target="#b46">[47]</ref> for language modeling. Second, we would like to apply NAO to discovering better architectures for more applications such as Neural Machine Translation. Third, we plan to design better neural models from the view of teaching and learning to teach <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The general framework of NAO. Better viewed in color mode. The original architecture x is mapped to continuous representation e x via encoder network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Input:</head><label></label><figDesc>Initial candidate architectures set X to train NAO model. Initial architectures set to be evaluated denoted as X eval = X. Performances of architectures S = ∅. Number of seed architectures K. Step size η. Number of optimization iterations L. for l = 1, · · · , L do Train each architecture x ∈ X eval and evaluate it to obtain the dev set performances S eval = {sx}, ∀x ∈ X eval . Enlarge S: S = S S eval . Train encoder E, performance predictor f and decoder D by minimizing Eqn.(1), using X and S. Pick K architectures with top K performances among X, forming the set of seed architectures X seed . For x ∈ X seed , obtain a better representation e x from e x using Eqn. (2), based on encoder E and performance predictor f . Denote the set of enhanced representations as E = {e x }. Decode each x from e x using decoder, set X eval as the set of new architectures decoded out: X eval = {D(e x ), ∀e x ∈ E }. Enlarge X as X = X X eval . end for Output: The architecture within X with the best performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Left: the accuracy acc f of performance predictor f (red line) and performance dist D of decoder D (blue line) on the test set, w.r.t. the number of training data (i.e., evaluated architectures). Right: the mean dev set accuracy, together with its predicted value by f , of candidate architectures set X eval in each NAO optimization iteration l = 1, 2, 3. The architectures are trained for 25 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Best RNN Cell discovered by NAO for Penn Treebank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>we show the performances of performance predictor and decoder w.r.t. the number of training data, i.e., the number of evaluated architectures |X|. Specifically, among 600 randomly sampled architectures, we randomly choose 50 of them (denoted as X test ) and the corresponding performances (denoted as S test = {s x , ∀x ∈ X test }) as test set. Then we train NAO using the left 550 architectures (with their performances) as training set. We vary the number of training data as (100, 200, 300, 400, 500, 550) and observe the corresponding quality of performance predictor f , as well as the decoder D. To evaluate f , we compute the pairwise accuracy on X test and S test calculated via f , i.e., acc f = x 1 ∈X test ,x 2 ∈X test 1 f (E(x 1 ))≥f (E(x 2 )) 1 sx 1 ≥sx 2 x 1 ∈X test ,x 2 ∈X test 1 , where 1 is the 0-1 indicator function. To evaluate decoder D, we compute the Hamming distance (denoted as Dist) between the sequence representation</figDesc><table><row><cell>Model</cell><cell>B</cell><cell>N</cell><cell>F</cell><cell cols="3">#op Error(%) #params</cell><cell>M</cell><cell>GPU Days</cell></row><row><cell>DenseNet-BC [20]</cell><cell></cell><cell>100</cell><cell>40</cell><cell>/</cell><cell>3.46</cell><cell>25.6M</cell><cell>/</cell><cell>/</cell></row><row><cell>ResNeXt-29 [46]</cell><cell></cell><cell></cell><cell></cell><cell>/</cell><cell>3.58</cell><cell>68.1M</cell><cell>/</cell><cell>/</cell></row><row><cell>NASNet-A [51]</cell><cell>5</cell><cell>6</cell><cell>32</cell><cell>13</cell><cell>3.41</cell><cell>3.3M</cell><cell>20000</cell><cell>2000</cell></row><row><cell>NASNet-B [51]</cell><cell>5</cell><cell>4</cell><cell>N/A</cell><cell>13</cell><cell>3.73</cell><cell>2.6M</cell><cell>20000</cell><cell>2000</cell></row><row><cell>NASNet-C [51]</cell><cell>5</cell><cell>4</cell><cell>N/A</cell><cell>13</cell><cell>3.59</cell><cell>3.1M</cell><cell>20000</cell><cell>2000</cell></row><row><cell>Hier-EA [28]</cell><cell>5</cell><cell>2</cell><cell>64</cell><cell>6</cell><cell>3.75</cell><cell>15.7M</cell><cell>7000</cell><cell>300</cell></row><row><cell>AmoebaNet-A [39]</cell><cell>5</cell><cell>6</cell><cell>36</cell><cell>10</cell><cell>3.34</cell><cell>3.2M</cell><cell>20000</cell><cell>3150</cell></row><row><cell>AmoebaNet-B [39]</cell><cell>5</cell><cell>6</cell><cell>36</cell><cell>19</cell><cell>3.37</cell><cell>2.8M</cell><cell>27000</cell><cell>3150</cell></row><row><cell>AmoebaNet-B [39]</cell><cell>5</cell><cell>6</cell><cell>80</cell><cell>19</cell><cell>3.04</cell><cell>13.7M</cell><cell>27000</cell><cell>3150</cell></row><row><cell>AmoebaNet-B [39]</cell><cell>5</cell><cell>6</cell><cell>128</cell><cell>19</cell><cell>2.98</cell><cell>34.9M</cell><cell>27000</cell><cell>3150</cell></row><row><cell cols="2">AmoebaNet-B + Cutout [39] 5</cell><cell>6</cell><cell>128</cell><cell>19</cell><cell>2.13</cell><cell>34.9M</cell><cell>27000</cell><cell>3150</cell></row><row><cell>PNAS [27]</cell><cell>5</cell><cell>3</cell><cell>48</cell><cell>8</cell><cell>3.41</cell><cell>3.2M</cell><cell>1280</cell><cell>225</cell></row><row><cell>ENAS [37]</cell><cell>5</cell><cell>5</cell><cell>36</cell><cell>5</cell><cell>3.54</cell><cell>4.6M</cell><cell>/</cell><cell>0.45</cell></row><row><cell>Random-WS</cell><cell>5</cell><cell>5</cell><cell>36</cell><cell>5</cell><cell>3.92</cell><cell>3.9M</cell><cell>/</cell><cell>0.25</cell></row><row><cell>DARTS + Cutout [29]</cell><cell>5</cell><cell>6</cell><cell>36</cell><cell>7</cell><cell>2.83</cell><cell>4.6M</cell><cell>/</cell><cell>4</cell></row><row><cell>NAONet</cell><cell>5</cell><cell>6</cell><cell>36</cell><cell>11</cell><cell>3.18</cell><cell>10.6M</cell><cell>1000</cell><cell>200</cell></row><row><cell>NAONet</cell><cell>5</cell><cell>6</cell><cell>64</cell><cell>11</cell><cell>2.98</cell><cell>28.6M</cell><cell>1000</cell><cell>200</cell></row><row><cell>NAONet + Cutout</cell><cell>5</cell><cell>6</cell><cell>36</cell><cell>11</cell><cell>2.48</cell><cell>10.6M</cell><cell>1000</cell><cell>200</cell></row><row><cell>NAONet + Cutout</cell><cell>5</cell><cell>6</cell><cell>128</cell><cell>11</cell><cell>1.93</cell><cell>144.6M</cell><cell>1000</cell><cell>200</cell></row><row><cell>NAONet-WS</cell><cell>5</cell><cell>5</cell><cell>36</cell><cell>5</cell><cell>3.53</cell><cell>2.5M</cell><cell>/</cell><cell>0.3</cell></row><row><cell>NAONet-WS + Cutout</cell><cell>5</cell><cell>5</cell><cell>36</cell><cell>5</cell><cell>2.93</cell><cell>2.5M</cell><cell>/</cell><cell>0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performances of different CNN models on ImageNet dataset. 'NAONet' represents the best architecture discovered by NAO on CIFAR-10.</figDesc><table><row><cell>Models and Techniques</cell><cell cols="2">#params Test Perplexity</cell><cell>GPU Days</cell></row><row><cell>Vanilla LSTM [48]</cell><cell>66M</cell><cell>78.4</cell><cell>/</cell></row><row><cell>LSTM + Zoneout [24]</cell><cell>66M</cell><cell>77.4</cell><cell>/</cell></row><row><cell>Variational LSTM [14]</cell><cell>19M</cell><cell>73.4</cell><cell></cell></row><row><cell>Pointer Sentinel-LSTM [34]</cell><cell>51M</cell><cell>70.9</cell><cell>/</cell></row><row><cell>Variational LSTM + weight tying [21]</cell><cell>51M</cell><cell>68.5</cell><cell>/</cell></row><row><cell>Variational Recurrent Highway Network + weight tying [50]</cell><cell>23M</cell><cell>65.4</cell><cell>/</cell></row><row><cell>4-layer LSTM + skip connection + averaged weight drop + weight penalty + weight tying [32]</cell><cell>24M</cell><cell>58.3</cell><cell>/</cell></row><row><cell>LSTM + averaged weight drop + Mixture of Softmax + weight penalty + weight tying [47]</cell><cell>22M</cell><cell>56.0</cell><cell>/</cell></row><row><cell>NAS + weight tying [51]</cell><cell>54M</cell><cell>62.4</cell><cell>1e4 CPU days</cell></row><row><cell>ENAS + weight tying + weight penalty [37]</cell><cell>24M</cell><cell>58.6 5</cell><cell>0.5</cell></row><row><cell>Random-WS + weight tying + weight penalty</cell><cell>27M</cell><cell>58.81</cell><cell>0.4</cell></row><row><cell>DARTS+ weight tying + weight penalty [29]</cell><cell>23M</cell><cell>56.1</cell><cell>1</cell></row><row><cell>NAONet + weight tying + weight penalty</cell><cell>27M</cell><cell>56.0</cell><cell>300</cell></row><row><cell>NAONet-WS + weight tying + weight penalty</cell><cell>27M</cell><cell>56.6</cell><cell>0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of different models and techniques on PTB dataset. Similar to CIFAR-10 experiment, 'NAONet-WS' represents NAO accompanied with weight sharing,and 'Random-WS' is the corresponding random search baseline.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For ease of introduction, even though some notations have been used before (e.g., ht in defining RNN search space), they are slightly abused here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">If we have x = x, i.e., the new architecture is exactly the same with the previous architecture, we ignore it and keep increasing the step-size value by η 2 , until we found a different decoded architecture different with x.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We adopt the number reported via<ref type="bibr" target="#b28">[29]</ref> which is similar to our reproduction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Normal Cell(b) Reduction CellFigure 3: Basic NAONet building blocks. NAONet normal cell (left) and reduction cell (right).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>We thank Hieu Pham for the discussion on some details of ENAS implementation, and Hanxiao Liu for the code base of language modeling task in DARTS <ref type="bibr" target="#b28">[29]</ref>. We furthermore thank the anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMASH: One-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reinforcement learning for architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04873</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Path-level network transformation for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02639</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Peephole: Predicting network performance before training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03351</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cascade-correlation learning architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fahlman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lebiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="524" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to teach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shake-shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno>abs/1705.07485</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno>abs/1612.04426</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting compositionality to explore a large space of model structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI&apos;12</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI&apos;12<address><addrLine>Arlington, Virginia, United States</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="306" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04964</idno>
		<title level="m">Learning deep resnet blocks sequentially using boosting theory</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01462</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07191</idno>
		<title level="m">Neural architecture search with bayesian optimisation and optimal transport</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms with graph generation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Kitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems Journal</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="461" to="476" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoneout</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01305</idno>
		<title level="m">Regularizing rnns by randomly preserving hidden activations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>Eric P. Xing and Tony Jebara</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<title level="m">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The penn treebank: annotating predicate argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Schasberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="114" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Risto Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bala</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hormoz</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshak</forename><surname>Shahrzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duffy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00548</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Evolving deep neural networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to teach with dynamic loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><surname>Jian-Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6465" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4189" to="4198" />
		</imprint>
	</monogr>
	<note>Recurrent highway networks</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">of Model Training on Image Classification and Language Modelling CIFAR-10 contains 50k and 10k images for training and testing. We randomly choose 5000 images within training set as the dev set for measuring the performance of each candidate network in the optimization process of NAO. Standard data pre-processing and augmentation, such as whitening, randomly cropping 32 × 32 patches from unsampled images of size 40 × 40, and randomly flipping images horizontally are applied to original training set. The CNN models are trained using SGD with momentum set to 0.9, where the arrange of learning rate follows a single period cosine schedule with l max = 0.024 proposed in [30]. For the purpose of regularization, We apply stochastic drop-connect on each path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Learning transferable architectures for scalable image recognition. and an l 2 weight decay of 5 × 10 −4 . All the models are trained with batch size 128</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">We use the open-source code of ENAS [37] released by the authors and exactly follow their setups. Specifically, we apply variational dropout, l 2 regularization with weight decay of 5 × 10 −7 , and tying word embeddings and softmax weights [38]. We train the models using SGD with an initial learning rate of 10.0, decayed by a factor of 0.9991 after every epoch staring at epoch 15. To avoid gradient explosion, we clip the norm of gradient with the threshold value 0.25. For WikiText-2, we use embdedding size 700</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penn</forename><surname>Treebank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PTB) [31] is one of the most widely adopted benchmark dataset for language modeling task</title>
		<imprint/>
	</monogr>
	<note>weight decay of 5 × 10 −7 and variational dropout 0.15. Others unstated are the same as in PTB, such as weight tying</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">2 Search Space In searching convolutional cell architectures without weight sharing</title>
		<imprint/>
	</monogr>
	<note>following previous works of [39, 52], we adopt 11 possible ops as follow</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">In searching for recurrent cell architectures, we exactly follow the search space of ENAS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
	<note>where possible activation functions are</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Furthermore we plot the best architecture of recurrent cell discovered by our NAO algorithm in Fig</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
