<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Heriot-Watt University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel, conceptually simple and general framework for instance segmentation on 3D point clouds. Our method, called 3D-BoNet, follows the simple design philosophy of per-point multilayer perceptrons (MLPs). The framework directly regresses 3D bounding boxes for all instances in a point cloud, while simultaneously predicting a point-level mask for each instance. It consists of a backbone network followed by two parallel network branches for 1) bounding box regression and 2) point mask prediction. 3D-BoNet is single-stage, anchor-free and end-to-end trainable. Moreover, it is remarkably computationally efficient as, unlike existing approaches, it does not require any post-processing steps such as non-maximum suppression, feature sampling, clustering or voting. Extensive experiments show that our approach surpasses existing work on both ScanNet and S3DIS datasets while being approximately 10× more computationally efficient. Comprehensive ablation studies demonstrate the effectiveness of our design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Enabling machines to understand 3D scenes is a fundamental necessity for autonomous driving, augmented reality and robotics. Core problems on 3D geometric data such as point clouds include semantic segmentation, object detection and instance segmentation. Of these problems, instance segmentation has only started to be tackled in the literature. The primary obstacle is that point clouds are inherently unordered, unstructured and non-uniform. Widely used convolutional neural networks require the 3D point clouds to be voxelized, incurring high computational and memory costs.</p><p>The first neural algorithm to directly tackle 3D instance segmentation is SGPN <ref type="bibr" target="#b49">[50]</ref>, which learns to group per-point features through a similarity matrix. Similarly, ASIS <ref type="bibr" target="#b50">[51]</ref>, JSIS3D <ref type="bibr" target="#b33">[34]</ref>, MASC <ref type="bibr" target="#b29">[30]</ref>, 3D-BEVIS <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b27">[28]</ref> apply the same per-point feature grouping pipeline to segment 3D instances. Mo et al. formulate the instance segmentation as a per-point feature classification problem in PartNet <ref type="bibr" target="#b31">[32]</ref>. However, the learnt segments of these proposal-free methods do not have high objectness as they do not explicitly detect the object boundaries. In addition, they inevitably require a post-processing step such as mean-shift clustering <ref type="bibr" target="#b5">[6]</ref> to obtain the final instance labels, which is computationally heavy. Another pipeline is the proposal-based 3D-SIS <ref type="bibr" target="#b14">[15]</ref> and GSPN <ref type="bibr" target="#b57">[58]</ref>, which usually rely on two-stage training and the expensive non-maximum suppression to prune dense object proposals.</p><p>In this paper, we present an elegant, efficient and novel framework for 3D instance segmentation, where objects are loosely but uniquely detected through a single-forward stage using efficient MLPs, and then each instance is precisely segmented through a simple point-level binary classifier. To this end, we introduce a new bounding box prediction module together with a series of carefully designed loss functions to directly learn object boundaries. Our framework is significantly different from the existing proposal-based and proposal-free approaches, since we are able to efficiently segment all instances with high objectness, but without relying on expensive and dense object proposals. Our code and data are available at https://github.com/Yang7879/3D-BoNet. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our framework, called 3D-BoNet, is a single-stage, anchor-free and end-to-end trainable neural architecture. It first uses an existing backbone network to extract a local feature vector for each point and a global feature vector for the whole input point cloud. The backbone is followed by two branches: 1) instance-level bounding box prediction, and 2) point-level mask prediction for instance segmentation. The bounding box prediction branch is the core of our framework. This branch aims to predict a unique, unoriented and rectangular bounding box for each instance in a single forward stage, without relying on predefined spatial anchors or a region proposal network <ref type="bibr" target="#b38">[39]</ref>. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we believe that roughly drawing a 3D bounding box for an instance is relatively achievable, because the input point clouds explicitly include 3D geometry information, while it is extremely beneficial before tackling point-level instance segmentation since reasonable bounding boxes can guarantee high objectness for learnt segments. However, to learn instance boxes involves critical issues: 1) the number of total instances is variable, i.e., from 1 to many, 2) there is no fixed order for all instances. These issues pose great challenges for correctly optimizing the network, because there is no information to directly link predicted boxes with ground truth labels to supervise the network. However, we show how to elegantly solve these issues. This box prediction branch simply takes the global feature vector as input and directly outputs a large and fixed number of bounding boxes together with confidence scores. These scores are used to indicate whether the box contains a valid instance or not. To supervise the network, we design a novel bounding box association layer followed by a multi-criteria loss function. Given a set of ground-truth instances, we need to determine which of the predicted boxes best fit them. We formulate this association process as an optimal assignment problem with an existing solver. After the boxes have been optimally associated, our multi-criteria loss function not only minimizes the Euclidean distance of paired boxes, but also maximizes the coverage of valid points inside of predicted boxes.</p><p>The predicted boxes together with point and global features are then fed into the subsequent point mask prediction branch, in order to predict a point-level binary mask for each instance. The purpose of this branch is to classify whether each point inside of a bounding box belongs to the valid instance or the background. Assuming the estimated instance box is reasonably good, it is very likely to obtain an accurate point mask, because this branch is simply to reject points that are not part of the detected instance. A random guess may bring about 50% corrections.</p><p>Overall, our framework distinguishes from all existing 3D instance segmentation approaches in three folds. 1) Compared with the proposal-free pipeline, our method segments instance with high objectness by explicitly learning 3D object boundaries. 2) Compared with the widely-used proposalbased approaches, our framework does not require expensive and dense proposals. 3) Our framework is remarkably efficient, since the instance-level masks are learnt in a single-forward pass without requiring any post-processing steps. Our key contributions are:</p><p>• We propose a new framework for instance segmentation on 3D point clouds. The framework is single-stage, anchor-free and end-to-end trainable, without requiring any post-processing steps.</p><p>• We design a novel bounding box association layer followed by a multi-criteria loss function to supervise the box prediction branch.</p><p>• We demonstrate significant improvement over baselines and provide intuition behind our design choices through extensive ablation studies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Point Cloud</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bounding Box Scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bounding Boxes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bounding Box Association Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Association Index</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-criteria Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">3D-BoNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, our framework consists of two branches on top of the backbone network. Given an input point cloud P with N points in total, i.e., P ∈ R N ×k0 , where k 0 is the number of channels such as the location {x, y, z} and color {r, g, b} of each point, the backbone network extracts point local features, denoted as F l ∈ R N ×k , and aggregates a global point cloud feature vector, denoted as F g ∈ R 1×k , where k is the length of feature vectors.</p><p>The bounding box prediction branch simply takes the global feature vector F g as input, and directly regresses a predefined and fixed set of bounding boxes, denoted as B, and the corresponding box scores, denoted as B s . We use ground truth bounding box information to supervise this branch. During training, the predicted bounding boxes B and the ground truth boxes are fed into a box association layer. This layer aims to automatically associate a unique and most similar predicted bounding box to each ground truth box. The output of the association layer is a list of association index A. The indices reorganize the predicted boxes, such that each ground truth box is paired with a unique predicted box for subsequent loss calculation. The predicted bounding box scores are also reordered accordingly before calculating loss. The reordered predicted bounding boxes are then fed into the multi-criteria loss function. Basically, this loss function aims to not only minimize the Euclidean distance between each ground truth box and the associated predicted box, but also maximize the coverage of valid points inside of each predicted box. Note that, both the bounding box association layer and multi-criteria loss function are only designed for network training. They are discarded during testing. Eventually, this branch is able to predict a correct bounding box together with a box score for each instance directly.</p><p>In order to predict point-level binary mask for each instance, every predicted box together with previous local and global features, i.e., F l and F g , are further fed into the point mask prediction branch. This network branch is shared by all instances of different categories, and therefore extremely light and compact. Such class-agnostic approach inherently allows general segmentation across unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bounding Box Prediction</head><p>Bounding Box Encoding: In existing object detection networks, a bounding box is usually represented by the center location and the length of three dimensions <ref type="bibr" target="#b2">[3]</ref>, or the corresponding residuals <ref type="bibr" target="#b59">[60]</ref> together with orientations. Instead, we parameterize the rectangular bounding box by only two min-max vertices for simplicity:</p><p>{[x min y min z min ], [x max y max z max ]} Neural Layers: As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the global feature vector F g is fed through two fully connected layers with Leaky ReLU as the non-linear activation function. Then it is followed by another two parallel fully connected layers. One layer outputs a 6H dimensional vector, which is then reshaped as an H × 2 × 3 tensor. H is a predefined and fixed number of bounding boxes that the whole network are expected to predict in maximum. The other layer outputs an H dimensional vector followed by sigmoid function to represent the bounding box scores. The higher the score, the more likely that the predicted box contains an instance, thus the box being more valid.</p><p>Bounding Box Association Layer: Given the previously predicted H bounding boxes, i.e., B ∈ R H×2×3 , it is not straightforward to take use of the ground truth boxes, denoted asB ∈ R T ×2×3 , to supervise the network, because there are no predefined anchors to trace each predicted box back to a corresponding ground truth box in our framework. Besides, for each input point cloud P , the number of ground truth boxes T varies and it is usually different from the predefined number H, although we can safely assume the predefined number H ≥ T for all input point clouds. In addition, there is no box order for either predicted or ground truth boxes.</p><p>Optimal Association Formulation: To associate a unique predicted bounding box from B for each ground truth box ofB, we formulate this association process as an optimal assignment problem. Formally, let A be a boolean association matrix where A i,j =1 iff the i th predicted box is assigned to the j th ground truth box. A is also called association index in this paper. Let C be the association cost matrix where C i,j represents the cost that the i th predicted box is assigned to the j th ground truth box. Basically, the cost C i,j represents the similarity between two boxes; the less the cost, the more similar the two boxes. Therefore, the bounding box association problem is to find the optimal assignment matrix A with the minimal cost overall:</p><formula xml:id="formula_0">A = arg min A H i=1 T j=1 Ci,jAi,j subject to H i=1 Ai,j = 1, T j=1 Ai,j ≤ 1, j ∈ {1..T }, i ∈ {1..H} (1)</formula><p>To solve the above optimal association problem, the existing Hungarian algorithm [20; 21] is applied. Association Matrix Calculation: To evaluate the similarity between the i th predicted box and the j th ground truth box, a simple and intuitive criterion is the Euclidean distance between two pairs of min-max vertices. However, it is not optimal. Basically, we want the predicted box to include as many valid points as possible. As illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>, the input point cloud is usually sparse and distributed non-uniformly in 3D space. Regarding the same ground truth box #0 (blue), the candidate box #2 (red) is believed to be much better than the candidate #1 (black), because the box #2 has more valid points overlapped with #0. Therefore, the coverage of valid points should be included to calculate the cost matrix C. In this paper, we consider the following three criteria:</p><formula xml:id="formula_1">true box #0 candidate box #1 candidate box #2</formula><p>Algorithm 1 An algorithm to calculate point-in-pred-boxprobability. H is the number of predicted bounding boxes B, N is the number of points in point cloud P , θ1 and θ2 are hyperparameters for numerical stability. We use θ1 = 100, θ2 = 20 in all our implementation.</p><formula xml:id="formula_2">for i ← 1 to H do • the i th box min-vertex B i min = [x i min y i min z i min ]. • the i th box max-vertex B i max = [x i max y i max z i max ]. for n ← 1 to N do • the n th point location P n = [x n y n z n ]. • step 1: ∆xyz ← (B i min − P n )(P n − B i max ). • step 2: ∆xyz ← max [min(θ1∆xyz, θ2), −θ2]. • step 3: probability p xyz = 1 1+exp(−∆xyz ) . • step 4: point probability q n i = min(p xyz ). • obtain the soft-binary vector q i = [q 1 i · · · q N i ].</formula><p>The above two loops are only for illustration. They are easily replaced by standard and efficient matrix operations.</p><p>(1) Euclidean Distance between Vertices. Formally, the cost between the i th predicted box B i and the j th ground truth boxB j is calculated as follows:</p><formula xml:id="formula_3">C ed i,j = 1 6 (B i −B j ) 2<label>(2)</label></formula><p>(2) Soft Intersection-over-Union on Points. Given the input point cloud P and the j th ground truth instance boxB j , it is able to directly obtain a hard-binary vectorq j ∈ R N to represent whether each point is inside of the box or not, where '1' indicates the point being inside and '0' outside. However, for a specific i th predicted box of the same input point cloud P , to directly obtain a similar hardbinary vector would result in the framework being non-differentiable, due to the discretization operation. Therefore, we introduce a differentiable yet simple algorithm 1 to obtain a similar but soft-binary vector q i , called point-in-pred-box-probability, where all values are in the range (0, 1). The deeper the corresponding point is inside of the box, the higher the value. The farther away the point is outside, the smaller the value. Formally, the Soft Intersection-over-Union (sIoU) cost between the i th predicted box and the j th ground truth box is defined as follows:</p><formula xml:id="formula_4">C sIoU i,j = − N n=1 (q n i * q n j ) N n=1 q n i + N n=1q n j − N n=1 (q n i * q n j )<label>(3)</label></formula><p>where q n i andq n j are the n th values of q i andq j . (3) Cross-Entropy Score. In addition, we also consider the cross-entropy score between q i andq j . Being different from sIoU cost which prefers tighter boxes, this score represents how confident a predicted bounding box is able to include valid points as many as possible. It prefers larger and more inclusive boxes, and is formally defined as:</p><formula xml:id="formula_5">C ces i,j = − 1 N N n=1 q n j log q n i + (1 −q n j ) log(1 − q n i )<label>(4)</label></formula><p>Overall, the criterion (1) guarantees the geometric boundaries for learnt boxes and criteria <ref type="formula" target="#formula_3">(2)(3)</ref> maximize the coverage of valid points and overcome the non-uniformity as illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>The final association cost between the i th predicted box and the j th ground truth box is defined as:</p><formula xml:id="formula_6">C i,j = C ed i,j + C sIoU i,j + C ces i,j<label>(5)</label></formula><p>Loss Functions After the bounding box association layer, both the predicted boxes B and scores B s are reordered using the association index A, such that the first predicted T boxes and scores are well paired with the T ground truth boxes.</p><p>Multi-criteria Loss for Box Prediction: The previous association layer finds the most similar predicted box for each ground truth box according to the minimal cost including: 1) vertex Euclidean distance, 2) sIoU cost on points, and 3) cross-entropy score. Therefore, the loss function for bounding box prediction is naturally designed to consistently minimize those cost. It is formally defined as follows:</p><formula xml:id="formula_7">bbox = 1 T T t=1 (C ed t,t + C sIoU t,t + C ces t,t )<label>(6)</label></formula><p>where C ed t,t , C sIoU t,t and C ces t,t are the cost of t th paired boxes. Note that, we only minimize the cost of T paired boxes; the remaining H − T predicted boxes are ignored because there is no corresponding ground truth for them. Therefore, this box prediction sub-branch is agnostic to the predefined value of H. Here raises an issue. Since the H − T negative predictions are not penalized, it might be possible that the network predicts multiple similar boxes for a single instance. Fortunately, the loss function for the parallel box score prediction is able to alleviate this problem.</p><p>Loss for Box Score Prediction: The predicted box scores aim to indicate the validity of the corresponding predicted boxes. After being reordered by the association index A, the ground truth scores for the first T scores are all '1', and '0' for the remaining invalid H − T scores. We use cross-entropy loss for this binary classification task:</p><formula xml:id="formula_8">bbs = − 1 H T t=1 log B t s + H t=T +1 log(1 − B t s )<label>(7)</label></formula><p>where B t s is the t th predicted score after being associated. Basically, this loss function rewards the correctly predicted bounding boxes, while implicitly penalizing the cases where multiple similar boxes are regressed for a single instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Point Mask Prediction</head><p>Given the predicted bounding boxes B, the learnt point features F l and global features F g , the point mask prediction branch processes each bounding box individually with shared neural layers. Neural Layers: As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, both the point and global features are compressed to be 256 dimensional vectors through fully connected layers, before being concatenated and further compressed to be 128 dimensional mixed point features F l . For the i th predicted bounding box B i , the estimated vertices and score are fused with features F l through concatenation, producing box-aware features F l . These features are then fed through shared layers, predicting a point-level binary mask, denoted as M i . We use sigmoid as the last activation function. This simple box fusing approach is extremely computationally efficient, compared with the commonly used RoIAlign in prior art <ref type="bibr">[58; 15; 13]</ref> which involves the expensive point feature sampling and alignment.</p><p>Loss Function: The predicted instance masks M are similarly associated with the ground truth masks according to the previous association index A. Due to the imbalance of instance and background point numbers, we use focal loss <ref type="bibr" target="#b28">[29]</ref> with default hyper-parameters instead of the standard cross-entropy loss to optimize this branch. Only the valid T paired masks are used for the loss pmask .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">End-to-End Implementation</head><p>While our framework is not restricted to any point cloud network, we adopt PointNet++ <ref type="bibr" target="#b37">[38]</ref> as the backbone to learn the local and global features. Parallelly, another separate branch is implemented to learn per-point semantics with the standard sof tmax cross-entropy loss function sem . The architecture of the backbone and semantic branch is the same as used in <ref type="bibr" target="#b49">[50]</ref>. Given an input point cloud P , the above three branches are linked and end-to-end trained using a single combined multi-task loss: all = sem + bbox + bbs + pmask (8) We use Adam solver <ref type="bibr" target="#b17">[18]</ref> with its default hyper-parameters for optimization. Initial learning rate is set to 5e −4 and then divided by 2 every 20 epochs. The whole network is trained on a Titan X GPU from scratch. We use the same settings for all experiments, which guarantees the reproducibility of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation on ScanNet Benchmark</head><p>We first evaluate our approach on ScanNet(v2) 3D semantic instance segmentation benchmark <ref type="bibr" target="#b6">[7]</ref>. Similar to SGPN <ref type="bibr" target="#b49">[50]</ref>, we divide the raw input point clouds into 1m × 1m blocks for training, while using all points for testing followed by the BlockMerging algorithm <ref type="bibr" target="#b49">[50]</ref> to assemble blocks into complete 3D scenes. In our experiment, we observe that the performance of the vanilla PointNet++ based semantic prediction sub-branch is limited and unable to provide satisfactory semantics. Thanks to the flexibility of our framework, we therefore easily train a parallel SCN network <ref type="bibr" target="#b10">[11]</ref> to estimate more accurate per-point semantic labels for the predicted instances of our 3D-BoNet. The average precision (AP) with an IoU threshold 0.5 is used as the evaluation metric.</p><p>We compare with the leading approaches on 18 object categories in <ref type="table" target="#tab_0">Table 1</ref>. Particularly, the SGPN <ref type="bibr" target="#b49">[50]</ref>, 3D-BEVIS <ref type="bibr" target="#b7">[8]</ref>, MASC <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b27">[28]</ref> are point feature clustering based approaches; the R-PointNet <ref type="bibr" target="#b57">[58]</ref> learns to generate dense object proposals followed by point-level segmentation; 3D-SIS <ref type="bibr" target="#b14">[15]</ref> is a proposal-based approach using both point clouds and color images as input. PanopticFusion <ref type="bibr" target="#b32">[33]</ref> learns to segment instances on multiple 2D images by Mask-RCNN <ref type="bibr" target="#b12">[13]</ref> and then uses the SLAM system to reproject back to 3D space. Our approach surpasses them all using point clouds only. Remarkably, our framework performs relatively satisfactory on all categories without preferring specific classes, demonstrating the superiority of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Point Cloud</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PartNet</head><p>Ground Truth ASIS 3D-BoNet (Ours) <ref type="figure">Figure 7</ref>: This shows a lecture room with hundreds of objects (e.g., chairs, tables), highlighting the challenge of instance segmentation. Different color indicates different instance. The same instance may not have the same color. Our framework predicts more precise instance labels than others. We further evaluate the semantic instance segmentation of our framework on S3DIS <ref type="bibr" target="#b0">[1]</ref>, which consists of 3D complete scans from 271 rooms belonging to 6 large areas. Our data preprocessing and experimental settings strictly follow PointNet <ref type="bibr" target="#b36">[37]</ref>, SGPN <ref type="bibr" target="#b49">[50]</ref>, ASIS <ref type="bibr" target="#b50">[51]</ref>, and JSIS3D <ref type="bibr" target="#b33">[34]</ref>. In our experiments, H is set as 24 and we follow the 6-fold evaluation <ref type="bibr">[1; 51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation on S3DIS Dataset</head><p>We compare with ASIS <ref type="bibr" target="#b50">[51]</ref>, the state of art on S3DIS, and the PartNet baseline <ref type="bibr" target="#b31">[32]</ref>. For fair comparison, we carefully train the PartNet baseline with the same PointNet++ backbone and other settings as used in our framework. For evaluation, the classical metrics mean precision (mPrec) and mean recall (mRec) with IoU threshold 0.5 are reported. Note that, we use the same BlockMerging algorithm <ref type="bibr" target="#b49">[50]</ref> to merge the instances from different blocks for both our approach and the PartNet baseline. The final scores are averaged across the total 13 categories. <ref type="table" target="#tab_1">Table 2</ref> presents the mPrec/mRec scores and <ref type="figure">Figure 7</ref> shows qualitative results. Our method surpasses PartNet baseline <ref type="bibr" target="#b31">[32]</ref> by large margins, and also outperforms ASIS <ref type="bibr" target="#b50">[51]</ref>, but not significantly, mainly because our semantic prediction branch (vanilla PointNet++ based) is inferior to ASIS which tightly fuses semantic and instance features for mutual optimization. We leave the feature fusion as our future exploration. To evaluate the effectiveness of each component of our framework, we conduct 6 groups of ablation experiments on the largest Area 5 of S3DIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>(1) Remove Box Score Prediction Sub-branch. Basically, the box score serves as an indicator and regularizer for valid bounding box prediction. After removing it, we train the network with: ab1 = sem + bbox + pmask Initially, the multi-criteria loss function is a simple unweighted combination of the Euclidean distance, the soft IoU cost, and the cross-entropy score. However, this may not be optimal, because the density of input point clouds is usually inconsistent and tends to prefer different criterion. We conduct the below 3 groups of experiments on ablated bounding box loss function.</p><p>(2)-(4) Use Single Criterion. Only one criterion is used for the box association and loss bbox .</p><formula xml:id="formula_9">ab2 = sem + 1 T T t=1 C ed t,t + bbs + pmask · · · ab4 = sem + 1 T T t=1</formula><p>C ces t,t + bbs + pmask (5) Do Not Supervise Box Prediction. The predicted boxes are still associated according to the three criteria, but we remove the box supervision signal. The framework is trained with: ab5 = sem + bbs + pmask (6) Remove Focal Loss for Point Mask Prediction. In the point mask prediction branch, the focal loss is replaced by the standard cross-entropy loss for comparison.</p><p>Analysis. <ref type="table" target="#tab_2">Table 3</ref> shows the scores for ablation experiments. (1) The box score sub-branch indeed benefits the overall instance segmentation performance, as it tends to penalize duplicated box predictions. (2) Compared with Euclidean distance and cross-entropy score, the sIoU cost tends to be better for box association and supervision, thanks to our differentiable Algorithm 1. As the three individual criteria prefer different types of point structures, a simple combination of three criteria may not always be optimal on a specific dataset. (3) Without the supervision for box prediction, the performance drops significantly, primarily because the network is unable to infer satisfactory instance 3D boundaries and the quality of predicted point masks deteriorates accordingly. (4) Compared with focal loss, the standard cross entropy loss is less effective for point mask prediction due to the imbalance of instance and background point numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Computation Analysis</head><p>(1) For point feature clustering based approaches including SGPN <ref type="bibr" target="#b49">[50]</ref>, ASIS <ref type="bibr" target="#b50">[51]</ref>, JSIS3D <ref type="bibr" target="#b33">[34]</ref>, 3D-BEVIS <ref type="bibr" target="#b7">[8]</ref>, MASC <ref type="bibr" target="#b29">[30]</ref>, and <ref type="bibr" target="#b27">[28]</ref>, the computation complexity of the post clustering algorithm such as Mean Shift <ref type="bibr" target="#b5">[6]</ref> tends towards O(T N 2 ), where T is the number of instances and N is the number of input points. (2) For dense proposal-based methods including GSPN <ref type="bibr" target="#b57">[58]</ref>, 3D-SIS <ref type="bibr" target="#b14">[15]</ref> and PanopticFusion <ref type="bibr" target="#b32">[33]</ref>, region proposal network and non-maximum suppression are usually required to generate and prune the dense proposals, which is computationally expensive <ref type="bibr" target="#b32">[33]</ref>. (3) Both PartNet baseline <ref type="bibr" target="#b31">[32]</ref> and our 3D-BoNet have similar efficient computation complexity O(N ). Empirically, our 3D-BoNet takes around 20 ms GPU time to process 4k points, while most approaches in (1)(2) need more than 200ms GPU/CPU time to process the same number of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>To extract features from 3D point clouds, traditional approaches usually craft features manually <ref type="bibr">[5; 42]</ref>. Recent learning based approaches mainly include voxel-based <ref type="bibr">[42; 46; 41; 23; 40; 11; 4]</ref> and point-based schemes <ref type="bibr">[37; 19; 14; 16; 45]</ref>.</p><p>Semantic Segmentation PointNet <ref type="bibr" target="#b36">[37]</ref> shows leading results on classification and semantic segmentation, but it does not capture context features. To address it, a number of approaches <ref type="bibr">[38; 57; 43; 31; 55; 49; 26; 17]</ref> have been proposed recently. Another pipeline is convolutional kernel based approaches <ref type="bibr">[55; 27; 47]</ref>. Basically, most of these approaches can be used as our backbone network, and parallelly trained with our 3D-BoNet to learn per-point semantics.</p><p>Object Detection The common way to detect objects in 3D point clouds is to project points onto 2D images to regress bounding boxes <ref type="bibr">[25; 48; 3; 56; 59; 53]</ref>. Detection performance is further improved by fusing RGB images in <ref type="bibr">[3; 54; 36; 52]</ref>. Point clouds can be also divided into voxels for object detection <ref type="bibr">[9; 24; 60]</ref>. However, most of these approaches rely on predefined anchors and the two-stage region proposal network <ref type="bibr" target="#b38">[39]</ref>. It is inefficient to extend them on 3D point clouds. Without relying on anchors, the recent PointRCNN <ref type="bibr" target="#b43">[44]</ref> learns to detect via foreground point segmentation, and the VoteNet <ref type="bibr" target="#b34">[35]</ref> detects objects via point feature grouping, sampling and voting. By contrast, our box prediction branch is completely different from them all. Our framework directly regresses 3D object bounding boxes from the compact global features through a single forward pass.</p><p>Instance Segmentation SGPN <ref type="bibr" target="#b49">[50]</ref> is the first neural algorithm to segment instances on 3D point clouds by grouping the point-level embeddings. ASIS <ref type="bibr" target="#b50">[51]</ref>, JSIS3D <ref type="bibr" target="#b33">[34]</ref>, MASC <ref type="bibr" target="#b29">[30]</ref>, 3D-BEVIS <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b27">[28]</ref> use the same strategy to group point-level features for instance segmentation. Mo et al. introduce a segmentation algorithm in PartNet <ref type="bibr" target="#b31">[32]</ref> by classifying point features. However, the learnt segments of these proposal-free methods do not have high objectness as it does not explicitly detect object boundaries. By drawing on the successful 2D RPN <ref type="bibr" target="#b38">[39]</ref> and RoI <ref type="bibr" target="#b12">[13]</ref>, GSPN <ref type="bibr" target="#b57">[58]</ref> and 3D-SIS <ref type="bibr" target="#b14">[15]</ref> are proposal-based methods for 3D instance segmentation. However, they usually rely on two-stage training and a post-processing step for dense proposal pruning. By contrast, our framework directly predicts a point-level mask for each instance within an explicitly detected object boundary, without requiring any post-processing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our framework is simple, effective and efficient for instance segmentation on 3D point clouds. However, it also has some limitations which lead to the future work. (1) Instead of using unweighted combination of three criteria, it is better to design a module to automatically learn the weights, so to adapt to different types of input point clouds. (2) Instead of training a separate branch for semantic prediction, more advanced feature fusion modules can be introduced to mutually improve both semantic and instance segmentation. (3) Our framework follows the MLP design and is therefore agnostic to the number and order of input points. It is desirable to directly train and test on large-scale input point clouds instead of the divided small blocks, by drawing on the recent work [10] <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiments on ScanNet Benchmark</head><p>ScanNet(v2) consists of 1613 complete 3D scenes acquired from real-world indoor spaces. The official split has 1201 training scenes, 312 validation scenes and 100 hidden testing scenes. The original large point clouds are divided into 1m × 1m blocks with 0.5m overlapped between neighbouring blocks. This data proprocessing step is the same as being used by PointNet <ref type="bibr" target="#b36">[37]</ref> for the S3DIS dataset. We sample 4096 points from each block for training, but use all points of a block for testing. Each point is represented by a 9D vector (normalized xyz in the block, rgb, normalized xyz in the room). H is set as 20 in our experiments. We train our 3D-BoNet to predict object bounding boxes and point-level masks, and parallelly train an officially released ResNet-based SCN network <ref type="bibr" target="#b10">[11]</ref> to predict point-level semantic labels. <ref type="figure" target="#fig_7">Figure 8</ref> shows qualitative results of our 3D-BoNet for instance segmentation on ScanNet validation split. It can be seen that our approach tends to predict complete object instances, instead of inferring tiny and but invalid fragments. This demonstrates that our framework indeed guarantees high objectness for segmented instances. The red circles showcase the failure cases, where the very similar instances are unable to be well segmented by our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Point Clouds Predicted Instance Labels Ground Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiments on S3DIS Dataset</head><p>The original large point clouds are divided into 1m × 1m blocks with 0.5m overlapped between neighbouring blocks. It is the same as being used in PointNet <ref type="bibr" target="#b36">[37]</ref>. We sample 4096 points from each block for training, but use all points of a block for testing. Each point is represented by a 9D vector (normalized xyz in the block, rgb, normalized xyz in the room). H is set as 24 in our experiments. We train our 3D-BoNet to predict object bounding boxes and point-level masks, and parallelly train a vanilla PointNet++ based sub-branch to predict point-level semantic labels. Particularly, all the semantic, bounding box and point mask sub-branches share the same PointNet++ backbone to extract point features, and are end-to-end trained from scratch. <ref type="figure" target="#fig_8">Figure 9</ref> shows the training curves of our proposed loss functions on Areas (1,2,3,4,6) of S3DIS dataset. It demonstrates that all the proposed loss functions are able to converge consistently, thus jointly optimizing the semantic segmentation, bounding box prediction, and point mask prediction branches in an end-to-end fashion. <ref type="figure" target="#fig_0">Figure 10</ref> presents the qualitative results of predicted bounding boxes and scores. It can be seen that the predicted boxes are not necessarily tight and precise. Instead, they tend to be inclusive but with high objectness. Fundamentally, this highlights the simple but effective concept of our bounding box prediction network. Given these bounded points, it is extremely easy to segment the instance inside. <ref type="figure" target="#fig_0">Figure 11</ref> visualizes the predicted instance masks, where the black points have ∼ 0 probability and the brighter points have ∼ 1 probability to be an instance within each predicted mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation Loss</head><p>Bounding Box Score Loss Bounding Box Loss (Euclidean Distance)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bounding Box Loss (soft IoU cost)</head><p>Bounding Box Loss (cross entropy score)    <ref type="table" target="#tab_3">Table 4</ref> compares the time consumption of four existing approaches using their released codes on the validation split (312 scenes) of ScanNet(v2) dataset. SGPN <ref type="bibr" target="#b49">[50]</ref>, ASIS <ref type="bibr" target="#b50">[51]</ref>, GSPN <ref type="bibr" target="#b57">[58]</ref> and our 3D-BoNet are implemented by Tensorflow 1.4, 3D-SIS <ref type="bibr" target="#b14">[15]</ref> by Pytorch 0.4. All approaches are running on a single Titan X GPU and the pre/post-processing steps on an i7 CPU core with a single thread. Note that 3D-SIS automatically uses CPU for computing when some large scenes are unable to be processed by the single GPU. Overall, our approach is much more computationally efficient than existing methods, even achieving up to 20× faster than ASIS <ref type="bibr" target="#b50">[51]</ref>. Given the predicted bounding boxes, B, and ground-truth boxes,B, we compute the assignment cost matrix, C. This matrix is converted to a permutation matrix, A, using the Hungarian algorithm. Here we focus on the euclidean distance component of the loss, C ed . The derivative of our loss component w.r.t the network parameters, θ, in matrix form is:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Mask Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiments for Computation Efficiency</head><formula xml:id="formula_10">∂C ed ∂θ = −2(AB −B) A + ∂A ∂C ∂C ∂B B T ∂B ∂θ<label>(9)</label></formula><p>The components are easily computable except for ∂A ∂C which is the gradient of the permutation w.r.t the assignment cost matrix which is zero nearly everywhere. In our implementation, we found that the network is able to converge when setting this term to zero.</p><p>However, convergence could be sped up using the straight-through-estimator <ref type="bibr" target="#b1">[2]</ref> , which assumes that the gradient of the rounding is identity (or a small constant), ∂A ∂C = 1. This would speed up convergence as it allows both the error in the bounding box alignment (1st term of Eq. 9) to be backpropagated and the assignment to be reinforced (2nd term of Eq. 9). This approach has been shown to work well in practice for many problems including for differentiating through permutations for solving combinatorial optimization problems and for training binary neural networks . More complex approaches could also be used in our framework for computing the gradient of the assignment such as <ref type="bibr" target="#b11">[12]</ref> which uses a Plackett-Luce distribution over permutations and a reparameterized gradient estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Generalization to Unseen Scenes and Categories</head><p>Our framework learns the object bounding boxes and point masks from raw point clouds without coupling with semantic information, which inherently allows the generalization across new categories and scenes. We conduct extra experiments to qualitatively demonstrate the generality of our framework. In particular, we use the well-trained model from S3DIS dataset (Areas 1/2/3/4/6) to directly test on the validation split of ScanNet(v2) dataset. Since ScanNet dataset consists of much more object categories than S3DIS dataset, there are a number of categories (e.g., toilet, desk, sink, bathtub) that the trained model has never seen before.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 12</ref>, our model is still able to predict high-quality instance labels even though the scenes and some object categories have not been seen before. This shows that our model does not simply fit the training dataset. Instead, it tends to learn the underlying geometric features which are able to be generalized across new objects and scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Point Clouds Predicted Instance Labels Ground Truth</head><p>Cross cat <ref type="figure" target="#fig_0">Figure 12</ref>: Qualitative results of instance segmentation on ScanNet dataset. Although the model is trained on S3DIS dataset and then directly tested on ScanNet validation split, it is still able to predict high-quality instance labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>33rdFigure 1 :</head><label>1</label><figDesc>Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1906.01140v2 [cs.CV] 5 Sep 2019 The 3D-BoNet framework for instance segmentation on 3D point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Rough instance boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The general workflow of 3D-BoNet framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>AFigure 4 :</head><label>4</label><figDesc>This module is only used for training.It is discarded in testing. The architecture of bounding box regression branch. The predicted H boxes are optimally associated with T ground truth boxes before calculating the multi-criteria loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>A sparse input point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The architecture of point mask prediction branch. The point features are fused with each bounding box and score, after which a point-level binary mask is predicted for each instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results of our approach for instance segmentation on ScanNet(v2) validation split. Different color indicates different instance. The same instance may not be indicated by the same color. Black points are uninterested and belong to none of the 18 object categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Training losses on S3DIS Areas<ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results of predicted bounding boxes and scores on S3DIS Area 2. The point clouds inside of the blue boxes are fed into our framework which then estimates the red boxes to roughly detect instances. The tight blue boxes are the ground truth. Input PC Pred Mask #1 Pred Mask #2 Pred Mask #3 Pred Mask #4 GT Masks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Qualitative results of predicted instance masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Instance segmentation results on ScanNet(v2) benchmark (hidden test set). The metric is AP(%) with IoU threshold 0.5. Accessed on 2 June 2019. mean bathtub bed bookshelf cabinet chair counter curtain desk door other picture refrig showerCur sink sofa table toilet window</figDesc><table><row><cell>MaskRCNN [13]</cell><cell>5.8</cell><cell>33.3 0.2</cell><cell>0.0</cell><cell>5.3</cell><cell>0.2</cell><cell>0.2</cell><cell>2.1</cell><cell cols="3">0.0 4.5 2.4 23.8 6.5</cell><cell>0.0</cell><cell>1.4 10.7 2.0 11.0</cell><cell>0.6</cell></row><row><cell>SGPN [50]</cell><cell cols="2">14.3 20.8 39.0</cell><cell>16.9</cell><cell cols="3">6.5 27.5 2.9</cell><cell>6.9</cell><cell>0.0 8.7 4.3</cell><cell>1.4</cell><cell>2.7</cell><cell>0.0</cell><cell>11.2 35.1 16.8 43.8 13.8</cell></row><row><cell>3D-BEVIS [8]</cell><cell cols="2">24.8 66.7 56.6</cell><cell>7.6</cell><cell cols="3">3.5 39.4 2.7</cell><cell>3.5</cell><cell>9.8 9.9 3.0</cell><cell>2.5</cell><cell>9.8</cell><cell>37.5</cell><cell>12.6 60.4 18.1 85.4 17.1</cell></row><row><cell>R-PointNet [58]</cell><cell cols="2">30.6 50.0 40.5</cell><cell>31.1</cell><cell cols="3">34.8 58.9 5.4</cell><cell cols="4">6.8 12.6 28.3 29.0 2.8 21.9</cell><cell>21.4</cell><cell>33.1 39.6 27.5 82.1 24.5</cell></row><row><cell>UNet-Backbone [28]</cell><cell cols="2">31.9 66.7 71.5</cell><cell>23.3</cell><cell cols="3">18.9 47.9 0.8</cell><cell cols="4">21.8 6.7 20.1 17.3 10.7 12.3</cell><cell>43.8</cell><cell>15.0 61.5 35.5 91.6</cell><cell>9.3</cell></row><row><cell cols="3">3D-SIS (5 views) [15] 38.2 100.0 43.2</cell><cell>24.5</cell><cell cols="3">19.0 57.7 1.3</cell><cell cols="4">26.3 3.3 32.0 24.0 7.5 42.2</cell><cell>85.7</cell><cell>11.7 69.9 27.1 88.3 23.5</cell></row><row><cell>MASC [30]</cell><cell cols="2">44.7 52.8 55.5</cell><cell>38.1</cell><cell cols="3">38.2 63.3 0.2</cell><cell cols="4">50.9 26.0 36.1 43.2 32.7 45.1</cell><cell>57.1</cell><cell>36.7 63.9 38.6 98.0 27.6</cell></row><row><cell cols="3">ResNet-Backbone [28] 45.9 100.0 73.7</cell><cell>15.9</cell><cell cols="3">25.9 58.7 13.8</cell><cell cols="4">47.5 21.7 41.6 40.8 12.8 31.5</cell><cell>71.4</cell><cell>41.1 53.6 59.0 87.3 30.4</cell></row><row><cell>PanopticFusion [33]</cell><cell cols="2">47.8 66.7 71.2</cell><cell>59.5</cell><cell cols="3">25.9 55.0 0.0</cell><cell cols="4">61.3 17.5 25.0 43.4 43.7 41.1</cell><cell>85.7</cell><cell>48.5 59.1 26.7 94.4 35.9</cell></row><row><cell>MTML</cell><cell cols="2">48.1 100.0 66.6</cell><cell>37.7</cell><cell cols="3">27.2 70.9 0.1</cell><cell cols="4">57.9 25.4 36.1 31.8 9.5 43.2</cell><cell>100.0</cell><cell>18.4 60.1 48.7 93.8 38.4</cell></row><row><cell>3D-BoNet(Ours)</cell><cell cols="2">48.8 100.0 67.2</cell><cell>59.0</cell><cell cols="3">30.1 48.4 9.8</cell><cell cols="4">62.0 30.6 34.1 25.9 12.5 43.4</cell><cell>79.6</cell><cell>40.2 49.9 51.3 90.9 43.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Instance segmentation re-</cell></row><row><cell cols="2">sults on S3DIS dataset.</cell></row><row><cell></cell><cell>mPrec mRec</cell></row><row><cell>PartNet [32]</cell><cell>56.4 43.4</cell></row><row><cell>ASIS [51]</cell><cell>63.6 47.5</cell></row><row><cell cols="2">3D-BoNet (Ours) 65.6 47.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Instance segmentation results of all</cell></row><row><cell cols="2">ablation experiments on Area 5 of S3DIS.</cell></row><row><cell></cell><cell>mPrec mRec</cell></row><row><cell>(1) Remove Box Score Sub-branch</cell><cell>50.9 40.9</cell></row><row><cell>(2) Euclidean Distance Only</cell><cell>53.8 41.1</cell></row><row><cell>(3) Soft IoU Cost Only</cell><cell>55.2 40.6</cell></row><row><cell>(4) Cross-Entropy Score Only</cell><cell>51.8 37.8</cell></row><row><cell cols="2">(5) Do Not Supervise Box Prediction 37.3 28.5</cell></row><row><cell>(6) Remove Focal Loss</cell><cell>50.8 39.2</cell></row><row><cell>(7) The Full Framework</cell><cell>57.5 40.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Time consumption of different approaches on the validation split (312 scenes) of ScanNet(v2) (seconds).</figDesc><table><row><cell></cell><cell>SGPN [50]</cell><cell>ASIS [51]</cell><cell>GSPN [58]</cell><cell>3D-SIS [15]</cell><cell>3D-BoNet(Ours)</cell></row><row><cell></cell><cell>network(GPU): 650</cell><cell>network(GPU): 650</cell><cell>network(GPU): 500</cell><cell>voxelization, projection,</cell><cell>network(GPU): 650</cell></row><row><cell></cell><cell>group merging(CPU): 46562</cell><cell>mean shift(CPU): 53886</cell><cell>point sampling(GPU): 2995</cell><cell>network, etc. (GPU+CPU):</cell><cell>SCN (GPU parallel): 208</cell></row><row><cell></cell><cell>block merging(CPU): 2221</cell><cell>block merging(CPU): 2221</cell><cell>neighbour search(CPU): 468</cell><cell>38841</cell><cell>block merging(CPU): 2221</cell></row><row><cell>total</cell><cell>49433</cell><cell>56757</cell><cell>3963</cell><cell>38841</cell><cell>2871</cell></row></table><note>D Gradient Estimation for Hungarian Algorithm</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<title level="m">3D Semantic Parsing of Large-Scale Indoor Spaces. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-View 3D Object Detection Network for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Point signatures: A new representation for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jarvis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="85" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<title level="m">Mean Shift: A Robust Approach toward Feature Space Analysis. TPAMI</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<title level="m">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">3D-BEVIS: Birds-Eye-View Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<title level="m">Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks. ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<title level="m">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stochastic Optimization of Sorting Networks via Continuous Relaxations. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<title level="m">3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<title level="m">Pointwise Convolutional Neural Networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recurrent Slice Networks for 3D Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Escape from Cells: Deep Kd-Networks for The Recognition of 3D Point Cloud Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Hungarian Method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Variants of the hungarian method for assignment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="253" to="258" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<title level="m">PointGrid: A Deep Network for 3D Shape Understanding. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D Fully Convolutional Network for Vehicle Detection in Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vehicle Detection from 3D Lidar Using Fully Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">SO-Net: Self-Organizing Network for Point Cloud Analysis. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PointCNN : Convolution On X -Transformed Points. NeurlPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">3D Graph Embedding Learning with a Structure-aware Loss Function for Point Cloud Semantic Instance Segmentation. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<title level="m">MASC: Multi-scale Affinity with Sparse Convolution for 3D Instance Segmentation. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<title level="m">Attentional ShapeContextNet for Point Cloud Recognition. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<title level="m">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kaji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep Hough Voting for 3D Object Detection in Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<title level="m">Fully-Convolutional Point Networks for Large-Scale Point Clouds. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">OctNet: Learning Deep 3D Representations at High Resolutions. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fast point feature histograms (fpfh) for 3d registration. ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">SEGCloud: Semantic Segmentation of 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">KPConv: Flexible and Deformable Convolution for Point Clouds. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deconvolutional Networks for Point-Cloud Vehicle Detection and Tracking in Driving Scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Soì</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrade-Cetto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Local Spectral Graph Convolution for Point Set Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<title level="m">Associatively Segmenting Instances and Semantics in Point Clouds. CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Fusing Bird View LIDAR Point Cloud and Front View Camera Image for Deep Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Squeezeseg</surname></persName>
		</author>
		<title level="m">Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<title level="m">SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters. ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning Single-View 3D Reconstruction with Limited Pose Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">RT3D: Real-Time 3D Vehicle Detection in LiDAR Point Cloud for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3434" to="3440" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
