<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Training for Neural Network Transition-Based Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
							<email>djweiss@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
							<email>chrisalberti@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<email>mjcollins@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Training for Neural Network Transition-Based Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this fixed network representation, we learn a final layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide indepth ablative analysis to determine which aspects of our model provide the largest gains in accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention. Lately, dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages <ref type="bibr" target="#b3">(Buchholz and Marsi, 2006;</ref><ref type="bibr" target="#b25">Nivre et al., 2007;</ref><ref type="bibr" target="#b22">McDonald et al., 2013)</ref> and the efficiency of dependency parsers.</p><p>Transition-based parsers <ref type="bibr" target="#b27">(Nivre, 2008)</ref> have been shown to provide a good balance between efficiency and accuracy. In transition-based parsing, sentences are processed in a linear left to right pass; at each position, the parser needs to choose from a set of possible actions defined by the transition strategy. In greedy models, a classifier is used to independently decide which transition to take based on local features of the current parse configuration. This classifier typically uses hand-engineered features and is trained on individual transitions extracted from the gold transition sequence. While extremely fast, these greedy models typically suffer from search errors due to the inability to recover from incorrect decisions. <ref type="bibr" target="#b41">Zhang and Clark (2008)</ref> showed that a beamsearch decoding algorithm utilizing the structured perceptron training algorithm can greatly improve accuracy. Nonetheless, significant manual feature engineering was required before transitionbased systems provided competitive accuracy with graph-based parsers <ref type="bibr" target="#b43">(Zhang and Nivre, 2011)</ref>, and only by incorporating graph-based scoring functions were <ref type="bibr" target="#b0">Bohnet and Kuhn (2012)</ref> able to exceed the accuracy of graph-based approaches.</p><p>In contrast to these carefully hand-tuned approaches, <ref type="bibr" target="#b5">Chen and Manning (2014)</ref> recently presented a neural network version of a greedy transition-based parser. In their model, a feedforward neural network with a hidden layer is used to make the transition decisions. The hidden layer has the power to learn arbitrary combinations of the atomic inputs, thereby eliminating the need for hand-engineered features. Furthermore, because the neural network uses a distributed representation, it is able to model lexical, part-of-speech (POS) tag, and arc label similarities in a continuous space. However, although their model outperforms its greedy hand-engineered counterparts, it is not competitive with state-of-the-art dependency parsers that are trained for structured search.</p><p>In this work, we combine the representational power of neural networks with the superior search enabled by structured training and inference, making our parser one of the most accurate dependency parsers to date. Training and testing on the Penn Treebank <ref type="bibr" target="#b19">(Marcus et al., 1993)</ref>, our transition-based parser achieves 93.99% unlabeled (UAS) / 92.05% labeled (LAS) attachment accuracy, outperforming the 93.22% UAS / 91.02% LAS of <ref type="bibr" target="#b42">Zhang and McDonald (2014)</ref> and 93.27 UAS / 91.19 LAS of <ref type="bibr" target="#b0">Bohnet and Kuhn (2012)</ref>. In addition, by incorporating unlabeled data into training, we further improve the accuracy of our model to 94.26% UAS / 92.41% LAS (93.46% UAS / 91.49% LAS for our greedy model).</p><p>In our approach we start with the basic structure of <ref type="bibr" target="#b5">Chen and Manning (2014)</ref>, but with a deeper architecture and improvements to the optimization procedure. These modifications (Section 2) increase the performance of the greedy model by as much as 1%. As in prior work, we train the neural network to model the probability of individual parse actions. However, we do not use these probabilities directly for prediction. Instead, we use the activations from all layers of the neural network as the representation in a structured perceptron model that is trained with beam search and early updates (Section 3). On the Penn Treebank, this structured learning approach significantly improves parsing accuracy by 0.8%.</p><p>An additional contribution of this work is an effective way to leverage unlabeled data. Neural networks are known to perform very well in the presence of large amounts of training data; however, obtaining more expert-annotated parse trees is very expensive. To this end, we generate large quantities of high-confidence parse trees by parsing unlabeled data with two different parsers and selecting only the sentences for which the two parsers produced the same trees (Section 3.3). This approach is known as "tri-training" <ref type="bibr" target="#b18">(Li et al., 2014)</ref> and we show that it benefits our neural network parser significantly more than other approaches. By adding 10 million automatically parsed tokens to the training data, we improve the accuracy of our parsers by almost ∼1.0% on web domain data.</p><p>We provide an extensive exploration of our model in Section 5 through ablative analysis and other retrospective experiments. One of the goals of this work is to provide guidance for future refinements and improvements on the architecture and modeling choices we introduce in this paper.</p><p>Finally, we also note that neural network representations have a long history in syntactic parsing <ref type="bibr" target="#b11">(Henderson, 2004;</ref><ref type="bibr" target="#b35">Titov and Henderson, 2007;</ref><ref type="bibr" target="#b36">Titov and Henderson, 2010)</ref>; however, like <ref type="bibr" target="#b5">Chen and Manning (2014)</ref>, our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. <ref type="bibr" target="#b28">Peng et al. (2009)</ref> and Do and <ref type="bibr" target="#b9">Artires (2010)</ref> for Conditional Random Field (CRF) training of neural networks). Our paper ex-</p><formula xml:id="formula_0">h0 = [XgEg]</formula><p>Embedding Layer Input Hidden Layers</p><formula xml:id="formula_1">argmax y2GEN(x) m X j=1 v(y j ) · (x, c j ) h2 = max{0, W2h1 + b2} h1 = max{0, W1h0 + b1} P (y) / exp{ &gt; y h2 + by} 8g 2 {word, tag, label} Buffer NN DT news The det NN JJ VBD nsubj had little effect . ROOT ROOT Stack … … … … … Softmax Layer</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perceptron Layer</head><p>Features Extracted early updates (section 3). Structured learning reduces bias and significantly improves parsing accuracy by 0.6%. We demonstrate empirically that beam search based on the scores from the neural network does not work as well, perhaps because of the label bias problem.</p><p>A second contribution of this work is an effective way to leverage unlabeled data and other parsers. Neural networks are known to perform very well in the presence of large amounts of training data. It is however unlikely that the amount of hand parsed data will increase significantly because of the high cost for syntactic annotations. To this end we generate large quantities of high-confidence parse trees by parsing an unlabeled corpus and selecting only the sentences on which two different parsers produced the same parse trees. This idea comes from tri-training <ref type="bibr" target="#b18">(Li et al., 2014)</ref> and while applicable to other parsers as well, we show that it benefits neural network parsers more than models with discrete features. Adding 10 million automatically parsed tokens to the training data improves the accuracy of our parsers further by 0.7%. Our final greedy parser achieves an unlabeled attachment score (UAS) of 93.46% on the Penn Treebank test set, while a model with a beam of size 8 produces an UAS of 94.08% (section 4. To the best of our knowledge, these are some of the very best dependency accuracies on this corpus.</p><p>We provide an extensive exploration of our model in section 5. In ablation experiments we tease apart our various contributions and modeling choices in order to shed some light on what matters in practice. Neural network representations have been used in structured models before <ref type="bibr" target="#b28">(Peng et al., 2009;</ref><ref type="bibr" target="#b9">Do and Artires, 2010)</ref>, and have also been used for syntactic parsing <ref type="bibr" target="#b35">(Titov and Henderson, 2007;</ref><ref type="bibr" target="#b36">Titov and Henderson, 2010)</ref>, alas with fairly complex architectures and constraints. Our work on the other hand introduces a general approach for structured perceptron training with a neural network representation and achieves stateof-the-art parsing results for English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Network Model</head><p>In this section, we describe the architecture of our model, which is summarized in figure 2. Note that we separate the embedding processing to a distinct "embedding layer" for clarity of presentation. Our model is based upon that of <ref type="bibr">Chen</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Layer</head><p>Perceptron Layer </p><formula xml:id="formula_2">argmax d GEN(x) m X j=1 v(yj) · (x, cj) h2 = max{0, W2h1 + b2}, h1 = max{0, W1h0 + b1},</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Groups (2014) and we discuss the differences between our model and theirs in detail at the end of this section.</p><formula xml:id="formula_3">si, bi i 2 {1, 2, 3, 4} All lc1(si), lc2(si) i 2 {1, 2} All rc1(si), rc2(si) i 2 {1, 2} All rc1(rc1(si)) i 2 {1, 2} All lc1(lc1(si)) i 2 {1, 2} All</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input layer</head><p>Given a parse configuration c, we extract a rich set of discrete features which we feed into the neural network. Following Chen and Manning (2014), we group these features by their input source: words, POS tags, and arc labels. The full set of features is given in <ref type="table" target="#tab_4">Table 2</ref>. The features extracted for each group are represented as a sparse F ⇥ V matrix X, where V is the size of the vocabulary of the feature group and F is the number of features: the value of element X fv is 1 if the f 'th feature takes on value v. We produce three input matrices: X word for words features, Xtag for POS tag features, and X label for arc labels. For all feature groups, we add additional special … <ref type="figure" target="#fig_0">Figure 1</ref>: Schematic overview of our neural network model. Atomic features are extracted from the i'th elements on the stack (s i ) and the buffer (b i ); lc i indicates the i'th leftmost child and rc i the i'th rightmost child. We use the top two elements on the stack for the arc features and the top four tokens on stack and buffer for words, tags and arc labels.</p><p>tends this line of work to the setting of inexact search with beam decoding for dependency parsing; <ref type="bibr" target="#b44">Zhou et al. (2015)</ref> concurrently explored a similar approach using a structured probabilistic ranking objective. <ref type="bibr" target="#b10">Dyer et al. (2015)</ref> concurrently developed the Stack Long Short-Term Memory (S-LSTM) architecture, which does incorporate recurrent architecture and look-ahead, and which yields comparable accuracy on the Penn Treebank to our greedy model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Network Model</head><p>In this section, we describe the architecture of our model, which is summarized in <ref type="figure" target="#fig_0">Figure 1</ref>. Note that we separate the embedding processing to a distinct "embedding layer" for clarity of presentation. Our model is based upon that of <ref type="bibr" target="#b5">Chen and Manning (2014)</ref> and we discuss the differences between our model and theirs in detail at the end of this section. We use the arc-standard (Nivre, 2004) transition system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input layer</head><p>Given a parse configuration c (consisting of a stack s and a buffer b), we extract a rich set of discrete features which we feed into the neural network. Following Chen and Manning (2014), we group these features by their input source: words, POS tags, and arc labels. The features extracted for each group are represented as a sparse F × V matrix X, where V is the size of the vocabulary of the feature group and F is the number of features. The value of element X f v is 1 if the f 'th feature takes on value v. We produce three input matrices: X word for words features, X tag for POS tag features, and X label for arc labels, with F word = F tag = 20 and F label = 12 ( <ref type="figure" target="#fig_0">Figure 1</ref>). For all feature groups, we add additional special values for "ROOT" (indicating the POS or word of the root token), "NULL" (indicating no valid feature value could be computed) or "UNK" (indicating an out-of-vocabulary item).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embedding layer</head><p>The first learned layer h 0 in the network transforms the sparse, discrete features X into a dense, continuous embedded representation. For each feature group X g , we learn a V g × D g embedding matrix E g that applies the conversion:</p><formula xml:id="formula_4">h 0 = [X g E g | g ∈ {word, tag, label}],<label>(1)</label></formula><p>where we apply the computation separately for each group g and concatenate the results. Thus, the embedding layer has E = g F g D g outputs, which we reshape to a vector h 0 . We can choose the embedding dimensionality D for each group freely. Since POS tags and arc labels have much smaller vocabularies, we show in our experiments (Section 5.1) that we can use smaller D tag and D label , without a loss in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hidden layers</head><p>We experimented with one and two hidden layers composed of M rectified linear (Relu) units <ref type="bibr" target="#b24">(Nair and Hinton, 2010)</ref>. Each unit in the hidden layers is fully connected to the previous layer:</p><formula xml:id="formula_5">h i = max{0, W i h i−1 + b i },<label>(2)</label></formula><p>where W 1 is a M 1 × E weight matrix for the first hidden layer and W i are M i × M i−1 matrices for all subsequent layers. The weights b i are bias terms. Relu layers have been well studied in the neural network literature and have been shown to work well for a wide domain of problems <ref type="bibr" target="#b17">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b40">Zeiler et al., 2013)</ref>. Through most of development, we kept M i = 200, but we found that significantly increasing the number of hidden units improved our results for the final comparison.</p><p>2.4 Relationship to <ref type="bibr" target="#b5">Chen and Manning (2014)</ref> Our model is clearly inspired by and based on the work of <ref type="bibr" target="#b5">Chen and Manning (2014)</ref>. There are a few structural differences: (1) we allow for much smaller embeddings of POS tags and labels, (2) we use Relu units in our hidden layers, and <ref type="formula" target="#formula_6">(3)</ref> we use a deeper model with two hidden layers. Somewhat to our surprise, we found these changes combined with an SGD training scheme (Section 3.1) during the "pre-training" phase of the model to lead to an almost 1% accuracy gain over <ref type="bibr" target="#b5">Chen and Manning (2014)</ref>. This trend held despite carefully tuning hyperparameters for each method of training and structure combination. Our main contribution from an algorithmic perspective is our training procedure: as described in the next section, we use the structured perceptron for learning the final layer of our model. We thus present a novel way to leverage a neural network representation in a structured prediction setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semi-Supervised Structured Learning</head><p>In this work, we investigate a semi-supervised structured learning scheme that yields substantial improvements in accuracy over the baseline neural network model. There are two complementary contributions of our approach: (1) incorporating structured learning of the model and (2) utilizing unlabeled data. In both cases, we use the neural network to model the probability of each parsing action y as a soft-max function taking the final hidden layer as its input:</p><formula xml:id="formula_6">P(y) ∝ exp{β y h i + b y },<label>(3)</label></formula><p>where β y is a M i dimensional vector of weights for class y and i is the index of the final hidden layer of the network. At a high level our approach can be summarized as follows:</p><p>• First, we pre-train the network's hidden representations by learning probabilities of parsing actions. Fixing the hidden representations, we learn an additional final output layer using the structured perceptron that uses the output of the network's hidden layers. In practice this improves accuracy by ∼0.6% absolute.</p><p>• Next, we show that we can supplement the gold data with a large corpus of high quality automatic parses. We show that incorporating unlabeled data in this way improves accuracy by as much as 1% absolute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Backpropagation Pretraining</head><p>To learn the hidden representations, we use mini-batched averaged stochastic gradient descent (ASGD) <ref type="bibr" target="#b2">(Bottou, 2010)</ref> with momentum <ref type="bibr" target="#b12">(Hinton, 2012)</ref> to learn the parameters Θ of the network,</p><formula xml:id="formula_7">where Θ = {E g , W i , b i , β y | ∀g, i, y}.</formula><p>We use backpropagation to minimize the multinomial logistic loss:</p><formula xml:id="formula_8">L(Θ) = − j log P(y j | c j , Θ) + λ i ||W i || 2 2 , (4)</formula><p>where λ is a regularization hyper-parameter over the hidden layer parameters (we use λ = 10 −4 in all experiments) and j sums over all decisions and configurations {y j , c j } extracted from gold parse trees in the dataset.</p><p>The specific update rule we apply at iteration t is as follows:</p><formula xml:id="formula_9">g t = µg t−1 − ∆L(Θ t ),<label>(5)</label></formula><formula xml:id="formula_10">Θ t+1 = Θ t + η t g t ,<label>(6)</label></formula><p>where the descent direction g t is computed by a weighted combination of the previous direction g t−1 and the current gradient ∆L(Θ t ). The parameter µ ∈ [0, 1) is the momentum parameter while η t is the traditional learning rate. In addition, since we did not tune the regularization parameter λ, we apply a simple exponential step-wise decay to η t ; for every γ rounds of updates, we multiply η t = 0.96η t−1 . The final component of the update is parameter averaging: we maintain averaged parameters</p><formula xml:id="formula_11">Θ t = α tΘt−1 + (1 − α t )Θ t ,</formula><p>where α t is an averaging weight that increases from 0.1 to 0.9999 with 1/t. Combined with averaging, careful tuning of the three hyperparameters µ, η 0 , and γ using heldout data was crucial in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structured Perceptron Training</head><p>Given the hidden representations, we now describe how the perceptron can be trained to utilize these representations. The perceptron algorithm with early updates <ref type="bibr" target="#b7">(Collins and Roark, 2004</ref>) requires a feature-vector definition φ that maps a sentence x together with a configuration c to a feature vector φ(x, c) ∈ R d . There is a one-to-one mapping between configurations c and decision sequences y 1 . . . y j−1 for any integer j ≥ 1: we will use c and y 1 . . . y j−1 interchangeably.</p><p>For a sentence x, define GEN(x) to be the set of parse trees for x. Each y ∈ GEN(x) is a sequence of decisions y 1 . . . y m for some integer m. We use Y to denote the set of possible decisions in the parsing model. For each decision y ∈ Y we assume a parameter vector v(y) ∈ R d . These parameters will be trained using the perceptron.</p><p>In decoding with the perceptron-trained model, we will use beam search to attempt to find:</p><formula xml:id="formula_12">argmax y∈GEN(x) m j=1 v(y j ) · φ(x, y 1 . . . y j−1 ).</formula><p>Thus each decision y j receives a score: v(y j ) · φ(x, y 1 . . . y j−1 ).</p><p>In the perceptron with early updates, the parameters v(y) are trained as follows. On each training example, we run beam search until the goldstandard parse tree falls out of the beam. 1 Define j to be the length of the beam at this point. A structured perceptron update is performed using the gold-standard decisions y 1 . . . y j as the target, and the highest scoring (incorrect) member of the beam as the negative example.</p><p>A key idea in this paper is to use the neural network to define the representation φ(x, c). Given the sentence x and the configuration c, assuming two hidden layers, the neural network defines values for h 1 , h 2 , and P(y) for each decision y. We experimented with various definitions of φ (Section 5.2) and found that φ(x, c) = [h 1 h 2 P(y)] (the concatenation of the outputs from both hidden layers, as well as the probabilities for all decisions y possible in the current configuration) had the best accuracy on development data.</p><p>Note that it is possible to continue to use backpropagation to learn the representation φ(x, c) during perceptron training; however, we found using ASGD to pre-train the representation always led to faster, more accurate results in preliminary experiments, and we left further investigation for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Incorporating Unlabeled Data</head><p>Given the high capacity, non-linear nature of the deep network we hypothesize that our model can be significantly improved by incorporating more data. One way to use unlabeled data is through unsupervised methods such as word clusters <ref type="bibr" target="#b16">(Koo et al., 2008)</ref>; we follow <ref type="bibr" target="#b5">Chen and Manning (2014)</ref> and use pretrained word embeddings to initialize our model. The word embeddings capture similar distributional information as word clusters and give consistent improvements by providing a good initialization and information about words not seen in the treebank data.</p><p>However, obtaining more training data is even more important than a good initialization. One potential way to obtain additional training data is by parsing unlabeled data with previously trained models. <ref type="bibr" target="#b21">McClosky et al. (2006)</ref> and <ref type="bibr" target="#b14">Huang and Harper (2009)</ref> showed that iteratively re-training a single model ("self-training") can be used to improve parsers in certain settings; <ref type="bibr" target="#b31">Petrov et al. (2010)</ref> built on this work and showed that a slow and accurate parser can be used to "up-train" a faster but less accurate parser.</p><p>In this work, we adopt the "tri-training" approach of <ref type="bibr" target="#b18">Li et al. (2014)</ref>: Two parsers are used to process the unlabeled corpus and only sentences for which both parsers produced the same parse tree are added to the training data. The intuition behind this idea is that the chance of the parse being correct is much higher when the two parsers agree: there is only one way to be correct, while there are many possible incorrect parses. Of course, this reasoning holds only as long as the parsers suffer from different biases.</p><p>We show that tri-training is far more effective than vanilla up-training for our neural network model. We use same setup as <ref type="bibr" target="#b18">Li et al. (2014)</ref>, intersecting the output of the BerkeleyParser <ref type="bibr" target="#b30">(Petrov et al., 2006)</ref>, and a reimplementation of ZPar <ref type="bibr" target="#b43">(Zhang and Nivre, 2011)</ref> as our baseline parsers. The two parsers agree only 36% of the time on the tune set, but their accuracy on those sentences is 97.26% UAS, approaching the inter annotator agreement rate. These sentences are of course easier to parse, having an average length of 15 words, compared to 24 words for the tune set overall. However, because we only use these sentences to extract individual transition decisions, the shorter length does not seem to hurt their utility. We generate 10 7 tokens worth of new parses and use this data in the backpropagation stage of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we present our experimental setup and the main results of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We conduct our experiments on two English language benchmarks: (1) the standard Wall Street Journal (WSJ) part of the Penn Treebank <ref type="bibr" target="#b19">(Marcus et al., 1993)</ref> and <ref type="formula" target="#formula_5">(2)</ref> a more comprehensive union of publicly available treebanks spanning multiple domains. For the WSJ experiments, we follow standard practice and use sections 2-21 for training, section 22 for development and section 23 as the final test set. Since there are many hyperparameters in our models, we additionally use section 24 for tuning. We convert the constituency trees to Stanford style dependencies <ref type="bibr" target="#b8">(De Marneffe et al., 2006)</ref> using version 3.3.0 of the converter. We use a CRF-based POS tagger to generate 5fold jack-knifed POS tags on the training set and predicted tags on the dev, test and tune sets; our tagger gets comparable accuracy to the Stanford POS tagger <ref type="bibr" target="#b37">(Toutanova et al., 2003)</ref> with 97.44% on the test set. We report unlabeled attachment score (UAS) and labeled attachment score (LAS) excluding punctuation on predicted POS tags, as is standard for English.</p><p>For the second set of experiments, we follow the same procedure as above, but with a more diverse dataset for training and evaluation. Following <ref type="bibr" target="#b38">Vinyals et al. (2015)</ref>, we use (in addition to the WSJ), the OntoNotes corpus version 5 <ref type="bibr" target="#b13">(Hovy et al., 2006)</ref>, the English Web Treebank <ref type="bibr" target="#b29">(Petrov and McDonald, 2012)</ref>, and the updated and corrected Question Treebank <ref type="bibr" target="#b15">(Judge et al., 2006)</ref>. We train on the union of each corpora's training set and test on each domain separately. We refer to this setup as the "Treebank Union" setup.</p><p>In our semi-supervised experiments, we use the corpus from <ref type="bibr" target="#b4">Chelba et al. (2013)</ref> as our source of unlabeled data. We process it with the Berkeley-Parser <ref type="bibr" target="#b30">(Petrov et al., 2006)</ref>, a latent variable constituency parser, and a reimplementation of ZPar <ref type="bibr" target="#b43">(Zhang and Nivre, 2011)</ref>, a transition-based parser with beam search. Both parsers are included as baselines in our evaluation. We select the first 10 7 tokens for which the two parsers agree as additional training data. For our tri-training experiments, we re-train the POS tagger using the POS tags assigned on the unlabeled data from the Berkeley constituency parser. This increases POS  <ref type="table" target="#tab_1">Table 1</ref>: Final WSJ test set results. We compare our system to state-of-the-art graph-based and transition-based dependency parsers. denotes our own re-implementation of the system so we could compare tri-training on a competitive baseline. All methods except <ref type="bibr" target="#b5">Chen and Manning (2014)</ref> and <ref type="bibr" target="#b10">Dyer et al. (2015)</ref> were run using predicted tags from our POS tagger. For reference, the accuracy of the Berkeley constituency parser (after conversion) is 93.61% UAS / 91.51% LAS.</p><p>accuracy slightly to 97.57% on the WSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Initialization &amp; Hyperparameters</head><p>In all cases, we initialized W i and β randomly using a Gaussian distribution with variance 10 −4 . We used fixed initialization with b i = 0.2, to ensure that most Relu units are activated during the initial rounds of training. We did not systematically compare this random scheme to others, but we found that it was sufficient for our purposes. For the word embedding matrix E word , we initialized the parameters using pretrained word embeddings. We used the publicly available word2vec 2 tool  to learn CBOW embeddings following the sample configuration provided with the tool. For words not appearing in the unsupervised data and the special "NULL" etc. tokens, we used random initialization. In preliminary experiments we found no difference between training the word embeddings on 1 billion or 10 billion tokens. We therefore trained the word embeddings on the same corpus we used for tri-training <ref type="bibr" target="#b4">(Chelba et al., 2013)</ref>.</p><p>We   tron layer, we used φ(x, c) = [h 1 h 2 P(y)] (concatenation of all intermediate layers). All hyperparameters (including structure) were tuned using Section 24 of the WSJ only. When not tri-training, we used hyperparameters of γ = 0.2, η 0 = 0.05, µ = 0.9, early stopping after roughly 16 hours of training time. With the tri-training data, we decreased η 0 = 0.05, increased γ = 0.5, and decreased the size of the network to M 1 = 1024, M 2 = 256 for run-time efficiency, and trained the network for approximately 4 days. For the Treebank Union setup, we set M 1 = M 2 = 1024 for the standard training set and for the tri-training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Table 1 shows our final results on the WSJ test set, and <ref type="table" target="#tab_4">Table 2</ref> shows the cross-domain results from the Treebank Union. We compare to the best dependency parsers in the literature. For <ref type="bibr" target="#b5">(Chen and Manning, 2014)</ref> and <ref type="bibr" target="#b10">(Dyer et al., 2015)</ref>, we use reported results; the other baselines were run by Bernd Bohnet using version 3.3.0 of the Stanford dependencies and our predicted POS tags for all datasets to make comparisons as fair as possible. On the WSJ and Web tasks, our parser outperforms all dependency parsers in our comparison by a substantial margin. The Question (QTB) dataset is more sensitive to the smaller beam size we use in order to train the models in a reasonable time; if we increase to B = 32 at inference time only, our perceptron performance goes up to 92.29% LAS. Since many of the baselines could not be directly compared to our semi-supervised approach, we re-implemented <ref type="bibr" target="#b43">Zhang and Nivre (2011)</ref> and trained on the tri-training corpus. Although tritraining did help the baseline on the dev set <ref type="figure" target="#fig_3">(Figure 4)</ref>, test set performance did not improve significantly. In contrast, it is quite exciting to see that after tri-training, even our greedy parser is more accurate than any of the baseline dependency parsers and competitive with the Berkeley-Parser used to generate the tri-training data. As expected, tri-training helps most dramatically to increase accuracy on the Treebank Union setup with diverse domains, yielding 0.4-1.0% absolute LAS improvement gains for our most accurate model.</p><p>Unfortunately we are not able to compare to several semi-supervised dependency parsers that achieve some of the highest reported accuracies on the WSJ, in particular <ref type="bibr" target="#b33">Suzuki et al. (2009</ref><ref type="bibr" target="#b34">), Suzuki et al. (2011</ref> and . These parsers use the <ref type="bibr" target="#b39">Yamada and Matsumoto (2003)</ref> dependency conversion and the accuracies are therefore not directly comparable. The highest of these is <ref type="bibr" target="#b34">Suzuki et al. (2011)</ref>, with a reported accuracy of 94.22% UAS. Even though the UAS is not directly comparable, it is typically similar, and this suggests that our model is competitive with some of the highest reported accuries for dependencies on WSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this section, we investigate the contribution of the various components of our approach through ablation studies and other systematic experiments. We tune on Section 24, and use Section 22 for comparisons in order to not pollute the official test set (Section 23). We focus on UAS as we found the LAS scores to be strongly correlated. Unless otherwise specified, we use 200 hidden units in each layer to be able to run more ablative experiments in a reasonable amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Impact of Network Structure</head><p>In addition to initialization and hyperparameter tuning, there are several additional choices about model structure and size a practitioner faces when implementing a neural network model. We explore these questions and justify the particular choices we use in the following. Note that we do Figure 2: Effect of hidden layers and pre-training on variance of random restarts. Initialization was either completely random or initialized with word2vec embeddings ("Pretrained"), and either one or two hidden layers of size 200 were used <ref type="bibr">("200" vs "200x200")</ref>. Each point represents maximization over a small hyperparameter grid with early stopping based on WSJ tune set UAS score. D word = 64, D tag , D label = 16. not use a beam for this analysis and therefore do not train the final perceptron layer. This is done in order to reduce training times and because the trends persist across settings.</p><p>Variance reduction with pre-trained embeddings. Since the learning problem is nonconvex, different initializations of the parameters yield different solutions to the learning problem. Thus, for any given experiment, we ran multiple random restarts for every setting of our hyperparameters and picked the model that performed best using the held-out tune set. We found it important to allow the model to stop training early if tune set accuracy decreased.</p><p>We visualize the performance of 32 random restarts with one or two hidden layers and with and without pretrained word embeddings in <ref type="figure">Figure 2</ref>, and a summary of the figure in <ref type="table" target="#tab_7">Table 3</ref>. While adding a second hidden layer results in a large gain on the tune set, there is no gain on the dev set if pre-trained embeddings are not used. In fact, while the overall UAS scores of the tune set and dev set are strongly correlated (ρ = 0.64, p &lt; 10 −10 ), they are not significantly correlated if pre-trained embeddings are not used (ρ = 0.12, p &gt; 0.3). This suggests that an additional benefit of pre-trained embeddings, aside from allowing learning to reach a more accurate solution, is to push learning towards a solution that generalizes to more data.   Diminishing returns with increasing embedding dimensions. For these experiments, we fixed one embedding type to a high value and reduced the dimensionality of all others to very small values. The results are plotted in <ref type="figure" target="#fig_2">Figure  3</ref>, suggesting larger embeddings do not significantly improve results. We also ran tri-training on a very compact model with D word = 8 and D tag = D label = 2 (8× fewer parameters than our full model) which resulted in 92.33% UAS accuracy on the dev set. This is comparable to the full model without tri-training, suggesting that more training data can compensate for fewer parameters.</p><p>Increasing hidden units yields large gains. For these experiments, we fixed the embedding sizes D word = 64, D tag = D label = 32 and tried increasing and decreasing the dimensionality of the hidden layers on a logarthmic scale. Improvements in accuracy did not appear to saturate even with increasing the number of hidden units by an order of magnitude, though the network became too slow to train effectively past M = 2048. These results suggest that there are still gains to be made by increasing the efficiency of larger networks, even for greedy shift-reduce parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of Structured Perceptron</head><p>We now turn our attention to the importance of structured perceptron training as well as the impact of different latent representations.</p><p>Bias reduction through structured training.</p><p>To evaluate the impact of structured training, we   compare using the estimates P(y) from the neural network directly for beam search to using the activations from all layers as features in the structured perceptron. Using the probability estimates directly is very similar to <ref type="bibr" target="#b32">Ratnaparkhi (1997)</ref>, where a maximum-entropy model was used to model the distribution over possible actions at each parser state, and beam search was used to search for the highest probability parse. A known problem with beam search in this setting is the label-bias problem. <ref type="table" target="#tab_10">Table 5</ref> shows the impact of using structured perceptron training over using the softmax function during beam search as a function of the beam size used. For reference, our reimplementation of <ref type="bibr" target="#b43">Zhang and Nivre (2011)</ref> is trained equivalently for each setting. We also show the impact on beam size when tri-training is used. Although the beam does marginally improve accuracy for the softmax model, much greater gains are achieved when perceptron training is used.</p><p>Using all hidden layers crucial for structured perceptron. We also investigated the impact of connecting the final perceptron layer to all prior hidden layers <ref type="table" target="#tab_11">(Table 6</ref>). Our results suggest that all intermediate layers of the network are indeed discriminative. Nonetheless, aggregating all of their activations proved to be the most effective representation for the structured perceptron. This suggests that the representations learned by the network collectively contain the information re- quired to reduce the bias of the model, but not when filtered through the softmax layer. Finally, we also experimented with connecting both hidden layers to the softmax layer during backpropagation training, but we found this did not significantly affect the performance of the greedy model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Impact of Tri-Training</head><p>To evaluate the impact of the tri-training approach, we compared to up-training with the Berkely-Parser <ref type="bibr" target="#b30">(Petrov et al., 2006)</ref> alone. The results are summarized in <ref type="figure" target="#fig_3">Figure 4</ref> for the greedy and perceptron neural net models as well as our reimplementated <ref type="bibr" target="#b43">Zhang and Nivre (2011)</ref> baseline.</p><p>For our neural network model, training on the output of the BerkeleyParser yields only modest gains, while training on the data where the two parsers agree produces significantly better results. This was especially pronounced for the greedy models: after tri-training, the greedy neural network model surpasses the BerkeleyParser in accuracy. It is also interesting to note that up-training improved results far more than tri-training for the baseline. We speculate that this is due to the a lack of diversity in the tri-training data for this model, since the same baseline model was intersected with the BerkeleyParser to generate the tritraining data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Error Analysis</head><p>Regardless of tri-training, using the structured perceptron improved error rates on some of the common and difficult labels: ROOT, ccomp, cc, conj, and nsubj all improved by &gt;1%. We inspected the learned perceptron weights v for the softmax probabilities P(y) (see Appendix) and found that the perceptron reweights the softmax probabilities based on common confusions; e.g. a strong negative weight for the action RIGHT(ccomp) given the softmax model outputs RIGHT(conj). Note that this trend did not hold when φ(x, c) = [P(y)]; without the hidden layer, the perceptron was not able to reweight the softmax probabilities to account for the greedy model's biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a new state of the art in dependency parsing: a transition-based neural network parser trained with the structured perceptron and ASGD. We then combined this approach with unlabeled data and tri-training to further push state-of-the-art in semi-supervised dependency parsing. Nonetheless, our ablative analysis suggests that further gains are possible simply by scaling up our system to even larger representations. In future work, we will apply our method to other languages, explore end-to-end training of the system using structured learning, and scale up the method to larger datasets and network structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic overview of our neural network model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>set D word = 64 and D tag = D label = 32 for embedding dimensions and M 1 = M 2 = 2048 hidden units in our final experiments. For the percep-2 http://code.google.com/p/word2vec/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Effect of embedding dimensions on the WSJ tune set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>ZN'11 (B=1) ZN'11 (B=32) Ours (B=1) Ours (Semi-supervised training with 10 7 additional tokens, showing that tri-training gives significant improvements over up-training for our neural net model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Features used in the model. si and bi are elements on the stack and buffer, respectively. lci indicates i'th leftmost child and rci the i'th rightmost child.</figDesc><table><row><cell>Features that are included in addition to those from Chen and Manning (2014) are marked with ?. Groups indicates which values were ex-tracted from each feature location (e.g. words, tags, labels).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Final Treebank Union test set results. We report LAS only for brevity; see Appendix for full results. For these tri-training results, we sampled sentences to ensure the dis-tribution of sentence lengths matched the distribution in the training set, which we found marginally improved the ZPar tri-training performance. For reference, the accuracy of the Berkeley constituency parser (after conversion) is 91.66% WSJ, 85.93% Web, and 93.45% QTB.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Impact of network architecture on UAS for greedy inference. We select the best model from 32 random restarts based on the tune set and show the resulting dev set accuracy. We also show the standard deviation across the 32 restarts.</figDesc><table><row><cell># Hidden 64 128 256 512 1024 2048</cell></row><row><cell>1 Layer 91.73 92.27 92.48 92.73 92.74 92.83 2 Layers 91.89 92.40 92.71 92.70 92.96 93.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Increasing hidden layer size increases WSJ Dev UAS. Shown is the average WSJ Dev UAS across hyperpa-rameter tuning and early stopping with 3 random restarts with a greedy model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>91.36 92.54 92.62 92.88 93.09 Softmax 92.74 93.07 93.16 93.25 93.24 93.24 Perceptron 92.73 93.06 93.40 93.47 93.50 93.58 Tri-training ZN'11 91.65 92.37 93.37 93.24 93.21 93.18 Softmax 93.71 93.82 93.86 93.87 93.87 93.87 Perceptron 93.69 94.00 94.23 94.33 94.31 94.32</figDesc><table><row><cell>Beam</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>WSJ Only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ZN'11</cell><cell>90.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Beam search always yields significant gains but using perceptron training provides even larger benefits, especially for the tri-trained neural network model. The best result for each model is highlighted in bold.</figDesc><table><row><cell>φ(x, c)</cell><cell cols="2">WSJ Only Tri-training</cell></row><row><cell>[h 2 ] [P(y)]</cell><cell>93.16 93.26</cell><cell>93.93 93.80</cell></row><row><cell cols="2">93.33 [h 1 h 2 P(y)] 93.47 [h 1 h 2 ]</cell><cell>93.95 94.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Utilizing all intermediate representations improves performance on the WSJ dev set. All results are with B = 8.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If the gold parse tree stays within the beam until the end of the sentence, conventional perceptron updates are used.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Bernd Bohnet for training his parsers and TurboParser on our setup. This paper benefitted tremendously from discussions with Ryan McDonald, Greg Coppola, Emily Pitler and Fernando Pereira. Finally, we are grateful to all members of the Google Parsing Team.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The best of both worlds: a graph-based completion model for transition-based parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COL-ING</title>
		<meeting>COL-ING</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COMPSTAT</title>
		<meeting>COMPSTAT</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conll-x shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised feature transformation for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2013 EMNLP</title>
		<meeting>2013 EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1303" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL, Main Volume</title>
		<meeting>ACL, Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trinh</forename><forename type="middle">Minh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Artires</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL, Main Volume</title>
		<meeting>ACL, Main Volume</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="599" to="619" />
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ontonotes: The 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selftraining PCFG grammars with latent annotations across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2009 EMNLP</title>
		<meeting>2009 EMNLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="832" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Questionbank: Creating a corpus of parseannotated questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ambiguity-aware ensemble training for semisupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Quirmbach-Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Bedini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
	<note>Núria Bertomeu Castelló, and Jungmee Lee</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th ICML</title>
		<meeting>27th ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The CoNLL 2007 shared task on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mc-Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="915" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL Workshop on Incremental Parsing</title>
		<meeting>ACL Workshop on Incremental Parsing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conditional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1419" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<title level="m">Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uptraining for accurate deterministic question parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="705" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A linear observed time statistical parser based on maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An empirical study of semisupervised structured conditional models for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2009 EMNLP</title>
		<meeting>2009 EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning condensed feature representations from large unsupervised data sets for supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="636" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast and robust multilingual dependency parsing with a generative latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="947" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A latent variable model for generative dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Parsing Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="35" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<title level="m">Grammar as a foreign language</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On rectified linear units for speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3517" to="3521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enforcing structural diversity in cube-pruned dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="656" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A neural probabilistic structured-prediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
