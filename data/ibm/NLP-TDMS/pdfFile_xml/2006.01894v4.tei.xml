<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An efficient manifold density estimator for all recommendation systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Dąbrowski</surname></persName>
							<email>jack.dabrowski@synerise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
							<email>barbara.rychalska@synerise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Daniluk</surname></persName>
							<email>michal.daniluk@synerise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominika</forename><surname>Basaj</surname></persName>
							<email>dominika.basaj@synerise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Gołuchowski</surname></persName>
							<email>konrad.goluchowski@synerise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bąbel</surname></persName>
							<email>piotr.babel@synerise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Michałowski</surname></persName>
							<email>andrzej.michalowski@synerise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Jakubowski</surname></persName>
							<email>adam.jakubowski@synerise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synerise 2 Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An efficient manifold density estimator for all recommendation systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many unsupervised representation learning methods belong to the class of similarity learning models. While various modality-specific approaches exist for different types of data, a core property of many methods is that representations of similar inputs are close under some similarity function. We propose EMDE (Efficient Manifold Density Estimator) -a framework utilizing arbitrary vector representations with the property of local similarity to succinctly represent smooth probability densities on Riemannian manifolds. Our approximate representation has the desirable properties of being fixed-size and having simple additive compositionality, thus being especially amenable to treatment with neural networks -both as input and output format, producing efficient conditional estimators. We generalize and reformulate the problem of multi-modal recommendations as conditional, weighted density estimation on manifolds. Our approach allows for trivial inclusion of multiple interaction types, modalities of data as well as interaction strengths for any recommendation setting. Applying EMDE to both top-k and session-based recommendation settings, we establish new state-of-the-art results on multiple open datasets in both uni-modal and multi-modal settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of recommender systems is to suggest items which a user might find interesting, often in the setting of online stores or social media. A common problem setting in the domain of recommenders is that predictions must be made from data which is inherently sequential, representing user actions over a time-span <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Thus, the input is an ordered collection of items based on a single user's shopping session, and the task consists of predicting which item will be clicked or added to cart next. Many session-based recommendation (SRS) systems use methods which explicitly model sequentiality, such as recurrent neural networks (RNNs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> or graph neural networks (GNNs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. However, these methods are known to scale poorly to growing item sets and increasing sequence size <ref type="bibr" target="#b10">[11]</ref>. They also exhibit a number of specific efficiencyrelated problems, such as neighborhood explosion in GNNs (the number of neighbors often grows exponentially when increasing node distances are considered). Such problems demand additional remedial measures which often hurt performance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Yet, as the sequential aspect of recommendation is considered vital, most efforts are focused on researching ever more complex neural network architectures in order to represent the ordered relations accurately.</p><p>We propose to shift the focus from sequentiality in rec- <ref type="figure">Figure 1</ref>: Encoding densities in a multidimensional space. Each item is represented by an embedding vector of arbitrary modality (interactions, textual description, image). Embedding spaces of items are represented with multidimensional histograms. Items from each session are encoded into fixed-size sketch structures.</p><p>ommenders to an accurate representation of user behaviors understood as item sets. Our hypothesis is that the sequential aspect could be largely discarded if we manage to represent the density of aggregate user preference spaces with sufficiently high fidelity. This approach could simplify the neural architecture dramatically, as probability densities do not have a temporal nature and can be represented by approximate, fixed-size structures, serving as simple 1-d input vectors to feed-forward neural networks. Thus, the core challenge in our setting becomes to 1) create a succinct vector representation of densities, and 2) teach a model to compute conditional mappings between input and output densities. To this end, we propose EMDE (Efficient Manifold Density Estimator) -a method which exploits locality sensitive hashing to create sketches -histogram-like structures which represent density on multidimensional manifolds. The sketches allow conditional mapping from one density estimate to another with simple shallow feed-forward neural networks. We show that EMDE achieves state-of-the-art results on multiple SRS datasets, going against the trend of sequential focus. We confirm the versatility of EMDE applying it to another recommender setting -top-k recommendation task, which operates on large item collections viewed/bought over an extended time period, and achieving competitive results. Thus, we postulate that direct density estimation of item spaces is indeed a viable research pathway in the general area of recommendation.</p><p>EMDE also exhibits a number of unique properties crucial to real-life recommendation tasks, which marks its efficiency against other neural methods:</p><p>• The sketches have constant size independent of the total number of items modeled, the number of items a given user has interacted with, and the original embedding dimensions. Thus the size of a downstream model does not depend on these values as well.</p><p>• The sketches are additive: aggregation of multiple samples to create item sets is done with a simple summation, with the option to include attached weights.</p><p>• Retrieving items from a sketch is simple and efficient (small, constant number of lookups per item).</p><p>Moreover, we show that EMDE allows for easy and natural incorporation of multiple modalities, such as various types of interactions (click/purchase/add to favorites), item names or images. Multiple modalities can be introduced simply as additional manifolds whose density is estimated and mapped by EMDE to the same sketch structures. A simple concatenation of per-modality sketches is then fed to a shallow, feed-forward network. This feature is important even in a unimodal setting, because multiple versions of the same embedding can be easily used (e.g. computed with a different seed or iteration number) -a feature we exploit in our experiments.</p><p>Our conclusions are backed by experiments inspired by findings from <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>. They observe the phantom progress problem in recommender systems: carefully tuned simple heuristics (such as nearest-neighbor methods) in practice often outperform complex deep learning models, while algorithm performance is heavily dependent on the dataset and chosen performance metrics. In response to this, we use the benchmark suites from <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref> to test our approach on a wide range of referential metrics and datasets, outperforming both the most advanced models and simple heuristics alike.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural session-based recommenders.</head><p>A popular branch of neural models use recurrent neural networks (RNN), suitable for data with temporal ordering. Gru4Rec <ref type="bibr" target="#b3">[4]</ref> uses the gated recurrent unit (GRU), in a sequential setting. NARM <ref type="bibr" target="#b4">[5]</ref> extends the approach with an item-level attention mechanism. STAMP <ref type="bibr" target="#b16">[17]</ref> is an attention-based non-RNN model using neural memory modules to represent item long-term and short-term sequences. NextItNet <ref type="bibr" target="#b17">[18]</ref> is based on stacked 1D dilated convolutions, instead of an RNN. SR-GNN <ref type="bibr" target="#b7">[8]</ref> features a graph neural network (GNN) able to exploit additional paths between sessions, still focusing on sequentiality. TAGNN <ref type="bibr" target="#b8">[9]</ref> adds a target-aware attention module to the GNN. The GNN-based models do not scale easily and are usually evaluated on small datasets (e.g. <ref type="bibr" target="#b8">[9]</ref> evaluate on just 1/64 of the RSC15 dataset, while EMDE and most methods evaluated by <ref type="bibr" target="#b14">[15]</ref> work on the full dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-neural session-based recommenders.</head><p>Although neural recommenders are mostly regarded as stateof-the art, <ref type="bibr" target="#b14">[15]</ref> show that simple non-neural baselines can often outperform them in practice. They propose S-KNN -a session-based nearest-neighbor technique, and its variant VS-KNN which favors recent items in sessions. AR is based on association rules, with its variant SR counting associations in item sequences. CT <ref type="bibr" target="#b18">[19]</ref> uses context trees for reflecting possible decision pathways during sessions.</p><p>Sketching-based density estimators. <ref type="bibr" target="#b19">[20]</ref> introduce methods for kernel density estimation (KDE) based on locality-sensitive hashing (LSH). Subsequently <ref type="bibr" target="#b20">[21]</ref> use a sketch-based structure for a compressed representation of multiple LSH partitions for KDE. Both methods require a computation-heavy sampling procedure to arrive at density estimates. <ref type="bibr" target="#b21">[22]</ref> introduce RACE -a LSH sketch-based method for KDE, which does not require sampling to arrive at density estimates. <ref type="bibr" target="#b22">[23]</ref> further explores the technique of LSH sketching for approximate nearest-neighbor search on streaming data. In these methods, the considered manifold is R n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Our solution is inspired by two algorithms: count-min sketch (CMS) <ref type="bibr" target="#b23">[24]</ref> and locality sensitive hashing (LSH) <ref type="bibr" target="#b24">[25]</ref>.</p><p>Count-min sketch. CMS is a data structure used to count the number of times items appear in a data stream. The algorithm provides efficiency at the cost of accuracy: it can encode multi-sets in an compact, compressed way, but returned item counts are approximate. Often used when the input space is too large to fit in memory, the algorithm works in sublinear space ( <ref type="figure" target="#fig_1">Figure 3</ref>). It consists of two operations: 1) incrementing item counts; 2) querying item counts by computing a minimum across hashed tables. The CMS sketch structure is represented by a two-dimensional array count with width w and depth d, representing hash functions h d , which output a value h d (it) ∈ (0, .., w − 1). Each row of the array represents a separate frequency table, and multiple such tables are introduced in order to alleviate the problem of hash conflicts. The array count d×w is initialized with zeros. When an item it arrives, its count is updated by a value of ct (1 by default). The procedure is expressed by the following equation:</p><formula xml:id="formula_0">count d [h d (it)] ← count d [h d (it)] + ct.</formula><p>The retrieval procedure of an item count consists of checking the values contained in the d respective item buckets in each row of the sketch. The minimum value of all of these is finally selected:</p><formula xml:id="formula_1">item_count = d min j=1 countj[hj(it)].</formula><p>While CMS is a very efficient, compressed data structure for count estimation, the algorithm applies random hashes to item labels, therefore is blind to the geometry of the input space and any item-item relationships.</p><p>Locality-sensitive hashing. As a geometry-aware approach to hashing, LSH aims to assign input vectors to hash codes, ensuring that the probability of being in the same hash bucket is much higher for inputs which are close together in the input space, than for those which are far apart <ref type="bibr" target="#b24">[25]</ref>.</p><p>In contrast to CMS random hashing, LSH methods allow to preserve the geometric prior of the original input space when available. As we further show, CMS and LSH can be combined and adapted for compressed, geometry-aware density estimation in high dimensional spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficient Manifold Density Estimator</head><p>EMDE can be likened to a weighted, compressed, geometryaware histogram for high dimensional spaces. Similarly to a histogram accumulating normalized observation counts in disjoint regions of a 1-dimensional input space, EMDE also  functions as a piece-wise constant observation counter over disjoint regions of an input space lying on the data manifold. As dimensionality of the input space grows, performance of a simple histogram degrades dramatically. Thus, to efficiently map high dimensional spaces to a histogramlike structure, we utilize the CMS compression approach -maintaining an ensemble of multiple, independent histograms. Originally CMS uses random hash functions to map inputs onto sketch buckets, which is unsuitable for continuous spaces. Instead, we utilize data-dependent LSH methods, to ensure a piecewise-continuous mapping, analogous to histogram buckets. The resulting partitionings define regions of the data manifold corresponding to CMS input-to-bucket mapping. EMDE operates on manifolds spanned by vector representations coming from upstream metric representation learning methods (text/image/graph node embeddings). This helps to ensure that semantically similar items frequently share their assigned buckets in the sketch, preserving the geometric prior from upstream embeddings. Combining the compressive properties of CMS, the LSH inspired preservation of geometric priors, with the simplicity of histograms, EMDE is a form of a probability density estimator which can efficiently scale to extremely large input dimensionalities.</p><p>A typical application of EMDE consists of 4 steps: 1) computing multiple independent partitionings of the data manifold (analogous to computing the boundaries of histogram buckets), usually done once; 2) filling the resulting structure with weighted observations, done once for each set of observations (e.g. single user's interactions) to be summarized; 3) using a simple neural network to model input density into output density transformation 4) querying the output structure to obtain density estimates.</p><p>Step 3) is optional and required only for conditional estimation, it can be skipped in case of simple density estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Session-based Recommendation with EMDE</head><p>Below we describe EMDE on the example of item recommendation. The procedure is also displayed in <ref type="figure" target="#fig_0">Figure 2</ref>. We focus on the scenario of SRS, where the data is comprised of user interaction sessions. Based on the items which have appeared in the session so far (input chunk), our objective is to predict the item which will be purchased/clicked next. The application of EMDE to SRS proceeds in the following steps:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Obtaining upstream item embeddings</head><p>As the first step we need to precompute input embeddings using an external embedding method. EMDE can handle all sorts of embeddings, for example capturing interaction data (often interactions of different types, e.g. click, purchase, favorite), item names, attributes, or images. In practice, virtually all popular embedding schemes fulfill the theoretical conditions which make them applicable for our method (see Section 3.4 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Encoding</head><p>A central concept of EMDE is the data manifold M locally embedded in R n . The manifold M is spanned by embedding vectors computed for a particular data modality. Our objective is to perform multiple partitionings of of the data manifold into regions capturing the local metric prior induced by the underlying embedding vectors (see Encoding section in <ref type="figure" target="#fig_0">Figure 2</ref>). As such, semantically similar inputs should be assigned to the same manifold regions frequently. In a statistical interpretation, this operation corresponds to computing bucket boundaries of histograms, given samples from their input distributions.</p><p>To this end, we propose a modified version of LSH algorithm we call Density-dependent LSH (DLSH). We start with choosing K random vectors ri, then for v ∈ M we let</p><formula xml:id="formula_2">hashi(v) = sgn(v · ri − bi), where the bias value bi is drawn from Qi(U ∼ Unif[0, 1]) , where Qi is the quantile func- tion of {v · ri : v ∈ M}.</formula><p>In contrast to LSH, this scheme is density-dependent, cutting the the manifold into nonempty parts, thus avoiding unutilized regions. As each hash independently divides the manifold into two parts, we treat these K binary hashcodes as bits of a K-bit integer. We thus obtain a partitioning of the manifold into 2 K disjoint regions (or n i=0 K i if K &gt; n; we further assume K &lt; n for simplicity), which correspond to the set of all geometric intersections of regions spanned by single-bit hashes. The 2 K regions of the manifold form a single depth level of our sketch, analogous to a simple histogram with 2 K buckets, but in multiple dimensions. As a piecewise-constant estimator, the resolution of a histogram is quite poor -being only capable of outputting as many different values, as there are buckets / regions, with expressivity scaling linearly with structure size.</p><p>To overcome linear scaling with respect to the number of regions, we use a procedure inspired by CMS. Instead of increasing the number of regions into which a manifold is partitioned, we maintain N independent partitionings, akin to different depth levels of a CMS. Since any given point is located in a single region (out of 2 K ) of each independent partitioning (out of N ), we know that it also lies in the geometric intersection of all N regions it belongs to. This intersection can be extremely small, which leads to an exponential growth of the structure's resolution power.</p><p>Thus we perform the above partitioning procedure N times independently, starting with new random vectors ri every time, resulting in a sketch structure of width 2 K and depth N , named by analogy to CMS. In <ref type="figure" target="#fig_0">Figure 2</ref> item embeddings are encoded into a sketch of depth N = 2 and K = 3.</p><p>We apply DLSH to obtain separate partitionings for each input modality. In item recommendation context, for each item, we store N region indices (or buckets) in per-modality matrices M n_items×N . The region indices stored in M are integers in the range [0, 2 K − 1] and form a sparse item encoding, suitable for efficient storage. In <ref type="figure" target="#fig_0">Figure 2</ref>, for item Adidas Sleek Shoes, its row in M is equal to [0, <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>For the purposes of item-set aggregation, we one-hot encode each region index from the per-modality matrices M , obtaining per-modality matrices Bn_items×j, where j ∈ {1, . . . , N × 2 K }. Each row of the matrix B represents an item sketch. Such an item sketch can be interpreted as N concatenated histograms of 2 K buckets each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Aggregation</head><p>Representations of (weighted) item multi-sets are obtained by simple elementwise (weighted) summation of individual item sketches. This follows from the additive compositionality of histograms with the same bucket ranges, or alternatively from the definition of CMS. Thanks to this property, sketches can also be constructed incrementally from data streams, which simplifies their application in industrial settings. Any subsequent normalization of sketches is performed along the width dimension, as every level of depth represents a separate, independent histogram. For a given user and some temporal split into past and future chunks of a session, we encode each input and target item into dense sketches representation (in form of matrix B) and then we aggregate sketches of input items into single sketch S input and sketches of target items into target sketch S target . The aggregations are performed separately for each modality. While in our setting, the target sketch contains only a single item to be predicted, the general formulation admits multiple target items. We arrive at a concise vector representation of weighted, multi-modal multi-sets, efficiently representing user interaction behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Model and loss function</head><p>We use a simple feed-forward neural network as a conditional density estimator with sketch structures for all inputs, outputs and targets -making both the model size and training time fully independent of the total number of items, original upstream embedding sizes, or the lengths of user sessions.</p><p>As a simple L1-normalized histogram can be considered to approximate a probability mass function of a discrete distribution, sketches normalized across width are ensembles of such histograms for many individual distributions. We train our model to minimize mean Kullback-Leibler divergence between individual target and output probability mass functions in the ensemble. For every row in a batch, this entails: 1) L1-normalizing the target sketch S target across width; 2) applying the Sof tmax function to the network output logits S output across width; 3) calculating KLdivergence between targets and outputs across width; 4) averaging the loss across depth. It is worth noting that our formulation bypasses the need for either: a) an extremely wide softmax layer with weight vectors for all items; b) negative sampling; both of which negatively affect stability and training time of standard methods.</p><p>To improve the stability of pre-activations in the neural network, we L2-normalize the input sketch S input across width.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Prediction</head><p>In order to produce a recommendation score for an item, we query the output sketch in a similar way to CMS queries. As the matrix M contains mappings of every item, to a single histogram bucket for all levels of sketch depth, a simple lookup of S output at the respective indices from M is sufficient. This operation can also be efficiently realized in dense batch format amenable to GPU acceleration, as matrix multiplication S output × B. Both versions yield N independent probability estimates from individual elements of the depth-wise ensemble for each item. We aggregate the estimates using geometric mean (see Prediction section in <ref type="figure" target="#fig_0">Figure 2)</ref>. The difference between CMS, which uses minimum operation for aggregation, and our sketches stems from operating on counts (CMS) versus probabilities (EMDE). Strong theoretical arguments for optimality of geometric mean can be found in <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref>, given the interpretation of sketches as an ensemble of density estimators over manifolds. We verify the choice of aggregation function empirically in <ref type="table" target="#tab_5">Table 6</ref>. The resulting probability estimates are unnormalized i.e. don't sum to 1 across all items (normalization can be performed but is unnecessary for the recommendation setting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">EMDE Input: the Requirement of Local Similarity</head><p>EMDE uses DLSH to preserve the metric prior from its input space. Intuitively, the transferred prior has 3 effects: 1) a local smoothing effect for estimates on the data manifold;  2) errors due to hash collisions have a high likelihood to mistake an item for a semantically similar one (still relevant for recommendation); 3) in neural models a hash bucket holding semantically similar items helps to reduce variance and speed up convergence, as opposed to a hash bucket with random items.</p><p>In the absence of any local geometry (e.g. fully random item embeddings), EMDE degenerates to a normalized CMS with random hash codes. In Section 4.3 we show that such a geometry-blind approach yields sub-par results. Thus, it is essential for our method that the property of local similarity under a locally-Euclidean metric holds at least approximately in the input space.</p><p>Naively, it would seem that not all popular methods of deep representation learning follow the metric learning paradigm, optimizing e.g. skip-gram, masked-languagemodel, next sentence prediction, CPC, InfoNCE or Deep-InfoMax objectives <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Fortunately, <ref type="bibr" target="#b31">[32]</ref> show that all the aforementioned self-supervised objectives correspond to InfoNCE, while <ref type="bibr" target="#b32">[33]</ref> observe that InfoNCE has a direct formulation in terms of metric learning. Thus, most existing representation learning methods exhibit local similarity and can be utilized by our method to capture the metric prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EMDE as a Recommender System</head><p>We report results for unimodal EMDE (no multimodal data, only basic user-item interactions found in the original datasets), and EMDE MM (configurations where selected multimodal channels are present). We evaluate all algorithms in the same frameworks of <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b14">[15]</ref>, keeping to their selected performance measures and datasets. In order to disentangle the gains induced by our method from the quality of the embeddings themselves, we use a very simple graph node embedding scheme <ref type="bibr" target="#b39">[40]</ref>, which is the same for representing both text and interaction networks. For embedding text we simply create a graph of item-token edges. We leave experiments with elaborate embeddings such as BERT <ref type="bibr" target="#b28">[29]</ref> for future research.</p><p>We conduct our experiments on a machine with 28 CPUs, 128 GB RAM and a commodity nVidia GeForce RTX 2080 Ti 11GB RAM GPU card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Session-Based Recommendation</head><p>We conduct experiments on six popular datasets used in <ref type="bibr" target="#b14">[15]</ref>. We reuse data preprocessing, training framework, hyperparameter search procedure and evaluation from the framework of <ref type="bibr" target="#b14">[15]</ref> 1 without introducing any changes. Within the framework, each dataset is split into five contiguous in time slices in order to minimize the risk of random effects. For each dataset we locate and use multimodal data whenever possible. We experiment on the following datasets:</p><p>• RETAIL dataset has been collected from a real-world e-commerce website. It consists of behaviour data and item properties such as price, category, vendor, product type etc. The dataset consists of 212,182 actions in 59,962 sessions over 31,968 items.</p><p>• DIGI is an e-commerce dataset shared by the company Diginetica, introduced at a CIKM 2016 challenge. The goal is to predict next item view in each session. As multimodal data we use purchases, product name tokens and items returned by search queries.</p><p>• RSC15 also known as Yoochoose dataset, is an ecommerce dataset used in the 2015 ACM RecSys Challenge. It contains 5,426,961 actions in 1,375,128 sessions over 28,582 items. Purchased products were used as additional data source.</p><p>• 30Music dataset consists of music listening logs obtained from Last.fm. It contains 638,933 actions in 37,333 sessions over 210,633 items. Playlists created by the users were used as additional multimodal data.</p><p>• AOTM is a music dataset containing user-contributed playlists from the Art of the Mix webpage. It contains 306,830 actions in 21,888 sessions over 91,166 items.</p><p>• NOWP contains song playlists collected from Twitter. It consists of 271,177 actions in 27,005 sessions over 75,169 items.</p><p>For AOTM and NOWP datasets, we were unable to find multimodal data with matching item identifiers. Benchmarks. We compare against the benchmark methods from <ref type="bibr" target="#b14">[15]</ref>, which have been fine-tuned extensively by the benchmark's authors. We include two recent graph neural models: SR-GNN <ref type="bibr" target="#b38">[39]</ref> and TAGNN <ref type="bibr" target="#b8">[9]</ref>, running an extensive grid search on their parameters and using the best configurations found on train/validation split from <ref type="bibr" target="#b14">[15]</ref>. We confirm the correctness of our configurations by crosschecking against results reported in the original author papers, using their original data splits (e.g. we achieve 0.1811 MRR@20 for TAGNN on DIGI dataset with the original author train/validation split, while the metric reported by authors is 0.1803 MRR@20). We confirm that the best configurations are the same for both original author data preprocessing and preprocessing from <ref type="bibr" target="#b14">[15]</ref>. Following <ref type="bibr" target="#b14">[15]</ref> we skip datasets requiring more than a week to complete hyperparameter tuning. Therefore we mark TAGNN on RSC15, 30MU, NOWP and SR-GNN on 30MU, RSC15 as timeout.</p><p>Metrics. Hit rate (HR@20) and mean reciprocal rank (MRR@20) are based on measuring to what extent an algorithm is able to predict the immediate next item in a session. Precision (Precision@20), recall (Recall@20) and  mean average precision (MAP@20) consider all items that appear in the currently hidden part of the session. EMDE Configuration. Items interactions are grouped by user, artist (for music datasets) and session. Then we create a graph with those interactions, where items are graph nodes with edges between them if an interaction appears. We embed these graphs with <ref type="bibr" target="#b39">[40]</ref>. As a result, we obtain an embedding vector for each item. For every dataset we perform a small random search over number of iterations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, embedding dimension <ref type="bibr">[512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048,</ref><ref type="bibr">4096]</ref>, items interactions [user, session, artist] and different sources of multimodal data depending on the dataset. We also observe that adding random sketch codes (not based on LSH) for each item improves the model performance, allowing the model to separate very similar items to differentiate their popularity. All input modalities are presented in <ref type="table" target="#tab_8">Table 9</ref> in the Appendix.</p><p>We train a three layer residual feed forward neural network with 3000 neurons in each layer, with leaky ReLU activations and batch normalization. We use Adam <ref type="bibr" target="#b44">[45]</ref> for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999 2 . The initial learning rate was decayed by parameter γ after every epoch. Refer to the Appendix for our full hyperparameter configuration.</p><p>The input of the network consists of two width-wise L2normalized, concatenated, separate sketches, one of them represents the last single item in the session, the second one all other items. In order to create representation of user's behaviour for the second sketch, we aggregate the sketches of user's items, multiplying them with exponential time decay. Decay of sketch between time t1 and t2 is defined as:</p><formula xml:id="formula_3">sketch(t2) = αw (t 2 −t 1 ) sketch(t1)<label>(1)</label></formula><p>2 Standard configuration recommended by <ref type="bibr" target="#b44">[45]</ref> The parameters α and w define decay force. We perform a small random search over network parameters for each dataset, which is summarized in <ref type="table" target="#tab_0">Table 10</ref> in the Appendix. For all datasets we used sketch of width K = 7 and w = 0.01. EMDE Performance. As presentend in <ref type="table" target="#tab_0">Table 1</ref>, EMDE outperforms competing approaches for most metricdataset combinations, although <ref type="bibr" target="#b14">[15]</ref> find that neural methods generally underperform compared to simple heuristics. The results with multimodal data shown in <ref type="table" target="#tab_1">Table 2</ref> (EMDE MM) are significantly better than the basic configuration in all cases. Thus, the ability to easily ingest multimodal data proves important in practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Top-k Recommendation</head><p>We conduct experiments on two popular, real-world, largescale datasets: Netflix Prize <ref type="bibr" target="#b45">[46]</ref> and MovieLens20M. The datasets contain movie ratings from viewers. We reuse the code for dataset preprocessing, training and evaluation from <ref type="bibr" target="#b14">[15]</ref> 3 .</p><p>Benchmarks. We compare against the baselines used by <ref type="bibr" target="#b15">[16]</ref>, including a recent state-of-the-art VAE-based neural model: MultVAE <ref type="bibr" target="#b41">[42]</ref>, and a non-neural algorithm EASE <ref type="bibr" target="#b40">[41]</ref>. EMDE Configuration. On top of EMDE, we use a simple one-hidden-layer feed-forward neural network with 12,000 neurons. We put 80% of randomly shuffled user items into the input sketch, and the remaining 20% into the output sketch to reflect train/test split ratio of items for a single user. In our multimodal configuration we include the interactions of users with disliked items (items which received a rating lower than 4). Detailed information on item embeddings configuration can be found in <ref type="table" target="#tab_7">Table 8</ref> in the Appendix. EMDE Performance. The results are show in <ref type="table" target="#tab_2">Table  3</ref>. We observe that our approach consistently and significantly outperforms the baselines for lower k values in the top-k recommended item rankings for Movielens20M, which is consistent with CMS being a heavy-hitters estimator. In practice, the very top recommended items are key for user satisfaction as they are given the most attention by users, considering the limitation of item display capabilities and user's attention in the real world. For Netflix, we are able to outperform all competitors only on top-1 recommendation, which is probably caused by comparatively lower density   estimation scores we achieve on this dataset (see <ref type="figure" target="#fig_3">Figure  4</ref>). This is probably due to the simplistic graph embedding method we use. <ref type="table" target="#tab_3">Table 4</ref> presents results with added multimodal data (EMDE MM), which again are significantly higher than the unimodal case and competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EMDE as a Density Estimator</head><p>Most kernel density estimation methods such as <ref type="bibr" target="#b46">[47]</ref> are not applicable for our solution since they do not scale to high input dimensionalities or require sampling during inference, thus preventing usage as input/output for neural networks. We compare against a recent SOTA fast hashing-based estimator (HBE) method for multidimensional data <ref type="bibr" target="#b47">[48]</ref>, which utilizes sampling during inference. This system can handle input dimensions on the scale of thousands, which is optimal for embedding systems currently in use. We repeat the Laplacian kernel testing procedure from <ref type="bibr" target="#b47">[48]</ref> on multiple datasets: MovieLens20M and Netflix (embedded with <ref type="bibr" target="#b39">[40]</ref>), GloVe <ref type="bibr" target="#b48">[49]</ref> and MNIST <ref type="bibr" target="#b49">[50]</ref>. Results in <ref type="table" target="#tab_4">Table 5</ref> show that with optimal parameter values EMDE is a competitive density estimator despite its simplicity. Pearson correlation is used as EMDE estimates are un-normalized. <ref type="figure" target="#fig_3">Figure 4</ref> shows the relation between N/K parameters and estimation quality. Large values of N have an ensembling effect due to averaging of smaller regions, increasing N is always beneficial. With too small K the number of created regions is low and the details in data distribution are not represented. Too large K makes similar data points spread over many regions instead of being contained in one representative region and the gain from the metric prior is lost. The optimal value of K needs to be established empirically.</p><p>Note that Fast HBE receives an advantage since the input space is constructed with Laplacian kernel, for which their method is purposefully aligned while EMDE is kernelagnostic. In spite of this, EMDE still proves competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Pure vs. Conditional EMDE</head><p>We study the benefits of using a neural network on top of EMDE. EMDE can be used in two ways:</p><p>1. Pure EMDE. The rationale behind this simple KNNlike approach is that users might like the items which are closest to their previous selection on the embedding manifold (located within the same or close buckets). It is done with a simple EMDE query on the user's liked items sketch.</p><p>2. Conditional EMDE. This approach involves a neural model for conditional estimation, which learns to map between input and output sketches. Reflecting complex patterns in user behavior, the model transforms the input sketch and produces a new sketch at output, on which EMDE query is performed. We use this approach in all our SRS and top-k experiments.</p><p>A marked difference in preformance between these approaches on MovieLens20M dataset is displayed in <ref type="table" target="#tab_6">Table  7</ref> (including also a version of pure EMDE with scores weighted by item popularity via simple multiplication). Conditional EMDE achieves a significant boost in scores due to a number of factors: 1) it learns a global structure, enabling it to model relationships between far away regions of the manifold, while pure EMDE models only the  local structure (as evidenced by the spatial spread of high score points in <ref type="figure" target="#fig_4">Figure 5</ref>); 2) it considers popularity of items where pure EMDE does not, yet only selectively (high score points in <ref type="figure" target="#fig_4">Figure 5</ref> loosely correspond to clusters of popular items); 3) conditional EMDE is fully aligned with the training objective. In this way, conditional EMDE can hit the right balance between uniqueness and popularity, and the spatial distances between recommended items, while being able to model an arbitrary transformation of one density into another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Configuration Selection</head><p>In order to understand the effects of crucial parameters for training and decoding of EMDE, we conduct additional experiments in session-based scenario. We run experiments on RETAIL dataset, treating the best configuration for this dataset as our baseline. Results are reported in <ref type="table" target="#tab_5">Table 6</ref>. Manifold partitioning. In addition to DLSH we verify the impact of partitioning the manifold with product quantization methods (PQ) which decompose high-dimensional space into the Cartesian product of low-dimensional subspaces which are quantized separately. We also analyze its enhanced and optimized version (OPQ) <ref type="bibr" target="#b51">[52]</ref>. We can see that DLSH is a strong baseline, leading at MRR@20 in comparison to other coders. However, we note that OPQ achieves competitive results, which indicates potential for improvement in density-based manifold partitioning methods.</p><p>Score ensembling. We perform point estimates from the output sketch by ensembling independent probability mass functions across sketch depth using geometric mean. While arguments behind the choice of geometric mean for ensembling probability measures can be found in <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref>, we empirically confirm the choice, comparing with: minimum, arithmetic mean and harmonic mean.</p><p>Choice of manifold We investigate how the choice of input embedding manifold influences performance of downstream recommendation tasks. We compare our multimodal item sketches (MM) with uni-modal sketches built on item-user interaction embeddings (inter), item attribute metadata embeddings (metadata), and random item embeddings without a metric prior (random). All sketches have the same dimensions for a fair comparison. Not only random item vectors have the lowest performance due to the lack of a metric prior, but the manifolds for item interaction similarity and item attribute similarity confer different benefits, outperforming each other in MRR@20 and P@20 respectively. The multi-modal sketch is a clear winner in both metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present EMDE -a compact density estimator for high dimensional manifolds inspired by CMS and LSH techniques, especially amenable to treatment with neural networks optimized with KL-divergence loss. We show that both sequential and top-k recommendation problems can be cast in a simple framework of conditional density estimation. Despite simplified treatment of sequential data as weighted multi-sets, our method achieves state-of-theart results in recommendation systems setting. Easy incorporation of multiple data modalities, combined with scalablility and the potential for incremental operation on data streams make EMDE especially suitable for industry applications. Natural extensions to sequence-aware settings, such as item pair encoding or joint encoding of item and positional vectors are promising avenues for future research.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental details</head><p>Here we give an overview of experimental details for the results presented in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Session-based Recommendation: Details</head><p>Our full hyperparameter configurations for each dataset (both basic EMDE and multimodal EMDE MM) are given in <ref type="table" target="#tab_0">Table 10</ref>. Input embeddings for both EMDE and EMDE MM are described in <ref type="table" target="#tab_8">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Top-k Recommender System</head><p>We empirically verify that a wide and simple network is optimal for the top-k problem. We train a simple one-layer feed-forward neural network with 12,000 neurons and leaky ReLU activations. We use Adam <ref type="bibr" target="#b44">[45]</ref> for optimization with a learning rate of 0.001. Batch size for each experiment equals 256. Input sketches are counted on interactions in the same way as in session-based recommendations (Section 1.1). For Netflix and Movielens20M, we use interactions of users with liked items (rating equal or above 4) and disliked items (rating below 4). We embed interactions with our basic embedding scheme, noting that the best results are achived on just 1 iteration of the embedding algorithm. This suggests that the tested top-k datasets are fundamentally different than session-based datasets, where valuable embedding vectors were obtained with even 4 or 5 iterations. Higher density of top-k datasets makes <ref type="bibr" target="#b39">[40]</ref> degrade with more iterations, rendering all items' embedding vectors too similar to each other and losing the cluster structure.</p><p>We use input sketches of size 30x350 (N x 2 K ), and output sketches (counted on just liked item interactions) are of size 30x350. Sketch sizes are estimated on the validation set.</p><p>Datasets. After preprocessing, we obtain 20,108 movies and 116,677/10,000/10,000 users in the train/valid/test sets in MovieLens20M. The lowest number of movie ratings for single user is 5 (due to preprocessing constraints), the median is 38 and maximum is 3,177.</p><p>For Netflix, we obtain 17,768 movies and 383,435/40,000/40,000 users in the train/valid/test  <ref type="table" target="#tab_0">Dataset  RETAIL  RETAIL MD  DIGI /  DIGI MD   RSC15 /  RSC15 MD  NOWP  30M  30M MD  AOTM   Epochs  5  5  5  7  5  50  50  9  Batch size  256  256  512  512  256  512  512  256  lr</ref> 0.004 0.004 0.004 0.0005 0.001 0.0005 0.0005 0.0005 γ 0.5 0.5 0.5 1.0 0.75 1 1 0.9 N 10 10 10 10 10 10 9 9 α 0.95 0.9 0.97 0.9 0.9 0.9 0.9 0.9</p><p>sets. The lowest number of movie ratings for single user is 5 (due to preprocessing constraints), the median is 60 and maximum is 12,206. Metrics. We focus on metrics reported by <ref type="bibr" target="#b15">[16]</ref>: Re-call@K and NDCG@K. Recall is the total percentage of correct items retrieved in ranked item list cut off at the threshold K. NDCG is a measure which additionally considers the exact rankings of the retrieved items -the closer to the actual user ranking, the higher the measure. We rank algorithms by their performance on Recall@20 and NDCG@20, adhering to <ref type="bibr" target="#b15">[16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>EMDE recommender architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>An example of sketch produced by the count-min sketch algorithm. The displayed sketch stores three items, with hash collisions of two items in buckets containing the value 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Density estimation quality with various N/K configurations. Metric reported is Pearson correlation coefficient against true distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example of the effects of 2 modes of operation of EMDE. Pure EMDE is a simple query on user liked items sketch. Cond EMDE has a neural network mapping from user liked item sketches to output sketches. We draw the plots by casting our embedded MovieLens item vectors onto a 2D plane with UMAP<ref type="bibr" target="#b50">[51]</ref> and scaling the color by the score a particular item gets with each mode of recommendation. Both plots are drawn for a single user. Darker color means higher score. Item Popularity shows item popularity scores across the whole dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Session-based recommendation results. Top 5 competitors are displayed for each dataset. * are from<ref type="bibr" target="#b33">[34]</ref> </figDesc><table><row><cell>Model</cell><cell cols="2">MRR@20 P@20</cell><cell>R@20</cell><cell cols="3">HR@20 MAP@20 Model</cell><cell cols="2">MRR@20 P@20</cell><cell>R@20</cell><cell cols="2">HR@20 MAP@20</cell></row><row><cell></cell><cell></cell><cell>RETAIL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RSC15</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell>0.3524</cell><cell>0.0526</cell><cell cols="3">0.4709 0.5879 0.0282</cell><cell>EMDE</cell><cell>0.3104</cell><cell>0.0730</cell><cell>0.4936</cell><cell>0.6619</cell><cell>0.0346</cell></row><row><cell>VS-KNN*</cell><cell>0.3395</cell><cell cols="2">0.0531 0.4632</cell><cell>0.5745</cell><cell>0.0278</cell><cell>CT [35]</cell><cell>0.3072</cell><cell>0.0654</cell><cell>0.471</cell><cell>0.6359</cell><cell>0.0316</cell></row><row><cell>S-KNN*</cell><cell>0.337</cell><cell>0.0532</cell><cell>0.4707</cell><cell>0.5788</cell><cell>0.0283</cell><cell>NARM [5]</cell><cell>0.3047</cell><cell cols="4">0.0735 0.5109 0.6751 0.0357</cell></row><row><cell>TAGNN [9]</cell><cell>0.3266</cell><cell>0.0463</cell><cell>0.4237</cell><cell>0.5240</cell><cell>0.0249</cell><cell>STAMP [5]</cell><cell>0.3033</cell><cell>0.0713</cell><cell>0.4979</cell><cell>0.6654</cell><cell>0.0344</cell></row><row><cell cols="2">Gru4Rec [36] 0.3237</cell><cell>0.0502</cell><cell>0.4559</cell><cell>0.5669</cell><cell>0.0272</cell><cell>SR [37]</cell><cell>0.301</cell><cell>0.0684</cell><cell>0.4853</cell><cell>0.6506</cell><cell>0.0332</cell></row><row><cell>NARM [5]</cell><cell>0.3196</cell><cell>0.044</cell><cell>0.4072</cell><cell>0.5549</cell><cell>0.0239</cell><cell>AR [38]</cell><cell>0.2894</cell><cell>0.0673</cell><cell>0.476</cell><cell>0.6361</cell><cell>0.0325</cell></row><row><cell></cell><cell></cell><cell>DIGI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NOWP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell>0.1724</cell><cell cols="4">0.0602 0.3753 0.4761 0.0258</cell><cell>EMDE</cell><cell>0.1108</cell><cell cols="4">0.0617 0.1847 0.2665 0.0179</cell></row><row><cell>VS-KNN*</cell><cell>0.1784</cell><cell>0.0584</cell><cell>0.3668</cell><cell>0.4729</cell><cell>0.0249</cell><cell>CT [35]</cell><cell>0.1094</cell><cell>0.0287</cell><cell>0.0893</cell><cell>0.1679</cell><cell>0.0065</cell></row><row><cell>S-KNN*</cell><cell>0.1714</cell><cell>0.0596</cell><cell>0.3715</cell><cell>0.4748</cell><cell>0.0255</cell><cell cols="2">Gru4Rec [36] 0.1076</cell><cell>0.0449</cell><cell>0.1361</cell><cell>0.2261</cell><cell>0.0116</cell></row><row><cell>TAGNN [9]</cell><cell>0.1697</cell><cell>0.0573</cell><cell>0.3554</cell><cell>0.4583</cell><cell>0.0249</cell><cell>SR [37]</cell><cell>0.1052</cell><cell>0.0466</cell><cell>0.1366</cell><cell>0.2002</cell><cell>0.0133</cell></row><row><cell cols="2">Gru4Rec [36] 0.1644</cell><cell>0.0577</cell><cell>0.3617</cell><cell>0.4639</cell><cell>0.0247</cell><cell>SR-GNN [39]</cell><cell>0.0981</cell><cell>0.0414</cell><cell>0.1194</cell><cell>0.1968</cell><cell>0.0101</cell></row><row><cell>SR-GNN [39]</cell><cell>0.1551</cell><cell>0.0571</cell><cell>0.3535</cell><cell>0.4523</cell><cell>0.0240</cell><cell>S-KNN*</cell><cell>0.0687</cell><cell>0.0655</cell><cell>0.1809</cell><cell>0.245</cell><cell>0.0186</cell></row><row><cell></cell><cell></cell><cell>30MU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AOTM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell>0.2673</cell><cell cols="4">0.1102 0.2502 0.4104 0.0330</cell><cell>EMDE</cell><cell>0.0123</cell><cell>0.0083</cell><cell>0.0227</cell><cell>0.0292</cell><cell>0.0020</cell></row><row><cell>CT [35]</cell><cell>0.2502</cell><cell>0.0308</cell><cell>0.0885</cell><cell>0.2882</cell><cell>0.0058</cell><cell>CT [35]</cell><cell>0.0111</cell><cell>0.0043</cell><cell>0.0126</cell><cell>0.0191</cell><cell>0.0006</cell></row><row><cell>SR [37]</cell><cell>0.241</cell><cell>0.0816</cell><cell>0.1937</cell><cell>0.3327</cell><cell>0.024</cell><cell>NARM [5]</cell><cell>0.0088</cell><cell>0.005</cell><cell>0.0146</cell><cell>0.0202</cell><cell>0.0009</cell></row><row><cell cols="2">Gru4Rec [36] 0.2369</cell><cell>0.0617</cell><cell>0.1529</cell><cell>0.3273</cell><cell>0.015</cell><cell>STAMP [5]</cell><cell>0.0088</cell><cell>0.002</cell><cell>0.0063</cell><cell>0.0128</cell><cell>0.0003</cell></row><row><cell>NARM [5]</cell><cell>0.1945</cell><cell>0.0675</cell><cell>0.1486</cell><cell>0.2956</cell><cell>0.0155</cell><cell>SR [37]</cell><cell>0.0074</cell><cell>0.0047</cell><cell>0.0134</cell><cell>0.0186</cell><cell>0.001</cell></row><row><cell>VS-KNN*</cell><cell>0.1162</cell><cell>0.109</cell><cell>0.2347</cell><cell>0.383</cell><cell>0.0309</cell><cell>S-KNN*</cell><cell>0.0054</cell><cell cols="2">0.0139 0.039</cell><cell cols="2">0.0417 0.0037</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of adding multimodal data to our sessionbased EMDE recommenders.</figDesc><table><row><cell>Model</cell><cell>MRR@20</cell><cell>P@20</cell><cell>R@20</cell><cell>HR@20</cell><cell>MAP@20</cell></row><row><cell></cell><cell></cell><cell cols="2">RETAIL</cell><cell></cell><cell></cell></row><row><cell>EMDE MM</cell><cell>0.3664</cell><cell>0.0571</cell><cell>0.5073</cell><cell>0.6330</cell><cell>0.0309</cell></row><row><cell>EMDE</cell><cell>0.3524</cell><cell>0.0526</cell><cell>0.4709</cell><cell>0.5879</cell><cell>0.0282</cell></row><row><cell></cell><cell></cell><cell cols="2">RSC15</cell><cell></cell><cell></cell></row><row><cell>EMDE MM</cell><cell>0.3116</cell><cell>0.0743</cell><cell>0.5000</cell><cell>0.6680</cell><cell>0.0352</cell></row><row><cell>EMDE</cell><cell>0.3104</cell><cell>0.0730</cell><cell>0.4936</cell><cell>0.6619</cell><cell>0.0346</cell></row><row><cell></cell><cell></cell><cell>DIGI</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE MM</cell><cell>0.1731</cell><cell>0.0620</cell><cell>0.3849</cell><cell>0.4908</cell><cell>0.0268</cell></row><row><cell>EMDE</cell><cell>0.1724</cell><cell>0.0602</cell><cell>0.3753</cell><cell>0.4761</cell><cell>0.0258</cell></row><row><cell></cell><cell></cell><cell>30MU</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE MM</cell><cell>0.2703</cell><cell>0.1106</cell><cell>0.2503</cell><cell>0.4105</cell><cell>0.0331</cell></row><row><cell>EMDE</cell><cell>0.2673</cell><cell>0.1102</cell><cell>0.2502</cell><cell>0.4104</cell><cell>0.0330</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Top-k recommendation results.</figDesc><table><row><cell>Model</cell><cell cols="7">Recall@1 NDCG@5 Recall@5 NDCG@10 Recall@10 NDCG@20 Recall@20</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MovieLens 20M</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell cols="2">0.0529 0.2017</cell><cell cols="2">0.1662 0.2535</cell><cell>0.2493</cell><cell>0.3053</cell><cell>0.3523</cell></row><row><cell>EASE [41]</cell><cell>0.0507</cell><cell>0.1990</cell><cell>0.1616</cell><cell>0.2530</cell><cell>0.2465</cell><cell>0.3078</cell><cell>0.3542</cell></row><row><cell cols="2">MultVAE [42] 0.0514</cell><cell>0.1955</cell><cell>0.1627</cell><cell>0.2493</cell><cell>0.2488</cell><cell>0.3052</cell><cell>0.3589</cell></row><row><cell>SLIM [43]</cell><cell>0.0474</cell><cell>0.1885</cell><cell>0.1533</cell><cell>0.2389</cell><cell>0.2318</cell><cell>0.2916</cell><cell>0.3350</cell></row><row><cell cols="2">RP3beta [44] 0.0380</cell><cell>0.1550</cell><cell>0.1279</cell><cell>0.2004</cell><cell>0.2007</cell><cell>0.2501</cell><cell>0.3018</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Netflix Prize</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell cols="2">0.0328 0.1512</cell><cell>0.1101</cell><cell>0.1876</cell><cell>0.1652</cell><cell>0.2332</cell><cell>0.2432</cell></row><row><cell>EASE [41]</cell><cell>0.0323</cell><cell>0.1558</cell><cell cols="2">0.1120 0.2050</cell><cell>0.1782</cell><cell>0.2589</cell><cell>0.2677</cell></row><row><cell cols="2">MultVAE [42] 0.0313</cell><cell>0.1485</cell><cell>0.1109</cell><cell>0.1957</cell><cell>0.1756</cell><cell>0.2483</cell><cell>0.2645</cell></row><row><cell>SLIM [43]</cell><cell>0.0307</cell><cell>0.1484</cell><cell>0.1062</cell><cell>0.1952</cell><cell>0.1688</cell><cell>0.2474</cell><cell>0.2552</cell></row><row><cell cols="2">RP3beta [44] 0.0243</cell><cell>0.0946</cell><cell>0.0595</cell><cell>0.1191</cell><cell>0.0863</cell><cell>0.1578</cell><cell>0.1390</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of adding multimodal data to our top-k EMDE recommenders.</figDesc><table><row><cell>Model</cell><cell cols="7">Recall@1 NDCG@5Recall@5 NDCG@10 Recall@10 NDCG@20 Recall@20</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MovieLens 20M</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell cols="7">0.0670 0.2378 0.1963 0.2890 0.2780 0.3358 0.3710</cell></row><row><cell>MM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell>0.0529</cell><cell>0.2017</cell><cell>0.1662</cell><cell>0.2535</cell><cell>0.2493</cell><cell>0.3053</cell><cell>0.3523</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Netflix Prize</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell cols="7">0.0388 0.1574 0.1253 0.2155 0.1875 0.2619 0.2645</cell></row><row><cell>MM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMDE</cell><cell>0.0328</cell><cell>0.1512</cell><cell>0.1101</cell><cell>0.1876</cell><cell>0.1652</cell><cell>0.2332</cell><cell>0.2432</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Density estimation results. Metric reported isPearson correlation coefficient against true distribution created as in<ref type="bibr" target="#b47">[48]</ref>.</figDesc><table><row><cell cols="4">Method GloVE MNIST MovieLens20M Netflix</cell></row><row><cell>EMDE 0.996</cell><cell>0.809</cell><cell>0.983</cell><cell>0.700</cell></row><row><cell>FastHBE 0.997</cell><cell>0.998</cell><cell>0.986</cell><cell>0.695</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study results. Bolded headers indicate parameters used in experiments.</figDesc><table><row><cell cols="2">Partitioning method</cell><cell cols="3">Ensembling method</cell><cell>Input embedding</cell></row><row><cell>Metrics DLSH OPQ</cell><cell>PQ</cell><cell>gmean min</cell><cell>mean</cell><cell>hmean MM</cell><cell cols="2">interact metadata random</cell></row><row><cell cols="6">P@20 0.05564 0.05708 0.0519 0.05564 0.04634 0.05254 0.05172 0.0556 0.05067 0.05144</cell><cell>0.03044</cell></row><row><cell cols="6">MRR@20 0.36493 0.35937 0.35138 0.36493 0.31195 0.35922 0.34112 0.3649 0.35019 0.32813</cell><cell>0.27865</cell></row><row><cell cols="6">MAP@20 0.03016 0.03092 0.02838 0.03016 0.02508 0.02865 0.02785 0.0302 0.02747 0.02789</cell><cell>0.01685</cell></row><row><cell cols="6">R@20 0.49743 0.50752 0.47328 0.49743 0.42569 0.4786 0.46592 0.49743 0.46087 0.46769</cell><cell>0.30577</cell></row><row><cell cols="6">HR@20 0.62015 0.63284 0.58562 0.62015 0.52502 0.59597 0.57769 0.6202 0.57501 0.58113</cell><cell>0.37567</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Pure EMDE vs Conditional EMDE results on MovieLens dataset. Neural network setting is the same as in the experiment in top-k setting. TopPop is a baseline of most popular items.</figDesc><table><row><cell cols="5">Metric Pure EMDE TopPop Pure EMDE+Pop Cond EMDE</cell></row><row><cell>NDCG@20</cell><cell>0.0930</cell><cell>0.1201</cell><cell>0.1582</cell><cell>0.3053</cell></row><row><cell>Recall@20</cell><cell>0.1253</cell><cell>0.1441</cell><cell>0.2003</cell><cell>0.3523</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Input modalities of items for each top-k dataset.</figDesc><table><row><cell>Dataset</cell><cell>input modalities</cell></row><row><cell>Movielens20M</cell><cell>liked items dim 1024 iter 1 disliked items dim 1024 iter 1</cell></row><row><cell>Netflix</cell><cell>liked items dim 1024 iter [1,2] disliked items dim 1024 iter 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Input modalities of items for each session-based dataset.</figDesc><table><row><cell cols="2">Dataset input modalities</cell><cell>Dataset</cell><cell>input modalities</cell></row><row><cell></cell><cell></cell><cell></cell><cell>session dim 1024 iter [2,4]</cell></row><row><cell>RETAIL</cell><cell>session dim 4096 iter [2,3,4,5] user dim 4096 iter [2,3,4,5] random codes</cell><cell>RETAIL MD</cell><cell>user dim 1024 iter [3] property 6 iter 4 dim 1024 property 839 iter 4 dim 1024 property 776 iter 4 dim 1024</cell></row><row><cell></cell><cell></cell><cell></cell><cell>random codes</cell></row><row><cell></cell><cell></cell><cell></cell><cell>session dim 1024 iter [1,2,3,4,5]</cell></row><row><cell></cell><cell>session dim 1024 iter [1,2,5]</cell><cell></cell><cell>purchases iter 4 dim 1024</cell></row><row><cell>DIGI</cell><cell>session dim 2048 iter [1,2,4,5]</cell><cell>DIGI MD</cell><cell>product names iter 4 dim 1024</cell></row><row><cell></cell><cell>random codes</cell><cell></cell><cell>item in search queries iter 4 dim 1024</cell></row><row><cell></cell><cell></cell><cell></cell><cell>random codes</cell></row><row><cell>RSC15</cell><cell>session dim 1024 iter [2,3,4] session dim 2048 iter [2,3,4] random codes</cell><cell>RSC15 MD</cell><cell>session dim 1024 iter [2,3,4] session dim 2048 iter [2,3,4] purchases iter 4 dim 1024 random codes</cell></row><row><cell>30M</cell><cell>session dim 2048 iter [1,2,3,5] artist dim 2048 iter [2,3,4,5] user dim 2048 iter [2,3,4] random codes</cell><cell>30M MD</cell><cell>session dim 2048 iter [1,2,3,5] artist dim 2048 iter [2,3,4,5] user dim 2048 iter [2,3,4] playlist iter 4 dim 1024 random codes</cell></row><row><cell></cell><cell>session dim 2048 iter [1,2,3,4]</cell><cell></cell><cell>session dim 2048 iter [1,2,3]</cell></row><row><cell>NOWP</cell><cell>artist dim 2048 iter [1,2,4,5] users dim 2048 iter [2,4, 5]</cell><cell>AOTM</cell><cell>artist dim 2048 iter [1,4,5] users dim 2048 iter [1,2,3]</cell></row><row><cell></cell><cell>random codes</cell><cell></cell><cell>random codes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Training hyperparameters for each SRS dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/rn5l/session-rec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/MaurizioFD/RecSys2019_DeepLearning_ Evaluation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Evaluation of session-based recommendation algorithms. User Modeling and User-Adapted Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ludewig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="331" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ben-Shimon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tsikinovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Friedmann</surname></persName>
		</author>
		<title level="m">Bracha Shapira, Lior Rokach, and Johannes Hoerle. Recsys challenge 2015 and the yoochoose dataset. In RecSys&apos;15</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">nowplaying-rs: A new benchmark dataset for building context-aware music recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asmita</forename><surname>Poddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Zangerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with top-k gains for session-based recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;18</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Inter-session modeling for sessionbased recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ruocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Steinar Lillestøl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Skrede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langseth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Yanxiang Ling, and Maarten de Rijke. An Intent-Guided Collaborative Machine for Session-Based Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">TAGNN: Target Attentive Graph Neural Networks for Session-Based Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computing Machinery</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Handling Information Loss of Graph Neural Networks for Session-Based Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond Chi-Wing</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unbiasing truncated backpropagation through time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08209</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Accurate, efficient and scalable training of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ripple walk training: A subgraph-based training framework for large and deep graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2002.07206</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Performance comparison of neural and non-neural approaches to session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ludewig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemi</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Latifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In RecSys&apos;19</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Are we really making much progress? a worrying analysis of recent neural recommendation approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Maurizio Ferrari Dacrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jannach</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>RecSys &apos;19</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stamp: Short-term attention/memory priority model for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Refuoe</forename><surname>Mokhosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A simple but hard-to-beat baseline for session-based recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context tree for adaptive session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
		<idno>abs/1806.03733</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hashing-basedestimators for kernel density in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siminelakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rehashing kernel evaluation in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Siminelakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Levis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sublinear race sketches for approximate kernel density estimation on streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">04</biblScope>
			<biblScope unit="page" from="1739" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Race: Sub-linear memory sketches for approximate near-neighbor search on streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An improved data stream summary: The count-min sketch and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<editor>Martín Farach-Colton</editor>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Theoretical Informatics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: Towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Proceedings of the Annual ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Wasserstein barycenter model ensembling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Geometric mean of probability measures and geodesics of fisher information metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuhiro</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Satoh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyprien</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Performance comparison of neural and non-neural approaches to session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ludewig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemi</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Latifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>RecSys &apos;19</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Context tree for adaptive session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A comparison of frequent pattern techniques and a deep learning method for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iman</forename><surname>Kamehkhosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ludewig</surname></persName>
		</author>
		<editor>RecTemp@RecSys</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining association rules between sets of items in large databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 1993 ACM SIGMOD International Conference on Management of Data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cleora: A simple, strong and scalable graph embedding scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bąbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Gołuchowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Michałowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Dąbrowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Embarrassingly shallow autoencoders for sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
		<imprint>
			<publisher>WWW &apos;19</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jebara</surname></persName>
		</author>
		<imprint>
			<publisher>WWW &apos;18</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<title level="m">Sparse linear methods for top-n recommender systems. ICDM &apos;11</title>
		<meeting><address><addrLine>Slim</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Updatable, accurate, diverse, and scalable recommendations for interactive applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Christoffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The netflix prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Lanning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netflix</forename><surname>Netflix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Cup and Workshop in conjunction with KDD</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The fast gauss transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Greengard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Strain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Space and time efficient kernel density estimation in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturs</forename><surname>Backurs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Grossberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Optimized product quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
