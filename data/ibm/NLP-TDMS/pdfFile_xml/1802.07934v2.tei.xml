<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Learning for Semi-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang: Adversarial</forename><surname>Learning</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semi-Segmentation</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Google Cloud</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Learning for Semi-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method for semi-supervised semantic segmentation using an adversarial network. While most existing discriminators are trained to classify input images as real or fake on the image level, we design a discriminator in a fully convolutional manner to differentiate the predicted probability maps from the ground truth segmentation distribution with the consideration of the spatial resolution. We show that the proposed discriminator can be used to improve semantic segmentation accuracy by coupling the adversarial loss with the standard cross entropy loss of the proposed model. In addition, the fully convolutional discriminator enables semi-supervised learning through discovering the trustworthy regions in predicted results of unlabeled images, thereby providing additional supervisory signals. In contrast to existing methods that utilize weakly-labeled images, our method leverages unlabeled images to enhance the segmentation model. Experimental results on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the effectiveness of the proposed algorithm.</p><p>Although CNN-based approaches have achieved astonishing performance, they require an enormous amount of training data. Different from image classification and object detection, semantic segmentation requires accurate per-pixel annotations for each training image, which can cost considerable expense and time. To ease the effort of acquiring high-quality data, semi/weakly-supervised methods have been applied to the task of semantic segmentation. These methods often assume that there are additional annotations on the image level <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, box level [6], or point level <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this paper, we propose a semi-supervised semantic segmentation algorithm based on adversarial learning. The recent success of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">[10]</ref> facilitate effective unsupervised and semi-supervised learning in numerous tasks. A typical GAN consists of two sub-networks, i.e., generator and discriminator, in which these two subnetworks play a min-max game in the training process. The generator takes a sample vector and outputs a sample of the target data distribution, e.g., human faces, while the discriminator aims to differentiate generated samples from target ones. The generator is then trained to confuse the discriminator through back-propagation and therefore generates samples that are similar to those from the target distribution. In this paper, we apply a similar methodology and treat the segmentation network as the generator in a GAN framework. Different from the typical generators that are trained to generate images from noise vectors, our segmentation network outputs the probability maps of the semantic labels given an input image. Under this setting, we enforce the outputs of the segmentation network as close as possible to the ground truth label maps spatially.</p><p>To this end, we adopt an adversarial learning scheme and propose a fully convolutional discriminator that learns to differentiate ground truth label maps from probability maps of segmentation predictions. Combined with the cross-entropy loss, our method uses an adversarial loss that encourages the segmentation network to produce predicted probability maps close to the ground truth label maps in a high-order structure. The idea is similar to the use of probabilistic graphical models such as Conditional Random Fields (CRFs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">44]</ref>, but without the extra post-processing module during the testing phase. In addition, the discriminator is not required during inference, and thus the proposed framework does not increase any computational load during testing. By employing the adversarial learning, we further exploit the proposed scheme under the semi-supervised setting.</p><p>In this work, we combine two semi-supervised loss terms to leverage the unlabeled data. First, we utilize the confidence maps generated by our discriminator network as the supervisory signal to guide the cross-entropy loss in a self-taught manner. The confidence maps indicate which regions of the prediction distribution are close to the ground truth label distribution so that these predictions can be trusted and trained by the segmentation network via a masked cross-entropy loss. Second, we apply the adversarial loss on unlabeled data as adopted in the supervised setting, which encourages the model to predict segmentation outputs of unlabeled data close to the ground truth distributions.</p><p>The contributions of this work are summarized as follows. First, we develop an adversarial framework that improves semantic segmentation accuracy without requiring additional computation loads during inference. Second, we propose a semi-supervised framework and show that the segmentation accuracy can be further improved by adding images without any annotations. Third, we facilitate the semi-supervised learning by leveraging the discriminator network response of unlabeled images to discover trustworthy regions that facilitate the training process for segmentation. Experimental results on the PASCAL VOC 2012 [9] and Cityscapes [5] datasets validate the effectiveness of the proposed adversarial framework for semi-supervised semantic segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation aims to assign a semantic label, e.g., person, dog, or road, to each pixel in images. This task is of essential importance to a wide range of applications, such as autonomous driving and image editing. Numerous methods have been proposed to tackle this task <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, and abundant benchmark datasets have been constructed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref> with focus on different sets of scene/object categories as well as various realworld applications. However, this task remains challenging because of large object/scene appearance variations, occlusions, and lack of context understanding. Convolutional Neural Network (CNN) based methods, such as the Fully Convolutional Network (FCN) <ref type="bibr" target="#b27">[28]</ref>, have recently achieved significant improvement on the task of semantic segmentation, and most state-of-the-art algorithms are based on FCN and additional modules.</p><p>Semantic segmentation. Recent state-of-the-art methods for semantic segmentation are based on the advances of CNNs. As proposed in <ref type="bibr" target="#b27">[28]</ref>, one can transform a classification CNN, e.g., AlexNet <ref type="bibr" target="#b20">[21]</ref>, VGG <ref type="bibr" target="#b38">[39]</ref>, or ResNet <ref type="bibr" target="#b11">[12]</ref>, to a fully-convolutional network (FCN) for the semantic segmentation task. However, it is usually expensive and difficult to label images with pixel-level annotations. To reduce the heavy efforts of labeling segmentation ground truth, numerous weakly-supervised approaches are proposed in recent years. In the weakly-supervised setting, the segmentation network is not trained at the pixel level with the fully annotated ground truth. Instead, the network is trained with various weak-supervisory signals that can be obtained easily. Image-level labels are exploited as the supervisory signal in most recent approaches. The methods in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b34">[35]</ref> use Multiple Instance Learning (MIL) to generate latent segmentation label maps for supervised training. On the other hand, Papandreou et al. <ref type="bibr" target="#b32">[33]</ref> use the image-level labels to penalize the prediction of non-existent object classes, while Qi et al. <ref type="bibr" target="#b36">[37]</ref> use object localization to refine the segmentation. Hong et al. <ref type="bibr" target="#b14">[15]</ref> leverage the labeled images to train a classification network as the feature extractor for deconvolution. In addition to image-level supervisions, the segmentation network can also be trained with bounding boxes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, point supervision <ref type="bibr" target="#b1">[2]</ref>, or web videos <ref type="bibr" target="#b15">[16]</ref>.</p><p>However, these weakly supervised approaches do not perform as well as the fullysupervised methods especially because it is difficult to infer the detailed boundary information from weak-supervisory signals. Hence semi-supervised learning is also considered in some methods to enhance the prediction performance. In such settings, a set of fully-annotated data and weakly-labeled samples are used for training. Hong et al. <ref type="bibr" target="#b14">[15]</ref> jointly train a network with image-level supervised images and a few fully-annotated frames in the encoder-decoder framework. The approaches in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b32">[33]</ref> are generalized from the weakly-supervised to the semi-supervised setting for utilizing additional annotated image data.</p><p>Different from the aforementioned methods, the proposed algorithm can leverage unlabeled images in model training, hence greatly alleviating the task of manual annotation. We treat the output of a fully convolutional discriminator as the supervisory signals, which compensate for the absence of image annotations and enable semi-supervised semantic segmentation. On the other hand, the proposed self-taught learning framework for segmentation is related to <ref type="bibr" target="#b33">[34]</ref> where the prediction maps of unlabeled images are used as ground truth. However, in <ref type="bibr" target="#b33">[34]</ref>, the prediction maps are refined by several hand-designed constraints before training, while we learn the selection criterion for self-taught learning based on the proposed adversarial network model.</p><p>Generative adversarial networks. Since the GAN framework with its theoretical foundation is proposed <ref type="bibr" target="#b9">[10]</ref>, it draws significant attention with several improvements in implementation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref> and applciations including image generation <ref type="bibr" target="#b37">[38]</ref>, superresolution <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>, optical flow <ref type="bibr" target="#b22">[23]</ref>, object detection <ref type="bibr" target="#b41">[42]</ref>, domain adaptation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41]</ref> and semantic segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref>. The work closest in scope to ours is the one proposed by <ref type="bibr" target="#b28">[29]</ref>, where the adversarial network is used to help the training process for semantic segmentation. However, this method does not achieve substantial improvement over the baseline scheme and does not tackle the semi-supervised setting. On the other hand, Souly et al. <ref type="bibr" target="#b39">[40]</ref> propose to generate adversarial examples using GAN for semi-supervised semantic segmentation. However, these generated examples may not be sufficiently close to real images to help the segmentation network since view synthesis from dense labels is still challenging.  <ref type="figure">Figure 1</ref>: Overview of the proposed system for semi-supervised semantic segmentation. With a fully-convolution discriminator network trained using the loss L D , we optimize the segmentation network using three loss functions during the training process: cross-entropy loss L ce on the segmentation ground truth, adversarial loss L adv to fool the discriminator, and semi-supervised loss L semi based on the confidence map, i.e., output of the discriminator.</p><p>3 Algorithm Overview <ref type="figure">Figure 1</ref> shows the overview of the proposed algorithm. The proposed model consists of two modules: segmentation and discriminator networks. The former can be any network designed for semantic segmentation, e.g., FCN <ref type="bibr" target="#b27">[28]</ref>, DeepLab <ref type="bibr" target="#b3">[4]</ref>, and DilatedNet <ref type="bibr" target="#b42">[43]</ref>. Given an input image of dimension H ×W × 3, the segmentation network outputs the class probability maps of size H ×W ×C, where C is the number of semantic categories. Our discriminator network baed on a FCN, which takes class probability maps as the input, either from the segmentation network or ground truth label maps and then outputs spatial probability maps of size H ×W × 1. Each pixel p of the discriminator outputs map represents whether that pixel is sampled from the ground truth label (p = 1) or the segmentation network (p = 0). In contrast to the typical GAN discriminators which take fix-sized input images (64 × 64 in most cases) and output a single probability value, we transform our discriminator to a fully-convolutional network that can take inputs of arbitrary sizes. More importantly, we show this transformation is essential for the proposed adversarial learning scheme.</p><p>During the training process, we use both labeled and unlabeled images under the semisupervised setting. When using the labeled data, the segmentation network is supervised by both the standard cross-entropy loss L ce with the ground truth label map and the adversarial loss L adv with the discriminator network. Note that we train the discriminator network only with the labeled data. For the unlabeled data, we train the segmentation network with the proposed semi-supervised method. After obtaining the initial segmentation prediction of the unlabeled image from the segmentation network, we compute a confidence map by passing the segmentation prediction through the discriminator network. We in turn treat this confidence map as the supervisory signal using a self-taught scheme to train the segmentation network with a masked cross-entropy loss L semi . This confidence map indicates the quality of the predicted segmented regions such that the segmentation network can trust during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semi-Supervised Training with Adversarial Network</head><p>In this section, we present the proposed network architecture and learning schemes for the segmentation as well as discriminator modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Architecture</head><p>Segmentation network. We adopt the DeepLab-v2 <ref type="bibr" target="#b3">[4]</ref> framework with ResNet-101 <ref type="bibr" target="#b11">[12]</ref> model pre-trained on the ImageNet dataset <ref type="bibr" target="#b6">[7]</ref> and MSCOCO <ref type="bibr" target="#b25">[26]</ref> as our segmentation baseline network (see <ref type="figure">Figure 1</ref>). However, we do not use the multi-scale fusion proposed in <ref type="bibr" target="#b3">[4]</ref> since it would occupy all memory of a single GPU and make it impractical to train the discriminator. Similar to the recent semantic segmentation method <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref>, we remove the last classification layer and modify the stride of the last two convolution layers from 2 to 1, thereby making the resolution of the output feature maps effectively 1/8 of the input image size. To enlarge the receptive fields, we apply the dilated convolution <ref type="bibr" target="#b42">[43]</ref> in conv4 and conv5 layers with strides of 2 and 4, respectively. In addition, we use the Atrous Spatial Pyramid Pooling (ASPP) method <ref type="bibr" target="#b3">[4]</ref> in the last layer. Finally, we apply an up-sampling layer along with the softmax output to match the size of the input image.</p><p>Discriminator network. We use the structure similar to <ref type="bibr" target="#b37">[38]</ref> for the discriminator network. It consists of 5 convolution layers with 4 × 4 kernel and {64, 128, 256, 512, 1} channels in the stride of 2. Each convolution layer is followed by a Leaky-ReLU <ref type="bibr" target="#b29">[30]</ref> parameterized by 0.2 except the last layer. To transform the model into a fully convolutional network, an up-sampling layer is added to the last layer to rescale the output to the size of the input map. Note that we do not employ any batch-normalization layer <ref type="bibr" target="#b17">[18]</ref> as it only performs well when the batch size is sufficiently large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Function</head><p>Given an input image X n of size H ×W × 3, we denote the segmentation network by S(·) and the predicted probability map by S(X n ) of size H ×W ×C where C is the category number. We denote the fully convolutional discriminator by D(·) which takes a probability map of size H ×W ×C and outputs a confidence map of size H ×W × 1. In the proposed method, there are two possible inputs to the discriminator network: segmentation prediction S(X n ) or one-hot encoded ground truth vector Y n .</p><p>Discriminator network. To train the discriminator network, we minimize the spatial crossentropy loss L D with respect to two classes using:</p><formula xml:id="formula_0">L D = − ∑ h,w (1 − y n ) log(1 − D(S(X n )) (h,w) ) + y n log(D(Y n ) (h,w) ),<label>(1)</label></formula><p>where y n = 0 if the sample is drawn from the segmentation network, and y n = 1 if the sample is from the ground truth label. In addition, D(S(X n )) (h,w) is the confidence map of X at location (h, w), and D(Y n ) (h,w) is defined similarly. To convert the ground truth label map with discrete labels to a C-channel probability map, we use the one-hot encoding scheme on the ground truth label map where Y (h,w,c) n takes value 1 if pixel X (h,w) n belongs to class c, and 0 otherwise.</p><p>One potential issue with the discriminator network is that it may easily distinguish whether the probability maps come from the ground truth by detecting the one-hot probability <ref type="bibr" target="#b28">[29]</ref>. However, we do not encounter this problem during the training phase. One reason is that we use a fully-convolutional scheme to predict spatial confidence, which increases the difficulty of learning the discriminator. In addition, we evaluate the Scale scheme <ref type="bibr" target="#b28">[29]</ref> where the ground truth probability channel is slightly diffused to other channels according to the distribution of segmentation network output. However, the results show no difference, and thus we do not adopt this scheme in this work.</p><p>Segmentation network. We train the segmentation network by minimizing a multi-task loss function:</p><formula xml:id="formula_1">L seg = L ce + λ adv L adv + λ semi L semi ,<label>(2)</label></formula><p>where L ce , L adv , and L semi denote the spatial multi-class cross entropy loss, adversarial loss, and semi-supervised loss, respectively. In <ref type="formula" target="#formula_1">(2)</ref>, λ adv and λ semi are two weights for minimizing the proposed multi-task loss function. We first consider the scenario of using annotated data. Given an input image X n , its onehot encoded ground truth Y n and prediction result S(X n ), the cross-entropy loss is obtained by:</p><formula xml:id="formula_2">L ce = − ∑ h,w ∑ c∈C Y (h,w,c) n log(S(X n ) (h,w,c) ).<label>(3)</label></formula><p>We use the adversarial learning process through the loss L adv given a fully convolutional discriminator network D(·):</p><formula xml:id="formula_3">L adv = − ∑ h,w log(D(S(X n )) (h,w) ).<label>(4)</label></formula><p>With this loss, we train the segmentation network to fool the discriminator by maximizing the probability of the predicted results being generated from the ground truth distribution.</p><p>Training with unlabeled data. In this work, we consider the adversarial training under the semi-supervised setting. For unlabeled data, we do not apply L ce since there is no ground truth annotation. The adversarial loss L adv is still applicable as it only requires the discriminator network. However, we find that it is crucial to choose a smaller λ adv than the one used for labeled data. It is because the adversarial loss may over-correct the prediction to fit the ground truth distribution without the cross entropy loss. In addition, we use the trained discriminator with unlabeled data within a self-taught learning framework. The main idea is that the trained discriminator can generate a confidence map D(S(X n )) which can be used to infer the regions sufficiently close to those from the ground truth distribution. We then binarize this confidence map with a threshold to highlight the trustworthy region. Furthermore, the self-taught, one-hot encoded ground truthŶ n is element-wise set withŶ (h,w,c * ) n = 1 if c * = arg max c S(X n ) (h,w,c) . The resulting semi-supervised loss is defined by:</p><formula xml:id="formula_4">L semi = − ∑ h,w ∑ c∈C I(D(S(X n )) (h,w) &gt; T semi ) ·Ŷ (h,w,c) n log(S(X n ) (h,w,c) ),<label>(5)</label></formula><p>where I(·) is the indicator function and T semi is the threshold to control the sensitivity of the self-taught process. Note that during training we treat both the self-taught targetŶ n and the value of indicator function as constant, and thus (5) can be simply viewed as a masked spatial cross entropy loss. In practice, we find that this strategy works robustly with T semi ranging between 0.1 and 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Implementation details. We implement the proposed algorithm using the PyTorch framework. We train the proposed model on a single TitanX GPU with 12 GB memory. To train the segmentation network, we use the Stochastic Gradient Descent (SGD) optimization method, where the momentum is 0.9, and the weight decay is 10 −4 . The initial learning rate is set as  2.5 × 10 −4 and is decreased with polynomial decay with power of 0.9 as mentioned in <ref type="bibr" target="#b3">[4]</ref>.</p><p>For training the discriminator, we adopt Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with the learning rate 10 −4 and the same polynomial decay as the segmentation network. For the hyper-parameters in the proposed method, λ adv is set as 0.01 and 0.001 when training with labeled and unlabeled data, respectively. We set λ semi as 0.1 and T semi as 0.2. For semi-supervised training, we randomly interleave labeled and unlabeled data while applying the training scheme described in Section 4.2. Note that, to prevent the model suffering from initial noisy masks and predictions, we start the semi-supervised learning after training for 5000 iterations with labeled data. We update both the segmentation network and discriminator network jointly. In each iteration, only the batch containing the ground truth data are used for training the discriminator. When randomly sampling partial labeled and unlabeled data from the datasets, we average several experiment results with different random seeds to ensure the evaluation robustness. The code and model are available at https://github.com/hfslyc/AdvSemiSeg. Evaluation datasets and metric. In this work, we conduct experiments on two semantic segmentation datasets: PASCAL VOC 2012 <ref type="bibr" target="#b8">[9]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref>. On both datasets, we use the mean intersection-over-union (mean IU) as the evaluation metric.</p><p>The PASCAL VOC 2012 dataset comprises 20 common objects with annotations on daily captured photos. In addition, we utilize the extra annotated images from the Segmentation Boundaries Dataset (SBD) <ref type="bibr" target="#b10">[11]</ref> and obtain a set of total 10,582 training images. We evaluate our models on the standard validation set with 1,449 images. During training, we use the random scaling and cropping operations with size 321 × 321. We train each model on the PASCAL VOC dataset for 20k iterations with batch size 10.</p><p>The Cityscapes dataset contains 50 videos in driving scenes from which 2975, 500, 1525 images are extracted and annotated with 19 classes for training, validation, and testing, respectively. Each annotated frame is the 20-th frame in a 30-frames snippet, where only these images with annotations are considered in the training process. We resize the input image to 512 × 1024 without any random cropping/scaling. We train each model on the Cityscapes dataset for 40k iterations with batch size 2. PASCAL VOC 2012. <ref type="table" target="#tab_0">Table 1</ref> shows the evaluation results on the PASCAL VOC 2012 dataset. To validate the semi-supervising scheme, we randomly sample 1/8, 1/4, 1/2 images as labeled data, and use the rest of training images as unlabeled data. We compare the proposed algorithm against the FCN <ref type="bibr" target="#b27">[28]</ref>, Dilation10 <ref type="bibr" target="#b42">[43]</ref>, and DeepLab-v2 <ref type="bibr" target="#b3">[4]</ref> methods. to demonstrate that our baseline model performs comparably with the state-of-the-art schemes. Note that our baseline model is equivalent to the DeepLab-v2 model without multi-scale fusion. The adversarial loss brings consistent performance improvement (from 1.6%to2.8%) over different amounts of training data. Incorporating the proposed semi-supervised learning scheme brings overall 3.5% to 4.0% improvement. <ref type="figure">Figure 2</ref> shows visual comparisons of the segmentation results generated by the proposed method. We observe that the segmentation boundary achieves significant improvement when compared to the baseline model. <ref type="bibr">image</ref> annotation baseline +L adv +L adv + L semi <ref type="figure">Figure 2</ref>: Comparisons on the PASCAL VOC 2012 dataset using 1/2 labeled data.  Cityscapes. <ref type="table" target="#tab_1">Table 2</ref> shows evaluation results on the Cityscapes dataset. By applying the adversarial loss L adv , the model achieves 0.5% to 1.9% gain over the baseline model under the semi-supervised setting. This shows that our adversarial training scheme can encourage the segmentation network to learn the structural information from the ground truth distribution.</p><p>Combining with the adversarial learning and the proposed semi-supervised scheme, the proposed algorithm achieves the performance gain of 1.6% to 3.3%.</p><p>Comparisons with state-of-the-art methods. <ref type="table" target="#tab_2">Table 3</ref> shows comparisons with <ref type="bibr" target="#b28">[29]</ref> which utilizes adversarial learning for segmentation. There are major differences between <ref type="bibr" target="#b28">[29]</ref> and our method in the adversarial learning processes. First, we design a universal discriminator for various segmentation tasks, while <ref type="bibr" target="#b28">[29]</ref> utilizes one network structures for each dataset. Second, our discriminator does not require RGB images as additional inputs but directly operates on the prediction maps from the segmentation network. <ref type="table" target="#tab_2">Table 3</ref> shows that our method achieves 1.2% gain in mean IU over the method in <ref type="bibr" target="#b28">[29]</ref>. We present the results under the semi-supervised setting in <ref type="table" target="#tab_3">Table 4</ref>. To compare with <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b39">[40]</ref>, our model is trained on the original PASCAL VOC 2012 train set (1,464 images) and use the SBD <ref type="bibr" target="#b10">[11]</ref> set as unlabeled data. It is worth noticing that in <ref type="bibr" target="#b32">[33]</ref>, image-level labels are available for the SBD <ref type="bibr" target="#b10">[11]</ref> set, and in <ref type="bibr" target="#b39">[40]</ref> additional unlabeled images are generated through their generator during the training stage.</p><p>Hyper-parameter analysis. The proposed algorithm is governed by three hyper parameters: λ adv and λ semi for balancing the multi-task learning in <ref type="bibr" target="#b1">(2)</ref>, and T semi used to control the sensitivity in the semi-supervised learning described in <ref type="bibr" target="#b4">(5)</ref>. <ref type="table" target="#tab_7">Table 8</ref> shows sensitivity analysis on hyper parameters using the PASCAL VOC dataset under the semi-supervised setting. More analysis and results are provided in the supplementary material.</p><p>We first show comparisons of different values of λ semi with 1/8 amount of data under the semi-supervised setting. We set λ adv = 0.01 and T semi = 0.2 for the comparisons. Overall, the input image confidence map input image confidence map <ref type="figure">Figure 3</ref>: Visualization of the confidence maps. Given the prediction results generated by the segmentation network, the confidence maps are obtained from the discriminator. In the confidence maps, the brighter regions indicate that they are closer to the ground truth distribution, and we utilize these brighter regions for semi-supervised learning.  proposed method achieves the best mean IU of 69.5% with 1.9% gain. when λ semi is set to 0.1. Second, we perform the experiments with different values of T semi by setting λ adv = 0.01 and λ semi = 0.1 . With higher T semi , the proposed model only trusts regions of high structural similarity as the ground truth distribution. Overall, the proposed model achieves the best results when T semi = 0.2 and performs well for a wide range of T semi (0.1 to 0.3). When T semi = 0, we trust all the pixel predictions in unlabeled images, which results in performance degradation. <ref type="figure">Figure 3</ref> shows sample confidence maps from the predicted probability maps.</p><p>Ablation study. We present the ablation study of our proposed system in <ref type="table" target="#tab_5">Table 6</ref> on the PASCAL VOC dataset. First, we examine the effect of using fully convolutional discriminator (FCD). To construct a discriminator that is not fully-convolutional, we replace the last convolution layer of the discriminator with a fully-connected layer that outputs a single neuron as in typical GAN models. Without using FCD, the performance drops 1.0% and 0.9% with all and one-eighth data, respectively. This shows that the use of FCD is essential to adversarial learning. Second, we apply the semi-supervised learning method without the adversarial loss. The results show that the adversarial training on the labeled data is important to our semi-supervised scheme. If the segmentation network does not seek to fool the discriminator, the confidence maps generated by the discriminator would be meaningless, providing weaker supervisory signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we propose an adversarial learning scheme for semi-supervised semantic segmentation. We train a discriminator network to enhance the segmentation network with both labeled and unlabeled data. With labeled data, the adversarial loss for the segmentation network is designed to learn higher order structural information without post-processing. For unlabeled data, the confidence maps generated by the discriminator network act as the selftaught signal for refining the segmentation network. Extensive experiments on the PASCAL VOC 2012 and Cityscapes datasets validate the effectiveness of the proposed algorithm.</p><p>A Pixel Accuracy in Semi-Supervised Learning</p><p>In <ref type="table" target="#tab_6">Table 7</ref>, we show the average segmentation accuracy with respect to the number of selected pixels based on different threshold values of T semi as in (5) of the paper on the Cityscapes dataset. With a higher T semi , the discriminator outputs are more confident (similar to ground truth label distributions) and lead to more accurate pixel predictions. Also, as a trade-off, the higher threshold (T semi ), the fewer pixels are selected for back-propagation. This trade-off could also be observed in <ref type="table" target="#tab_4">Table 5</ref> of the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Hyper-parameter Analysis</head><p>In <ref type="table" target="#tab_7">Table 8</ref>, we show the complete hyper-parameter analysis. In addition to the analysis of λ semi and T semi in <ref type="table" target="#tab_4">Table 5</ref> of the paper, we show that the proposed adversarial learning is also robust to different values of λ adv . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Parameters</head><p>In <ref type="table" target="#tab_8">Table 9</ref>, we show the training parameters for both datasets. We use the PyTorch implementation, and we will release our code and models for the public.</p><p>14HUNG, TSAI, LIOU, LIN, YANG: ADVERSARIAL LEARNING FOR SEMI-SEGMENTATION </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Qualitative Results</head><p>In <ref type="figure">Figure 4</ref>-5, we show additional qualitative comparisons with the models using half training data of the PSCAL VOC dataset. In <ref type="figure" target="#fig_2">Figure 6</ref>, we show more qualitative comparisons with the models using half training data of the Cityscapes dataset. The results show that both the adversarial learning and the semi-supervised training scheme improve the segmentation quality.</p><p>image annotation baseline +L adv +L adv + L semi  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Comparisons on the PASCAL VOC dataset using 1/2 training data. 16HUNG, TSAI, LIOU, LIN, YANG: ADVERSARIAL LEARNING FOR SEMI-SEGMENTATION image annotation baseline +L adv +L adv + L semi Comparisons on the PASCAL VOC dataset using 1/2 training data. HUNG, TSAI, LIOU, LIN, YANG: ADVERSARIAL LEARNING FOR SEMI-SEGMENTATION 17 image annotation baseline +L adv +L adv + L semi</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Comparisons on the Cityscapes dataset using 1/2 training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the VOC 2012 val set.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Data Amount</cell><cell></cell></row><row><cell>Methods</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>Full</cell></row><row><cell>FCN-8s [28]</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>67.2</cell></row><row><cell>Dilation10 [43]</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>73.9</cell></row><row><cell>DeepLab-v2 [4]</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>77.7</cell></row><row><cell>our baseline</cell><cell>66.0</cell><cell>68.3</cell><cell>69.8</cell><cell>73.6</cell></row><row><cell>baseline + L adv</cell><cell>67.6</cell><cell>71.0</cell><cell>72.6</cell><cell>74.9</cell></row><row><cell>baseline + L adv + L semi</cell><cell>69.5</cell><cell>72.1</cell><cell>73.8</cell><cell>N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on the Cityscapes val set.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Data Amount</cell><cell></cell></row><row><cell>Methods</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>Full</cell></row><row><cell>FCN-8s [28]</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>65.3</cell></row><row><cell>Dilation10 [43]</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>67.1</cell></row><row><cell>DeepLab-v2 [4]</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>70.4</cell></row><row><cell>our baseline</cell><cell>55.5</cell><cell>59.9</cell><cell>64.1</cell><cell>66.4</cell></row><row><cell>baseline + L adv</cell><cell>57.1</cell><cell>61.8</cell><cell>64.6</cell><cell>67.7</cell></row><row><cell>baseline + L adv + L semi</cell><cell>58.8</cell><cell>62.3</cell><cell>65.7</cell><cell>N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Adversarial learning comparison with [29] on the VOC 2012 validation set.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell>Adversarial</cell></row><row><cell>[29]</cell><cell>71.8</cell><cell>72.0</cell></row><row><cell>ours</cell><cell>73.6</cell><cell>74.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Semi-supervised learning comparisons on the VOC 2012 validation set without using additional labels of the SBD.</figDesc><table><row><cell></cell><cell>Data</cell><cell>Fully-</cell><cell>Semi-</cell></row><row><cell></cell><cell>Amount</cell><cell>supervised</cell><cell>supervised</cell></row><row><cell>[33]</cell><cell>Full</cell><cell>62.5</cell><cell>64.6</cell></row><row><cell>[40]</cell><cell>Full</cell><cell>59.5</cell><cell>64.1</cell></row><row><cell>ours</cell><cell>Full</cell><cell>66.3</cell><cell>68.4</cell></row><row><cell>[40]</cell><cell>30%</cell><cell>38.9</cell><cell>42.2</cell></row><row><cell>ours</cell><cell>30%</cell><cell>57.4</cell><cell>60.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hyper parameter analysis.</figDesc><table><row><cell>Data Amount</cell><cell>λ adv</cell><cell>λ semi</cell><cell>T semi</cell><cell>Mean IU</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0</cell><cell>N/A</cell><cell>67.6</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.05</cell><cell>0.2</cell><cell>68.4</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0.2</cell><cell>69.5</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.2</cell><cell>0.2</cell><cell>69.1</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0</cell><cell>67.2</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0.1</cell><cell>68.8</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0.2</cell><cell>69.5</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0.3</cell><cell>69.2</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>1.0</cell><cell>67.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of the proposed method on the PASCAL VOC dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Data Amount</cell></row><row><cell>L adv</cell><cell>L semi</cell><cell>FCD</cell><cell>1/8</cell><cell>Full</cell></row><row><cell></cell><cell></cell><cell></cell><cell>66.0</cell><cell>73.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.6</cell><cell>74.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>66.6</cell><cell>74.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>65.7</cell><cell>N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell>69.5</cell><cell>N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Selected pixel accuracy.</figDesc><table><row><cell cols="3">T semi Selected Pixels (%) Accuracy</cell></row><row><cell>0</cell><cell>100%</cell><cell>92.65%</cell></row><row><cell>0.1</cell><cell>36%</cell><cell>99.84%</cell></row><row><cell>0.2</cell><cell>31%</cell><cell>99.91%</cell></row><row><cell>0.3</cell><cell>27%</cell><cell>99.94%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Hyper parameter analysis.</figDesc><table><row><cell>Data Amount</cell><cell>λ adv</cell><cell cols="3">λ semi T semi Mean IU</cell></row><row><cell>Full</cell><cell>0</cell><cell>0</cell><cell>N/A</cell><cell>73.6</cell></row><row><cell>Full</cell><cell>0.005</cell><cell>0</cell><cell>N/A</cell><cell>74.0</cell></row><row><cell>Full</cell><cell>0.01</cell><cell>0</cell><cell>N/A</cell><cell>74.9</cell></row><row><cell>Full</cell><cell>0.02</cell><cell>0</cell><cell>N/A</cell><cell>74.6</cell></row><row><cell>Full</cell><cell>0.04</cell><cell>0</cell><cell>N/A</cell><cell>74.1</cell></row><row><cell>Full</cell><cell>0.05</cell><cell>0</cell><cell>N/A</cell><cell>73.0</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0</cell><cell>N/A</cell><cell>67.6</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.05</cell><cell>0.2</cell><cell>68.4</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0.2</cell><cell>69.5</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.2</cell><cell>0.2</cell><cell>69.1</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0</cell><cell>67.2</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0.1</cell><cell>68.8</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0.2</cell><cell>69.5</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>0.3</cell><cell>69.2</cell></row><row><cell>1/8</cell><cell>0.01</cell><cell>0.1</cell><cell>1.0</cell><cell>67.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Training parameters.</figDesc><table><row><cell>Parameter</cell><cell cols="2">Cityscaps PASCAL VOC</cell></row><row><cell>Trained iterations</cell><cell>40,000</cell><cell>20,000</cell></row><row><cell>Learning rate</cell><cell>2.5e-4</cell><cell>2.5e-4</cell></row><row><cell>Learning rate (D)</cell><cell>1e-4</cell><cell>1e-4</cell></row><row><cell>Polynomial decay</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>Momentum</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>Optimizer</cell><cell>SGD</cell><cell>SGD</cell></row><row><cell>Optimizer (D)</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Nesterov</cell><cell>True</cell><cell>True</cell></row><row><cell>Batch size</cell><cell>2</cell><cell>10</cell></row><row><cell>Weight decay</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>Crop size</cell><cell>512x1024</cell><cell>321x321</cell></row><row><cell>Random scale</cell><cell>No</cell><cell>Yes</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">HUNG, TSAI, LIOU, LIN, YANG: ADVERSARIAL LEARNING FOR SEMI-SEGMENTATION</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">HUNG, TSAI, LIOU, LIN, YANG: ADVERSARIAL LEARNING FOR SEMI-SEGMENTATION</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. W.-C. Hung is supported in part by the NSF CAREER Grant #1149783, gifts from Adobe and NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">WhatâȂŹs the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixellevel adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cycada: Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghun</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scene parsing with global context embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for optical flow with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Dan Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation via Deep Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-class generative adversarial networks with the l2 loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04076</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Role of Context for Object Detection and Semantic Segmentation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Augmented feedback in semantic segmentation under image level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi and weakly supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
