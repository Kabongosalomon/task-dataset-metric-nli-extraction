<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncovering Coresets for Classification With Multi-Objective Evolutionary Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Barbiero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Squillero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Tonda</surname></persName>
						</author>
						<title level="a" type="main">Uncovering Coresets for Classification With Multi-Objective Evolutionary Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A coreset is a subset of the training set, using which a machine learning algorithm obtains performances similar to what it would deliver if trained over the whole original data. Coreset discovery is an active and open line of research as it allows improving training speed for the algorithms and may help human understanding the results. Building on previous works, a novel approach is presented: candidate corsets are iteratively optimized, adding and removing samples. As there is an obvious trade-off between limiting training size and quality of the results, a multiobjective evolutionary algorithm is used to minimize simultaneously the number of points in the set and the classification error. Experimental results on non-trivial benchmarks show that the proposed approach is able to deliver results that allow a classifier to obtain lower error and better ability of generalizing on unseen data than state-of-theart coreset discovery techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A coreset is like a small set of paradigmatic examples: a concept can be more effectively explained to a learner resorting to them than by enumerating a longer list of cases. Such an analogy, however, should not be pushed too far: a coreset in Machine Learning (ML) is more formally defined as the minimal set of training samples that allows a supervised algorithm to deliver a result as good as the one obtained when the whole set is used <ref type="bibr" target="#b1">(Bachem et al., 2017)</ref>. Traditionally, the process of discovering a coreset consists in pinpointing the minimal subset of the data that is sufficient to achieve such good performances. The definition does not specify what is the task of the algorithm <ref type="bibr">(classification, regression, or other)</ref>, nor what is the quantitative measure used to evaluate its performances. The relevance of coresets * Equal contribution 1 DAUIN, Politecnico di Torino, Torino, Italy 2 UMR 518 MIA, INRAE, Université Paris-Saclay, France. Correspondence to: Pietro <ref type="bibr">Barbiero &lt;pietro.barbiero@polito.it&gt;.</ref> is manifold: reducing the size of the training set can boost the performance of training, limit the memory requirements; and pinpointing the key elements required may provide an insight on the internal process, helping to explain, if not understand, ML results.</p><p>While remarkable contributions to the problem date back to 1960s <ref type="bibr">(M. A., 1960)</ref>, discovering coresets for a specific ML task is still an open and active research line. Plainly, reducing the number of samples could impair the performance of the ML algorithm, and sometimes even state-of-the-art methodologies may not be able to preserve all the information of the original training set. However, coresets have so many advantages that the reduced quality might be acceptable. Therefore, the problem should be more productively framed as multi-objective: minimize the size of the corset and maximize the quality of the final result, and eventually let the user to pinpoint the best compromise of these two conflicting goals. Moreover, as ML algorithms employ different techniques to accomplish the same goals, it is also reasonable to assume that they would need coresets of different size and shape to operate at the best of their possibilities. Starting from these two assumptions, and building on previous works <ref type="bibr" target="#b2">(Barbiero &amp; Tonda, 2019a;</ref><ref type="bibr" target="#b2">Barbiero et al., 2019)</ref>, an Evolutionary Algorithm (EA) for coreset discovery is proposed.</p><p>When compared to similar works in literature <ref type="bibr" target="#b2">(Barbiero et al., 2019)</ref>, the proposed EA improves the state of the art as follows:</p><p>• it exploits a new representation of coreset candidates in the EA, making it possible to tackle datasets of larger size;</p><p>• it does not require parameter tuning, as all relevant parameters are empirically derived from the size of the dataset.</p><p>The proposed approach exploits an Evolutionary Algorithm (EA) to drive the automatic selection of coresets. Preliminary experiments suggest that classifiers trained with such evolved samples are able to generalize better than those trained with the whole set. The rest of paper is organized as follows: a few necessary background elements are reported arXiv:2002.08645v1 [cs.</p><p>LG] 20 Feb 2020</p><p>in Section 2, the proposed approach is outlined in Section 3, while Section 4 reports the experimental evaluation, and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>ML literature reports a number of approaches for the identification of coresets in different scenarios, starting from Forward Stagewise published in the 1966 <ref type="bibr">(M. A., 1960)</ref> up to the Greedy Iterative Geodesic Ascent (GIGA) appeared in 2018 <ref type="bibr">(Campbell &amp; Broderick, 2018)</ref>. Other remarkable contributions to this research include Matching Pursuit <ref type="bibr" target="#b13">(Mallat &amp; Zhang, 1993)</ref>, Orthogonal Matching Pursuit (Ortho Pursuit) <ref type="bibr" target="#b16">(Pati et al., 1993)</ref>, Frank-Wolfe <ref type="bibr" target="#b4">(Clarkson, 2010)</ref>, and Least-angle regression (LAR) <ref type="bibr" target="#b6">(Efron et al., 2004;</ref><ref type="bibr">Boutsidis et al., 2013)</ref>. The original Frank-Wolfe algorithm applies in the context of maximizing a concave function within a feasible polytope by means of a local linear approximation.</p><p>In Section 4, we refer to the Bayesian implementation of the Frank-Wolfe algorithm designed for core set discovery. This technique, described in <ref type="bibr">(Campbell &amp; Broderick, 2017)</ref>, aims to find a linear combination of approximated likelihoods (which depends on the core set samples) that is similar to the full likelihood as much as possible. GIGA is a greedy algorithm that further improves Frank-Wolfe. In <ref type="bibr">(Campbell &amp; Broderick, 2018)</ref>, the authors show that computing the residual error between the full and the approximated likelihoods by using a geodesic alignment guarantees a lower upper bound to the error at the same computational cost. On the other hand, Forward Stagewise, Least-angle regression, Matching Pursuit and Orthogonal Matching Pursuit were all originally devised as greedy algorithms for dimensionality reduction, but have been later applied to coreset discovery, as this last problem represents the transpose of feature selection, choosing samples instead of features. The simplest of the group is Forward Stagewise, which projects highdimensional data in a lower dimensional space by selecting, one at a time, the features whose inclusion in the model gives the most statistically significant improvement. Matching Pursuit, on the other hand, includes features having the highest inner product with a target signal, while its improved version Orthogonal Matching Pursuit at each step carries an orthogonal projection out. Similarly, Least-angle regression increases the weight of each feature in the direction equiangular to its correlations with the target signal.</p><p>While all the above approaches take the size of the coreset as an input of the problem, recently multi-objective EAs were proposed to determine the best trade-off between final performances and coreset size <ref type="bibr" target="#b2">(Barbiero &amp; Tonda, 2019a;</ref><ref type="bibr" target="#b2">Barbiero et al., 2019)</ref>. Since the 1990s, Evolutionary Computation (EC) techniques have been used to optimize ML frameworks, demonstrating the possibility to determine semi-optimal topologies or to efficiently train artificial neu-ral networks <ref type="bibr" target="#b0">(Angeline et al., 1994;</ref><ref type="bibr" target="#b14">Maniezzo, 1994;</ref><ref type="bibr" target="#b7">Frean, 1990)</ref>. A topic where the use of EC soon appeared promising is feature selection <ref type="bibr" target="#b20">(Vafaie &amp; De Jong, 1992;</ref><ref type="bibr" target="#b10">Kim et al., 2000)</ref>: as the performance of ML algorithms are quite sensitive to the choice of the features, a smart selection procedure is quite beneficial, and EC allowed to automatize the procedure. Selecting features using the eventual performance of a ML algorithm as fitness function is indisputably related to the problem of selecting training-set elements with the same goal.</p><p>EAs are stochastic optimization techniques, loosely inspired by the neo-Darwinian paradigm of natural selection. Candidate solutions (individuals) are encoded in appropriate data structures (genomes); the algorithm manipulates a set of them (population), trying to generate better ones. An evaluation function (fitness function) is used to assess the extent to which a individual solves the problem; such value controls the probability that the individual is selected for reproduction and survival. In each discrete step of the algorithm (generation), new individual (offspring) are first generated using recombination and mutation (cumulatively called genetic operators), then evaluated, and eventually the less fit are discarded. The EA stops when a user-defined threshold is reached, typically a limit on the number of generations.</p><p>Among the most successful applications of EC, multiobjective optimization often takes the center stage. Optimization problems with contrasting objectives have no single optimal solution. Each candidate represents a different compromise between the multiple conflicting aims. Yet, it is still possible to search for optimal trade-offs, for which an objective cannot be improved without degrading the others. The set of such optimal compromises is called Pareto front from the 19 th century Italian engineer Vilfredo Pareto. Multi-objective evolutionary algorithms (MOEA) currently represent the state of the art for problems with contradictory objectives, and are able to obtain good approximations of the true Pareto front in a reasonable amount of time. One of the most known MOEAs is the Non-Sorting Genetic Algorithm II (NSGA-II) <ref type="bibr" target="#b5">(Deb et al., 2002)</ref>, that makes use of a special mechanism to spread candidate solutions on the Pareto front as evenly as possible, with considerable efficiency for problems with few objectives.</p><p>Interestingly, conflicting objectives to optimize abound in ML, with models being trade-offs between fitting and complexity, and coresets being compromises between number of samples considered and quality of the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed approach</head><p>In ML, coresets are defined as a subset of the initial (training) dataset that, used on its own, does not significantly  <ref type="figure">Figure 1</ref>. Scheme of the proposed approach. Given a dataset, split into training and test, the original training set is used to seed the initial population with coresets, consisting of sets training samples. Candidate solutions are evaluated on compactness (number of samples) and error (classifier trained on a candidate solution, tested on the original training set). New solutions are obtained through evolutionary operators. Once the evolution is complete, the coresets on the Pareto front undergo a final evaluation, training the classifier that will then compute its error on the (unseen) original test set, to evaluate the generality of the approach.</p><p>impact the quality of the results of a ML algorithm with respect to the original dataset. In other terms, coresets can be seen as a summary of the information contained in the original dataset, encompassing all the essential data to obtain the correct behavior of the ML algorithm.</p><p>As the search space of all possible coresets of varying size for a given problem is clearly vast, it is necessary to resort to stochastic optimization to efficiently explore it. As said before, oreset discovery is inherently a multi-objective problem. Building on previous works on evolutionary coreset discovery <ref type="bibr" target="#b2">(Barbiero &amp; Tonda, 2019a;</ref><ref type="bibr" target="#b2">Barbiero et al., 2019)</ref>, the proposed algorithm extends and improves the evolutionary approach making it suitable for tackling datasets of larger size without requiring parameter tuning. While the focus of this work is on coreset discovery for classification, it could easily be extended to regression problems. A summary of the proposed algorithm is presented in <ref type="figure">Figure 1</ref>.</p><p>The algorithm can be summarized as follows. The dataset D is split into three groups for training (D t , 90% of samples), validating (D v , 10% of samples), and testing (D u , 10% of samples). Given the training sample</p><formula xml:id="formula_0">s i = ( x i , y i ) ∈ D t ,</formula><p>where x i is the feature vector of s i and y i the corresponding class label, respectively, the objective is to estimate the probability that it belongs to the coreset C j , given the ML model (i.e., the classifier) f :</p><formula xml:id="formula_1">p(s i ∈ C j |f )<label>(1)</label></formula><p>Each coreset candidate C j is then used to fit the model parameters:</p><formula xml:id="formula_2">p(f j |C j )<label>(2)</label></formula><p>Finally, the trained classifier is used to make inferences on the training set:</p><formula xml:id="formula_3">p(y i t | x i t , f j )<label>(3)</label></formula><p>The proposed approach makes it possible to evolve several solutions C j approximating the coreset problem, reducing both the set size and the classification error η:</p><formula xml:id="formula_4">arg min Cj |C j | η(f j , D t )<label>(4)</label></formula><p>The classification error has been defined as 1 minus the weighted F 1 score <ref type="bibr" target="#b18">(Sørensen et al., 1948;</ref><ref type="bibr" target="#b3">Chinchor, 1991)</ref> to account for class imbalance in multi-class datasets:</p><formula xml:id="formula_5">η = 1 − 1 l∈L |y l | l∈L |y l |F 1 (ŷ l , y l )<label>(5)</label></formula><p>where L is the set of labels,ŷ the set of predicted sample/label pairs, y the set of true sample/label pairs, y l the subset of y with label l, and F 1 given by:</p><formula xml:id="formula_6">F 1 = 2 × precision × recall precision + recall<label>(6)</label></formula><p>At the end of the evolution, the validation set D v is used in order to evaluate the final solutions along the Pareto front. The maximum likelihood estimation for the evolved solutions is given by the core set providing the best score on the validation set:</p><formula xml:id="formula_7">C M LE = arg max Cj 1 − η(f j , D v )<label>(7)</label></formula><p>Finally,Ĉ M LE is used to train the model:</p><formula xml:id="formula_8">p(f M LE |Ĉ M LE )<label>(8)</label></formula><p>to make inferences on an unseen test set D u and to compute the classification error:</p><formula xml:id="formula_9">η(f M LE , D u )<label>(9)</label></formula><p>3.1. Genotype of a candidate solution</p><p>The genotype of a candidate solution is represented by a list of integers encoding the position of core samples in the original dataset. The list size may vary for different candidate solutions according to the number of core samples selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fitness functions</head><p>The two fitness functions used in this multi-objective problem are the size of the coreset (to be minimized), and the error of the target classifier trained on core samples, evaluated on the original dataset (to be minimized).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Population initialization</head><p>The starting population of the MOEA is initialized with coresets of random size. The minimum size corresponds to the number of classes in the problem, so that each candidate solution has at least one data point associated to each class; the maximum size is defined as a 1/10 of the original dataset size. Data points in each of such coresets are randomly drawn from the original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evolutionary operators</head><p>When generating new candidate solutions, the MOEA randomly selects two individuals and applies the following operators: i. cross over the two candidate solutions, by randomly distributing core samples contained in both between two children solutions; ii. mutate the children solutions adding or removing one sample from their respective list;</p><p>iii. repair children if the obtained representations violate feasibility constraints (e.g. minimum number of classes, maximum and minimum coreset size).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Parameters setting</head><p>Most parameters in an EA can be derived from a single value, the population size µ. In many practical applications, µ is set through a trial-and-error approach; but there are alternatives, for example the empirical formula used by the state-of-theart single-objective optimizer Covariance Matrix Adaptation Evolution Strategy <ref type="bibr" target="#b9">(Hansen &amp; Ostermeier, 2001)</ref>, where population size is determined starting from problem size.</p><p>We decided to follow a similar approach, and derive the parameters of the proposed algorithm from the size of the problem, that in the case of coreset discovery can be estimated as 2 k , the total number of possible different coresets of size <ref type="bibr">[1, k]</ref>, that represents the search space that the algorithm will need to explore in order to find the best coreset candidate. We fix k = 0.1 * N , where N is the total number of samples in the considered data set, as in most practical cases desirable coresets are 10% or less of the initial samples.</p><p>Following the empirical formulas presented in <ref type="bibr" target="#b9">(Hansen &amp; Ostermeier, 2001)</ref>, starting from k we can then derive the other necessary parameters for NSGA-II: µ = max(100, log 10 2 k ) (10) λ = 2 * µ (11) G = max(100, log 10 2 k·0.5 )</p><p>Where µ, as mentioned above, is the size of the population; λ is the size of the offspring, the number of new candidate solutions generated at each iteration; and G is the number of iterations after which the algorithm will stop. When compared to the empirical formulas of CMA-ES for deriving parameters, the main difference is that we increased the minimum size of the population and the minimum number of iterations, as the specific problem we are tackling is quite challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>All the necessary code for the experiments has been implemented in Python 3, relying upon open-source libraries as scikit-learn v0.22 1 <ref type="bibr" target="#b17">(Pedregosa et al., 2011)</ref> and inspyred v1.0 2 <ref type="bibr" target="#b8">(Garrett, 2012)</ref>, and bayesiancoresets v0.8 3 <ref type="bibr">(Campbell &amp; Beronov, 2019)</ref>. The code is freely available under GNU Public License from a GitHub public repository 4 . NSGA-II is used with the default parameters set by inspyred, with the exceptions described in subsection 3.5. In order to generate reproducible results, all algorithms that exploit pseudorandom elements in their training process have been set with a fixed seed. Before running coreset selection algorithms, each dataset has been standardized removing the mean and scaling to unit variance (StandardScaler <ref type="bibr" target="#b22">(Zill et al., 2011)</ref>). All the experiments have been run on the same machine: nn AMD EPYC 7301 16-Core Processor at 2 GHz equipped with 64 MiB memory. The proposed approach has been tested over a 10-fold cross-validation against state-of-the-art coreset discovery algorithms: GIGA (Campbell &amp; Broderick, 2018), Frank-Wolfe <ref type="bibr" target="#b4">(Clarkson, 2010)</ref>, Matching Pursuit <ref type="bibr" target="#b16">(Pati et al., 1993)</ref>, Orthogonal Mathcing Pursuit <ref type="bibr" target="#b16">(Pati et al., 1993)</ref>, LAR <ref type="bibr" target="#b6">(Efron et al., 2004;</ref><ref type="bibr">Boutsidis et al., 2013)</ref>, and Forward Stagewise (M. <ref type="bibr">A., 1960)</ref>. For each fold, coresets have been extracted and used to train an instance of the Ridge <ref type="bibr" target="#b19">(Tikhonov, 1943)</ref> classifier. This classifier has been chosen both for its high train speed and its generalization ability in a variety of experimental settings. <ref type="figure" target="#fig_0">Figures 2 and  3</ref> show for each benchmark dataset the quality of extracted coresets as a function of coreset size (lower is better), and classification error (lower is better). The coreset size has been reported both as the absolute number of core samples (in round brackets) and as the ratio |Ĉ M LE |/|D t | (number of core samples over the total number of training samples). The error bars represent the standard error of the mean. Details about experimental results are shown in the <ref type="table" target="#tab_4">Table 2</ref> and 3. The results can be summarized as follows:</p><p>• Concerning just mean classification error, EvoCore outperformed state-of-the-art techniques on all datasets but two, namely Micro-mass and MNIST;</p><p>• Considering coreset discovery as a multi-objective problem (minimize classification error and minimize coreset size), EvoCore's coresets are never dominated by the coresets uncovered by other techniques, for all considered datasets. On the contrary, they often dominate a considerable number of other solutions.</p><p>• In most cases, the solution provided by EvoCore was in a region of the search space far away from the solutions provided by the other techniques, thus revealing compromises unexplored by competitors.</p><p>When compared to the state-of-the-art in coreset discovery, EvoCore thus proves to be extremely effective. The main drawback of the proposed approach is represented by the longer running time. However, the problem can be mitigated by parallelizing evaluations, as EAs can easily evaluate all individuals in the same generation at the same time, provided that enough computational resources are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, a novel alternative to coreset discovery is presented. Experimental results suggest that the performance of ML classifiers would not be a function of the size of the training set, but rather a function of the mutual position of the training samples in the feature space. Future works will explore the possibility of extending coreset discovery to regression and clustering.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software and Data</head><p>Should this paper be accepted, a link to a repository with all the code needed to reproduce the experiments will be provided.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The figures show for each benchmark dataset the quality of extracted coresets as a function of coreset size (lower is better), and classification error obtained using an instance of the Ridge classifier (lower is better). The error bars represent the standard error of the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The figures show for each benchmark dataset the quality of extracted coresets as a function of coreset size (lower is better), and classification error obtained using an instance of the Ridge classifier (lower is better). The error bars represent the standard error of the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fitting using the whole training set</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Fitting using a core set</cell></row><row><cell></cell><cell></cell><cell>Test set</cell><cell></cell><cell>Test set</cell></row><row><cell cols="3">Proposed Approach</cell><cell></cell></row><row><cell cols="3">INDIVIDUAL</cell><cell>F2</cell><cell>Multi-objective</cell></row><row><cell>Encoding</cell><cell></cell><cell>Fitting</cell><cell></cell><cell>optimization</cell></row><row><cell>Sample 0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sample 1</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>Sample 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sample 3</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>Sample 4 Sample 5 … Sample N</cell><cell>N …</cell><cell>Core set Validation set</cell><cell></cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>EVOLUTION</cell></row><row><cell cols="2">F1: minimize</cell><cell>F2: minimize</cell><cell></cell></row><row><cell cols="2">coreset size</cell><cell>classification error</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 summarizes</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Table 1. Benchmark datasets</cell><cell></cell></row><row><cell>DATA SET</cell><cell>SAMPLES</cell><cell>FEATURES</cell><cell>CLASSES</cell><cell>NANS</cell></row><row><cell>MICRO-MASS</cell><cell>571</cell><cell>1,301</cell><cell>20</cell><cell>0</cell></row><row><cell>SOYBEAN</cell><cell>683</cell><cell>36</cell><cell>19</cell><cell>2,337</cell></row><row><cell>CREDIT-G</cell><cell>1,000</cell><cell>21</cell><cell>2</cell><cell>0</cell></row><row><cell>KR-VS-KP</cell><cell>3,196</cell><cell>37</cell><cell>2</cell><cell>0</cell></row><row><cell>ABALONE</cell><cell>4,177</cell><cell>9</cell><cell>28</cell><cell>0</cell></row><row><cell>ISOLET</cell><cell>7,797</cell><cell>618</cell><cell>26</cell><cell>0</cell></row><row><cell>JM1</cell><cell>10,885</cell><cell>22</cell><cell>2</cell><cell>25</cell></row><row><cell>GAS-DRIFT</cell><cell>13,910</cell><cell>129</cell><cell>6</cell><cell>0</cell></row><row><cell>MOZILLA4</cell><cell>15,545</cell><cell>6</cell><cell>2</cell><cell>0</cell></row><row><cell>LETTER</cell><cell>20,000</cell><cell>17</cell><cell>26</cell><cell>0</cell></row><row><cell>AMAZON</cell><cell>32,769</cell><cell>10</cell><cell>2</cell><cell>0</cell></row><row><cell>ELECTRICITY</cell><cell>45,312</cell><cell>9</cell><cell>2</cell><cell>0</cell></row><row><cell>MNIST</cell><cell>70,000</cell><cell>785</cell><cell>10</cell><cell>0</cell></row><row><cell>COVERTYPE</cell><cell>581,012</cell><cell>55</cell><cell>17</cell><cell>0</cell></row></table><note>the main characteristics of the bench- mark datasets used for the experiments. All of them have been downloaded from the OpenML public repository (Van- schoren et al., 2013; Matthias Feurer).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>10-fold cross-validation results. The mean and the standard error of the mean (s.e.m.) is reported in each column. The result with the best (highest or lowest) mean value for each metric is highlighted in bold.</figDesc><table><row><cell>MICRO-MASS</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>299.50 ± 0.67</cell><cell>0.832 ± 0.024</cell><cell cols="2">0.995 ± 0.002 21.80 ± 0.22</cell></row><row><cell>FRANK-WOLFE</cell><cell>323.60 ± 0.27</cell><cell>0.848 ± 0.017</cell><cell cols="2">1.000 ± 0.000 59.55 ± 0.28</cell></row><row><cell>MATCHING PURSUIT</cell><cell>289.00 ± 1.29</cell><cell>0.850 ± 0.022</cell><cell cols="2">0.992 ± 0.002 61.40 ± 0.20</cell></row><row><cell cols="2">FORWARD STAGEWISE 165.60 ± 1.36</cell><cell>0.830 ± 0.033</cell><cell cols="2">0.945 ± 0.007 60.49 ± 0.32</cell></row><row><cell>ORTHO PURSUIT</cell><cell>323.00 ± 0.00</cell><cell cols="3">0.851 ± 0.018 1.000 ± 0.000 231.35 ± 139.34</cell></row><row><cell>LAR</cell><cell>323.00 ± 0.00</cell><cell cols="3">0.851 ± 0.018 1.000 ± 0.000 21.84 ± 0.29</cell></row><row><cell>EVOCORE</cell><cell>71.60 ± 9.54</cell><cell>0.839 ± 0.023</cell><cell cols="2">0.968 ± 0.009 227.98 ± 2.95</cell></row><row><cell>SOYBEAN</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>362.70 ± 14.11</cell><cell>0.820 ± 0.029</cell><cell cols="2">0.860 ± 0.008 3.56 ± 0.12</cell></row><row><cell>FRANK-WOLFE</cell><cell>420.70 ± 10.74</cell><cell>0.842 ± 0.018</cell><cell cols="2">0.865 ± 0.012 1.79 ± 0.10</cell></row><row><cell>MATCHING PURSUIT</cell><cell>342.10 ± 13.52</cell><cell>0.801 ± 0.022</cell><cell cols="2">0.829 ± 0.011 2.34 ± 0.11</cell></row><row><cell cols="2">FORWARD STAGEWISE 105.80 ± 6.62</cell><cell>0.737 ± 0.024</cell><cell cols="2">0.786 ± 0.017 1.94 ± 0.12</cell></row><row><cell>ORTHO PURSUIT</cell><cell>30.60 ± 1.50</cell><cell>0.624 ± 0.033</cell><cell cols="2">0.680 ± 0.028 0.81 ± 0.19</cell></row><row><cell>LAR</cell><cell>37.10 ± 0.35</cell><cell>0.694 ± 0.024</cell><cell cols="2">0.717 ± 0.012 0.80 ± 0.16</cell></row><row><cell>EVOCORE</cell><cell>152.80 ± 12.95</cell><cell cols="3">0.911 ± 0.019 0.956 ± 0.001 96.87 ± 0.91</cell></row><row><cell>CREDIT-G</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>195.50 ± 27.65</cell><cell>0.718 ± 0.015</cell><cell cols="2">0.721 ± 0.005 1.19 ± 0.18</cell></row><row><cell>FRANK-WOLFE</cell><cell>537.20 ± 57.91</cell><cell>0.738 ± 0.015</cell><cell cols="2">0.739 ± 0.006 1.93 ± 0.07</cell></row><row><cell>MATCHING PURSUIT</cell><cell>398.20 ± 79.64</cell><cell>0.724 ± 0.011</cell><cell cols="2">0.737 ± 0.006 2.09 ± 0.09</cell></row><row><cell cols="2">FORWARD STAGEWISE 67.20 ± 1.91</cell><cell>0.671 ± 0.019</cell><cell cols="2">0.676 ± 0.011 2.06 ± 0.11</cell></row><row><cell>ORTHO PURSUIT</cell><cell>19.20 ± 0.80</cell><cell>0.668 ± 0.015</cell><cell cols="2">0.658 ± 0.009 0.63 ± 0.12</cell></row><row><cell>LAR</cell><cell>20.30 ± 0.21</cell><cell>0.636 ± 0.017</cell><cell cols="2">0.636 ± 0.014 0.67 ± 0.10</cell></row><row><cell>EVOCORE</cell><cell>29.60 ± 8.28</cell><cell cols="3">0.743 ± 0.015 0.773 ± 0.007 124.58 ± 0.92</cell></row><row><cell>KR-VS-KP</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>2227.30 ± 32.20</cell><cell>0.889 ± 0.022</cell><cell cols="2">0.939 ± 0.002 4.48 ± 0.22</cell></row><row><cell>FRANK-WOLFE</cell><cell>2395.60 ± 16.58</cell><cell>0.891 ± 0.025</cell><cell cols="2">0.940 ± 0.001 5.18 ± 0.11</cell></row><row><cell>MATCHING PURSUIT</cell><cell>2425.70 ± 19.22</cell><cell>0.896 ± 0.024</cell><cell cols="2">0.942 ± 0.002 6.25 ± 0.17</cell></row><row><cell cols="2">FORWARD STAGEWISE 297.40 ± 24.03</cell><cell>0.821 ± 0.039</cell><cell cols="2">0.878 ± 0.011 5.71 ± 0.13</cell></row><row><cell>ORTHO PURSUIT</cell><cell>28.70 ± 2.42</cell><cell>0.695 ± 0.029</cell><cell cols="2">0.732 ± 0.014 0.76 ± 0.17</cell></row><row><cell>LAR</cell><cell>36.10 ± 0.18</cell><cell>0.702 ± 0.039</cell><cell cols="2">0.743 ± 0.014 0.87 ± 0.16</cell></row><row><cell>EVOCORE</cell><cell>145.50 ± 28.12</cell><cell cols="3">0.937 ± 0.012 0.968 ± 0.001 146.55 ± 0.53</cell></row><row><cell>ABALONE</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>56.60 ± 2.87</cell><cell>0.088 ± 0.005</cell><cell cols="2">0.089 ± 0.004 1.69 ± 0.25</cell></row><row><cell>FRANK-WOLFE</cell><cell>73.60 ± 2.40</cell><cell>0.092 ± 0.007</cell><cell cols="2">0.098 ± 0.004 2.96 ± 0.11</cell></row><row><cell>MATCHING PURSUIT</cell><cell>61.00 ± 2.03</cell><cell>0.089 ± 0.007</cell><cell cols="2">0.095 ± 0.002 3.13 ± 0.12</cell></row><row><cell cols="2">FORWARD STAGEWISE 40.40 ± 1.94</cell><cell>0.083 ± 0.010</cell><cell cols="2">0.083 ± 0.009 3.32 ± 0.15</cell></row><row><cell>ORTHO PURSUIT</cell><cell>29.80 ± 0.49</cell><cell>0.092 ± 0.006</cell><cell cols="2">0.096 ± 0.005 1.14 ± 0.26</cell></row><row><cell>LAR</cell><cell>29.80 ± 0.47</cell><cell>0.082 ± 0.007</cell><cell cols="2">0.090 ± 0.009 1.61 ± 0.61</cell></row><row><cell>EVOCORE</cell><cell>232.90 ± 32.43</cell><cell cols="3">0.186 ± 0.009 0.194 ± 0.003 44.52 ± 0.08</cell></row><row><cell>ISOLET</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>1763.10 ± 13.97</cell><cell>0.856 ± 0.007</cell><cell cols="2">0.902 ± 0.002 208.60 ± 0.35</cell></row><row><cell>FRANK-WOLFE</cell><cell>1814.50 ± 97.73</cell><cell>0.854 ± 0.005</cell><cell cols="2">0.902 ± 0.002 543.19 ± 0.44</cell></row><row><cell>MATCHING PURSUIT</cell><cell>1581.20 ± 23.01</cell><cell>0.839 ± 0.006</cell><cell cols="2">0.883 ± 0.002 560.61 ± 1.67</cell></row><row><cell cols="2">FORWARD STAGEWISE 364.90 ± 5.67</cell><cell>0.620 ± 0.015</cell><cell cols="2">0.655 ± 0.009 564.02 ± 0.62</cell></row><row><cell>ORTHO PURSUIT</cell><cell>82.20 ± 10.12</cell><cell>0.536 ± 0.033</cell><cell cols="2">0.559 ± 0.026 6.58 ± 0.89</cell></row><row><cell>LAR</cell><cell>617.00 ± 0.00</cell><cell>0.614 ± 0.008</cell><cell cols="2">0.659 ± 0.002 433.81 ± 40.76</cell></row><row><cell>EVOCORE</cell><cell cols="4">3025.50 ± 150.26 0.905 ± 0.006 0.952 ± 0.002 7029.46 ± 49.88</cell></row><row><cell>JM1</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>284.10 ± 17.68</cell><cell>0.748 ± 0.010</cell><cell cols="2">0.748 ± 0.001 8.33 ± 0.52</cell></row><row><cell>FRANK-WOLFE</cell><cell>531.70 ± 19.20</cell><cell>0.741 ± 0.006</cell><cell cols="2">0.745 ± 0.002 26.58 ± 0.13</cell></row><row><cell>MATCHING PURSUIT</cell><cell>287.50 ± 13.64</cell><cell>0.744 ± 0.005</cell><cell cols="2">0.748 ± 0.002 27.25 ± 0.12</cell></row><row><cell cols="2">FORWARD STAGEWISE 68.50 ± 1.29</cell><cell>0.743 ± 0.005</cell><cell cols="2">0.744 ± 0.006 27.61 ± 0.11</cell></row><row><cell>ORTHO PURSUIT</cell><cell>14.20 ± 1.21</cell><cell>0.739 ± 0.006</cell><cell cols="2">0.745 ± 0.005 0.87 ± 0.10</cell></row><row><cell>LAR</cell><cell>21.30 ± 0.15</cell><cell>0.752 ± 0.012</cell><cell cols="2">0.754 ± 0.003 1.11 ± 0.13</cell></row><row><cell>EVOCORE</cell><cell>54.00 ± 14.35</cell><cell cols="3">0.771 ± 0.013 0.788 ± 0.002 844.78 ± 1.94</cell></row><row><cell cols="5">Barbiero, P. and Tonda, A. Making sense of economics</cell></row><row><cell cols="5">datasets with evolutionary coresets. In The Interna-</cell></row><row><cell cols="5">tional Conference on Decision Economics, pp. 162-170.</cell></row><row><cell cols="2">Springer, 2019b.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Barbiero, P., Squillero, G., and Tonda, A. Evolutionary</cell></row><row><cell cols="5">discovery of coresets for classification. In Proceedings of</cell></row><row><cell cols="5">the Genetic and Evolutionary Computation Conference</cell></row><row><cell cols="3">Companion, pp. 1747-1754, 2019.</cell><cell></cell><cell></cell></row><row><cell cols="5">Boutsidis, C., Drineas, P., and Magdon-Ismail, M. Near-</cell></row><row><cell cols="5">optimal Coresets For Least-Squares Regression. Techni-</cell></row><row><cell cols="5">cal report, 2013. URL https://arxiv.org/pdf/</cell></row><row><cell cols="2">1202.3505.pdf.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>10-fold cross-validation results. The mean and the standard error of the mean (s.e.m.) is reported in each column. The result with the best (highest or lowest) mean value for each metric is highlighted in bold. ± 0.000 39.43 ± 0.21 FORWARD STAGEWISE 45.40 ± 4.84 0.894 ± 0.010 0.894 ± 0.010 39.09 ± 0.16 ORTHO PURSUIT 9.80 ± 0.29 0.853 ± 0.016 0.852 ± 0.015 0.89 ± 0.08 LAR 10.20 ± 0.33 0.812 ± 0.037 0.811 ± 0.037 8.73 ± 5.36 EVOCORE 10.40 ± 2.38 0.915 ± 0.000 0.915 ± 0.000 14098.09 ± 177.97 ± 0.024 0.99 ± 0.10 EVOCORE 41.40 ± 25.20 0.693 ± 0.029 0.759 ± 0.003 38331.78 ± 84.84 ± 319.09 0.797 ± 0.003 0.807 ± 0.003 1.41 ± 0.08 FRANK-WOLFE 7306.20 ± 240.28 0.819 ± 0.004 0.826 ± 0.002 3075.35 ± 172.95 MATCHING PURSUIT 6250.90 ± 157.99 0.807 ± 0.004 0.816 ± 0.002 3080.56 ± 128.81 FORWARD STAGEWISE 587.00 ± 5.79 0.442 ± 0.004 0.449 ± 0.004 3009.10 ± 166.47 ORTHO PURSUIT 128.80 ± 20.96 0.588 ± 0.019 0.590 ± 0.018 46.48 ± 5.72 LAR 711.00 ± 0.26 0.468 ± 0.007 0.477 ± 0.005 746.32 ± 41.71 EVOCORE 82.00 ± 3.86 0.772 ± 0.006 0.773 ± 0.003 4001.79 ± 157.18 ± 265.86 0.583 ± 0.027 0.637 ± 0.006 1319.12 ± 121.27 FRANK-WOLFE 4735.80 ± 413.56 0.584 ± 0.027 0.626 ± 0.007 1888.11 ± 99.88 MATCHING PURSUIT 5421.30 ± 129.91 0.590 ± 0.027 0.630 ± 0.007 1893.71 ± 96.84 FORWARD STAGEWISE 264.20 ± 12.45 0.500 ± 0.019 0.533 ± 0.006 1975.60 ± 46.11 ORTHO PURSUIT 52.60 ± 0.22 0.486 ± 0.016 0.501 ± 0.012 17.25 ± 1.44 LAR 53.30 ± 0.21 0.476 ± 0.021 0.505 ± 0.014 29.26 ± 2.52 EVOCORE 328.40 ± 59.92 0.643 ± 0.032 0.701 ± 0.003 4881.47 ± 106.46 Campbell, T. and Beronov, B. Sparse variational inference: Bayesian coresets from scratch. In Advances in Neural Information Processing Systems, pp. 11457-11468, 2019. Campbell, T. and Broderick, T. Automated Scalable Bayesian Inference via Hilbert Coresets. 2017. URL http://arxiv.org/abs/1710.05053. Campbell, T. and Broderick, T. Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent. In International Conference on Machine Learning (ICML), 2018. URL https://arxiv.org/pdf/1802. 01737.pdf.</figDesc><table><row><cell>GAS-DRIFT</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>402.70 ± 6.11</cell><cell>0.834 ± 0.026</cell><cell cols="2">0.876 ± 0.016 77.82 ± 0.48</cell></row><row><cell>FRANK-WOLFE</cell><cell>531.70 ± 23.01</cell><cell>0.825 ± 0.025</cell><cell cols="2">0.886 ± 0.011 195.76 ± 0.21</cell></row><row><cell>MATCHING PURSUIT</cell><cell>324.40 ± 8.06</cell><cell>0.801 ± 0.031</cell><cell cols="2">0.859 ± 0.017 195.28 ± 0.17</cell></row><row><cell>FORWARD STAGEWISE</cell><cell>82.80 ± 3.13</cell><cell>0.710 ± 0.034</cell><cell cols="2">0.742 ± 0.025 195.12 ± 0.17</cell></row><row><cell>ORTHO PURSUIT</cell><cell>21.40 ± 2.66</cell><cell>0.513 ± 0.046</cell><cell cols="2">0.579 ± 0.034 1.21 ± 0.10</cell></row><row><cell>LAR</cell><cell>128.10 ± 0.10</cell><cell>0.826 ± 0.031</cell><cell cols="2">0.887 ± 0.007 20.83 ± 1.15</cell></row><row><cell>EVOCORE</cell><cell>813.10 ± 162.08</cell><cell cols="3">0.946 ± 0.016 0.986 ± 0.001 3517.78 ± 30.70</cell></row><row><cell>MOZILLA4</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>17.30 ± 1.17</cell><cell>0.670 ± 0.037</cell><cell cols="2">0.665 ± 0.029 1.04 ± 0.21</cell></row><row><cell>FRANK-WOLFE</cell><cell>37.90 ± 1.31</cell><cell>0.624 ± 0.038</cell><cell cols="2">0.630 ± 0.035 6.26 ± 0.15</cell></row><row><cell>MATCHING PURSUIT</cell><cell>32.70 ± 1.30</cell><cell>0.636 ± 0.029</cell><cell cols="2">0.628 ± 0.021 6.46 ± 0.06</cell></row><row><cell>FORWARD STAGEWISE</cell><cell>15.90 ± 1.70</cell><cell>0.608 ± 0.035</cell><cell cols="2">0.578 ± 0.024 6.39 ± 0.08</cell></row><row><cell>ORTHO PURSUIT</cell><cell>5.20 ± 0.20</cell><cell>0.628 ± 0.037</cell><cell cols="2">0.604 ± 0.044 0.96 ± 0.18</cell></row><row><cell>LAR</cell><cell>6.70 ± 0.40</cell><cell>0.620 ± 0.034</cell><cell cols="2">0.611 ± 0.025 2.73 ± 1.31</cell></row><row><cell>EVOCORE</cell><cell>81.00 ± 8.98</cell><cell cols="3">0.912 ± 0.010 0.912 ± 0.001 2041.79 ± 4.50</cell></row><row><cell>LETTER</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>139.00 ± 5.31</cell><cell>0.213 ± 0.007</cell><cell cols="2">0.215 ± 0.007 1.13 ± 0.12</cell></row><row><cell>FRANK-WOLFE</cell><cell>258.20 ± 15.72</cell><cell>0.244 ± 0.008</cell><cell cols="2">0.246 ± 0.009 38.61 ± 0.17</cell></row><row><cell>MATCHING PURSUIT</cell><cell>180.50 ± 6.80</cell><cell>0.224 ± 0.009</cell><cell cols="2">0.227 ± 0.009 39.00 ± 0.14</cell></row><row><cell>FORWARD STAGEWISE</cell><cell>85.20 ± 5.90</cell><cell>0.188 ± 0.007</cell><cell cols="2">0.192 ± 0.008 39.17 ± 0.12</cell></row><row><cell>ORTHO PURSUIT</cell><cell>29.30 ± 0.40</cell><cell>0.247 ± 0.010</cell><cell cols="2">0.246 ± 0.009 0.95 ± 0.11</cell></row><row><cell>LAR</cell><cell>30.30 ± 0.37</cell><cell>0.238 ± 0.007</cell><cell cols="2">0.243 ± 0.007 8.54 ± 5.20</cell></row><row><cell>EVOCORE</cell><cell>724.30 ± 66.09</cell><cell cols="3">0.659 ± 0.004 0.669 ± 0.002 6916.12 ± 33.55</cell></row><row><cell>AMAZON-EMPLOYEE-ACCESS</cell><cell>SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>56.70 ± 4.33</cell><cell>0.912 ± 0.001</cell><cell cols="2">0.912 ± 0.001 1.28 ± 0.23</cell></row><row><cell>FRANK-WOLFE</cell><cell>150.00 ± 3.96</cell><cell>0.912 ± 0.002</cell><cell cols="2">0.912 ± 0.002 41.27 ± 0.17</cell></row><row><cell cols="4">MATCHING PURSUIT 0.914 ELECTRICITY 95.00 ± 14.50 0.914 ± 0.000 SIZE TEST F1 TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>81.10 ± 9.03</cell><cell>0.482 ± 0.040</cell><cell cols="2">0.513 ± 0.020 1.27 ± 0.18</cell></row><row><cell>FRANK-WOLFE</cell><cell>190.40 ± 16.92</cell><cell>0.525 ± 0.028</cell><cell cols="2">0.553 ± 0.014 47.35 ± 0.17</cell></row><row><cell>MATCHING PURSUIT</cell><cell>86.00 ± 9.74</cell><cell>0.552 ± 0.046</cell><cell cols="2">0.553 ± 0.036 47.97 ± 0.14</cell></row><row><cell>FORWARD STAGEWISE</cell><cell>38.20 ± 4.29</cell><cell>0.592 ± 0.049</cell><cell cols="2">0.618 ± 0.037 47.47 ± 0.14</cell></row><row><cell>ORTHO PURSUIT</cell><cell>7.90 ± 0.10</cell><cell>0.581 ± 0.026</cell><cell cols="2">0.622 ± 0.027 0.97 ± 0.14</cell></row><row><cell cols="4">LAR 0.607 MNIST 8.40 ± 0.22 0.607 ± 0.034 SIZE TEST F1 TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell cols="2">GIGA 5552.30 COVERTYPE SIZE</cell><cell>TEST F1</cell><cell>TRAIN F1</cell><cell>FIT TIME (S)</cell></row><row><cell>GIGA</cell><cell>5788.40</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">scikit-learn: Machine Learning in Python, http:// scikit-learn.org/ 2 inspyred: Bio-inspired Algorithms in Python, https:// pythonhosted.org/inspyred/ 3 bayesiancoresets: Coresets for approximate Bayesian inference, https://github.com/trevorcampbell/ bayesian-coresets/ 4 GitHub, https://github.com/albertotonda/ prototype-set-discovery</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An evolutionary algorithm that constructs recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Angeline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="65" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Practical coreset constructions for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06476</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fundamental flowers: Finding core sets for classification using evolutionary computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barbiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Muc-3 evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chinchor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd conference on Message understanding</title>
		<meeting>the 3rd conference on Message understanding</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse Greedy Approximation, and the Frank-Wolfe Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coresets</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.9299{&amp;}rep=rep1{&amp;}type=pdf" />
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Algorithms</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: Nsgaii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Least Angle Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno>doi: 10.1214/ 009053604000000067</idno>
		<ptr target="https://arxiv.org/pdf/math/0406456.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="451" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The upstart algorithm: A method for constructing and training feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="209" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">inspyred (version 1.0.1) inspired intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garrett</surname></persName>
		</author>
		<ptr target="https://github.com/aarongarrett/inspyred" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Completely derandomized self-adaptation in evolution strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ostermeier</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.2713540</idno>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="195" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature selection in unsupervised learning via evolutionary search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the sixth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="365" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning</title>
		<editor>Langley, P.</editor>
		<meeting>the 17th International Conference on Machine Learning<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple Regression Analysis. Mathematical Methods for Digital Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matching pursuits with timefrequency dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">33973415</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Genetic evolution of the topology and weight distribution of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Maniezzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="53" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Openml-python: an extensible python api for openml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K P G N M S R A M J V F H</forename></persName>
		</author>
		<idno>1911.02490</idno>
		<ptr target="https://arxiv.org/pdf/1911.02490.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishnaprasad</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACSSC.1993.342465</idno>
		<ptr target="http://ieeexplore.ieee.org/document/342465/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 27th Asilomar Conference on Signals, Systems and Computers</title>
		<meeting>27th Asilomar Conference on Signals, Systems and Computers</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="40" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on danish commons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Biering-Sørensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the stability of inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Tikhonov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="195" to="198" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Genetic algorithms as a tool for feature selection in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vafaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>De Jong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools with Artificial Intelligence, 1992. TAI&apos;92, Proceedings., Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="200" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Networked science in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openml</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2641190.2641198</idno>
		<ptr target="http://doi.acm.org/10.1145/2641190.2641198" />
	</analytic>
	<monogr>
		<title level="m">SIGKDD Explorations</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Advanced engineering mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Cullen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Jones &amp; Bartlett Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
