<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BoxE: A Box Embedding Model for Knowledge Base Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Abboud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">İsmaililkan</forename><surname>Ceylan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BoxE: A Box Embedding Model for Knowledge Base Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge base completion (KBC) aims to automatically infer missing facts by exploiting information already present in a knowledge base (KB). A promising approach for KBC is to embed knowledge into latent spaces and make predictions from learned embeddings. However, existing embedding models are subject to at least one of the following limitations: (1) theoretical inexpressivity, (2) lack of support for prominent inference patterns (e.g., hierarchies), (3) lack of support for KBC over higher-arity relations, and (4) lack of support for incorporating logical rules. Here, we propose a spatio-translational embedding model, called BoxE, that simultaneously addresses all these limitations. BoxE embeds entities as points, and relations as a set of hyper-rectangles (or boxes), which spatially characterize basic logical properties. This seemingly simple abstraction yields a fully expressive model offering a natural encoding for many desired logical properties. BoxE can both capture and inject rules from rich classes of rule languages, going well beyond individual inference patterns. By design, BoxE naturally applies to higher-arity KBs. We conduct a detailed experimental analysis, and show that BoxE achieves state-of-the-art performance, both on benchmark knowledge graphs and on more general KBs, and we empirically show the power of integrating logical rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge bases (KBs) are fundamental means for representing, storing, and processing information, and are widely used to enhance the reasoning and learning capabilities of modern information systems. KBs can be viewed as a collection of facts of the form r(e 1 , . . . , e n ), which represent a relation r between the entities e 1 , . . . , e n , and knowledge graphs (KGs) as a special case, where all the relations are binary (i.e., composed of two entities). KBs such as YAGO [26], NELL [28], Knowledge Vault [10], and Freebase [2] contain millions of facts, and are increasingly important in academia and industry, for applications such as question answering [3], recommender systems [42], information retrieval [47], and natural language processing [48].</p><p>KBs are, however, highly incomplete, which makes their downstream use more challenging. For instance, 71% of individuals in Freebase lack a connection to a place of birth [45]. Knowledge base completion (KBC), aiming at automatically inferring missing facts in a KB by exploiting the already present information, has thus become a focal point of research. One prominent approach for KBC is to learn embeddings for entities and relations in a latent space such that these embeddings, once learned from known facts, can be used to score the plausibility of unknown facts.</p><p>Currently, the main embedding approaches for KBC are translational models <ref type="bibr" target="#b3">[4,</ref> 37], which score facts based on distances in the embedding space, bilinear models [39, 49,<ref type="bibr" target="#b0">1]</ref>, which learn embeddings that factorize the truth tensor of a knowledge base, and neural models <ref type="bibr" target="#b7">[8,</ref> 34, 29], which score facts using dedicated neural architectures. Each of these models suffer from limitations, most of 34th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>which are well-known. Translational models, for instance, are theoretically inexpressive, i.e., cannot provably fit an arbitrary KG. Furthermore, none of these models can capture simple sets of logical rules: even capturing a simple relational hierarchy goes beyond the current capabilities of most existing models <ref type="bibr" target="#b13">[14]</ref>. This also makes it difficult to inject background knowledge (i.e., schematic knowledge), in the form of logical rules, into the model to improve KBC performance. Additionally, existing KBC models are primarily designed for KGs, and thus do not naturally extend to KBs with higher-arity relations, involving 3 or more entities, e.g., DegreeFrom(Turing, PhD, Princeton) <ref type="bibr" target="#b10">[11]</ref>, which hinders their applicability. Higher-arity relations are prevalent in modern KBs such as Freebase <ref type="bibr">[44]</ref>, and cannot always be reduced to a KG without loss of information <ref type="bibr" target="#b10">[11]</ref>. Despite the rich landscape for KBC, no existing model currently offers a solution to all these limitations.</p><p>In this paper, we address these problems by encoding relations as explicit regions in the embedding space, where logical properties such as relation subsumption and disjointness can naturally be analyzed and inferred. Specifically, we present BoxE, a spatio-translational box embedding model, which models relations as sets of d−dimensional boxes (corresponding to classes), and entities as d−dimensional points. Facts are scored based on the positions of entity embeddings with respect to relation boxes. Our contributions can be summarized as follows:</p><p>-We introduce BoxE and show that this model achieves state-of-the-art performance on both knowledge graph completion and knowledge base completion tasks across multiple datasets.</p><p>-We show that BoxE is fully expressive, a first for translation-based models, to our knowledge.</p><p>-We comprehensively analyze the inductive capacity of BoxE in terms of generalized inference patterns and rule languages, and show that BoxE can capture a rich rule language.</p><p>-We prove that BoxE additionally supports injecting a rich language of logical rules, and empirically show on a subset of NELL <ref type="bibr" target="#b27">[28]</ref>, that this can significantly improve KBC performance.</p><p>All proofs for theorems, as well as additional experiments and experimental details, can be found in the appendix of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Knowledge Base Completion: Problem, Properties, and Evaluation</head><p>In this section, we define knowledge bases and the problem of knowledge base completion (KBC). We also give an overview of standard approaches for evaluating KBC models.</p><p>Consider a relational vocabulary, which consists of a finite set E of entities and a finite set R of relations. A fact (also called atom) is of the form r(e 1 , . . . , e n ), where r ∈ R is an n-ary relation, and e i ∈ E are entities. A knowledge base (KB) is a finite set of facts, and a knowledge graph (KG) is a KB with only binary relations. In KGs, facts are also known as triples, and are of the form r(e h , e t ), with a head entity e h and a tail entity e t . Knowledge base completion (KBC) (resp., knowledge graph completion (KGC)) is the task of accurately predicting new facts from existing facts in a KB (resp., KG). KBC models are analyzed by means of (i) an experimental evaluation on existing benchmarks, (ii) their model expressiveness, and (iii) the set of inference patterns that they can capture.</p><p>Experimental evaluation. To evaluate KBC models empirically, true facts from the test set of a KB and corrupted facts, generated from the test set, are used. A corrupted fact is obtained by replacing one of the entities in a fact from the KB with a new entity: given a fact r(e 1 , . . . , e i , . . . , e n ) from the KB, a corrupted fact is a fact r(e 1 , . . . , e i , . . . , e n ) that does not occur in the training, validation, or test set. KBC models define a scoring function over facts, and are optimized to score true facts higher than corrupted facts. KBC performance is evaluated using metrics <ref type="bibr" target="#b3">[4]</ref> such as mean rank (MR), the average rank of facts against their corrupted counterparts, mean reciprocal rank (MRR), their average inverse rank (i.e., 1/rank), and Hits@K, the proportion of facts with rank at most K.</p><p>Expressiveness. A KBC model M is fully expressive if, for any given disjoint sets of true and false facts, there exists a parameter configuration for M such that M accurately classifies all the given facts. Intuitively, a fully expressive model can capture any knowledge base configuration, but this does not necessarily correlate with inductive capacity: fully expressive models can merely memorize training data and generalize poorly. Conversely, a model that is not fully expressive can fail to fit its training set properly, and thus can underfit. Hence, it is important to develop models that are jointly fully expressive and capture prominent and common inference patterns.</p><p>Inference patterns. Inference patterns are a common means to formally analyze the generalization ability of KBC systems. Briefly, an inference pattern is a specification of a logical property that may exist in a KB, which, if learned, enables further principled inferences from existing KB facts. One well-known example inference pattern is symmetry, which specifies that when a fact r(e 1 , e 2 ) holds, then r(e 2 , e 1 ) also holds. If a model learns a symmetry pattern for r, then it can automatically predict facts in the symmetric closure of r, thus providing a strong inductive bias. We present some prominent inference patterns in detail in Section 5, and also in <ref type="table" target="#tab_1">Table 1</ref>. Intuitively, inference patterns captured by a model serve as an indication of its inductive capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>In this section, we give an overview of closely related embedding methods for KBC/KGC and existing region-based embedding models. We exclude neural models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">35,</ref><ref type="bibr">29]</ref>, as these models are challenging to analyze, both from an expressiveness and inductive capacity perspective.</p><p>Translational models. Translational models represent entities as points in a high-dimensional vector space and relations as translations in this space. The seminal translational model is TransE <ref type="bibr" target="#b3">[4]</ref>, where a relation r, modeled by a vector r, holds between e 1 and e 2 iff e 1 + r = e 2 . However, TransE is not fully expressive, cannot capture one-to-many, many-to-one, many-to-many, and symmetric relations, and can only handle binary facts. This motivated extensions [43, <ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b11">12]</ref>, which each address some, but not all, these limitations.  <ref type="bibr" target="#b17">[18]</ref> and TuckER <ref type="bibr" target="#b0">[1]</ref> build on canonical polyadic (CP) <ref type="bibr" target="#b14">[15]</ref> and Tucker decomposition <ref type="bibr">[40]</ref>, respectively. TuckER subsumes RESCAL, its adaptations, and SimplE <ref type="bibr" target="#b0">[1]</ref>. Generally, all bilinear models except DistMult are fully expressive, but they are less interpretable compared to translational models.</p><p>Higher-arity KBC. KBs can encode knowledge that cannot be encoded in a KG <ref type="bibr" target="#b10">[11]</ref>. Hence, models such as HSimplE <ref type="bibr" target="#b10">[11]</ref>, m-TransH [44], m-DistMult, and m-CP <ref type="bibr" target="#b10">[11]</ref> are proposed as generalizations of SimplE, TransH <ref type="bibr">[43]</ref>, DistMult, and CP, respectively. HypE <ref type="bibr" target="#b10">[11]</ref> tackles higher-arity KBC through convolutions. Generalizations to TuckER, namely, m-TuckER and GETD <ref type="bibr" target="#b23">[24]</ref>, are also proposed, but these do not apply to KBs with different-arity relations. For most existing KGC models, there are conceptual and practical challenges (e.g., scalability) against generalizing them to KBC.</p><p>Region-based models. Region-based models explicitly define regions in the embedding space where an output property (e.g., membership to a class) holds. For instance, bounded axis-aligned hyperrectangles (boxes) <ref type="bibr">[41,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b21">22]</ref> are used for entity classification to define class regions and hierarchies, in which entity point embeddings appear. As boxes naturally represent sets of objects, they are also used to represent answer sets in the Query2Box query answering system <ref type="bibr" target="#b15">[16]</ref>. Query2Box can be applied to KBC but reduces to a translational model with a box correctness region for tail entities. Furthermore, entity classification approaches cannot be scalably generalized to KBC, as this would involve introducing an embedding per entity tuple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Box Embeddings for Knowledge Base Completion</head><p>In this section, we introduce an embedding model for KBC, called BoxE, that encodes relations as axis-aligned hyper-rectangles (or boxes) and entities as points in the d-dimensional Euclidian space.</p><p>Representation. In BoxE, every entity e i ∈ E is represented by two vectors e i , b i ∈ R d , where e i defines the base position of the entity, and b i defines its translational bump, which translates all the entities co-occuring in a fact with e i , from their base positions to their final embeddings by "bumping"</p><formula xml:id="formula_0">r (1) r (2) e 1 e 2 e 3 e 4 b 1 b 2 b 3 b 4 b 1 b 2 b 3 b 4 b 1 b 2 b 3 b 4 b 1 b 2 b 3 b 4 e 1 e 2</formula><p>e 4 e 3 <ref type="figure" target="#fig_6">Figure 1</ref>: A sample BoxE model is shown on the left for d = 2. The binary relation r is encoded via the box embeddings r <ref type="bibr" target="#b0">(1)</ref> and r <ref type="bibr" target="#b1">(2)</ref> . Every entity e i has an embedding e i , and defines a bump on other entities, as shown with distinct colors. This model induces the KG on r, shown on the right.</p><p>them. The final embedding of an entity e i relative to a fact r(e 1 , . . . , e n ) is hence given by:</p><formula xml:id="formula_1">e r(e1,...,en) i = (e i − b i ) + 1≤j≤n b j .<label>(1)</label></formula><p>Essentially, the entity representation is dynamic, as every entity can have a potentially different final embedding relative to a different fact. The main idea is that every entity translates the base positions of other entities co-appearing in a fact, that is, for a fact r(e 1 , e 2 ), b 1 and b 2 translate e 2 and e 1 respectively, to compute their final embeddings.</p><p>In BoxE, every relation r is represented by n hyper-rectangles, i.e., boxes, r (1) , . . . , r (n) ∈ R d , where n is the arity of r. Intuitively, this representation defines n regions in R d , one per arity position, such that a fact r(e 1 , ..., e n ) holds when the final embeddings of e 1 , ..., e n each appear in their corresponding position box, creating a class abstraction for the sets of all entities appearing at every arity position. For the special case of unary relations (i.e., classes), the definition given in Eq. 1 implies no translational bumps, and thus the base position of an entity is its final embedding. Example 4.1. Consider an example over a single binary relation r and the entities e 1 , e 2 , e 3 , e 4 . A BoxE model is given on the left in <ref type="figure" target="#fig_6">Figure 1</ref>, for d = 2, where every entity is represented as a point, and the binary relation r is represented with two boxes r <ref type="bibr" target="#b0">(1)</ref> and r <ref type="bibr" target="#b1">(2)</ref> . Every entity is translated by the bump vectors of all other entities. For example, r(e 1 , e 4 ) is a true fact in the model (e.g., to be ranked high), since (i) e r(e1,e4) 1 = (e 1 + b 4 ) is a point in r <ref type="bibr" target="#b0">(1)</ref> (e 1 appears in the head box), and (ii) e r(e1,e4) 4 = (e 4 + b 1 ) is a point in r <ref type="bibr" target="#b1">(2)</ref> (e 4 appears in the tail box). Similarly, r(e 3 , e 3 ) is a true fact in the model, as e r(e3,e3) 3 = (e 3 + b 3 ), which is a point in r <ref type="bibr" target="#b0">(1)</ref> and r <ref type="bibr" target="#b1">(2)</ref> , i.e., the entity is reflexive in r. The model encodes all (and only) the facts from the KG, shown on the right in <ref type="figure" target="#fig_6">Figure 1</ref>.</p><p>Translational bumps are very powerful, as they allow us to model complex interactions across entities in an effective manner. Observe that for the sample KG, there are 4 2 potential facts that can hold, and therefore 2 16 possible configurations. Nonetheless, they can all be compactly captured by choosing appropriate translational bumps to force entity embeddings in or out of the respective relation boxes as needed. Indeed, we later formally show that such a configuration can always be found for any KB, given sufficiently many dimensions, proving full expressiveness of the model. Scoring function. In the above example, we identified facts that ideally need to be ranked higher by our scoring function, to reflect the model properties adequately. To this end, we first define a distance function for evaluating entity positions relative to the box positions. The idea is to define a function that grows slowly if a point is in the box (relative to the center of the box), but grows rapidly if the point is outside of the box, so as to drive points more effectively into their target boxes and ensure they are minimally changed, and can remain there once inside.</p><p>Formally, let us denote by l (i) , u (i) ∈ R d the lower and upper boundaries of a box r (i) , respectively, by c (i) = (l (i) + u (i) )/2 its center, and by w (i) = u (i) − l (i) + 1 its width incremented by 1. We say that a point e i is inside a box r (i) , denoted e i ∈ r (i) , if l (i) ≤ e i ≤ u (i) . Furthermore, we denote the element-wise multiplication, division, and inversion operations by •, and •−1 respectively. Then, the distance function for the given entity embeddings relative to a given target box is defined piece-wise over two cases, as follows:</p><formula xml:id="formula_2">dist(e r(e1,...,en) i , r (i) ) = | e r(e1,...,en) i − c (i) | w (i) if e i ∈ r (i) , | e r(e1,...,en) i − c (i) | • w (i) − κ otherwise, where κ = 0.5 • (w (i) − 1) • (w (i) − w (i) •−1 )</formula><p>, is a width-dependent factor.  In both cases, dist factors in the size of the target box in its computation. In the first case, where the point is in its target box, distance inversely correlates with box size, to maintain low distance inside large boxes and provide a gradient to keep points inside. In the second case, box size linearly correlates with distance, to penalize points outside larger boxes more severely. Finally, κ is subtracted to preserve function continuity.</p><formula xml:id="formula_3">− c (i) dist w (i) = 1 w (i) = 3 w (i) = 5</formula><p>Plots for dist for one-dimensional w (i) are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Observe that, when w (i) = 1, r (i) is point-shaped, and dist reduces to standard L 1 distance. Conversely, as w (i) increases, dist gives lower values (and gradients) to the region inside the box, and severely punishes points outside. This function thus achieves three objectives. First, it treats points inside the box preferentially to points outside the box, unlike standard distance, which is agnostic to boxes. Second, it ensures that outside points receive high gradient through which they can more easily reach their target box, or escape it for negative samples. Third, it gives weight to the size of a box in distance computation, to yield a more comprehensive scoring mechanism.</p><p>Finally, we define the scoring function as the sum of the L-x norms of dist across all n entities and relation boxes, i.e.: score(r(e 1 , ..., e n )) = n i=1 dist(e r(e1,...,en) i , r (i) ) x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Properties</head><p>We analyze the representation power and inductive capacity of BoxE and show that BoxE is fully expressive, and can capture a rich language combining multiple inference patterns. We additionally show that BoxE can lucidly incorporate a given set of logical rules from a sublanguage of this language, i.e., rule injection. Finally, we analyze the complexity of BoxE in the appendix, and prove that it runs in time O(nd) and space O((|E| + n|R|)d), where n is the maximal relation arity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Full expressiveness</head><p>We prove that BoxE is fully expressive with d = |E| n−1 |R| dimensions. For KGs, this result implies d = |E||R|, so BoxE is fully expressive over KGs with dimensionality linear in |E|. The proof uses translational bumps to make an arbitrary true fact F false, while preserving the correctness of other facts. This result requires a careful technical construction, which (i) pushes a single entity representation within F outside its corresponding relation box at a specific dimension, and (ii) modifies all other model embeddings to prevent a change in the truth value of any other fact. Theorem 5.1. BoxE is a fully expressive model with the embedding dimensionality d of entities, bumps, and relations set to d = |E| n−1 |R|, where n &gt; 1 is the maximal arity of the relations in R.</p><p>We note that this result makes BoxE the first translation-based model that is fully expressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inference patterns and generalizations</head><p>We study the inductive capacity of BoxE in terms of common inference patterns appearing in the KGC literature, and compare it with earlier models. A comparison of BoxE against these models with respect to capturing prominent inference patterns is shown in <ref type="table" target="#tab_1">Table 1</ref>. A model captures an inference pattern if it admits a set of parameters exactly and exclusively satisfying the pattern. This is the standard definition of an inference pattern in the literature <ref type="bibr">[37]</ref>. For example, TransE can capture composition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">37]</ref>, but cannot capture hierarchy, as for TransE, r 1 (x, y) ⇒ r 2 (x, y) holds only if r 1 = r 2 , and thus r 2 (x, y) ⇒ r 1 (x, y), leading to loss of generality. However, this definition only addresses single applications of an inference pattern, which raises the question: can KBC models capture multiple, distinct instances of the same inference pattern jointly?</p><p>Capturing multiple inference patterns jointly is significantly more challenging. Indeed, TransE can capture r 1 (x, y) ∧ r 2 (y, z) ⇒ r 3 (x, z) and r 1 (x, y) ∧ r 4 (y, z) ⇒ r 3 (x, z) independently, but jointly capturing these compositions incorrectly forces r 2 ∼ r 4 . Similarly, bilinear models can capture the hierarchy rules r 1 (x, y) ⇒ r 3 (x, y) and r 2 (x, y) ⇒ r 3 (x, y) separately, but jointly capturing them incorrectly imposes either r 1 (x, y) ⇒ r 2 (x, y) or r 2 (x, y) ⇒ r 1 (x, y) <ref type="bibr" target="#b13">[14]</ref>. These examples are clearly not edge cases, and highlight severe limitations in how the inductive capacity of KBC models is analyzed. Therefore, we propose and study generalized inference patterns.</p><p>Definition 5.1. A rule is in one of the forms given in <ref type="table" target="#tab_1">Table 1</ref>, where r 1 = r 2 = r 3 ∈ R. To distinguish between types of rules, we write σ rule, where σ ∈ {symmetry, . . . , mutual exclusion}. A generalized σ pattern is a finite set of σ rules over R.</p><p>As before, a model captures a generalized inference pattern if the model admits a set of parameters, exactly and exclusively satisfying the generalized pattern. Our results for BoxE and all relevant models are summarized in <ref type="table" target="#tab_1">Table 1</ref>, and proven in the following theorem.</p><p>Theorem 5.2. All the results given in <ref type="table" target="#tab_1">Table 1</ref> for BoxE and other models hold.</p><p>Intuitively, BoxE captures all these generalized inference patterns through box configurations. For instance, BoxE captures (generalized) symmetry by setting the 2 boxes for a relation r to be equal, and captures (generalized) inverse relations r 1 and r 2 by setting r  respectively, and this extends to intersection in the usual sense. Finally, anti-symmetry and mutual exclusion, are captured through disjointness between relation boxes.</p><p>Generalized inference patterns are necessary to establish a more complete understanding of model inductive capacity, and, in this respect, our results show that BoxE goes well beyond any other model. However, generalized patterns are not sufficient. Indeed, different types of inference rules can appear jointly in practical applications, so KBC models must be able to jointly capture them. This is not the case for existing models. For instance, RotatE can capture composition and generalized symmetry, but to capture a single composition rule such as cousins(x, y) ∧ hasChild(y, z) ⇒ relatives(x, z), where relatives and cousins are symmetric relations, the model forces hasChild to be symmetric as well, i.e., hasChild(x, y) ⇒ hasChild(y, x), which is clearly absurd. Therefore, we also evaluate model inductive capacity relative to more general rule languages <ref type="bibr" target="#b13">[14]</ref>. We define a rule language as the union of different types of rules. Thus, generalized inference patterns are trivial rule languages allowing only one type of rule. BoxE can capture rules from a rich language, as stated next.</p><p>Theorem 5.3. Let L be the rule language that is the union of inverse, symmetry, hierarchy, intersection, mutual exclusion, and anti-symmetry rules. BoxE can capture any finite set of consistent rules from the rule language L. This result captures generalized inference patterns for BoxE as a special case. Such a result is implausible for other KBC models, given their limitations in capturing generalized inference patterns, and we are unaware of any analogous result in KBC. The only related result is for ontology embeddings, and for quasi-chained rules <ref type="bibr" target="#b13">[14]</ref>, but this result merely offers region structures enabling capturing a set of rules, without providing any viable model or means of doing so.</p><p>The strong inductive capacity of BoxE is advantageous from an interpretability perspective, as all the rules that BoxE can jointly capture can be simply "read" from the corresponding box configuration. Indeed, BoxE embeddings allow for rich rule extraction, and enable an informed understanding of what the model learns, and how it reaches its scores. This is a very useful consequence of inductive capacity, as better rule capturing directly translates into superior model interpretability. Finally, BoxE can seamlessly and naturally represent entity type information, e.g., country(UK) by modeling types as unary relations. In this setting, translational bumps are not applicable, and inference patterns deducible from classic box configurations can additionally be captured and extracted. By contrast, standard models require dedicated modifications to their parameters and scoring function <ref type="bibr">[46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref> to incorporate type information. This therefore further highlights the strong inductive capacity of BoxE, and its position as a unifying model for multi-arity knowledge base completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Rule injection</head><p>We now pose a complementary question to capturing inference patterns: can a KBC model be injected with a given set of rules such that it provably enforces them, improving its prediction performance? Formally, we say that a rule φ ⇒ ψ (resp., ψ ⇔ φ) can be injected to a model, if the model can be configured to force ψ to hold whenever φ holds (resp., φ holds whenever ψ holds and vice versa).</p><p>There is a subtle difference between capturing and injecting an inference pattern. Indeed, rules with negation, such a mutual exclusion, can be easily captured with any disjointness between r 1 and r 2 , but enforcing such a rule leads to non-determinism. To illustrate, r 1 and r 2 can be disjoint between their (i) head boxes, or (ii) tail boxes, or (iii) both, and at any combination of dimensions. This non-determinism only becomes more intricate as interactions across different rules are considered. We show that the positive fragment of the rule language that can be captured by BoxE, can be injected. Theorem 5.4. Let L + be the rule language that is the union of inverse, symmetry, hierarchy, and intersection rules. BoxE can be injected with any finite set of rules from the rule language L + .</p><p>Existing KGC rule injection methods (i) use rule-based training loss to inject rules <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">32]</ref>, potentially leveraging fuzzy logic <ref type="bibr" target="#b12">[13]</ref> and adversarial training <ref type="bibr" target="#b26">[27]</ref>, but cannot provably enforce rules, or (ii) constrain embeddings explicitly <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">32]</ref>, but only enforce very limited rules (e.g., inversion, linear implication). Indeed, most popular standard KGC methods fail to capture simple sets of rules <ref type="bibr" target="#b13">[14]</ref>. BoxE is a powerful model for rule injection in that it can explicitly and provably enforce such rules and incorporate a strong bias by appropriately constraining the learning space. Our study is related to the broader goal of making gradient-based optimization and learning compatible with reasoning <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Evaluation</head><p>In this section, we evaluate BoxE on a variety of tasks, namely, KGC, higher-arity KBC, and rule injection, and report state-of-the-art results, empirically confirming the theoretical strengths of BoxE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Knowledge graph completion</head><p>In this experiment, we run BoxE on the KGC benchmarks FB15k-237, WN18RR, and YAGO3-10, and compare it with translational models TransE <ref type="bibr" target="#b3">[4]</ref> and RotatE [37], both with uniform and self-adversarial negative sampling [37], and with bilinear models DistMult [49], ComplEx [39], and TuckER <ref type="bibr" target="#b0">[1]</ref>. We train BoxE for up to 1000 epochs, with validation checkpoints every 100 epochs and the checkpoint with highest MRR used for testing. We report the best published results on every dataset for all models, and, when unavailable, report our best computed results in italic. All results are for models with d ≤ 1000, to maintain comparison fairness <ref type="bibr" target="#b0">[1]</ref>. We therefore exclude results by ComplEx <ref type="bibr" target="#b19">[20]</ref> and DistMult [33] using d ≥ 2000. The best results by category are presented in bold, and the best results overall are highlighted by a surrounding rectangle. "(u)" indicates uniform negative sampling, and "(a)" denotes self-adversarial sampling. Further details about experimental setup, as well as hyperparameter choices and dataset properties, can be found in the appendix. Results. For every dataset and model, MR, MRR, and Hits@10 are reported in <ref type="table" target="#tab_2">Table 2</ref>. On FB15k-237, BoxE performs best among translational models, and is competitive with TuckER, especially in Hits@10. Furthermore, BoxE is comfortably state-of-the-art on YAGO3-10, significantly surpassing RotatE and TuckER. This result is especially encouraging considering that YAGO3-10 is the largest of all three datasets, and involves a challenging combination of inference patterns, and many fact appearances per entity. On YAGO3-10, we also observe that BoxE successfully learns symmetric relations, and learns box sizes correlating strongly with relational properties (cf. Appendix). Strong BoxE performance on FB15k-237, which contains several composition patterns, suggests that BoxE can perform well with compositions, despite not capturing them explicitly as an inference pattern.</p><p>On WN18RR, BoxE performs well in terms of MR, but is less competitive with RotatE in MRR. We investigated WN18RR more deeply, and identified two main factors for this. First, WN18RR primarily consists of hierarchical knowledge, which is logically flattened into deep tree-shaped compositions, such as hypernym(spoon, utensil). Second, symmetry is prevalent in WN18RR, e.g., derivationally_related_form accounts for 29,715 (∼34.5%) of WN18RR facts, which, combined with compositions, also helps RotatE. Indeed, in RotatE, the composition of two symmetric relations is (incorrectly) symmetric, but this is useful for WN18RR, where 4 of the the 11 relations are symmetric. That is, the modelling limitations of RotatE become an advantage given the setup of WN18RR, and enable it to achieve state-of-the-art performance on this dataset.</p><p>Overall, BoxE is competitive on all benchmarks , and is state of the art on YAGO3-10. Hence, it is a strong model for KGC on large, real-world KGs. We also evaluated the robustness of BoxE relative to dimensionality on YAGO3-10, and analyzed the resulting box configuration on this dataset from an interpretability perspective. These additional experiments can be found in the appendix. In this experiment, we evaluate BoxE on datasets with higher arity, namely the publicly available JF-17K and FB-AUTO. These datasets contain facts with arities up to 6 and 5, respectively, and include facts with different arities, i.e., 2, 3, 4, and 5. We compare BoxE with the best-known reported results over the same datasets <ref type="bibr" target="#b10">[11]</ref>. For this experiment, we set d = 200, for fairness with other models, and perform hyperparameter tuning analogously to Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Higher-arity knowledge base completion</head><p>Results. MRR and Hits@10 for all evaluated models are given in <ref type="table" target="#tab_3">Table 3</ref>. On both datasets, BoxE achieves state-of-the-art performance. This is primarily due to the natural extensibility of BoxE to non-uniform and higher arity. Indeed, BoxE defines unique boxes for every arity position, enabling a more natural representation of entity sets at every relation position. By contrast, all other models represent all relations with identical embedding structures, which can bottleneck the learning process, in particular when arities vary. Furthermore, the inductive capacity of BoxE also naturally extends to higher arities as a result of its structure, namely for higher-arity hierarchy, intersection, and mutual exclusion, which further improves its learning ability in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Rule injection</head><p>AgentBelongsToOrganization TeamPlaysInLeague PersonBelongsToOrganization AthletePlayedForSchool AthletePlaysForTeam AthletePlaysInLeague AthleteLedSportsTeam CoachesInLeague WorksFor CoachesTeam AthleteCoach In this experiment, we investigate the impact of rule injection on BoxE performance on the SportsNELL dataset, a subset of NELL <ref type="bibr" target="#b27">[28]</ref> with a known ontology, shown in <ref type="figure" target="#fig_4">Figure 3</ref>. We also consider the dataset SportsNELL C , which is precisely the logical closure of the SportsNELL dataset w.r.t. the given ontology (i.e., completion of SportsNELL under the rules).</p><p>We compare plain BoxE with BoxE injected with the Sport-sNELL ontology, denoted BoxE+RI. We train both models for 2000 epochs on a random subset (90%) of SportsNELL. First, we evaluate both models on all remaining facts from SportsNELL C , which we refer to as the full evaluation set, to measure the effect of rule injection. Second, we evaluate both models on a subset of the full evaluation set, only consisting of facts that are not directly deducible via the ontology from the training set (i.e., eliminating all inferences that can be made by a rule-based approach alone). This subset, which we call the filtered evaluation set, thus carefully tests the impact of rule injection on model inductive capacity. Results. The results on both evaluation datasets are shown in <ref type="table" target="#tab_4">Table 4</ref>. On the full evaluation dataset, BoxE+RI performs significantly better than BoxE. This shows that rule injection clearly improves the performance of BoxE. Importantly, this performance improvement cannot solely be attributed to the facts that can be deduced directly from the training set (with the help of the rules), as BoxE+RI performs much better than BoxE also over the filtered evaluation set. These experiments suggest that rule injection improves the inductive bias of BoxE, by enforcing all predictions to also conform with the given set of rules, as required. Intuitively, all predictions get amplified with the help of the rules, a very desired property, as many real-world KBs have an associated schema, or a simple ontology.</p><p>While allowing to amplify predictions, rule injection can potentially lead to poor performance with existing metrics. Indeed, if a model mostly predicts wrong facts, these would lead to further wrong conclusions due to rule application. Hence, a low-quality prediction model can find its performance further hindered by rule injection, as false predictions create yet more false positives, thereby lowering the rank of any good predictions in evaluation. Therefore, rule injection must be complemented with models having good inductive capacity (for sparser and simpler datasets) and expressiveness (for more complex and rich datasets), such that they yield high-quality predictions in all data settings [38].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>We presented BoxE, a spatio-translational model for KBC, and proved several strong results about its representational power and inductive capacity. We then empirically showed that BoxE achieves state-of-the-art performance both for KGC, and on higher-arity and different-arity KGC. Finally, we empirically validated the impact of rule injection, and showed it improves the overall inductive bias and capacity of BoxE. Overall, BoxE presents a strong theoretical backbone for KBC, combining theoretical expressiveness with strong inductive capacity and promising empirical performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Alan Turing Institute under the UK EPSRC grant EP/N510129/1, the AXA Research Fund, and by the EPSRC grants EP/R013667/1 and EP/M025268/1. Ralph Abboud is funded by the Oxford-DeepMind Graduate Scholarship and the Alun Hughes Graduate Scholarship. Experiments for this work were conducted on servers provided by the Advanced Research Computing (ARC) cluster administered by the University of Oxford.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The representation and inference of knowledge is essential for humanity, and thus any improvements in the quality and reliability of automated inference methods can significantly support endeavors in several application domains. This work provides a means for dealing with incomplete knowledge, and offers users to complete their knowledge bases with the help of automated machinery. The model predictions rely mostly on interpretable and explainable logical patterns, which makes it easier to analyze the model behavior. Furthermore, this work enables safely injecting background rules when completing knowledge bases, and this safety is of great value in settings where inferred knowledge is critical (e.g., completing medical knowledge bases). This work thus also provides a logically grounded approach that improves the quality of predictions and completions in safetycritical settings. The ability of the proposed model to naturally handle more general knowledge bases (beyond knowledge graphs) could also unlock the use of knowledge base completion technologies on important knowledge bases which were previously ignored.</p><p>[29] Deepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. Learning attention-based embeddings for relation prediction in knowledge graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 5.1 (Full Expressiveness)</head><p>We first prove the result for knowledge graphs, and then show how this can be lifted to arbitrary knowledge bases with higher-arity relations.</p><p>The result is shown by induction. We start with a base case where the KG G contains all facts from the universe as true facts, and subsequently prove in the induction step that a BoxE model with d = |E||R| can make any arbitrary fact in G false without affecting the correctness of other facts. In this induction step, facts are made false by pushing the representation of a single entity in the fact outside its corresponding relation box at a specific dimension, and modifying the remaining embeddings in the model to prevent a change in the truth value of any other fact.</p><p>Let us assume without loss of generality that all relations and entities are indexed. Specifically, we consider relations r i ∈ R, and entities e j ∈ E, where 0 ≤ i ≤ |R| − 1, and 0 ≤ j ≤ |E| − 1. We consider d-dimensional embedding vectors v with d = |E||R|, and write v(i, j) to refer to the vector index i|E| + j. Intuitively, in our construction, the sequence of indices v(i, 0), . . . , v(i, |E| − 1) corresponds to a "chunk" reserved for the relation r i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base case:</head><p>We initialize the KG G as the whole universe, i.e., the set of all possible facts over a given vocabulary. BoxE can trivially express G, by simply setting all entity and bump vectors to 0, and all boxes as the unit box centered at 0.</p><p>Induction step: In this step, we consider a true fact r i (e j , e k ), and make this fact false without affecting the remainder of G. This can be done as follows:</p><p>Step 1. Increment b j (i, k) by a value C, such that:</p><formula xml:id="formula_4">e k (i, k) + b j (i, k) + C &gt; u<label>(2)</label></formula><p>i (i, k).</p><p>Step 2. Decrement all entity embeddings except that of e k by C at dimension (i, k):</p><formula xml:id="formula_5">∀ k = k, e k (i, k) = e k (i, k) − C.</formula><p>Step 3. For the relation r i , grow the head box by C at dimension (i, k) both upwards and downwards, and grow the tail box downwards by C in this dimension:</p><formula xml:id="formula_6">l (1) i (i, k) = l (1) i (i, k) − C, u (1) i (i, k) = u (1) i (i, k) + C, l (2) i (i, k) = l (2) i (i, k) − C.</formula><p>Step 4. For all other relations r x ∈ R, x = i, grow all boxes by C at dimension (i, k) in both directions, that is, for β ∈ {1, 2}:</p><formula xml:id="formula_7">l (β) x (i, k) = l (β) x (i, k) − C, u (β) x (i, k) = u (β) x (i, k) + C.</formula><p>Observe first that Step 1 makes r i (e j , e k ) false, by pushing e ri(ej ,ek) k outside of r (2) i at dimension (i, k) from above. This flips the truth value of r i (e j , e k ), as required.</p><p>We now show that the results of Steps 1 &amp; 2, combined with the changes to relation boxes made in Steps 3 &amp; 4, which affect facts involving r i and other relations respectively, preserve the correctness/falsehood of all facts other than r i (e j , e k ). To this end, we consider any possible fact F = r i (e j , e k ) from the KG, and analyze the effect of the induction step at the head and tail of the fact. We need to consider the following cases: Case 1. The fact F is true: To verify that F = r i (e j , e k ) remains true after the inductive step, we analyze both the head entity e F j and the tail entity e F k .</p><p>(a) Head entity: Observe that (i) e F j can change by at most C following Steps 1 &amp; 2, and (ii) all relation head boxes are grown by C in both directions in Steps 3 &amp; 4. These together imply that e F j ∈ r <ref type="bibr" target="#b0">(1)</ref> is guaranteed to hold provided that it was true before the induction step. (b) Tail entity: If e k = e k , then e F k is not changed if e j = e j , and decremented by C at dimension (i, k) otherwise. Hence, the changes to both r (2) i and r <ref type="bibr" target="#b1">(2)</ref> x , x = i in Steps 3 &amp; 4 are sufficient to maintain e k ∈ r <ref type="bibr" target="#b1">(2)</ref> . Conversely, if e k = e k , then e F t is unchanged when e j = e j , and thus e k ∈ r (2) still holds. Otherwise, when e j = e j , e F k is incremented by C, which, for r = r i , makes F false, as required, and for r = r i , still keeps e k ∈ r <ref type="bibr" target="#b1">(2)</ref> , as all other tail boxes are grown upwards by C.</p><p>Hence, for any true fact in G, except the fact r i (e j , e k ), we conclude that e j ∈ r <ref type="bibr" target="#b0">(1)</ref> and e k ∈ r <ref type="bibr" target="#b1">(2)</ref> continues to hold after the induction step, as required.</p><p>Case 2. The fact F is false: To verify that F = r i (e j , e k ) remains false, after the inductive step, we again consider the head and tail entities.</p><p>(a) Head entity: By construction, all false facts r i (e j , e k ) satisfy the inequality e r i (e j ,e k ) k</p><formula xml:id="formula_8">(i , k ) &gt; u (2) i (i , k ),</formula><p>and any changes to e F j do not affect this inequality. (b) Tail entity: If e k = e k , then F verifies e F (i , k ) &gt; u</p><formula xml:id="formula_9">(2) i (i , k ), where k = k.</formula><p>This inequality continues to hold regardless of the changes to e F k (i, k). Otherwise, if e k = e k , and r i = r i , then e j = e j , as F is initially false, and r i (e j , e k ) is initially true. Furthermore, since e j = e j , e F k is unchanged, which maintains the falsehood inequality. Finally, if r i = r i , then the falsehood inequality for F holds at a dimension different than (i, k). Therefore, none of the changes in the induction step affect this inequality.</p><p>Hence, all false facts in G remain false after the induction step, as required.</p><p>Thus, the induction step can make any true fact r i (e j , e k ) in G false in a BoxE model with d = |E||R| without affecting the remainder of the facts in G. Hence, all fact configurations are possible and expressible by such a BoxE model, and this model is fully expressive, as required.</p><p>This proof can be generalized to higher-arity knowledge bases. Indeed, for a maximum arity of n, a dimensionality d = |E| n−1 |R| is needed for full expressiveness. All proof steps shown above would remain the same, except that (i) we define a higher-arity indexing function (θ 1 , θ 2 , ..., θ K ), which refers to vector index n a=1 |E| n−a θ i , and (ii) grow boxes for r i and all other r x at positions 3 and onwards in both Steps 3 and 4, in addition to position 1, by C in both directions (while the changes at position 2 remain the same).</p><p>Finally, we note that the proof can be trivially extended to knowledge bases with non-uniform arities (i.e., KBs containing relations with different arities) by introducing extra parameters to relations of lower arity, and setting the correctness of the n−arity facts solely based on the original facts. Hence, BoxE is a fully expressive model for general KBs containing both distinct and large relation arities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorem 5.2 (Inference Patterns and Generalizations)</head><p>We start by more explicitly reformulating Definition 5.1 in the main body of the paper. Definition C.1. Generalized inference patterns are defined as follows:</p><p>• A symmetry rule is of the form r 1 (x, y) ⇒ r 1 (y, x), where r 1 ∈ R. A generalized symmetry pattern is a finite set of symmetric rules over R.</p><p>• An anti-symmetry rule is of the form r 1 (x, y) ⇒ ¬r 1 (y, x), where r 1 ∈ R. A generalized anti-symmetry pattern is a finite set of anti-symmetric rules over R.</p><p>• An inversion rule is of the form r 1 (x, y) ⇔ r 2 (y, x), where r 1 = r 2 ∈ R. A generalized inversion pattern is a finite set of inverse rules over R.</p><p>• A composition rule is of the form r 1 (x, y) ∧ r 2 (y, z) ⇒ r 3 (x, z), where r 1 = r 2 = r 3 ∈ R.</p><p>A generalized composition pattern is a finite set of composition rules over R.</p><p>• A hierarchy rule is of the form r 1 (x, y) ⇒ r 2 (x, y),where r 1 = r 2 ∈ R. A generalized hierarchy pattern is a finite set of hierarchy rules over R.</p><p>• An intersection rule is of the form r 1 (x, y) ∧ r 2 (x, y) ⇒ r 3 (x, y), where r 1 = r 2 = r 3 ∈ R.</p><p>A generalized intersection pattern is a finite set of intersection rules over R.</p><p>• A mutual exclusion rule is of the form r 1 (x, y) ∧ r 2 (x, y) ⇒ ⊥, where r 1 = r 2 ∈ R. A generalized mutual exclusion pattern is a finite set of mutually exclusive rules over R.</p><p>Every generalized inference pattern defines a trivial rule language, consisting of a single type of rule. We define more general rule languages, by taking the union of different types of rules. A rule language L is defined in terms of the types of rules that are allowed in the language.</p><p>Remark. The requirement for setting the relations to be distinct is due to the existing conventions in the literature. This may appear somewhat unintuitive, but it is required to study the rules in isolation, i.e., a composition rule without this requirement can express transitivity by defining r 1 (x, y) ∧ r 2 (y, z) ⇒ r 1 (x, z) which cannot be captured by models that do capture composition. Nevertheless, this assumption does not lead to loss of generality for our study of generalized inference patterns, since we are allowed to use many rules, which in turn, can easily simulate cases that are excluded. For instance, the following rules: r 1 (x, y) ∧ r 2 (y, z) ⇒ r 3 (x, z), and r 3 (x, y) ⇒ r 1 (x, y) together simulate the transitivity rule given above.</p><p>We prove the statements in Theorem 5.2 in two seperate parts. First, we prove the results given for BoxE from <ref type="table" target="#tab_1">Table 1</ref>, and then we show the results given for the other models from <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Theorem 5.2: BoxE</head><p>We show that each generalized inference pattern can be captured by BoxE except for the composition pattern. For the latter, we argue why BoxE cannot capture this explicitly, as an inference pattern.</p><p>Generalized intersection. We first introduce the concept of boxicity. Let G = (V, E) be a graph, where V is the set of nodes, and E is the set of edges. The boxicity of G is the minimum embedding dimension in which G can be represented as an intersection of axis-aligned boxes, such that (i) every box corresponds to a specific node, (ii) boxes intersect iff an edge connects their respective nodes <ref type="bibr">[31]</ref>. It has been shown that the boxicity of a graph with p edges is O( p · log(p)) <ref type="bibr" target="#b4">[5]</ref>. This implies that, given a graph G, where every relation r ∈ R is represented as a node in the graph, and every edge between them represents an intersection, any finite combination of intersections between relations can be represented in a finite-dimensional vector space of worst-case dimensionality O(|R| log(|R|)).</p><p>For a given knowledge graph, we define a relation intersection graph. That is, for every relation r ∈ R, we define two nodes, corresponding to its head and tail boxes, and then set edges in the graph based on desired intersections between relation boxes, which are dictated by intersection rules.</p><p>Prior to encoding rules into relation intersection graph edges, we first compute the deductive closure of the set of intersection rules. In other words, we check whether any rule of the form r i (x, y) ⇒ r j (x, y), or r i (x, y) ∧ r j (x, y) ⇒ r k (x, y) for any i, j, k can be entailed from the given set of rules, and keep adding new rules to this initial set, until no more rules can be deduced. That is, we compute the logical closure of the initial set. This allows us to make all possible intersections between relations explicit.</p><p>Then, we map all rules in the computed deductive closure to edges as follows:</p><p>• For every intersection rule r 1 (x, y) ∧ r 2 (x, y) ⇒ r 3 (x, y), we set edges between the node corresponding to the head of r 3 and those of r 1 and r 2 , with the same done for tail nodes. • For every deduced hierarchy rule r 1 (x, y) ⇒ r 2 (x, y), we set edges between the head nodes of r 2 and r 1 , with the same done for tail nodes.</p><p>With the resulting relation intersection graph G, we have encoded necessary conditions for all rules to hold, namely that relations whose intersections are contained in other relations intersect with these relations. We now leverage the boxicity argument, and show that there exists a box configuration of finite dimensionality capturing all the intersections encoded in G. This box configuration captures all intersections needed between the respective boxes for the rules to hold, but is not necessarily sufficient to capture hierarchies and box containment. Hence, we modify the aforementioned box configuration using a procedure, which we apply iteratively over every intersection rule, such that the final configuration provably captures all rules, without capturing additional undesired rules.</p><p>Our box reconfiguration procedure is as follows:</p><p>1. Iterate over every intersection rule r 1 (x, y) ∧ r 2 (x, y) ⇒ r 3 (x, y): (a) If the r 3 head and tail boxes do not contain the head and tail box intersections r 1 ∩ r 2 , then we grow these r 3 boxes by the minimum possible amount to make this condition hold and establish the rule. In other words, we grow the r 3 boxes at every position to equal the boundary of either the r 1 boxes or the r 2 boxes at the dimensions where the rule does not hold due to r 1 or r 2 . This growth operation preserves all existing edges in G, and does not force new intersections, as all forced intersections due to rule capturing are already encoded by the existing edges. (b) Following Part (a), the growth of r 3 can violate another rule in the set, in particular if r 3 is in the body of this rule. Hence, when any r 3 boxes are grown in Part (a), check all other intersection rules in the rule set: If the change in r 3 makes a rule no longer hold (i.e., the rule was captured prior to growing r 3 and no longer is), then recursively call this procedure for this rule.</p><p>We now show that this procedure is correct, and then prove that it terminates, particularly with respect to the number of recursive calls made. First, we note that, following a successful iteration on a given rule, a rule is successfully captured (Part (a)), and no other rules are violated in the process (Part (b)). Thus, the final configuration returned by this procedure over the initial boxicity-given configuration returns a valid BoxE configuration. In particular, this configuration captures all and only the provided patterns within the deductive closure, which includes the original intersection rules. Furthermore, growing r 3 boxes in the configuration to satisfy an intersection rule does not induce any rules outside their deductive closure. Indeed, when r 3 boxes are grown, they are only grown in dimensions where they fail to capture r 1 ∩ r 2 . Thus, the procedure can only make r 3 intersect with boxes that intersect with r 1 or r 2 . As a result, the procedure can only force intersections between boxes within the deductive closure of the rule set.</p><p>It now remains to show that this procedure terminates, and thus that a configuration of this kind indeed can be found. In particular, we study the maximal number of recursive calls needed. Consider a rule ρ : r 1 (x, y) ∧ r 2 (x, y) ⇒ r 3 (x, y), where r 3 boxes are grown. For simplicity, we only consider a single box for r 3 , i.e., a unique arity position, as the analysis is analogous at every arity position. We define boundaries as being the lower and upper limits of a box at every dimension. Thus, a d-dimensional box has 2d boundaries. Therefore, in our binary BoxE configuration with |R| relations and d = O(|R| log |R|), there are O(|R| 2 log |R|) boundaries. For our analysis, we are interested in the number of distinct boundaries in our configuration.</p><p>We now consider the effect of an application of a call to Part (a) of the procedure on the number of distinct boundaries. If ρ is already captured, then no action is needed. Otherwise, r 3 needs to be grown. Hence, in this scenario, there exists at least one dimension in which the lower (resp., upper) boundary of r 3 is strictly higher (resp., lower) than the maximum (resp., minimum) lower (resp., upper) bound of either r 1 or r 2 . Therefore, when r 3 is grown, the value of the problematic bound(s) at this dimension is made equal to the corresponding bound(s) of r 1 or r 2 . As a result, the number of distinct boundaries is guaranteed to strictly drop by at least 1 following any growth operation.</p><p>Furthermore, we consider the recursive calls made in Part (b), after any growth to r 3 . Observe that recursion is only called when the change to r 3 exclusively makes the checked rule false. This condition ensures that all recursive calls are made only when the growing of the rule head boxes, in this case r 3 , is the exclusive cause for rule violation, and so eliminates all other possible causes of violation such that they are handled only when the outer loop iterating reaches the corresponding rule, and thus greatly simplifies the analysis. Finally, we observe that box growth can only be triggered when distinct boundaries exist. Hence, when the number of distinct boundaries drops to its (highly pessimistic and loose) minimum possible value of 1, no more recursive calls can be made. This observation, combined with the earlier finding that every box growth strictly reduces the number of distinct boundaries by at least 1, implies that the number of recursive calls in this procedure is upper bounded by O(|R| 2 log |R|). Hence, this procedure terminates, and a BoxE configuration capturing generalized intersections exists.</p><p>Generalized hierarchy. The proof for generalized intersection immediately applies to generalized hierarchies.</p><p>Generalized symmetry. The symmetry inference pattern is a single-relation pattern, and can appear at most once per relation. Symmetry can be easily captured for a relation r by setting r <ref type="bibr" target="#b0">(1)</ref> and r <ref type="bibr" target="#b1">(2)</ref> to be identical boxes. This can be independently done for any relation, and thus BoxE captures generalized symmetry.</p><p>Generalized anti-symmetry. Analogously to generalized symmetry, anti-symmetry is a singlerelation pattern. This pattern is captured by setting r <ref type="bibr" target="#b0">(1)</ref> and r <ref type="bibr" target="#b1">(2)</ref> to be disjoint for every antisymmetric r. Therefore, BoxE captures generalized anti-symmetry.</p><p>Generalized inversion. An inversion pattern r 1 (x, y) ⇔ r 2 (y, x) can be captured by setting r Generalized mutual exclusion. It is sufficient to observe that there exists a BoxE configuration for any arbitrary set of mutual exclusion rules due to the boxicity argument: simply consider a graph G with no edges connecting mutually exclusive relations. A simpler argument can be given directly: generalized mutual exclusion can be achieved by making one of the relation boxes (head, or tail) disjoint in a fixed-dimensional space.</p><p>(Generalized) composition. Consider the composition pattern r 1 (x, y) ∧ r 2 (y, z) → r 3 (x, z). In this pattern, we see that the entity that will appear in lieu of variable x will be bumped differently in every atom, as it appears with different entities. More concretely, if we replace variables x, y, z with entities e 1 , e 2 , e 3 respectively, then e r1 1 = e 1 + b 2 and e r3 1 = e 1 + b 3 . We can also view bumps as equivalently applying to boxes, i.e., instead of e 1 + b 2 ∈ r   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Theorem 5.2: Other models</head><p>In what follows, we generally define KGC embedding models such that every KG entity is represented by a vector in R d , and every relation defines two map functions r h , r t : R d → R d , which apply to head and tail embeddings, respectively. We further define the relation scoring function over a KG triple s r : R d × R d → R as a map from entity pair representations following the application of r h and r t to a real-valued score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Translational models: TransE and RotatE</head><p>We note that some of the results stated below are taken from the literature, but we included them nevertheless for completeness. The novel results are given for the generalized inference patterns.</p><p>For translational models, r h (e 1 ) encodes the translation (resp. rotation) operation, r t (e 2 ) = e 2 , and s r (e 1 , e 2 ) = r t (e 2 ) − r h (e 1 ) .</p><formula xml:id="formula_10">Hierarchy. Let M r = s −1 r ([0, ])</formula><p>, where s −1 r is the inverse map of s r , be the subset of embedding pairs (v, w) ∈ R d × R d such that s r (v, w) ≤ , i.e., the decision region of the relation r with margin . As a result, r 1 (x, y) ⇒ r 2 (x, y) holds iff M 1 ⊂ M 2 . In TransE (resp., RotatE), (e 1 , e 2 ) ∈ M r if e 1 + r − e 2 ∈ D (0) (resp., e 1 • r − e 2 ∈ D (0)), where D (0) is the disk of center 0 and radius . Since it is necessary that M 1 ⊂ M 2 , we require that the disk D 1, (e 1 + r 1 ) (resp., D 1, (e 1 • r 1 )) and radius is contained in the corresponding disk D 2 , defined analogously using r 2 . Since D 1 and D 2 have the same margin-induced radius, this is only possible if r 1 = r 2 , effectively enforcing relation equivalence. Thus, neither translational model can capture hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intersection. A model can represent the intersection pattern</head><formula xml:id="formula_11">r 1 (x, y) ∧ r 2 (x, y) ⇒ r 3 (x, y) if M 1 ∩M 2 ⊂ M 3 .</formula><p>In TransE and RotatE, this is satisfied if r 3 lies in the centre of the disk intersection of D (r 1 ) and D (r 2 ), thus both models capture intersection. However, both models clearly fail to capture generalized intersection. In particular, if we consider rules r 1 (x, y) ∧ r 2 (x, y) ⇒ r 3 (x, y) and r 3 (x, y) ∧ r 2 (x, y) ⇒ r 1 (x, y), the rule r 2 (x, y) ⇒ r 1 (x, y) is logically implied. But this is a hierarchy rule that clearly cannot be captured by either model. Hence, TransE and RotatE cannot capture generalized intersections.</p><p>Symmetry. In TransE, r(x, y) ⇒ r(y, x) holds iff r = 0, which implies that r is reflexive. Thus, TransE does not capture symmetry. In contrast, in RotatE, symmetry is captured iff r = {±kπ} d , k ∈ N, i.e., a rotation vector consisting exclusively of multiples of π. Symmetry is a single-relation pattern, and thus multiple rules, affecting different relations, can be captured independently. Hence, RotatE captures generalized symmetry.</p><p>Anti-symmetry. In TransE, a relation r is anti-symmetric iff r ≥ . The result for RotatE is proven in the original work <ref type="bibr">[37]</ref>. As anti-symmetry is a single-relation pattern, it can be applied independently across all relations. Thus, both TransE and RotatE capture generalized anti-symmetry. Mutual exclusion. To capture mutual exclusion between relations r 1 and r 2 , the model must satisfy M 1 ∩ M 2 = ∅. In TransE, this holds iff r 1 − r 2 ≥ 2 . Analogously, for RotatE, this holds if |r i − r j | ≥ arcsin(2 ) at every dimension and all node embeddings have a norm of at least 1. Such constructions can be set up for arbitrarily many mutual exclusion pairs, through decreasing or increasing the magnitude of embeddings. Thus, both TransE and RotatE can capture generalized mutual exclusions.</p><p>Composition. For TransE (resp., RotatE), two relations r 1 and r 2 compose a third relation r 3 iff r 1 + r 2 = r 3 (resp., r 1 • r 2 = r 3 ). On the other hand, both fail to capture generalized compositions. In particular, for the rules r 1 (x, y) ∧ r 2 (y, z) ⇒ r 3 (x, z) and r 1 (x, y) ∧ r 4 (y, z) ⇒ r 3 (x, z), both models force r 2 = r 4 (In RotatE, the equality is modulo 2π).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Bilinear models: DistMult, ComplEx, TuckER</head><p>TuckER is shown to subsume DistMult and ComplEx <ref type="bibr" target="#b0">[1]</ref>, so all positive results for either ComplEx and DistMult automatically follow for TuckER. Hence, these positive results for TuckER are omitted from the presentation. Analogously, when negative results are shown for TuckER, they automatically propagate to DistMult and ComplEx.</p><p>We now formally introduce TuckER. TuckER learns a tensor W ∈ R de×dr×de , d e , d r ∈ N, a vector e ∈ R de for every entity, and a vector r ∈ R dr for every relation, and s r (e 1 , e 2 ) = W · e 1 · r · e 2 . For ease of notation, we define v 1,r = W × e 1 × r. The scoring function can then be written as s r (e 1 , e 2 ) = v r,1 · e 2 . Given a head entity e 1 and a relation r, we define the space</p><formula xml:id="formula_12">A 1,r = {x ∈ R de | v r,1 · x ≥ }.</formula><p>Hierarchy. For bilinear models, it has been shown that individual hierarchies can be captured, but not generalized hierarchies <ref type="bibr" target="#b13">[14]</ref>. In particular, to satisfy the rules r 1 (x, y) ⇒ r 3 (x, y) and r 2 (x, y) ⇒ r 3 (x, y) simultanously, bilinear models must set either r 1 (x, y) ⇒ r 2 (x, y) or r 2 (x, y) ⇒ r 1 (x, y).</p><p>Intersection. We show that TuckER cannot capture intersections. In TuckER, a rule of the form</p><formula xml:id="formula_13">r 1 (x, y) ∧ r 2 (x, y) ⇒ r 3 (x, y) holds iff A r2,1 ∩ A r1,1 ⊂ A r3,1 , ∀e ∈ R de . This is true iff v r1,1 , v r2,1</formula><p>, v r3,1 are colinear, and thus that r 1 , r 2 , and r 3 are colinear. However, this also implies that either r 1 (x, y) ⇒ r 2 (x, y), or r 2 (x, y) ⇒ r 1 (x, y). Hence, TuckER fails to capture intersections.</p><p>Symmetry. ComplEx captures symmetry patterns by having real-only embedding matrices for its relations. DistMult is inherently symmetric by construction. Since symmetry is a single-relation pattern, multiple symmetries can be independently captured, and thus all three models can capture generalized symmetry.</p><p>Anti-symmetry. DistMult cannot capture anti-symmetry, as it is inherently a symmetric model. ComplEx captures anti-symmetry by having imaginary-only embedding matrices for its relations. Analogously to symmetry, anti-symmetry is also a single-relation pattern, and thus ComplEx (and TuckER) can capture generalized anti-symmetry.</p><p>Inversion. It is known that DistMult cannot capture inversions, while ComplEx can <ref type="bibr">[37]</ref>. Generalized inversion can also be captured in ComplEx, as symmetry, the only other type of rule deducible from multiple inversions, is also captured by ComplEx.</p><p>Mutual exclusion. In TuckER, two relations r 1 and r 2 are mutually exclusive iff r 1 = −r 2 . This implies TuckER can capture mutual exclusion, but cannot capture generalized mutual exclusions. In particular, to satisfy r 1 (x, y) ∧ r 2 (x, y) ⇒ ⊥ and r 1 (x, y) ∧ r 3 (x, y) ⇒ ⊥, TuckER forces r 2 = r 3 .</p><p>Composition. It is shown that both ComplEx and DistMult cannot capture composition patterns <ref type="bibr">[37,</ref><ref type="bibr" target="#b13">14]</ref>. Furthermore, it is also known that relation maps must be bijective to be able to represent composition [37]. This is not the case in TuckER, as relations are surjective maps from R de×dr to R de , and linear bijections between vector spaces are only possible with the same dimensionality. Hence, TuckER also cannot capture compositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Theorem 5.3 (Inference Patterns as Rule Languages)</head><p>We show that a BoxE model of dimensionality d = O(|R| 2 ) captures the rule language specified in Theorem 5.3. This is achieved by leveraging the ideas from the generalized inference patterns proof in Appendix C. Indeed, our existence proof also builds on the boxicity argument used in this proof.</p><p>Let S be a set of rules, and let S p , S a , and S m be subsets of S, where S p consists of hierarchy, symmetry, inversion, and intersection rules, S a consists of anti-symmetry rules, and S m consists of mutual exclusion rules. We first show that rules from S p ∪ S a can be captured, then extend this to additionally capture S m .</p><p>Step 1: Defining the relation intersection graph. We define a set of 2|R| nodes, where every relation is encoded with 2 nodes for its head and tail boxes. We now constrain this graph to eventually capture all rules in S p . First, we capture all symmetry and inversion rules as follows:</p><p>1. Symmetry: For every symmetry rule, we combine the corresponding head and tail nodes of a relation r to a single node. In other words, a single relation r is made symmetric by encoding both r <ref type="bibr" target="#b0">(1)</ref> and r <ref type="bibr" target="#b1">(2)</ref> with one same node. This encoding enforces that the head and tail boxes of r are identical, and thus that r is indeed symmetric, as required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Inversion:</head><p>For every inversion rule r 1 (x, y) ⇒ r 2 (y, x), we combine the respective head and tail nodes of r 1 and r 2 such that r </p><p>2 , are each represented by one node. This makes that their corresponding boxes are equal, effectively capturing inversion patterns.</p><p>Following this step, G now consists of at most 2|R| nodes, and captures symmetry and inversion rules jointly. It now remains to define edges in G, as needed to later capture intersection and hierarchy rules. This is done analogously to the proof for generalized intersections (cf. Appendix C.1): First, the deductive closure of all intersection and hierarchy rules is computed, and the corresponding edges are encoded in G. Note that the resulting graph G continues to capture inversion and symmetry, as these rules are encoded through nodes, and also encodes the deductive closure of all rules in S p . Indeed, any box intersection imposed by the deductive closure of intersection and hierarchy rules with a node capturing a symmetry or inversion rule automatically implies a box intersection with the multiple boxes that the node represents. Hence, G enables capturing symmetry and inversion rules a priori, as well as jointly sets up the necessary edges for hierarchy and inversion rules. Finally, we leverage the boxicity argument, and our final graph G, to obtain a box configuration where all the box intersections needed to later capture hierarchy and intersection rules are present (but not necessarily capturing hierarchy and intersection patterns at this stage), and which also successfully captures inversion and symmetry rules.</p><p>Step 2: Anti-symmetry (S a ). Anti-symmetry rules are captured by adding additional dimensions to the box configuration resulting from Step 1 to distinguish between the head and tail boxes of an anti-symmetric relation. S is consistent, therefore only anti-symmetry rules not contradicting the set of rules S p ∪ S m can be given. For example, if symmetry rule r(x, y) ⇔ r(y, x) ∈ S p , then r(x, y) ⇒ ¬r(y, x) / ∈ S a . This is important, as it implies that no combination of hierarchy, inversion, intersection symmetry, and mutual exclusion rule can force an intersection between r (1) and r <ref type="bibr" target="#b1">(2)</ref> , for any anti-symmetric r, and thus, that subsequent steps in this proof preserve the anti-symmetry captured in this step.</p><p>We now capture anti-symmetry rules by dedicating a new "disjointness" dimension for all boxes, such that, for an anti-symmetric relation r, the box ranges for head and tail boxes are made disjoint in this dimension, i.e., [l <ref type="bibr" target="#b0">(1)</ref> , u <ref type="bibr" target="#b0">(1)</ref> ]∩[l <ref type="bibr" target="#b1">(2)</ref> , u (2) ] = φ, and are set arbitrarily for all other relations, such that, for all rules in S p , if an anti-symmetric r is the head of a hierarchy rule r 1 =⇒ r, then the ranges of r 1 in this dimension respect the hierarchy and, for an intersection rule r 1 ∧ r 2 =⇒ r, then r 1 ∩ r 2 ⊂ r. This initialization exists, as S is consistent, so cannot create conflicting interval requirements for relations in rules. One can also observe this by considering this initialization a recursive pass through the rule sets affected by the anti-symmetric relations, where all other uninitialized relations in the deductive closure are not yet set. Hence, this new dimension captures r (1) ∩ r (2) = φ, so correctly captures anti-symmetry, and cannot be broken by subsequent rule-based box growth. It also is compatible with all symmetry and inversion rules, as box sharing is maintained. Hence, our current BoxE configuration captures any consistent set of anti-symmetry, symmetry, and inversion rules. Given S a , at most |S a | additional dimensions are needed, and since at most |R| anti-symmetry rules can exist, the worst-case dimensionality of our configuration remains O(|R| log (|R|)). We now build on this result and show that the current configuration can be modified to additionally capture intersection and hierarchy rules.</p><p>Step 3: Hierarchies and intersections. Given the box configuration at the end of Step 2, we now apply the box reconfiguration procedure presented in the generalized intersections proof (cf. Appendix C.1) to capture all hierarchy and intersection rules in S. We also note that, since S is consistent, no hierarchy and intersection rules force any inconsistency with the already captured symmetry, anti-symmetry and inversion rules, e.g., if r 1 (x, y) ⇒ r 1 (y, x), r 2 (x, y) ⇒ ¬r 2 (y, x) ∈ S, then r 1 (x, y) ⇒ r 2 (x, y) / ∈ S. Thus all symmetry, anti-symmetry, and inversion patterns, whose capture is based on structural concepts (box sharing and dedicated dimensions respectively), are preserved. In particular, box sharing is unaffected, and no box growth from this step can break the disjointness of anti-symmetric relation boxes, as S is consistent. The completeness of the procedure with respect to hierarchy and intersection rules is also shown in Appendix C.1.</p><p>Step 4: Mutual exclusion. Given the BoxE configuration from Step 3, capturing rules from S p ∪S a , we also capture rules from S m with additional dimensions. Indeed, we show that this can be done using a BoxE configuration with d = O(|R| 2 ) dimensions. Starting from the configuration after the completion of Step 3, we now dedicate a single dimension per mutual exclusion rule, and capture this pattern as follows: For every mutual exclusion rule, we set a dimension, where r 1 and r 2 have disjoint range intervals z 1 , z 2 ⊂ [0, 1], such that, without loss of generality, z 1 = [z 1,min , z 1,max ], z 2 = [z 2,min , z 2,max ] and z 2,min &gt; z 1,max . Then, we set the range of every other box in the configuration at this new dimension analogously to Step 2 (i.e., arbitrarily, but in a rule-aware fashion) by repeating the box reconfiguration procedure in Step 3 for capturing hierarchy and intersection rules starting from the current configuration.</p><p>Note that anti-symmetry, symmetry, and inversion rules play no part in this step, as anti-symmetry rules are captured with dedicated dimensions as shown earlier, whereas symmetry and inversion rules are already enforced, and thus captured, through box sharing and equality.</p><p>Intuitively, this step first makes r 1 and r 2 mutually exclusive in one dimension, then recursively traverses the set of hierarchy and intersection rules, as in Step 3, to preserve the capturing of these rules in this new dimension specifically. Clearly, anti-symmetry remains true, since its dedicated dimension is not affected by the repetition of Step 3. Furthermore, since S is consistent, all mutual exclusion rules in S can be captured without causing inconsistency. In other words, rule sets such as r 1 (x, y) ⇒ r 2 (x, y), r 1 (x, y) ⇒ r 3 (x, y), and r 2 (x, y) ∧ r 3 (x, y) ⇒ ⊥ are not possible.</p><p>Hence, since |S m | ≤ 0.5|R|(|R| − 1), the number of distinct pairs that can be selected from R, a BoxE model with d = 0.5|R|(|R| − 1) + |R| + |R| log |R| = O(|R| 2 ) dimensions can capture any consistent set of rules S from the language of intersection, hierarchy, symmetry, anti-symmetry, mutual exclusion, and inversion rules.</p><p>We finally highlight one subtle, but important detail: Whereas the inference pattern language just described can be captured by a BoxE model having d = O(|R| 2 ) dimensions, some individual generalized patterns (inversion, hierarchy, symmetry, anti-symmetry, mutual exclusion) can be captured with even constant number of dimensions, and generalized intersection can be captured with O(|R| log (|R|)) dimensions. Hence, an interesting contrast in dimensionality requirements arises between capturing individual generalized inference patterns, capturing the language of Theorem 5.4, and capturing rule language of Theorem 5.3, which highlights the significantly larger requirements that capturing joint generalized requirements, and the potential existence of cycles, can impose on any embedding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of Theorem 5.4 (Rule Injection)</head><p>We now prove that arbitrary sets S p of hierarchy, intersection, symmetry, and inversion rules can be injected into BoxE. To this end, we adapt the proof of Theorem 5.3 to this setting.</p><p>We start with a randomly initialized box configuration. First, we inject inversion and symmetry rules using box sharing: For symmetry rules, we set r (1) = r <ref type="bibr" target="#b1">(2)</ref> , and for inversion rules, we set r </p><p>1 , and this can be done in linear time with respect to the number of inversion and symmetry rules. This achieves the same result as the node sharing in Step 1 of the proof of Theorem 5.3, except that the box configuration is a concrete random initialization, as opposed to an abstract configuration known to exist due to boxicity. We then proceed with the box reconfiguration procedure in Step 3 of this same proof to enforce hierarchy and intersection rules on top of inversion Hence, rule injection for hierarchy and intersection rules runs at worst in near-quadratic time with respect to |R|, a typically small number, irrespective of the number of these rules. This result, combined with the efficiency of enforcing symmetry and hierarchy, imply that BoxE can be efficiently injected with arbitrary sets of symmetry, inversion, hierarchy and intersection rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experimental Details</head><p>In this section, we give further details on the experiments that we have conducted. In particular, we report details of every dataset, the hyperparameter tuning setup used when training BoxE, as well as the final set of hyperparameters used in the configurations whose results we report in the paper. Finally, we report the complete set of results for KGC, higher-arity, and rule injection experiments, i.e., MR, MRR, Hits@1, Hits@3, and Hits@10. All reported results for the KGC and KBC experiments are average results from 3 training runs, and empirically have very small variance. In particular, all MRR values fluctuate by no more than 0.002 between runs across all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Benchmark dataset details</head><p>In this subsection, we provide the details of of all benchmark datasets used in this paper (FB15k-237, WN18RR, YAGO3-10, JF17K, and FB-AUTO), namely the number of entities, relations, and facts in every split (training, validation, and test) in <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Hyperparameter settings for BoxE experiments</head><p>BoxE is trained using the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> Learning rate was varied between 10 −6 and 10 −2 , with root values of 1,2,5 and exponents from -6 to -2, i.e., 10 −6 , 2 × 10 −6 , 5 × 10 −6 , etc. . Margin was varied between 3 and 24 inclusive, in increments of 1.5, and in increments of 1 between 3 and 6. Adversarial temperature was varied between the integer values of 1 and 4 inclusive, and the number of negative samples was varied between 50, 100, and 150. Across all knowledge graph datasets, we additionally ran experiments with data augmentation, such that, for every relation r, a distinct inverse relation r is defined, and every  fact r(e 1 , e 2 ) is augmented with another fact r (e 2 , e 1 ). This setting, however, was only marginally beneficial on YAGO3-10, yielding a slightly improved MR.</p><p>Finally, the distance order was set to either 1 (Manhattan distance) or 2 (Euclidian distance), and batch sizes (for number of positive examples) were varied between all powers of two between 2 6 and 2 12 inclusive. Hyperparameters were initially selected randomly and tuned using grid search. The set of used hyperparameters in experiments is shown in <ref type="table" target="#tab_9">Table 6</ref>.</p><p>Aside from the reported hyperparameter settings, we have also attempted to fix box sizes, either in a hard fashion or softly by setting maximum total size. Hard sizes were based on statistical popularity of relations, whereas soft totals were tuned. However, neither of these settings yielded any improvements, and in fact both have been mostly detrimental to performance. This, in fact, further highlights the importance of box size variability to obtaining good predictive performance. Interestingly, it also confirms that statistical popularity alone is not sufficient to establish optimal box sizing. We also remain very confident that BoxE performance can further improve in the future, as more dedicated empirical studies and more comprehensive and bespoke tuning methods are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Complete experimental results</head><p>The complete results for KGC experiments on FB15k-237, WN18RR, and YAGO3-10 are reported across <ref type="table" target="#tab_10">Tables 7 and 8</ref>. Complete results for higher-arity KBC experiments on JF17K and FB-AUTO are reported in <ref type="table" target="#tab_12">Table 9</ref>, and complete rule injection results for BoxE and BoxE+RI on the two SportsNELL evaluation sets are reported in <ref type="table" target="#tab_1">Table 10</ref>.    In this experiment, we evaluate the dependence of BoxE on dimensionality d, to understand its prospective performance in a computationally restricted setting.</p><p>Experimental setup. We train BoxE with uniform negative sampling on YAGO3-10 using d = {25, 50, 100, 150, 200}.</p><p>We only tune the margin and fix the learning rate, batch size, and number of negative samples to 10 −3 , 4096, and 150, respectively. L2 norm and data augmentation are used across all experiments. We report peak MRR recorded over the validation set. The final margins were γ = 6 for d = 25 and γ = 10.5 otherwise. <ref type="figure" target="#fig_11">Figure 4</ref>. BoxE maintains very strong performance, even at d = 50, rivaling that of state-of-the-art translational model RotatE, even with just uniform negative sampling. Furthermore, it performs at near-optimal level with d = 100, and is already state-of-the-art on YAGO3-10 at this small dimensionality. Hence, BoxE proves to be very robust for performing knowledge base completion with restricted computational power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. A plot of validation MRR versus dimensionality is drawn in</head><p>G.2 Box volume information for BoxE following training on YAGO3-10 In <ref type="table" target="#tab_1">Table 11</ref>, we report the geometric mean of box volume across dimensions for all head and tail boxes for the 37 relations in YAGO3-10 following training. These numbers are computed from the same configuration whose results are reported in the main paper for BoxE(u). Note that, as explained in Appendix F, boxes are mapped to the space [−1, 1] d using the hyperbolic tangent function, so the geometric mean volume is upper-bounded by 2. From <ref type="table" target="#tab_1">Table 11</ref>, we can make the following four very interesting observations:</p><p>First, we see that more popular relations, in terms of entities they connect, tend to be represented with larger boxes in the embedding space. This confirms our intuition that the boxes effectively define entity classes, and thus larger classes, are met with larger boxes in the embedding space. For example, the less popular relation hasWebsite has very small boxes of mean volume about 0.15, as it is only makes up 68 facts in the YAGO training dataset. By contrast, the relation created has both boxes with mean volume above 0.9, and appears in over 1,400 facts.</p><p>Second, we observe that the size of relation boxes also correlates with implicit entity types, in addition to relation popularity. Indeed, the relation playsFor, despite appearing over 300,000 times, only has box volumes 0.284 and 0.469 respectively, whereas isLeaderOf, with less than 1,000 facts, has a tail box of mean volume exceeding 1. This is due to the diversity in entity types appearing at these relations: For playsFor, head entities are athletes, which cluster together in a smaller region of the embedding space, and tail entities are football/sports clubs, which are more diverse, but still quite similar semantically. By contrast, head entities for isLeaderOf are individuals, with medium variability, but tail entities can be anything from very different countries (e.g., Mali, Kuwait) to cities, districts, and towns (e.g., Toronto, Oxnard (California)), to political parties and associations (e.g., Democratic Governors Association, Hungarian Communist Party), which are vastly different types of entities, and this results in an extremely large tail box for isLeaderOf.</p><p>Third, we observe that relative box sizes accurately reflect the type of their underlying relation. More specifically, larger tail boxes tend to denote one-to-many relations, larger head boxes indicate a many-to-one relation, and similar sizes indicate many-to-many or one-to-one relations. This is especially evident for the one-to-many relations hasChild (0.299 vs 0.761), and isAffiliatedTo (0.257 vs 0.557), and for many-to-one relations isInterestedIn (0.644 vs 0.496), and graduatedFrom (0.608 vs 0.526).</p><p>Finally, we note that symmetric relations in YAGO3-10, namely hasNeighbor and isMarriedTo, are represented with near-identically sized boxes. This is a very important finding, as it indicates that BoxE succesfully captures the symmetry inference pattern, for which a necessary condition is having identical head and tail boxes.</p><p>All in all, these results further highlight the interpretability of BoxE, in terms of capturing inference patterns, accurately inferring and portraying entity classes, and inferring and successfully modelling relation types, which other models are unable to achieve. In this section, we provide additional information about the rule injection experiment presented in the paper. In particular, we give a more complete presentation of model convergence with and without rule injection, and provide further details on SportsNELL.</p><p>Learning curves of BoxE, BoxE+RI. The learning curves of BoxE, and Box+RI, defined with MRR as the performance metric, across the 2000 training epochs of the rule injection experiment, is shown in <ref type="figure" target="#fig_12">Figure 5</ref>. The two curves highlight a remarkable improvement stemming from injecting the SportsNELL ontology. Indeed, BoxE+RI converges to peak performance within 500 epochs, and mostly stablises its peak MRR following this point, whereas standard BoxE does not fully converge, even after the whole 2000 epochs have elapsed. Furthermore, the difference in performance between these two models is very significant. Hence, rule injection not only yields better-performing KBC systems, but also enables faster, more reliable training of these systems.</p><p>Further details about SportsNELL. SportsNELL initially consists of 181,936 facts, 11 relations and 4,252 sports-related entities, such that all its entities initially appear 50 or more times in NELL across these 11 relations. Its logical closure w.r.t the SportsNELL ontology is then computed., i.e., ontology rules are repeatedly applied to deduce new facts until no new facts can be deduced: new facts in the deductive closure are direct results of rule application, and thus their correct prediction indicates a good capturing of the underlying ontology. The resulting combined dataset, referred to as SportsNELL C , contains a total of 326,650 facts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>,...,en) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The dist function for width w (i) = 1, 3, 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The SportsNELL ontology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 ,</head><label>2</label><figDesc>to be identical boxes. This box sharing between inverse relations can easily be extended to any arbitrary set of inversion rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 −</head><label>1</label><figDesc>b 2 . Hence, it is equivalent to view BoxE as bumping relation boxes in the opposite direction. Now, we can see that r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>−b 2 , whereas r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>−b 3 . Therefore, since bumps are entity-specific and unknown a priori since the bump stems from an abstract variable, one cannot analyze the relative positions of r(1) 1 and r (1) 3 and draw conclusions. By contrast, all other captured rules in BoxE are such that relation boxes corresponding to the same variable are bumped identically, which in effect neutralizes the effect of bumping and enables the capturing of the patterns. Hence, translational bumps, which allow BoxE to be fully expressive, prevent BoxE from capturing compositions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :</head><label>4</label><figDesc>BoxE validation performance over YAGO3-10 versus dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>BoxE and BoxE+RI learning curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Beyond translations, RotatE [37] uses rotations to model relations, and thus can model symmetric relations with rotations of angle θ = ±π, but is otherwise as limited as TransE. Translational models are interpretable and can capture various inference patterns, but no known translational model is fully expressive.</figDesc><table /><note>Bilinear models. Bilinear models capture relations as a bilinear product between entity and relation embeddings. RESCAL [30] represents a relation r as a full-rank d × d matrix M , and entities as d- dimensional vectors e. DistMult [49] simplifies RESCAL by making M diagonal, but cannot capture non-symmetric relations. ComplEx [39] defines a diagonal M with complex numbers to capture anti-symmetry. SimplE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Inference patterns/generalized inference patterns captured by selected KBC models. TuckER coincides with ComplEx, so is omitted from the table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>KGC results (MR, MRR, Hits@10) for BoxE and competing approaches on FB15k-237, WN18RR, and YAGO3-10. Other approach results are best published, with sources cited per model.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell><cell cols="2">YAGO3-10</cell></row><row><cell></cell><cell cols="2">MR MRR</cell><cell>H@10</cell><cell>MR</cell><cell>MRR</cell><cell>H@10</cell><cell>MR</cell><cell>MRR</cell><cell>H@10</cell></row><row><cell>TransE(u) [33]</cell><cell>-</cell><cell>.313</cell><cell>.497</cell><cell>-</cell><cell>.228</cell><cell>.520</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RotatE(u) [37]</cell><cell>185</cell><cell>.297</cell><cell>.480</cell><cell>3254</cell><cell>.470</cell><cell>.564</cell><cell>1116</cell><cell>.459</cell><cell>.651</cell></row><row><cell>BoxE(u)</cell><cell>172</cell><cell>.318</cell><cell>.514</cell><cell>3117</cell><cell>.442</cell><cell>.523</cell><cell>1164</cell><cell>.567</cell><cell>.699</cell></row><row><cell>TransE(a) [37]</cell><cell>170</cell><cell>.332</cell><cell>.531</cell><cell>3390</cell><cell>.223</cell><cell>.529</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RotatE(a) [37]</cell><cell>177</cell><cell>.338</cell><cell>.533</cell><cell>3340</cell><cell>.476</cell><cell>.571</cell><cell>1767</cell><cell>.495</cell><cell>.670</cell></row><row><cell>BoxE(a)</cell><cell>163</cell><cell>.337</cell><cell>.538</cell><cell>3207</cell><cell>.451</cell><cell>.541</cell><cell>1022</cell><cell>.560</cell><cell>.691</cell></row><row><cell>DistMult [33, 49]</cell><cell>-</cell><cell>.343</cell><cell>.531</cell><cell>-</cell><cell>.452</cell><cell>.531</cell><cell>5926</cell><cell>.34</cell><cell>.54</cell></row><row><cell>ComplEx [33, 49]</cell><cell>-</cell><cell>.348</cell><cell>.536</cell><cell>-</cell><cell>.475</cell><cell>.547</cell><cell>6351</cell><cell>.36</cell><cell>.55</cell></row><row><cell>TuckER [1]</cell><cell>-</cell><cell>.358</cell><cell>.544</cell><cell>-</cell><cell>.470</cell><cell>.526</cell><cell>4423</cell><cell>.529</cell><cell>.670</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>KBC results on JF17K and FB-AUTO.</figDesc><table><row><cell>Model</cell><cell cols="2">JF17K</cell><cell cols="2">FB-AUTO</cell></row><row><cell></cell><cell cols="4">MRR H@10 MRR H@10</cell></row><row><cell>m-TransH</cell><cell>.446</cell><cell>.614</cell><cell>.728</cell><cell>.728</cell></row><row><cell>m-DistMult</cell><cell>.460</cell><cell>.635</cell><cell>.784</cell><cell>.845</cell></row><row><cell>m-CP</cell><cell>.392</cell><cell>.560</cell><cell>.752</cell><cell>.837</cell></row><row><cell>HypE</cell><cell>.492</cell><cell>.650</cell><cell>.804</cell><cell>.856</cell></row><row><cell>HSimplE</cell><cell>.472</cell><cell>.649</cell><cell>.798</cell><cell>.855</cell></row><row><cell>BoxE(u)</cell><cell>.553</cell><cell>.711</cell><cell>.837</cell><cell>.895</cell></row><row><cell>BoxE(a)</cell><cell>.560</cell><cell>.722</cell><cell>.844</cell><cell>.898</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Rule injection experiment results on the Sport-sNELL full and filtered evaluation sets. .577 .780 19.1 .713 .824 BoxE+RI 1.74 .979 .997 5.11 .954 .984</figDesc><table><row><cell>Model</cell><cell>Full Set</cell><cell>Filtered Set</cell></row><row><cell></cell><cell cols="2">MR MRR H@10 MR MRR H@10</cell></row><row><cell>BoxE</cell><cell>17.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL, 2019. [30] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In Proceedings of the Twenty-Eighth International Conference on Machine Learning, ICML, 2011. [31] Fred S. Roberts. On the Boxicity and Cubicity of a graph. In Recent Progress in Combinatorics, Academic Press, 1968. [32] Tim Rocktäschel, Sameer Singh, and Sebastian Riedel. Injecting logical background knowledge into embeddings for relation extraction. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL, 2015. [33] Daniel Ruffinelli, Samuel Broscheit, and Rainer Gemulla. You CAN teach an old dog new tricks! On training knowledge graph embeddings. In Proceedings of the Eighth International Conference on Learning Representations, ICLR, 2020. [34] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. Reasoning with neural tensor networks for knowledge base completion. In Proceedings of the Twenty-Seventh Annual Conference on Neural Information Processing Systems, NIPS, 2013. Dance, and Guillaume Bouchard. On inductive abilities of latent factor models for relational learning. J. Artif. Intell. Res., 64:21-53, 2019. [39] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In Proceedings of the Thirty-Third International Conference on Machine Learning, ICML, 2016. Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures. In Proceedings of the Fifty-Sixth Annual Meeting of the Association for Computational Linguistics (ACL), 2018. [42] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In Proceedings of the Twenty-Seventh ACM International Conference on Information and Knowledge Management, CIKM, 2018. [43] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI, 2014. [44] Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. On the Representation and Embedding of Knowledge Bases beyond Binary Relations. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI, 2016. 47] Chenyan Xiong, Russell Power, and Jamie Callan. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceedings of the Twenty-Sixth International Conference on World Wide Web, WWW, 2017. [48] Bishan Yang and Tom M. Mitchell. Leveraging knowledge bases in LSTMs for improving machine reading. In Proceedings of the Fifty-Fifth Annual Meeting of the Association for Computational Linguistics, ACL, 2017. [49] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of the Third International Conference on Learning Representations, ICLR, 2015. A Runtime and Space Complexity of BoxE Runtime. For any fact r(e 1 , . . . , e n ), we can compute the entity representations e r(e1,...,en) , in time O(nd), by first computing 1≤i≤n b i in O(nd), then subtracting b i from the overall sum for every entity e and finally adding the base position e, resulting in 3n d−dimensional addition/subtraction operations. The distance function dist runs in O(d) for every box and entity, as it involves a fixed number of d−dimensional operations. Thus, running dist for all n positions yields a running time of O(nd). Hence, BoxE scoring runs in O(nd) overall. This implies that BoxE scales linearly with the arity of the relations in a KB, and thus can be applied to this setting with minimal computational overhead. Assuming that n is bounded, as is the case for KGs, BoxE runs in linear time with respect to dimensionality d.</figDesc><table><row><cell>Psychometrika,</cell></row><row><cell>31(3):279-311, 1966.</cell></row><row><cell>[41]</cell></row></table><note>[35] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. Reasoning with neural tensor networks for knowledge base completion. In Proceedings of the Twenty-Sixth Annual Conference on Advances in Neural Information Processing Systems , NIPS, 2013.[36] Sandeep Subramanian and Soumen Chakrabarti. New embedded representations and evaluation protocols for inferring transitive relations. In Proceedings of the Fourty-First International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR, 2018.[37] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. RotatE: Knowledge graph em- bedding by relational rotation in complex space. In Proceedings of the Seventh International Conference on Learning Representations, ICLR, 2019.[38] Théo Trouillon, Éric Gaussier, Christopher R.[40] Ledyard R Tucker. Some mathematical notes on three-mode factor analysis.[45] Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. Knowledge base completion via search-based question answering. In Proceedings of the Twenty-Third International World Wide Web Conference, WWW, 2014.[46] Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Representation learning of knowledge graphs with hierarchical types. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI, 2016.[Space complexity. In terms of space complexity, BoxE stores 2 d−dimensional vectors per entity e, namely its base position e and bump b, and stores 2 d−dimensional vectors per box, denoting its lower and upper corners. Hence, for a KB with |E| entities and |R| relations with arity n, BoxE requires (|E| + n|R|)d parameters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Inversion. For both TransE and RotatE, inversion holds iff r 1 = −r 2 . However, whereas RotatE can capture generalized inversion through repeated application of the earlier equation across all inversion rules, since it can handle any deduced symmetry results, TransE cannot. More concretely, consider the rule set r 1 (x, y) ⇔ r 2 (y, x), r 2 (x, y) ⇔ r 3 (y, x), r 3 (x, y) ⇔ r 1 (y, x). This rule set implies r 1 (x, y) ⇔ r 1 (y, x), which RotatE can capture, but which TransE cannot. More generally, generalized inversion rules can yield symmetry rules, and thus only RotatE can capture generalized inversion.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Properties of benchmark datasets FB15k-237, WN18RR, YAGO3-10, JF17K, and FB-AUTO. This step is guaranteed to enforce these rules, and their deductive closure, as shown in Appendix D, and maintains box sharing, so preserves symmetry and inversion.We now analyze the worst-case runtime complexity of the box reconfiguration procedure. We assume the worst-case, that any pairwise intersections should be expressible, and thus use a dimensionality d = O(|R| log (|R|). The worst-case running time of the box reconfiguration procedure for enforcing a single hierarchy/intersection rule is O(|R|d) = O(|R| 2 log (|R|), corresponding to the maximum number of boundary changes needed per call. However, this upper bound is independent of the number of rules in S, as no more than O(|R|d) steps can be made across all rules. Thus, the worst-case running time for rule injection across all hierarchy and intersection rules is O(|R|d) = O(|R| 2 log (|R|)).</figDesc><table><row><cell>Dataset</cell><cell>|E|</cell><cell cols="4">|R| Training Facts Validation Facts Testing Facts</cell></row><row><cell cols="3">FB15k-237 14,541 237</cell><cell>272,115</cell><cell>17,535</cell><cell>20,466</cell></row><row><cell>WN18RR</cell><cell>40,943</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,034</cell></row><row><cell cols="3">YAGO3-10 123,182 37</cell><cell>1,079,040</cell><cell>5,000</cell><cell>5,000</cell></row><row><cell>JF17K</cell><cell cols="2">29,257 327</cell><cell>61,911</cell><cell>15,822</cell><cell>24,915</cell></row><row><cell>FB-AUTO</cell><cell>3,388</cell><cell>8</cell><cell>6,778</cell><cell>2,255</cell><cell>2,180</cell></row><row><cell>and symmetry rules.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>, to optimize negative sampling loss[37]. Training for every run was conducted on a Haswell CPU node with 12 cores, 64 GB RAM, and a V100 GPU. Hyperparameter tuning was conducted over its learning rate λ, dimensionality d, loss margin γ, distance order x, and number of negative examples m. For all BoxE experiments, points and boxes were projected into the hypercube [−1, 1] d , a bounded space, by simply applying the hyperbolic tangent function tanh element-wise on all final embedding representations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter settings of BoxE over different datasets.</figDesc><table><row><cell>Dataset</cell><cell>Embedding Dimension</cell><cell>Margin</cell><cell>Learning Rate</cell><cell>Adversarial Temperature</cell><cell>Negative Samples</cell><cell>Distance Order</cell><cell>Batch Size</cell><cell>Data Augmentation</cell></row><row><cell>FB15k-237(u)</cell><cell>500</cell><cell cols="2">12 1 × 10 −4</cell><cell>0.0</cell><cell>100</cell><cell>1</cell><cell>1024</cell><cell>No</cell></row><row><cell>FB15k-237(a)</cell><cell>1000</cell><cell>3</cell><cell>5 × 10 −5</cell><cell>4.0</cell><cell>100</cell><cell>2</cell><cell>1024</cell><cell>No</cell></row><row><cell>WN18RR(u)</cell><cell>500</cell><cell>5</cell><cell>1 × 10 −3</cell><cell>0.0</cell><cell>150</cell><cell>2</cell><cell>512</cell><cell>No</cell></row><row><cell>WN18RR(a)</cell><cell>500</cell><cell>3</cell><cell>1 × 10 −3</cell><cell>2.0</cell><cell>100</cell><cell>2</cell><cell>512</cell><cell>No</cell></row><row><cell>YAGO3-10(u)</cell><cell>200</cell><cell cols="2">10.5 1 × 10 −3</cell><cell>0.0</cell><cell>150</cell><cell>2</cell><cell>4096</cell><cell>Yes</cell></row><row><cell>YAGO3-10(a)</cell><cell>200</cell><cell>6</cell><cell>1 × 10 −3</cell><cell>2.0</cell><cell>150</cell><cell>2</cell><cell>4096</cell><cell>Yes</cell></row><row><cell>JF17K(u)</cell><cell>200</cell><cell cols="2">15 2 × 10 −3</cell><cell>0.0</cell><cell>100</cell><cell>2</cell><cell>1024</cell><cell>N/A</cell></row><row><cell>JF17K(a)</cell><cell>200</cell><cell>5</cell><cell>1 × 10 −4</cell><cell>2.0</cell><cell>100</cell><cell>2</cell><cell>1024</cell><cell>N/A</cell></row><row><cell>FB-AUTO(u)</cell><cell>200</cell><cell cols="2">18 2 × 10 −3</cell><cell>0.0</cell><cell>100</cell><cell>2</cell><cell>1024</cell><cell>N/A</cell></row><row><cell>FB-AUTO(a)</cell><cell>200</cell><cell>9</cell><cell>5 × 10 −4</cell><cell>2.0</cell><cell>100</cell><cell>2</cell><cell>1024</cell><cell>N/A</cell></row><row><cell>SportsNELL</cell><cell>200</cell><cell>6</cell><cell>1 × 10 −3</cell><cell>0.0</cell><cell>100</cell><cell>2</cell><cell>1024</cell><cell>No</cell></row><row><cell>SportsNELL+RI</cell><cell>200</cell><cell>6</cell><cell>1 × 10 −3</cell><cell>0.0</cell><cell>100</cell><cell>2</cell><cell>1024</cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Complete KGC results for BoxE and competing models on FB15K-237 and WN18RR. .205 .328 .480 3254 .470 .422 .488 .564 BoxE(u) 172 .318 .223 .351 .514 3117 .442 .398 .461 .523 TransE(a) [37] 170 .332 .233 .372 .531 3390 .223 .013 .401 .529 RotatE(a) [37] 177 .338 .241 .375 .533 3340 .476 .428 .492 .571 BoxE(a) 163 .337 .238 .374 .538 3207 .451 .400 .472 .541</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">FB15K-237</cell><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="10">MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10</cell></row><row><cell>TransE(u) [33]</cell><cell>-</cell><cell>.313</cell><cell>-</cell><cell>-</cell><cell>.497</cell><cell>-</cell><cell>.228</cell><cell>-</cell><cell>-</cell><cell>.520</cell></row><row><cell cols="3">RotatE(u) [37] 185 .297 DistMult [33, 49] -.343</cell><cell>-</cell><cell>-</cell><cell>.531</cell><cell>-</cell><cell>.452</cell><cell>-</cell><cell>-</cell><cell>.531</cell></row><row><cell cols="2">ComplEx [33, 49] -</cell><cell>.348</cell><cell>-</cell><cell>-</cell><cell>.536</cell><cell>-</cell><cell>.475</cell><cell>-</cell><cell>-</cell><cell>.547</cell></row><row><cell>TuckER [1]</cell><cell>-</cell><cell cols="4">.358 .266 .394 .544</cell><cell>-</cell><cell cols="4">.470 .443 .482 .526</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Complete KGC results for BoxE and competing models on YAGO3-10.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>YAGO3-10</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">MR MRR H@1 H@3 H@10</cell></row><row><cell>TransE(u) [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RotatE(u) [37]</cell><cell cols="5">1116 .459 .360 .509 .651</cell></row><row><cell>BoxE(u)</cell><cell cols="5">1164 .567 .494 .611 .699</cell></row><row><cell>TransE(a) [37]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RotatE(a) [37]</cell><cell cols="5">1767 .495 .402 .550 .670</cell></row><row><cell>BoxE(a)</cell><cell cols="5">1022 .560 .484 .608 .691</cell></row><row><cell>DistMult [33, 49]</cell><cell cols="4">5926 .34 .24 .38</cell><cell>.54</cell></row><row><cell>ComplEx [33, 49]</cell><cell cols="4">6351 .36 .26 .40</cell><cell>.55</cell></row><row><cell>TuckER [1]</cell><cell cols="5">4423 .529 .451 .576 .670</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Complete KBC results on higher-arity datasets JF17K and FB-AUTO.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>JF17K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FB-AUTO</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MR</cell><cell cols="5">MRR H@1 H@3 H@10 MR</cell><cell cols="4">MRR H@1 H@3 H@10</cell></row><row><cell>m-TransH</cell><cell>-</cell><cell>.446</cell><cell>.357</cell><cell>.495</cell><cell>.614</cell><cell>-</cell><cell>.728</cell><cell>.727</cell><cell>.728</cell><cell>.728</cell></row><row><cell>m-DistMult</cell><cell>-</cell><cell>.460</cell><cell>.367</cell><cell>.510</cell><cell>.635</cell><cell>-</cell><cell>.784</cell><cell>.745</cell><cell>.815</cell><cell>.845</cell></row><row><cell>m-CP</cell><cell>-</cell><cell>.392</cell><cell>.303</cell><cell>.441</cell><cell>.560</cell><cell>-</cell><cell>.752</cell><cell>.704</cell><cell>.785</cell><cell>.837</cell></row><row><cell>HypE</cell><cell>-</cell><cell>.492</cell><cell>.409</cell><cell>.533</cell><cell>.650</cell><cell>-</cell><cell>.804</cell><cell>.774</cell><cell>.823</cell><cell>.856</cell></row><row><cell>HSimplE</cell><cell>-</cell><cell>.472</cell><cell>.375</cell><cell>.523</cell><cell>.649</cell><cell>-</cell><cell>.798</cell><cell>.766</cell><cell>.821</cell><cell>.855</cell></row><row><cell>BoxE(u)</cell><cell>363</cell><cell>.553</cell><cell>.467</cell><cell>.596</cell><cell>.711</cell><cell>110</cell><cell>.837</cell><cell>.804</cell><cell>.858</cell><cell>.895</cell></row><row><cell>BoxE(a)</cell><cell>372</cell><cell>.560</cell><cell>.472</cell><cell>.604</cell><cell>.722</cell><cell>122</cell><cell>.844</cell><cell>.814</cell><cell>.863</cell><cell>.898</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Complete rule injection experiment results on the SportsNELL full and filtered sets. .577 .478 .623 .780 19.1 .713 .661 .732 .824 BoxE+RI 1.74 .979 .968 .988 .997 5.11 .954 .938 .964 .984 G Additional Experimental Insights and Discussions</figDesc><table><row><cell>Model</cell><cell>Full Set</cell><cell>Filtered Set</cell></row><row><cell></cell><cell cols="2">MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10</cell></row><row><cell>BoxE</cell><cell>17.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Geometric mean volume per dimension for all relation boxes in YAGO3-10 following training.</figDesc><table><row><cell>Relation</cell><cell cols="2">Head Box Tail Box</cell></row><row><cell>actedIn</cell><cell>0.456</cell><cell>0.479</cell></row><row><cell>created</cell><cell>0.966</cell><cell>0.905</cell></row><row><cell>dealsWith</cell><cell>0.373</cell><cell>0.366</cell></row><row><cell>diedIn</cell><cell>0.383</cell><cell>0.480</cell></row><row><cell>directed</cell><cell>0.474</cell><cell>0.461</cell></row><row><cell>edited</cell><cell>0.461</cell><cell>0.441</cell></row><row><cell>exports</cell><cell>0.238</cell><cell>0.260</cell></row><row><cell>graduatedFrom</cell><cell>0.608</cell><cell>0.526</cell></row><row><cell>happenedIn</cell><cell>0.453</cell><cell>0.363</cell></row><row><cell>hasAcademicAdvisor</cell><cell>0.655</cell><cell>0.605</cell></row><row><cell>hasCapital</cell><cell>0.390</cell><cell>0.347</cell></row><row><cell>hasChild</cell><cell>0.299</cell><cell>0.761</cell></row><row><cell>hasCurrency</cell><cell>0.228</cell><cell>0.239</cell></row><row><cell>hasGender</cell><cell>0.669</cell><cell>0.688</cell></row><row><cell>hasMusicalRole</cell><cell>0.328</cell><cell>0.427</cell></row><row><cell>hasNeighbor</cell><cell>0.311</cell><cell>0.312</cell></row><row><cell>hasOfficialLanguage</cell><cell>0.213</cell><cell>0.255</cell></row><row><cell>hasWebsite</cell><cell>0.159</cell><cell>0.143</cell></row><row><cell>hasWonPrize</cell><cell>0.264</cell><cell>0.381</cell></row><row><cell>imports</cell><cell>0.249</cell><cell>0.241</cell></row><row><cell>influences</cell><cell>0.510</cell><cell>0.567</cell></row><row><cell>isAffiliatedTo</cell><cell>0.257</cell><cell>0.557</cell></row><row><cell>isCitizenOf</cell><cell>0.544</cell><cell>0.614</cell></row><row><cell>isConnectedTo</cell><cell>0.403</cell><cell>0.388</cell></row><row><cell>isInterestedIn</cell><cell>0.644</cell><cell>0.496</cell></row><row><cell>isKnownFor</cell><cell>0.632</cell><cell>0.623</cell></row><row><cell>isLeaderOf</cell><cell>0.446</cell><cell>1.005</cell></row><row><cell>isLocatedIn</cell><cell>0.496</cell><cell>0.547</cell></row><row><cell>isMarriedTo</cell><cell>0.923</cell><cell>0.924</cell></row><row><cell>isPoliticianOf</cell><cell>0.361</cell><cell>0.521</cell></row><row><cell>livesIn</cell><cell>0.536</cell><cell>0.341</cell></row><row><cell>owns</cell><cell>0.907</cell><cell>0.485</cell></row><row><cell>participatedIn</cell><cell>0.389</cell><cell>0.471</cell></row><row><cell>playsFor</cell><cell>0.284</cell><cell>0.469</cell></row><row><cell>wasBornIn</cell><cell>0.465</cell><cell>0.445</cell></row><row><cell>worksAt</cell><cell>0.498</cell><cell>0.488</cell></row><row><cell>wroteMusicFor</cell><cell>0.450</cell><cell>0.646</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the Ninth International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the Ninth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A shared database of structured general human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Tufts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, AAAI</title>
		<meeting>the Twenty-Second AAAI Conference on Artificial Intelligence, AAAI</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth Annual Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Twenty-Sixth Annual Conference on Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric representation of graphs in low dimension using axis parallel boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><forename type="middle">C</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Sivadasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="140" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lifted rule injection for relation embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Annual Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Annual Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving knowledge graph embedding using simple constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth-Sixth Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the Fifth-Sixth Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge Vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD</title>
		<meeting>the Twentieth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Knowledge hypergraphs: Extending knowledge graphs beyond binary relations. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by flexible translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<meeting>the Fifteenth International Conference on the Principles of Knowledge Representation and Reasoning</meeting>
		<imprint>
			<publisher>KR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Annual Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Annual Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From knowledge graph embedding to ontology embedding? an analysis of the compatibility between vector space representations and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Víctor Gutiérrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basulto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schockaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<meeting>the Sixteenth International Conference on the Principles of Knowledge Representation and Reasoning</meeting>
		<imprint>
			<publisher>KR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Query2box: Reasoning over knowledge graphs in vector space using box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Learning Representations, ICLR</title>
		<meeting>the Eighth International Conference on Learning Representations, ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Annual Conference on Advances in Neural Information Processing Systems</title>
		<meeting>the Thirty-First Annual Conference on Advances in Neural Information Processing Systems<address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Learning Representations, ICLR</title>
		<meeting>the Third International Conference on Learning Representations, ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth International Conference on Machine Learning, ICML</title>
		<meeting>the Thirty-Fifth International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI, Invited Talk</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI, Invited Talk</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smoothing the geometry of probabilistic box embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Learning Representations, ICLR</title>
		<meeting>the Seventh International Conference on Learning Representations, ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalizing tensor decomposition for n-ary relational knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The Web Conference, WWW &apos;20</title>
		<meeting>the The Web Conference, WWW &apos;20</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differentiating concepts and instances for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">YAGO3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Biennial Conference on Innovative Data Systems Research, CIDR</title>
		<meeting>the Seventh Biennial Conference on Innovative Data Systems Research, CIDR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial sets for regularising neural link predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI</title>
		<meeting>the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Never-ending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Partha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Bhavana Dalvi Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<editor>Kathryn Mazaitis, Thahir Mohamed, Ndapandula Nakashole, Emmanouil A. Platanios, Alan Ritter, Mehdi Samadi, Burr Settles, Richard C. Wang, Derry Wijaya, Abhinav Gupta, Xinlei Chen, Abulhair Saparov, Malcolm Greaves, and Joel Welling</editor>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
