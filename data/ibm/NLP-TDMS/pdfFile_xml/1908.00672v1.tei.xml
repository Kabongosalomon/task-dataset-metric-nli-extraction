<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Indices Matter: Learning to Index for Deep Image Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
							<email>hao.lu@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
							<email>yutong.dai@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<email>chunhua.shen@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Indices Matter: Learning to Index for Deep Image Matting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>â€¡ Noah&apos;s Ark Lab, Huawei Technologies</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that existing upsampling operators can be unified with the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can recover boundary details much better than other upsampling operators such as bilinear interpolation. By looking at the indices as a function of the feature map, we introduce the concept of learning to index, and present a novel index-guided encoder-decoder framework where indices are self-learned adaptively from data and are used to guide the pooling and upsampling operators, without the need of supervision. At the core of this framework is a flexible network module, termed IndexNet, which dynamically predicts indices given an input. Due to its flexibility, IndexNet can be used as a plug-in applying to any off-the-shelf convolutional networks that have coupled downsampling and upsampling stages.</p><p>We demonstrate the effectiveness of IndexNet on the task of natural image matting where the quality of learned indices can be visually observed from predicted alpha mattes. Results on the Composition-1k matting dataset show that our model built on MobileNetv2 exhibits at least 16.1% improvement over the seminal VGG-16 based deep matting baseline, with less training data and lower model capacity. Code and models has been made available at: https://tinyurl.com/IndexNetV1. * Corresponding author.</p><p>Figure 1: Alpha mattes of different models. From left to right, Deeplabv3+ [4], RefineNet [30], Deep Matting [49] and Ours.</p><p>Bilinear upsampling fails to recover subtle details, but unpooling and our learned upsampling operator can produce much clear mattes with good local contrast.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Upsampling is an essential stage for most dense prediction tasks using deep convolutional neural networks (CNNs). The frequently used upsampling operators include transposed convolution <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b31">32]</ref>, unpooling <ref type="bibr" target="#b1">[2]</ref>, periodic shuffling <ref type="bibr" target="#b40">[41]</ref> (also known as depth-to-space), and naive interpolation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref> followed by convolution. These operators, however, are not general-purpose designs and often have different behaviors in different tasks.</p><p>The widely-adopted operator in semantic segmentation or depth estimation is bilinear interpolation, rather than unpooling. A reason is that the feature map generated by unpooling is too sparse, while bilinear interpolation is likely to generate the feature map that depicts semanticallyconsistent regions. This is particularly true for semantic segmentation and depth estimation where pixels in a region often share the same class label or have similar depth. However, bilinear interpolation performs much worse than unpooling in boundary-sensitive tasks such as image matting. A fact is that the leading deep image matting model <ref type="bibr" target="#b48">[49]</ref> largely borrows the design from the SegNet <ref type="bibr" target="#b1">[2]</ref>, where unpooling is introduced. When adapting other state-of-theart segmentation models, such as DeepLabv3+ <ref type="bibr" target="#b3">[4]</ref> and Re-fineNet <ref type="bibr" target="#b29">[30]</ref>, to this task, unfortunately, we observe both DeepLabv3+ and RefineNet fail to recover boundary details ( <ref type="figure">Fig. 1</ref>), compared to SegNet. This makes us to ponder over what is missing in these encoder-decoder models. After making a thorough comparison between different architectures and conducting ablative studies (Section 5.2), the answer is finally made clear-indices matter.</p><p>Compared to the bilinearly upsampled feature map, unpooling uses max-pooling indices to guide upsampling. Since boundaries in the shallow layers usually have the maximum responses, indices extracted from these responses record the boundary locations. The feature map projected by the indices thus shows improved boundary de-lineation. Above analyses reveal a fact that, different upsampling operators have different characteristics, and we expect a specific behavior of the upsampling operator when dealing with specific image content in a certain visual task.</p><p>It would be interesting to pose the question: Can we design a generic operator to upsample feature maps that better predict boundaries and regions simultaneously? A key observation of this work is that max unpooling, bilinear interpolation or other upsampling operators are some forms of index functions. For example, the nearest neighbor interpolation of a point is equivalent to allocating indices of one to its neighbor and then map the value of the point. In this sense, indices are models <ref type="bibr" target="#b23">[24]</ref>, therefore indices can be modeled and learned. In this work, we model indices as a function of the local feature map and learn an index function to perform upsampling within deep CNNs. In particular, we present a novel index-guided encoder-decoder framework, which naturally generalizes SegNet. Instead of using maxpooling and unpooling, we introduce indexed pooling and indexed upsampling operators where downsampling and upsampling are guided by learned indices. The indices are generated dynamically conditioned on the feature map and are learned using a fully convolutional network, termed In-dexNet, without supervision. IndexNet is a highly flexible module, which can be used as a plug-in applying to any offthe-shelf convolutional networks that have coupled downsampling and upsampling stages. Compared to the fixed max function, learned index functions show potentials for simultaneous boundary and region delineation.</p><p>We demonstrate the effectiveness of IndexNet on natural image matting as well as other visual tasks. In image matting, the quality of learned indices can be visually observed from predicted alpha mattes. By visualizing learned indices, we show that the indices automatically learn to capture the boundaries and textural patterns. We further investigate alternative ways to design IndexNet, and show through extensive experiments that IndexNet can effectively improve deep image matting both qualitatively and quantitatively. In particular, we observe that our best MobileNetv2-based <ref type="bibr" target="#b38">[39]</ref> model exhibits at least 16.1% improvement against the previous best deep model, i.e., the VGG-16-based model in <ref type="bibr" target="#b48">[49]</ref>, on the Composition-1k matting dataset. We achieve this with using less training data, and a much more compact model, therefore significantly faster inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review existing widely-used upsampling operators and the main application of IndexNet-deep image matting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upsampling in Deep Networks</head><p>Upsampling is an essential stage for almost all dense prediction tasks. It has been intensively studied about what is the principal way to recover the resolution of the downsampled feature map (decoding). The deconvolution operator, also known as transposed convolution, was initially used in <ref type="bibr" target="#b49">[50]</ref> to visualize convolutional activations and latter introduced to semantic segmentation <ref type="bibr" target="#b31">[32]</ref>. To avoid checkerboard artifacts, a follow-up suggestion is the "resize+convolution" paradigm, which has currently become the standard configuration in state-of-the-art semantic segmentation models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. Aside from these, perforate <ref type="bibr" target="#b34">[35]</ref> and unpooling <ref type="bibr" target="#b1">[2]</ref> are also two operators that generate sparse indices to guide upsampling. The indices are able to capture and keep boundary information, but the problem is that two operators induce sparsity after upsampling. Convolutional layers with large filter sizes must follow for densification. In addition, periodic shuffling (PS) was introduced in [41] as a fast and memory-efficient upsampling operator for image super-resolution. PS recovers resolution by rearranging the feature map of size H Ã— W Ã— Cr 2 to rH Ã— rW Ã— C.</p><p>Our work is primarily inspired by the unpooling operator <ref type="bibr" target="#b1">[2]</ref>. We remark that, it is important to keep the spatial information before loss of such information occurred in feature map downsampling, and more importantly, to use stored information during upsampling. Unpooling shows a simple and effective case of doing this, but we argue there is much room to improve. In this paper, we illustrate that the unpooling operator is a special form of index function, and we can learn an index function beyond unpooling.</p><p>Deep Image Matting In the past decades, image matting methods have been extensively studied from a low-level view <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45]</ref>; and particularly, they have been designed to solve the matting equation. Despite being theoretically elegant, these methods heavily rely on the color cues, rendering failures of matting in general natural scenes where colors cannot be used as reliable cues.</p><p>With the tremendous success of deep CNNs in highlevel vision tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>, deep matting methods are emerging. Some initial attempts appeared in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b39">[40]</ref>, where classic matting approaches, such as closed-form matting <ref type="bibr" target="#b28">[29]</ref> and KNN matting <ref type="bibr" target="#b5">[6]</ref>, are still used as the backends in deep networks. Although the networks are trained end-to-end and can extract powerful features, the final performance is limited by the conventional backends. These attempts may be thought as semi-deep matting. Recently fully-deep image matting was proposed <ref type="bibr" target="#b48">[49]</ref>. In <ref type="bibr" target="#b48">[49]</ref> the authors presented the first deep image matting approach based on SegNet <ref type="bibr" target="#b1">[2]</ref> and significantly outperformed other competitors. Interestingly, this SegNet-based architecture becomes the standard configuration in many recent deep matting methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>SegNet is effective in matting but also computationexpensive and memory-inefficient. For instance, the inference can only be executed on CPU when testing highresolution images, which is practically unattractive. We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indexed Pooling</head><p>Indexed Upsampling  show that, with our proposed IndexNet, even a lightweight backbone such as MobileNetv2-based model can surpass the VGG-16 based method in <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">An Indexing Perspective of Upsampling</head><p>With the argument that upsampling operators are index functions, here we offer an unified index perspective of upsampling operators. The unpooling operator is straightforward. We can define its index function in a k Ã— k local region as an indicator function</p><formula xml:id="formula_0">I max (x) = 1(x = max(X)) , x âˆˆ X ,<label>(1)</label></formula><p>where X âˆˆ R kÃ—k . Similarly, if one extracts indices from the average pooling operator, the index function takes the form</p><formula xml:id="formula_1">I avg (x) = 1(x âˆˆ X) .<label>(2)</label></formula><p>If further using I avg (x) during upsampling, it is equivalent to the nearest neighbor interpolation. Regarding the bilinear interpolation and deconvolution operators, their index functions have an identical form</p><formula xml:id="formula_2">I bilinear/dconv (x) = W âŠ— 1(x âˆˆ X) ,<label>(3)</label></formula><p>where W is the weight/filter of the same size as X, and âŠ— denotes the element-wise multiplication. The difference is that, W in deconvolution is learned, while W in bilinear interpolation stays fixed. Indeed, bilinear upsampling has been shown to be a special case of deconvolution <ref type="bibr" target="#b31">[32]</ref>. Notice that, in this case, the index function generates soft indices. The sense of index for the PS operator <ref type="bibr" target="#b40">[41]</ref> is even much clear, because the rearrangement of the feature map per se is an indexing process. Considering PS a tensor Z of size 1Ã—1Ã—r 2 to a matrix Z of size r Ã—r, the index function can be expressed by the one-hot encoding</p><formula xml:id="formula_3">I l ps (x) = 1(x = Z l ) , l = 1, ..., r 2 ,<label>(4)</label></formula><p>such that Z m,n = Z[I l ps (x)], where m = 1, ..., r, n = 1, ..., r, and l = (r âˆ’ 1) * m + n. Z l denotes the l-th element of Z. A similar notation applies to Z m,n .</p><p>Since upsampling operators can be unified by the notion of index function, in theory it is possible to learn an index function that adaptively captures local spatial patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Index-Guided Encoder-Decoder Framework</head><p>Our framework is a natural generalization of SegNet, as schematically illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. For ease of exposition, we assume the downsampling and upsampling rates are 2, and the pooling operator has a kernel size of 2 Ã— 2. At the core of our framework is the IndexNet module that dynamically generates indices given the feature map. The proposed indexed pooling and indexed upsampling operators further receive generated indices to guide the downsampling and upsampling, respectively. In practice, multiple such modules can be combined and used analogues to the max pooling layers. We provide details as follows.</p><p>concept for the index is that an index can either be represented in a natural order, e.g., 1, 2, 3, ..., or be represented in a logical form, i.e., 0, 1, 0, ..., which means an index map can be used as a mask. In fact, this is how we use the index map in downsampling and upsampling. The predicted index shares the same physical notation of the index in computer science, except that we generate soft indices for smooth optimization, i.e., for any index i, i âˆˆ [0, 1].</p><p>IndexNet consists of a predefined index block and two index normalization layers. An index block can simply be a heuristically defined function, e.g., a max function, or more generally, a neural network. In this work, the index block is designed to use a fully convolutional network. According to the shape of the output index map, we investigate two families of index networks: holistic index networks (HINs) and depthwise (separable) index networks (DINs). Their conceptual differences are shown in <ref type="figure">Fig. 3</ref>. HINs learn an index function I(X) : R HÃ—W Ã—C â†’ R HÃ—W Ã—1 . In this case, all channels of the feature map share a holistic index map. In contrast, DINs learn an index function I(X) : R HÃ—W Ã—C â†’ R HÃ—W Ã—C , where the index map is of the same size as the feature map. We will discuss concrete design of index networks in Sections 4.2 and 4.3.</p><p>Note that the index map sent to the encoder and decoder are normalized differently. The decoder index map only goes through a sigmoid function such that for any predicted index i âˆˆ (0, 1). As for the encoder index map, indices of a local region L are further normalized by a softmax function such that iâˆˆL i = 1. The reason behind the second normalization is to guarantee the magnitude consistency of the feature map after downsampling.</p><p>Indexed Pooling (IP) executes downsampling using generated indices. Given a local region E âˆˆ R kÃ—k , IP calculates a weighted sum of activations and corresponding indices over E as IP(E) = xâˆˆE I(x)x, where I(x) is the index of x. It is easy to infer that max pooling and average pooling are both special cases of IP. In practice, this operator can be easily implemented with an element-wise multiplication between the feature map and the index map, an average pooling layer, and a multiplication of a constant, as instantiated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Indexed Upsampling (IU) is the inverse operator of IP. IU upsamples d âˆˆ R 1Ã—1 that spatially corresponds to E taking the same indices into account. Let I âˆˆ R kÃ—k be the local index map formed by I(x)s, IU upsamples d as IU(d) = I âŠ— D, where âŠ— denotes the element-wise multiplication, and D is of the same size as I and is upsampled from d with the nearest neighbor interpolation. An important difference between deconvolution and IU is that, deconvolution applies a fixed kernel to all local regions, even if the kernel is learned, while IU upsamples different regions with different kernels (indices).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Holistic Index</head><p>Depthwise Index</p><formula xml:id="formula_4">2x2xC 1 x1x4 2x2xC 1 x1x4C</formula><p>HxWxC H xWx1 HxWxC HxWxC <ref type="figure">Figure 3</ref>: Conceptual differences between holistic index and depthwise index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Holistic Index Networks</head><p>Here we instantiate two types of HINs. Recall that HINs learn an index function I(X) : R HÃ—W Ã—C â†’ R HÃ—W Ã—1 . A naive design choice is to assume a linear relationship between the feature map and the index map.</p><p>Linear Holistic Index Networks. An example is shown in <ref type="figure" target="#fig_1">Fig. 4(a)</ref>. The network is implemented in a fully convolutional way. It first applies 2-stride 2 Ã— 2 convolution to the feature map of size H Ã— W Ã— C, generating a concatenated index map of size H/2 Ã— W/2 Ã— 4. Each slice of the index map (H/2 Ã— W/2 Ã— 1) is designed to correspond to the indices of a certain position of all local regions, e.g., the top-left corner of all 2 Ã— 2 regions. The network finally applies a PS-like shuffling operator to rearrange the index map to the size of H Ã— W Ã— 1.</p><p>In many situations, assuming a linear relationship is not sufficient. An obvious fact is that a linear function even cannot fit the max function. Naturally the second design choice is to add nonlinearity into the network.</p><p>Nonlinear Holistic Index Networks. <ref type="figure" target="#fig_1">Fig. 4(b)</ref> illustrates a nonlinear HIN where the feature map is first projected to a map of size H/2 Ã— W/2 Ã— 2C, followed by a batch normalization layer and a ReLU function for nonlinear mappings. We then use point-wise convolution to reduce the channel dimension to an indices-compatible size. The rest transformations follow its linear counterpart.</p><p>Remark 1. Note that, the holistic index map is shared by all channels of the feature map, which means the index map should be expanded to the size of H Ã— W Ã— C when feeding into IP and IU. Fortunately, many existing packages support implicit expansion over the singleton dimension. This index map could be thought as a collection of local attention maps <ref type="bibr" target="#b33">[34]</ref> applied to individual local spatial regions. In this case, the IP and IU operators can also be referred to "attentional pooling" and "attentional upsampling".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Depthwise Index Networks</head><p>In DINs, we find I(X) : R HÃ—W Ã—C â†’ R HÃ—W Ã—C , i.e., each spatial index corresponds to each spatial activation. This family of networks further has two high-level design strategies that correspond to two different assumptions.   One-to-One (O2O) Assumption assumes that each slice of the index map only relates to its corresponding slice of the feature map. It can be denoted by a local index function l(X) : R kÃ—kÃ—1 â†’ R kÃ—kÃ—1 , where k denotes the size of local region. Similar to HINs, DINs can also be designed to have linear/nonlinear modeling ability. <ref type="figure" target="#fig_2">Fig. 5</ref> shows an example when k = 2. Note that, different from HINs, DINs follow a multi-column architecture. Each column predicts indices specific to a certain spatial location of all local regions. The O2O assumption can be easily satisfied in DINs with grouped convolution.</p><p>Linear Depthwise Index Networks. As per <ref type="figure" target="#fig_2">Fig. 5</ref>, a feature map goes through four parallel convolutional layers with the same kernel size of 2 Ã— 2 Ã— C, a stride of 2, and C groups, leading to four downsampled feature maps of size H/2Ã—W/2Ã—C. The final index map is composed from the four feature maps by shuffling and rearrangement. Note that the parameters of four convolutional layers are not shared.</p><p>Nonlinear Depthwise Index Networks. Nonlinear DINs can be easily modified from linear DINs by inserting four extra convolutional layers. Each of them is followed by a BN layer and a ReLU unit, as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. The rest remains the same as the linear DINs.</p><p>Many-to-One (M2O) Assumption assumes that each slice of the index map relates with all channels of the feature map. The local index function is defined as l(X) : R kÃ—kÃ—C â†’ R kÃ—kÃ—1 . Compared to O2O DINs, the only difference in implementation is the use of standard convolution instead of group convolution, i.e., N = 1 in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p><p>Learning with Weak Context. A desirable property of In-dexNet is that it can predict indices even from a large local feature map, e.g., l(X) : R 2kÃ—2kÃ—C â†’ R kÃ—kÃ—1 . An intuition behind this idea is that, if one identifies a local maximum point from a k Ã— k region, its surrounding 2k Ã— 2k region can further support whether this point is a part of a boundary or just an isolated noise point. This idea can be easily implemented by enlarging the convolutional kernel and is also applicable to HINs. Remark 2. Both HINs and DINs have merits and drawbacks. It is clear that DINs have higher capacity than HINs, so DINs may capture more complex local patterns but also be at a risk of overfitting. By contrast, the index map generated by HINs is shared by all channels of the feature map, so the decoder feature map can reserve its expressibility without forcibly reducing its dimensionality to fit the shape of the index map during upsampling. This gives much flexibility for decoder design, while it is not the case for DINs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Relation to Other Networks</head><p>If considering the dynamic property of IndexNet, IndexNet shares a similar spirit with some recent networks. Spatial Transformer Networks (STNs) <ref type="bibr" target="#b20">[21]</ref>. The STN learns dynamic spatial transformation by regressing desired transformation parameters Î¸ with a localized network. A spatially-transformed output is then produced by a sampler parameterized by Î¸. Such a transformation is holistic for the feature map, which is similar to HINs. The differences between STN and IndexNet are that their learning targets have different physical definitions (spatial transformations vs. spatial indices), and that, STN is designed for global transformation, while IndexNet predicts local indices. Dynamic Filter Networks (DFNs) <ref type="bibr" target="#b21">[22]</ref>. The DFN dynamically generates filter parameters on-the-fly with a so-called filter generating network. Compared to conventional filter parameters that are initialized, learned, and stayed fixed during inference, filter parameters in DFN are dynamic and sample-specific. The main difference between DFN and In-dexNet lies in the motivation of the design. Dynamic filters are learned for adaptive feature extraction, but learned indices are used for dynamic downsampling and upsampling. Deformable Convolutional Networks (DCNs) <ref type="bibr" target="#b9">[10]</ref>. The DCN introduces deformable convolution and deformable RoI pooling. The key idea is to predict offsets for convolutional and pooling kernels, so DCN is also a dynamic network. While these convolution and pooling operators concern spatial transformations, they are still built upon stan-dard max pooling and are not designed for upsampling purposes. By contrast, index-guided IP and IU are fundamental operators and may be integrated into RoI pooling. Attention Networks <ref type="bibr" target="#b33">[34]</ref>. Attention networks are a broad family of networks that adopt attention mechanisms. The mechanisms introduce multiplicative interactions between inferred attention maps and feature maps. In Computer Vision, these mechanisms often refer to spatial attention <ref type="bibr" target="#b45">[46]</ref>, channel attention <ref type="bibr" target="#b19">[20]</ref> or both <ref type="bibr" target="#b47">[48]</ref>. As aforementioned, IP and IU in HINs can be viewed as attentional operators to some extent, which means indices are attention. In a reverse sense, attention is also indices. For example, maxpooling indices are a form of hard attention. Indices offer a new perspective to understand attention. It is worth noting that, despite IndexNet in its current implementation closely relates to attention, it has a distinct physical definition and specializes in upsampling rather than refining feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussions</head><p>We evaluate our framework and IndexNet on the task of image matting. This task is particularly suitable for visualizing the quality of learned indices. We mainly conduct experiments on the Adobe Image Matting dataset <ref type="bibr" target="#b48">[49]</ref>. This is so far the largest publicly available matting dataset. The training set has 431 foreground objects and ground-truth alpha mattes. <ref type="bibr" target="#b0">1</ref> Each foreground is composited with 100 background images randomly chosen from MS COCO <ref type="bibr" target="#b30">[31]</ref>. The test set termed Composition-1k includes 100 unique objects. Each of them is composited with 10 background images chosen from Pascal VOC <ref type="bibr" target="#b11">[12]</ref>. Overall, we have 43100 training images and 1000 testing images. We evaluate the results using widely-used Sum of Absolute Differences (SAD), Mean Squared Error (MSE), and perceptuallymotivated Gradient (Grad) and Connectivity (Conn) errors <ref type="bibr" target="#b36">[37]</ref>. The evaluation code implemented by <ref type="bibr" target="#b48">[49]</ref> is used. In what follows, we first describe our modified MobileNetv2-based architecture and training details. We then perform extensive ablation studies to justify choices of model design, make comparisons of different index networks, and visualize learned indices. We also report performance on the alphamatting.com online benchmark <ref type="bibr" target="#b36">[37]</ref> and extend IndexNet to other visual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Our implementation is based on PyTorch <ref type="bibr" target="#b35">[36]</ref>. Here we describe the network architecture used and some essential training details. Network Architecture. We build our model based on MobileNetv2 <ref type="bibr" target="#b38">[39]</ref> with only slight modifications to the backbone. An important reason why we choose Mo-bileNetv2 is that this lightweight model allows us to infer high-resolution images on a GPU, while other highcapacity backbones cannot. The basic network configuration is shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. It also follows the encoder-decoder paradigm same as SegNet. We simply change all 2-stride convolution to be 1-stride and attach 2-stride 2 Ã— 2 max pooling after each encoding stage for downsampling, which allows us to extract indices. If applying the IndexNet idea, max pooling and unpooling layers can be replaced with IP and IU, respectively. We also investigate alternative ways for low-level feature fusion and whether encoding context (Section 5.2). Notice that, the matting refinement stage <ref type="bibr" target="#b48">[49]</ref> is not considered in this paper.</p><p>Training Details. To enable a direct comparison with deep matting <ref type="bibr" target="#b48">[49]</ref>, we follow the same training configurations used in <ref type="bibr" target="#b48">[49]</ref>. The 4-channel input concatenates the RGB image and its trimap. We follow exactly the same data augmentation strategies, including 320Ã—320 random cropping, random flipping, random scaling, and random trimap dilation. All training samples are created on-the-fly. We use a combination of the alpha prediction loss and the composition loss during training as in <ref type="bibr" target="#b48">[49]</ref>. Only losses from the unknown region of the trimap are calculated. Encoder parameters are pretrained on ImageNet <ref type="bibr" target="#b10">[11]</ref>. Note that, the parameters of the 4-th input channel are initialized with zeros. All other parameters are initialized with the improved Xavier <ref type="bibr" target="#b15">[16]</ref>. The Adam optimizer <ref type="bibr" target="#b22">[23]</ref> is used. We update parameters with 30 epochs (around 90, 000 iterations). The learning rate is initially set to 0.01 and reduced by 10Ã— at  <ref type="table">Table 1</ref>: Ablation study of design choices. Fusion: fuse encoder features; Indices: max-pooling indices (when Indices is 'No', bilinear interpolation is used for upsampling); CRP: chained residual pooling <ref type="bibr" target="#b29">[30]</ref>; ASPP: atrous spatial pyramid pooling <ref type="bibr" target="#b3">[4]</ref>; OS: output stride. The lowest errors are boldfaced. the 20-th and 26-th epoch respectively. We use a batch size of 16 and fix the BN layers of the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Adobe Image Matting Dataset</head><p>Ablation Study on Model Design. Here we investigate strategies for fusing low-level features (no fusion, skip fusion as in ResNet <ref type="bibr" target="#b16">[17]</ref> or concatenation as in UNet <ref type="bibr" target="#b37">[38]</ref>) and whether encoding context for image matting. 11 baselines are consequently built to justify model design. Results on the Composition-1k testing set are reported in <ref type="table">Table 1</ref>. B3 is cited from <ref type="bibr" target="#b48">[49]</ref>. We can make the following observations: i) Indices are of great importance. Matting can significantly benefit from only indices (B3 vs. B4, B5 vs. B6); ii) Stateof-the-art semantic segmentation models cannot be directly applied to image matting (B1/B2 vs. B3); iii) Fusing lowlevel features help, and concatenation works better than the skip connection but at a cost of increased computation (B5 vs. B8 vs. B10 or B7 vs. B9 vs. B11); iv) Our intuition tells that the context may not help a low-level task like matting, while results show that encoding context is generally encouraged (B5 vs. B7 or B8 vs. B9 or B10 vs. B11). Indeed, we observe that the context sometimes can help to improve the quality of the background; v) A MobileNetv2based model can work as well as a VGG-16-based one with appropriate design choices (B3 vs. B11).</p><p>For the following experiments, we now mainly use B11.</p><p>Ablation Study on Index Networks. Here we compare different index networks and justify their effectiveness. The configurations of index networks used in the experiments follow Figs. 4 and 5. We primarily investigate the 2 Ã— 2 kernel with a stride of 2. Whenever the weak context is considered, we use a 4 Ã— 4 kernel in the first convolutional layer of index networks. To highlight the effectiveness of HINs, we further build a baseline called holistic max index (HMI) where max-pooling indices are extracted from a squeezed feature map X âˆˆ R HÃ—W Ã—1 . X is generated by applying the max function along the channel dimension of X âˆˆ R HÃ—W Ã—C . We also report the performance when setting the width multiplier of MobileNetV2 used in B11 to be 1.4 (B11-1.4). This allows us to justify whether the improved performance is due to increased model capacity.</p><p>Results on the Composition-1k testing dataset are listed in <ref type="table" target="#tab_4">Table 2</ref>. We observe that, except the most naive linear HIN, all index networks consistently reduce the errors. In particular, nonlinearity and the context generally have a positive effect on deep image matting. Compared to HMI, the direct baseline of HINs, the best HIN ("Nonlinear+Context") has at least 12.3% relative improvement. Compared to B11, the baseline of DINs, M2O DIN with "Nonlinear+Context" exhibits at least 16.5% relative improvement. Notice that, our best model even outperforms the state-of-the-art DeepMatting <ref type="bibr" target="#b48">[49]</ref> that has the refinement stage, and is also computationally efficient with less memory consumption-the inference can be performed on the GTX 1070 over 1920 Ã— 1080 high-resolution images. Some qualitative results are shown in <ref type="figure">Fig. 7</ref>. Our predicted mattes show improved delineation for edges and textures like hair and water drops.</p><p>Index Map Visualization. It is interesting to see what indices are learned by IndexNet. For the holistic index, the index map itself is a 2D matrix and is easily to be visualized. Regarding the depthwise index, we squeeze the index map along the channel dimension and calculate the average responses. Two examples of learned index maps are visualized in <ref type="figure" target="#fig_4">Fig. 8</ref>. We observe that, initial random indices have poor delineation for edges, while learned indices automatically capture the complex structural and textual patterns, e.g., the fur of the dog, and even air bubbles in the water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">alphamatting.com Online Benchmark</head><p>We also report results on the alphamatting.com online benchmark <ref type="bibr" target="#b36">[37]</ref>. We directly test our best model trained on the Adobe Image Dataset, without fine-tuning. Our approach (IndexNet Matting) ranks the first in terms of the gradient error among published methods, as shown in Ta- <ref type="figure">Figure 7</ref>: Qualitative results on the Composition-1k testing set. From left to right, the original image, trimap, ground-truth alpha matte, closed-form matting <ref type="bibr" target="#b28">[29]</ref>, deep image image <ref type="bibr" target="#b28">[29]</ref>, and ours (M2O DIN with "nonlinear + context").   ble 3. According to the qualitative results in <ref type="figure" target="#fig_5">Fig. 9</ref>, our approach produces significantly better mattes on hair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Extensions to Other Visual Tasks</head><p>We further evaluate IndexNet on other three visual tasks. For image classification, we compare three classification networks (LeNet <ref type="bibr" target="#b26">[27]</ref>, MobileNet <ref type="bibr" target="#b17">[18]</ref> and VGG-16 <ref type="bibr" target="#b42">[43]</ref>) on the CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b24">[25]</ref> with/without IndexNet. For monocular depth estimation, we attach IndexNet upon a recent ResNet-50 based baseline <ref type="bibr" target="#b18">[19]</ref> and report the performance on the NYUDv2 dataset <ref type="bibr" target="#b41">[42]</ref>. On the task of scene understanding, we evaluate SegNet <ref type="bibr" target="#b1">[2]</ref> with/without IndexNet on the SUN-RGBD dataset <ref type="bibr" target="#b43">[44]</ref>. Results show that IndexNet consistently improves the performance in all three tasks. We refer readers to the Supplement for quantitative and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Inspired by an observation in image matting, we delve deep into the role of indices and present an unified perspective of upsampling operators using the notion of index function. We show that an index function can be learned within a proposed index-guided encoder-decoder framework. In this framework, indices are learned with a flexible network module termed IndexNet, and are used to guide downsampling and upsampling using two operators called IP and IU. IndexNet itself is also a sub-framework that can be designed depending on the task at hand. We instantiated, investigated three index networks, compared their conceptual differences, discussed their properties, and demonstrated their effectiveness on the task of image matting, image classification, depth prediction and scene understanding. We report state-of-the-art performance on image matting with a modified MobileNetv2-based model on the Composition-1k dataset. We believe that IndexNet is an important step towards the design of generic upsampling operators.</p><p>Our model is simple with much room for improvement. It may be used as a strong baseline for future research. We plan to explore the applicability of IndexNet to other dense prediction tasks.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Index-guided encoder-decoder framework. The proposed IndexNet dynamically predicts indices for individual local regions, conditional on the input local feature map itself. The predicted indices are further utilized to guide the downsampling in the encoding stage and the upsampling in corresponding decoding stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Holistic index networks. (a) a linear index network; (b) a nonlinear index network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Depthwise index networks. N = C for the O2O assumption, and N = 1 for the M2O. The masked modules are invisible to linear networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Customized MobileNetv2-based encoder-decoder network architecture. Our modifications are boldfaced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of the randomly initialized index map (left) and the learned index map (right) of HINs (top) and DINs (bottom). Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results on the alphamatting.com dataset. From left to right, the original image, deep image matting, ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on the Composition-1k testing set. GFLOPs are measured on a 224 Ã— 224 Ã— 4 input. NL: Non-Linearity; C: Context. The lowest errors are boldfaced.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.6 12.3 0.2 0.2 0.2 0.1 0.1 0.3 0.2 0.2 0.2 0.2 0.2 0.4 1.7 1.9 2.5 1 1.1 1.3 1.1 1.2 1.2 0.4 0.5 0.5 AlphaGAN [33] 13.2 12 10.8 16.8 0.2 0.2 0.2 0.2 0.2 0.3 0.2 0.3 0.3 0.2 0.2 0.4 1.8 2.4 2.7 1.1 1.4 1.5 0.9 1.1 1 0.5 0.5 0.6 Deep Matting [49] 14.3 10.8 11 21 0.4 0.4 0.5 0.2 0.2 0.2 0.1 0.1 0.2 0.2 0.2 0.6 1.3 1.5 2.4 0.8 0.9 1.3 0.7 0.8 1.1 0.4 0.5 0.5</figDesc><table><row><cell>Gradient Error</cell><cell cols="2">Average Rank Overall S L</cell><cell>U</cell><cell>S</cell><cell>Troll L U</cell><cell>S</cell><cell>Doll L U</cell><cell>S</cell><cell>Donkey L U</cell><cell>S</cell><cell>Elephant L U</cell><cell>S</cell><cell>Plant L U</cell><cell>Pineapple S L U</cell><cell>Plastic Bag S L U</cell><cell>S</cell><cell>Net L U</cell></row><row><cell>IndexNet Matting</cell><cell>9</cell><cell>7.3 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Gradient errors (top 3) on the alphamatting.com online benchmark. The lowest errors are boldfaced.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The original paper reported that there were 491 images, but the released dataset only includes 431 images. As a result, we use fewer training data than the original paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Huawei Technologies for the donation of GPU cloud computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Ozan</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TOM-Net: Learning transparent object matting from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9233" to="9241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">KNN matting. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image matting with local and nonlocal smooth priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1902" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">264</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Easy matting-a stroke based approach for continuous image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="567" to="576" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1043" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The case for learned index structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Management of Data</title>
		<meeting>International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlocal matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2193" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RefineNet: Multipath refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AlphaGAN: Generative adversarial networks for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machince Vision Conference (BMVC)</title>
		<meeting>British Machince Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image superresolution with fast approximate convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Neural Information Processing (ICONIP)</title>
		<meeting>International Conference on Neural Information essing (ICONIP)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshops</title>
		<imprint>
			<publisher>NIPSW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Poisson matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep propagation based image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conferences on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="999" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
