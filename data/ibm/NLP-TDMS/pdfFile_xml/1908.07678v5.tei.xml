<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asymmetric Non-local Neural Networks for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Asymmetric Non-local Neural Networks for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the nonlocal block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256 × 128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/ MendelXu/ANN.git. * Equal contribution † Corresponding author &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h Z k z g Y O / g V + m e P P y R w 8 d X I Q M H C U = " &gt; A A A B 6 3 i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X E i Z c a P L g h u X F e w F 2 q F k 0 k w n N M k M S U Y o Q 1 / B j Q t F 3 P p C 7 n w b M + 0 s t P W H</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a long-standing challenging task in computer vision, aiming to predict pixel-wise semantic labels in an image accurately. This task is exceptionally important to tons of real-world applications, such as autonomous driving <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, medical diagnosing <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, etc. In recent years, the developments of deep neural networks encourage the emergence of a series of works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>. Shelhamer et al. <ref type="bibr" target="#b25">[26]</ref> proposed the seminal work called Fully Convolutional Network (FCN), which discarded the fully connected layer to support input of arbitrary sizes. Since then, a lot of works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> were inspired to manipulate FCN techniques into deep neural networks. Nonetheless, the segmentation accuracy is still far from satisfactory.     Some recent studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref> indicate that the performance could be improved if making sufficient use of long range dependencies. However, models that solely rely on convolutions exhibit limited ability in capturing these long range dependencies. A possible reason is the receptive field of a single convolutional layer is inadequate to cover correlated areas. Choosing a big kernel or composing a very deep network is able to enlarge the receptive field. However, such strategies require extensive computation and parameters, thus being very inefficient <ref type="bibr" target="#b43">[44]</ref>. Consequently, several works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref> resort to use global operations like non-local means <ref type="bibr" target="#b1">[2]</ref> and spatial pyramid pooling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>.</p><formula xml:id="formula_0">j I K 0 J R T n 2 K k t D Q 2 i 2 6 A 1 M z z 0 2 v o K h o Q C d u L s V m y K n W n W a 0 7 U J N m r V Z v a N K w b K d 6 C e 2 K t U Q J r N E Z m + / u J M R x Q L j C D E k 5 t K 1 I j V I k F M W M L A p u L E m E 8 B x N y V B T j v S d U b o M v 4 D n W p l A P x T 6 c Q W X 6 v e N F A V S J o G n J 7 O o 8 r e X i X 9 5 w 1 j 5 z V F K e R Q r w v H q k B 8 z q E K Y N Q E n V B C s W K I J w o L q r B D P k E B Y 6 b 4 K u o S v n 8 L / S e + i Y m t + Y 5 V a 5 X U d e X A K z k A Z 2 M A B L X A F O q A L M E j A A 3 g C</formula><p>In <ref type="bibr" target="#b32">[33]</ref>, Wang et al. combined CNNs and traditional nonlocal means <ref type="bibr" target="#b1">[2]</ref> to compose a network module named nonlocal block in order to leverage features from all locations in an image. This module improves the performance of existing methods <ref type="bibr" target="#b32">[33]</ref>. However, the prohibitive compu-tational cost and vast GPU memory occupation hinder its usage in many real applications. The architecture of a common non-local block <ref type="bibr" target="#b32">[33]</ref> is depicted in <ref type="figure" target="#fig_2">Fig. 1(a)</ref>. The block first calculates the similarities of all locations between each other, requiring a matrix multiplication of computational complexity O(CH 2 W 2 ), given an input feature map with size C × H × W . Then it requires another matrix multiplication of computational complexity O(CH 2 W 2 ) to gather the influence of all locations to themselves. Concerning the high complexity brought by the matrix multiplications, we are interested in this work if there are efficient ways to solve this without sacrificing the performance.</p><p>We notice that as long as the outputs of the key branch and value branch hold the same size, the output size of the non-local block remains unchanged. Considering this, if we could sample only a few representative points from key branch and value branch, it is possible that the time complexity is significantly decreased without sacrificing the performance. This motivation is demonstrated in <ref type="figure" target="#fig_2">Fig. 1</ref> when changing a large value N in the key branch and value branch to a much smaller value S (From (a) to (b)).</p><p>In this paper, we propose a simple yet effective nonlocal module called Asymmetric Pyramid Non-local Block (APNB) to decrease the computation and GPU memory consumption of the standard non-local module <ref type="bibr" target="#b32">[33]</ref> with applications to semantic segmentation. Motivated by the spatial pyramid pooling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">47]</ref> strategy, we propose to embed a pyramid sampling module into non-local blocks, which could largely reduce the computation overhead of matrix multiplications yet provide substantial semantic feature statistics. This spirit is also related to the sub-sampling tricks <ref type="bibr" target="#b32">[33]</ref> (e.g., max pooling). Our experiments suggest that APNB yields much better performance than those subsampling tricks with a decent decrease of computations. To better illustrate the boosted efficiency, we compare the GPU times of APNB and a standard non-local block in <ref type="figure" target="#fig_3">Fig. 2</ref>, averaging the running time of 10 different runs with the same configuration. Our APNB largely reduces the time cost on matrix multiplications, thus being nearly 6 times faster than a non-local block.</p><p>Besides, we also adapt APNB to fuse the features of different stages of a deep network, which brings a considerable improvement over the baseline model. We call the adapted block as Asymmetric Fusion Non-local Block (AFNB). AFNB calculates the correlations between every pixel of the low-level and high-level feature maps, yielding a fused feature with long range interactions. Our network is built based on a standard ResNet-FCN model by integrating APNB and AFNB together.</p><p>The efficacy of the proposed network is evaluated on Cityscapes <ref type="bibr" target="#b8">[9]</ref>, ADE20K <ref type="bibr" target="#b49">[50]</ref> and PASCAL Context <ref type="bibr" target="#b20">[21]</ref>, achieving the state-of-the-art performance 81.3%, 45.24% and 52.8%, respectively. In terms of time and space ef-ficiency, APNB is around 6 times faster than a non-local block on a GPU while 28 times smaller in GPU running memory occupation for a 256 × 128 input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review related works about semantic segmentation or scene parsing. Recent advances focus on exploring the context information and can be roughly categorized into five directions:</p><p>Encoder-Decoder. A encoder generally reduces the spatial size of feature maps to enlarge the receptive field. Then the encoded codes are fed to the decoder, which is responsible for recovering the spatial size of the prediction maps. Long et al. <ref type="bibr" target="#b25">[26]</ref> and Noh et al. <ref type="bibr" target="#b21">[22]</ref> used deconvolutions to perform the decoding pass. Ronneberger et al. <ref type="bibr" target="#b24">[25]</ref> introduced skip-connections to bridge the encoding features to their corresponding decoding features, which could enrich the segmentation output with more details. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> introduced a context encoding module to predict semantic category importance and selectively strengthen or weaken class-specific feature maps.</p><p>CRF. As a frequently-used operation that could leverage context information in machine learning, Conditional Random Field <ref type="bibr" target="#b14">[15]</ref> meets its new opportunity in combining with CNNs for semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b30">31]</ref>. CRF-CNN <ref type="bibr" target="#b48">[49]</ref> adopted this strategy, making the deep network end-to-end trainable. Chandra et al. <ref type="bibr" target="#b3">[4]</ref> and Vemulapalli et al. <ref type="bibr" target="#b30">[31]</ref> integrated Gaussian Conditional Random Fields into CNNs and achieved relatively good results. Different Convolutions. Chen et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and Yu et al. <ref type="bibr" target="#b39">[40]</ref> adapted generic convolutions to dilated ones, making the networks sensitive to global context semantics and thus improves the performance. Peng et al. <ref type="bibr" target="#b23">[24]</ref> found large kernel convolutions help relieve the contradiction between classification and localization in segmentation.</p><p>Spatial Pyramid Pooling. Inspired by the success of spatial pyramid pooling in object detection <ref type="bibr" target="#b11">[12]</ref>, Chen et al. <ref type="bibr" target="#b5">[6]</ref> replaced the pooling layers with dilated convolutions of different sampling weights and built an Atrous Spatial Pyramid Pooling layer (ASPP) to account for multiple scales explicitly. Chen et al. <ref type="bibr" target="#b7">[8]</ref> further combined ASPP and the encoder-decoder architecture to leverage the advantages of both and boost the performance considerably. Drawing inspiration from <ref type="bibr" target="#b15">[16]</ref>, PSPNet <ref type="bibr" target="#b46">[47]</ref> conducted spatial pyramid pooling after a specific layer to embed context features of different scales into the networks. Recently, Yang et al. <ref type="bibr" target="#b35">[36]</ref> pointed out the ASPP layer has a restricted receptive field and adapted ASPP to a densely connected version, which helps to overcome such limitation.</p><p>Non-local Network. Recently, researchers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref> noticed that skillful leveraging the long range dependencies      brings great benefits to semantic segmentation. Wang et al. <ref type="bibr" target="#b32">[33]</ref> proposed a non-local block module combining nonlocal means with deep networks and showcased its efficacy for segmentation.</p><formula xml:id="formula_1">O j t o l T z X i L x T L W 3 Y A a L o X i L R Q o e T f R n E a B 5 J 1 g c j P 3 O 0 9 c G x G r R 5 w m 3 I / o S I l Q M I p W e r g b N A f V m l t 3 c 5 B V 4 h W k B g V s / q s / j F k a c Y V M U m N 6 n p u g n 1 G N g k k + q / R T w x P K J n T E e 5 Y q G n H j Z / m q M 3 J m l S E J Y 2 2 f Q p K r v y c y G h k z j Q K b j C i O z b I 3 F / / z e i m G 1 3 4 m V J I i V 2 z x U Z h K g j G Z 3 0 2 G Q n O G c m o J Z V r Y X Q k b U 0 0 Z 2 n Y q t g R v + e R V</formula><formula xml:id="formula_2">V r S 1 0 C 4 l m 2 b b 0 G y y J L N C W f o T v H h Q x K u / y J v / x r T d g 7 a + E H h 4 Z 4 b M v F E q h U X f / / Z K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w q G 1 1 Z h h v M S 2 1 6 U T U c i k U b 6 F A y T u p 4 T S J J H + M x j e z + u M T N 1 Z o 9 Y C T l I c J H S o R C 0 b R W f e d v u x X a 3 7 d n 4 u s Q l B A D Q o 1 + 9 W v 3 k C z L O E K m a T W d g M / x T C n B g W T f F r p Z Z a n l I 3 p k H c d K p p w G + b z V a f k z D k D E m v j n k I y d 3 9 P 5 D S x d p J E r j O h O L L L t Z n 5 X 6 2 b Y X w d 5 k K l G X L F F h / F m S S o y e x u M h C G M 5 Q T B 5 Q Z 4 X Y l b E Q N Z e j S q b g Q g u W T V 6 F 9 W Q 8 c 3 / m 1 x k U R R x l O 4 B T O I Y A r a M A t N K E F D I b w D K / w 5 k n v x X v 3 P h a t J a + Y O Y Y / 8 j 5 / A C y A j a E = &lt; / l a t e x i t &gt; Pyramid Pooling</formula><formula xml:id="formula_3">2 o U l 2 S b J C W f o T v H h Q x K u / y J v / x r T d g 7 a + E H h 4 Z 4 b M v F E q u L G + / + 2 V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d s k m W b Y Y o l I d C e i B g V X 2 L L c C u y k G q m M B D 5 G 4 5 t Z / f E J t e G J e r C T F E N J h 4 r H n F H r r P t O f 9 S v 1 v y 6 P x d Z h a C A G h R q 9 q t f v U H C M o n K M k G N 6 Q Z + a s O c a s u Z w G m l l x l M K R v T I X Y d K i r R h P l 8 1 S k 5 c 8 6 A x I l 2 T 1 k y d 3 9 P 5 F Q a M 5 G R 6 5 T U j s x y b W b + V + t m N r 4 O c 6 7 S z K J i i 4 / i T B C b k N n d Z M A 1 M i s m D i j T 3 O 1 K 2 I h q y q x L p + J C C J Z P X o X 2 Z T 1 w f O f X G h d F H G U 4 g V M 4 h w C u o A G 3 0 I Q W M B j C</formula><formula xml:id="formula_4">2 E o = " &gt; A A A B 6 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 C A l 8 V K P B U G 8 W d F + Q B v K Z r t p l 2 4 2 Y X c i l N C f 4 M W D I l 7 9 R d 7 8 N 2 7 b H L T 1 h Y W H d 2 b Y m T d I p D D o u t 9 O Y W 1 9 Y 3 O r u F 3 a 2 d 3 b P y g f H r V M n G r G m y y W s e 4 E 1 H A p F G + i Q M k 7 i e Y 0 C i R v B + P r W b 3 9 x L U R s X r E S c L 9 i A 6 V C A W j a K 2 H u / 5 N v 1 x x q + 5 c Z B W 8 H C q Q q 9 E v f / U G M U s j r p B J a k z X c x P 0 M 6 p R M M m n p V 5 q e E L Z m A 5 5 1 6 K i E T d + N l 9 1 S s 6 s M y B h r O 1 T S O b u 7 4 m M R s Z M o s B 2 R h R H Z r k 2 M / + r d V M M r / x M q C R F r t j i o z C V B G M y u 5 s M h O Y M 5 c Q C Z V r Y X Q k b U U 0 Z 2 n R K N g R v + e R V a F 1 W P c v 3 b q V + k c d R</formula><formula xml:id="formula_5">k i 0 + C l N B T E x m d 5 M B V 8 i M m F i g T H G 7 K 2 E j q i g z N p 2 S D c F b P n k V W p d V z / K d W 6 l f 5 H E U 4 Q R O 4 R w 8 q E E d b q E B T W A w h G d 4 h T d H O C / O u / O</formula><p>Different from these works, our network uniquely incorporates pyramid sampling strategies with non-local blocks to capture the semantic statistics of different scales with only a minor budget of computation, while maintaining the excellent performance as the original non-local modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Asymmetric Non-local Neural Network</head><p>In this section, we firstly revisit the definition of nonlocal block <ref type="bibr" target="#b32">[33]</ref> in Sec. 3.1, then detail the proposed Asymmetrical Pyramid Non-local Block (APNB) and Asymmetrical Fusion Non-local Block (AFNB) in Sec. 3.2 and Sec. 3.3, respectively. While APNB aims to decrease the computational overhead of non-local blocks, AFNB improves the learning capacity of non-local blocks thereby improving the segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting Non-local Block</head><p>A typical non-local block <ref type="bibr" target="#b32">[33]</ref> is shown in <ref type="figure" target="#fig_2">Fig. 1</ref>. Consider an input feature X ∈ R C×H×W , where C, W , and H indicate the channel number, spatial width and height, respectively. Three 1 × 1 convolutions W φ , W θ , and W γ are used to transform X to different embeddings φ ∈ RĈ ×H×W , θ ∈ RĈ ×H×W and γ ∈ RĈ ×H×W as</p><formula xml:id="formula_6">φ = W φ (X ), θ = W θ (X ), γ = W γ (X ),<label>(1)</label></formula><p>whereĈ is the channel number of the new embeddings. Next, the three embeddings are flattened to sizeĈ × N , where N represents the total number of the spatial locations, that is, N = H · W . Then, the similarity matrix V ∈ R N ×N is calculated by a matrix multiplication as</p><formula xml:id="formula_7">V = φ T × θ.<label>(2)</label></formula><p>Afterward, a normalization is applied to V to get a unified similarity matrix as</p><formula xml:id="formula_8">V = f (V ).<label>(3)</label></formula><p>According to <ref type="bibr" target="#b32">[33]</ref>, the normalizing function f can take the form from softmax, rescaling, and none. We choose softmax here, which is equivalent to the self-attention mechanism and proved to work well in many tasks such as machine translation <ref type="bibr" target="#b29">[30]</ref> and image generation <ref type="bibr" target="#b43">[44]</ref>. For every location in γ, the output of the attention layer is</p><formula xml:id="formula_9">O = V × γ T ,<label>(4)</label></formula><p>where O ∈ R N ×Ĉ . By referring to the design of the nonlocal block, the final output is given by</p><formula xml:id="formula_10">Y = W o (O T ) + X or Y = cat(W o (O T ), X),<label>(5)</label></formula><p>where W o , also implemented by a 1 × 1 convolution, acts as a weighting parameter to adjust the importance of the nonlocal operation w.r.t. the original input X and moreover, recovers the channel dimension fromĈ to C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Asymmetric Pyramid Non-local Block</head><p>The non-local network is potent to capture the long range dependencies that are crucial for semantic segmentation. However, the non-local operation is very time and memory consuming compared to normal operations in the deep neural network, e.g., convolutions and activation functions.    time complexities of the two matrix multiplications are both O(ĈN 2 ) = O(ĈH 2 W 2 ). In semantic segmentation, the output of the network usually has a large resolution to retain detailed semantic features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47]</ref>. That means N is large (for example in our training phase, N = 96 × 96 = 9216). Hence, the large matrix multiplication is the main cause of the inefficiency of a non-local block (see our statistic in <ref type="figure" target="#fig_3">Fig. 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H 2 K j x i 9 N D 5 n g W 3 S z 0 t K g h D 4 w 9 t I = " &gt; A A A B 6 H i c b Z D L S g M x F I b P e K 3 1 V n X p J l g E V 2 X G j S 4 L b r p s w V 6 g H U o m P d P G J p k h y Q h l 6 B O 4 c a G I W 5 / E Z 3 D n 2 5 h e F t r 6 Q + D j P y e c c / 4 o F d x Y 3 / / 2 N j a 3 t n d 2 C 3 v F / Y P D o + P S y W n L J J l m 2 G S J S H Q n o g Y F V 9 i 0 3 A r s p B q p j A S 2 o / H d r N 5 + R G 1 4 o u 7 t J M V Q 0 q H i M W f U O q t R 6 5 f K f s W f i 6 x D s I Q y L F X v l 7 5 6 g 4 R l E p V l g h r T D f z U h j n V l j O B 0 2 I v M 5 h S N q Z D 7 D p U V K I J 8 / m i U 3 L p n A G J E + 2 e s m T u / v 6 R U 2 n M R E a u U 1 I 7 M q u 1 m f l f r Z v Z + D b M u U o z i 4 o t B s W Z I D Y h s 6 v J g G t k V k w c U K a 5 2 5 W w E d W U W Z d N 0 Y U Q r J 6 8 D q 3 r S u C 4 4 Z e r 0 e c i j g K c w w V c Q Q A 3 U I U a 1 K E J D B C e 4 A V e v Q f v 2 X v z 3 h e t G 9 4 y w j P 4 I + / j B 9 7 b j a g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T z p h / f H U i M A p G a f a + N Z / i z t i F + k = " &gt; A A A C C 3 i c b Z D L S g M x G I U z X m u 9 j b p 0 E 1 o E F 6 V M K m g 3 Q s G N y w r 2 A p 1 x y K S Z N j S T G Z K M U I b Z u / F V 3 L h Q x I U b X 8 C d b 2 N 6 W W j r g c D H O X 9 I / h M k n C n t O N / W y u r a + s Z m Y a u 4 v b O 7 t 2 8 f H L Z V n E p C W y T m s e w G W F H O B G 1 p p j n t J p L i K O C 0 E 4 y u J n n n n k r F Y n G r x w n 1 I j w Q L G Q E a 2 P 5 d s l V a e R n w m U C u h m q w L M K P K / A O n T z X N z V L h F y f L v s V J 2 p 4 D K g O Z T B X E 3 f / n L 7 M U k j K j T h W K k e c h L t Z V h q R j j N i 2 6 q a I L J C A 9 o z 6 D A E V V e N t 0 l h y f G 6 c M w l u Y I D a f u 7 x s Z j p Q a R 4 G Z j L A e q s V s Y v 6 X 9 V I d 1 r 2 M i S T V V J D Z Q 2 H K o Y 7 h p B j Y Z 5 I S z c c G M J H M / B W S I Z a Y a F N f 0 Z S A F l d e h n a t i g z f O O V G 8 D G r o w C O Q Q m c A g Q u Q A N c g y Z o A Q I e w B N 4 A a / W o / V s v V n v s 9 E V a 1 7 h E f g j 6 / M H v X C Y 9 w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x s 3 7 d j e c H v 1 4 u T D M 0 p s V U i D n q d w = " &gt; A A A B 8 X i c b Z D L S g N B E E V r f M b 4 i r p 0 0 x g E V 2 H G j S 4 D b l x G M A 9 M h t D T q U m a 9 P Q M 3 T V C C P k L N y 4 U c e t P +</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>g g F 1 L R h R A s n 7 w K j c t K 4 P j O L 1 e j z 3 k c B T i F M 7 i A A K 6 g C r d Q g z o I 0 P A E L / D q W e / Z e / P e 5 6 1 r 3 i L C E / h T 3 s c P i M y Q 2 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H T P k Z g S A U f m d 2 U X R 1 i E I K b 7 Z c t k = " &gt; A A A B 8 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J U Z X e i y 4 M Z l B X v B d i i Z N N O G Z j J D c k Y o Q 9 / C j Q t F 3 P o S P o M 7 3 8 b 0 s t D W A 4 G P / z / h n P O H q Z I W P e + b F N b W N z a 3 i t u l n d 2 9 / Y P y 4 V H T J p n h o s E T l Z h 2 y K x Q U o s G S l S i n R r B 4 l C J V j i 6 m f q t R 2 G s T P Q 9 j l M R x G y g Z S Q 5 Q y c 9 X N I u y l h Y e t k r V 7 y q N y u 6 C v 4 C K r C o e q / 8 1 e 0 n P I u F R q 6 Y t R 3 f S z H I m U H J l Z i U u p k V K e M j N h A d h 5 q 5 M U E + 2 3 h C z 5 z S p 1 F i 3 N N I Z + r v H z m L r R 3 H o e u M G Q 7 t s j c V / / M 6 G U b X Q S 5 1 m q H Q f D 4 o y h T F h E 7 P p 3 1 p B E c 1 d s C 4 k W 5 X y o f M M I 4 u p J I L w V 8 + e R W a F 1 X f 8 Z 1 X q Y W f 8 z i K c A K n c A 4 + X E E N b q E O D e C g 4 Q l e 4 J V Y 8 k z e y P u 8 t U A W E R 7 D n y I f P 4 7 u k N 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O B I r G 8 W f 9 8 F V 9 1 i Y y / I h 1 a s Y c i k = " &gt; A A A B 8 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J U Z F + q y 4 M Z l B X v B d i i Z N N O G Z j J D c k Y o Q 9 / C j Q t F 3 P o S P o M 7 3 8 b 0 s t D W A 4 G P / z / h n P O H q Z I W P e + b F N b W N z a 3 i t u l n d 2 9 / Y P y 4 V H T J p n h o s E T l Z h 2 y K x Q U o s G S l S i n R r B 4 l C J V j i 6 m f q t R 2 G s T P Q 9 j l M R x G y g Z S Q 5 Q y c 9 X N I u y l h Y e t k r V 7 y q N y u 6 C v 4 C K r C o e q / 8 1 e 0 n P I u F R q 6 Y t R 3 f S z H I m U H J l Z i U u p k V K e M j N h A d h 5 q 5 M U E + 2 3 h C z 5 z S p 1 F i 3 N N I Z + r v H z m L r R 3 H o e u M G Q 7 t s j c V / / M 6 G U b X Q S 5 1 m q H Q f D 4 o y h T F h E 7 P p 3 1 p B E c 1 d s C 4 k W 5 X y o f M M I 4 u p J I L w V 8 + e R W a F 1 X f 8 Z 1 X q Y W f 8 z i K c A K n c A 4 + X E E N b q E O D e C g 4 Q l e 4 J V Y 8 k z e y P u 8 t U A W E R 7 D n y I f P 5 g h k O Q = &lt; / l a t e x i t &gt;</head><p>A more straightforward pipeline is given as</p><formula xml:id="formula_11">R N ×Ĉ × RĈ ×N Eq.(2) → R N ×N × R N ×Ĉ Eq.(4) → R N ×Ĉ . (6)</formula><p>We hold a key yet intuitive observation that by changing N to another number S (S N ), the output size will remain the same, as</p><formula xml:id="formula_12">R N ×Ĉ × RĈ ×S → R N ×S × R S×Ĉ → R N ×Ĉ . (7)</formula><p>Returning to the design of the non-local block, changing N to a small number S is equivalent to sampling several representative points from θ and γ instead of feeding all the spatial points, as illustrated in <ref type="figure" target="#fig_2">Fig. 1</ref>. Consequently, the computational complexity could be considerably decreased.</p><p>Solution. Based on the above observation, we propose to add sampling modules P θ and P γ after θ and γ to sample several sparse anchor points denoted as θ P ∈ RĈ ×S and γ P ∈ RĈ ×S , where S is the number of sampled anchor points. Mathematically, this is computed by</p><formula xml:id="formula_13">θ P = P θ (θ), γ P = P γ (γ).<label>(8)</label></formula><p>The similarity matrix V P between φ and the anchor points θ P is thus calculated by</p><formula xml:id="formula_14">V P = φ T × θ P .<label>(9)</label></formula><p>Note that V P is an asymmetric matrix of size N × S. V P then goes through the same normalizing function as a standard non-local block, giving the unified similarity matrix V P . And the attention output is acquired by</p><formula xml:id="formula_15">O P = V P × γ P T ,<label>(10)</label></formula><p>where the output is in the same size as that of Eq. (4). Following non-local blocks, the final output Y P is given as</p><formula xml:id="formula_16">Y P = cat(W o (O P T ), X).<label>(11)</label></formula><p>The time complexity of such an asymmetric matrix multiplication is only O(ĈN S), significantly lower than O(ĈN 2 ) in a standard non-local block. It is ideal that S should be much smaller than N . However, it is hard to ensure that when S is small, the performance would not drop too much in the meantime.</p><p>As discovered by previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47]</ref>, global and multi-scale representations are useful for categorizing scene semantics. Such representations can be comprehensively carved by Spatial Pyramid Pooling <ref type="bibr" target="#b15">[16]</ref>, which contains several pooling layers with different output sizes in parallel. In addition to this virtue, spatial pyramid pooling is also parameter-free and very efficient. Therefore, we embed pyramid pooling in the non-local block to enhance the global representations while reducing the computational overhead.</p><p>By doing so, we now arrive at the final formulation of Asymmetric Pyramid Non-local Block (APNB), as given in <ref type="figure" target="#fig_9">Fig. 3</ref>. As can be seen, our APNB derives from the design of a standard non-local block <ref type="bibr" target="#b32">[33]</ref>. A vital change is to add a spatial pyramid pooling module after θ and γ respectively to sample representative anchors. This sampling process is clearly depicted in <ref type="figure" target="#fig_13">Fig. 4</ref>, where several pooling layers are applied after θ or γ and then the four pooling results are flattened and concatenated to serve as the input to the next layer. We denote the spatial pyramid pooling modules as P n θ and P n γ , where the superscript n means the width (or height) of the output size of the pooling layer (empirically, the width is equal to the height). In our model, we set n ⊆ {1, 3, 6, 8}. Then the total number of the anchor points is</p><formula xml:id="formula_17">S = 110 = n∈{1,3,6,8} n 2 .<label>(12)</label></formula><p>As a consequence, the complexity of our asymmetric matrix multiplication is only</p><formula xml:id="formula_18">T = S N<label>(13)</label></formula><p>times of the complexity of the non-local matrix multiplication. When H = 128 and W = 256, the asymmetrical matrix multiplication saves us 256×128</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>110</head><p>≈ 298 times of computation (see the results in <ref type="figure" target="#fig_3">Fig. 2)</ref>. Moreover, the spatial pyramid pooling gives sufficient feature statistics about the global scene semantic cues to remedy the potential performance deterioration caused by the decreased computation. We will analyze this in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Asymmetric Fusion Non-local Block</head><p>Fusing features of different levels are helpful to semantic segmentation and object tracking as hinted in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref>. Common fusing operations such as addition/concatenation, are conducted in a pixel-wise and local manner. We provide an alternative that leverages long range dependencies through a non-local block to fuse multi-level features, called Fusion Non-local Block.</p><p>A standard non-local block only has one input source while FNB has two: a high-level feature map X h ∈ R C h ×N h and a low-level feature map X l ∈ R C l ×N l . N h and N l are the numbers of spatial locations of X h and X l , respectively. C h and C l are the channel numbers of X h and X l , respectively. Likewise, 1 × 1 convolutions W h φ , W l θ and W l γ are used to transform X h and X l to embeddings φ h ∈ RĈ ×N h , θ l ∈ RĈ ×N l and γ l ∈ RĈ ×N l as</p><formula xml:id="formula_19">φ h = W h φ (X h ), θ l = W l θ (X l ), γ l = W l γ (X l ).<label>(14)</label></formula><p>Then, the similarity matrix V F ∈ R N h ×N l between φ h and θ l is computed by a matrix multiplication</p><formula xml:id="formula_20">V F = φ h T × θ l .<label>(15)</label></formula><p>We also put a normalization upon V F resulting in a unified similarity matrix V F ∈ R N h ×N l . Afterward, we integrate V F with γ l through a similar matrix multiplication as Eq.</p><p>(4) and Eq. (10), written as</p><formula xml:id="formula_21">O F = V F × γ T l .<label>(16)</label></formula><p>The output O F ∈ R N h ×Ĉ reflects the bonus of X l to X h , which are carefully selected from all locations in X l . Likewise, O F is fed to a 1×1 convolution to recover the channel number to C h . Finally, we have the output as</p><formula xml:id="formula_22">Y F = cat(W o (O F T ), X h ).<label>(17)</label></formula><p>Similar to the adaption of APNB w.r.t. the generic nonlocal block, incorporating spatial pyramid pooling into FNB could derive an efficient Asymmetric Fusion Non-local Block (AFNB), as illustrated in <ref type="figure" target="#fig_9">Fig. 3</ref>. Inheriting from the advantages of APNB, AFNB is more efficient than FNB without sacrificing the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture</head><p>The overall architecture of our network is depicted in <ref type="figure" target="#fig_9">Fig. 3</ref>. We choose ResNet-101 <ref type="bibr" target="#b12">[13]</ref> as our backbone network following the choice of most previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. We remove the last two down-sampling operations and use the dilation convolutions instead to hold the feature maps from the last two stages 1 1 8 of the input image. Concretely, all the feature maps in the last three stages have the same spatial size. According to our experimental trials, we fuse the features of Stage4 and Stage5 using AFNB. The fused features are thereupon concatenated with the feature maps after Stage5, avoiding situations that AFNB could not produce accurate strengthened features particularly when <ref type="bibr" target="#b0">1</ref> We refer to the stage with original feature map size <ref type="bibr">1 16</ref> as Stage4 and size <ref type="bibr">1 32</ref> as Stage5.</p><p>the training just begins and degrades the overall performance. Such features, full of rich long range cues from different feature levels, serve as the input to APNB, which then help to discover the correlations among pixels. As done for AFNB, the output of APNB is also concatenated with its input source. Note that in our implementation for APNB, W θ and W γ share parameters in order to save parameters and computation, following the design of <ref type="bibr" target="#b41">[42]</ref>. This design doesn't decrease the performance of APNB. Finally, a classifier is followed up to produce channel-wise semantic maps that later receive their supervisions from the ground truth maps. Note we also add another supervision to Stage4 following the settings of <ref type="bibr" target="#b46">[47]</ref>, as it is beneficial to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate our method, we carry out detailed experiments on three semantic segmentation datasets: Cityscapes <ref type="bibr" target="#b8">[9]</ref>, ADE20K <ref type="bibr" target="#b49">[50]</ref> and PASCAL Context <ref type="bibr" target="#b20">[21]</ref>. We have more competitive results on NYUD-V2 <ref type="bibr" target="#b28">[29]</ref> and COCO-Stuff-10K <ref type="bibr" target="#b2">[3]</ref> in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>Cityscapes <ref type="bibr" target="#b8">[9]</ref> is particularly created for scene parsing, containing 5,000 high quality finely annotated images and 20,000 coarsely annotated images. All images in this dataset are shot on streets and of size 2048 × 1024. The finely annotated images are divided into 2,975/500/1,525 splits for training, validation and testing, respectively. The dataset contains 30 classes annotations in total while only 19 classes are used for evaluation.</p><p>ADE20K <ref type="bibr" target="#b49">[50]</ref> is a large-scale dataset used in ImageNet Scene Parsing Challenge 2016, containing up to 150 classes. The dataset is divided into 20K/2K/3K images for training, validation, and testing, respectively. Different from Cityscapes, both scenes and stuff are annotated in this dataset, adding more challenge for participated methods.</p><p>PASCAL Context <ref type="bibr" target="#b20">[21]</ref> gives the segmentation labels of the whole image from PASCAL VOC 2010, containing 4,998 images for training and 5,105 images for validation. We use the 60 classes (59 object categories plus background) annotations for evaluation.</p><p>Evaluation Metric. We adopt Mean IoU (mean of classwise intersection over union) as the evaluation metric for all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Training Objectives. Following <ref type="bibr" target="#b46">[47]</ref>, our model has two supervisions: one after the final output of our model while another at the output layer of Stage4. Therefore, our loss function is composed by two cross entropy losses as</p><formula xml:id="formula_23">L = L final + λL Stage4 .<label>(18)</label></formula><p>For L final , we perform online hard pixel mining, which excels at coping with difficult cases. λ is set to 0.4.</p><p>Training Settings. Our code is based on an open source repository for semantic segmentation <ref type="bibr" target="#b36">[37]</ref> using PyTorch 1.0 framework <ref type="bibr" target="#b22">[23]</ref>. The backbone network ResNet-101 is pretrained on the ImageNet <ref type="bibr" target="#b9">[10]</ref>. We use Stochastic Gradient Descent (SGD) to optimize our network, in which we set the initial learning rate to 0.01 for Cityscapes and PASCAL Context and 0.02 for ADE20K. During training, the learning rate is decayed according to the "poly" leaning rate policy, where the learning rate is multiplied by 1 − ( iter max iter ) power with power = 0.9. For Cityscapes, we randomly crop out high-resolution patches 769 × 769 from the original images as the inputs for training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref>. While for ADE20K and PASCAL Context, we set the crop size to 520 × 520 and 480 × 480, respectively <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47]</ref>. For all datasets, we apply random scaling in the range of [0.5, 2.0], random horizontal flip and random brightness as additional data augmentation methods. Batch size is 8 in Cityscapes experiments and 16 in the other datasets. We choose the cross-GPU synchronized batch normalization in <ref type="bibr" target="#b42">[43]</ref> or apex to synchronize the mean and standard-deviation of batch normalization layer across multiple GPUs. We also apply the auxiliary loss L Stage4 and online hard example mining strategy in all the experiments as their effects for improving the performance are clearly discussed in previous works <ref type="bibr" target="#b46">[47]</ref>. We train on the training set of Cityscapes, ADE20K and PASCAL Context for 60K, 150K, 28K iterations, respectively. All the experiments are conducted using 8× Titan V GPUs.</p><p>Inference Settings. For the comparisons with state-of-theart methods, we apply multi-scale whole image and leftright flip testing for ADE20K and PASCAL Context while multi-scale sliding crop and left-right flip testing for the Cityscapes testing set. For quick ablation studies, we only employ single scale testing on the validation set of Cityscapes by feeding the whole original images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with Other Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Efficiency Comparison with Non-local Block</head><p>As discussed in Sec. 3.2, APNB is much more efficient than a standard non-local block. We hereby give a quantitative efficiency comparison between our APNB and a generic non-local block in the following aspects: GFLOPs, GPU memory (MB) and GPU computation time (ms  Besides the comparison of the single block efficiency, we also provide the whole network efficiency comparison with the two most advanced methods, PSANet <ref type="bibr" target="#b47">[48]</ref> and DenseA-SPP <ref type="bibr" target="#b35">[36]</ref>, in terms of inference time (s), GPU occupation with batch size set to 1 (MB) and the number of parameters (Million). According to Tab. 2, though our inference time and parameter number are larger than DenseASPP <ref type="bibr" target="#b35">[36]</ref>, the GPU memory occupation is obviously smaller. We attribute this to the different backbone networks: ResNet comparatively contains more parameters and layers while DenseNet is more GPU memory demanding. When comparing with the previous advanced method PSANet <ref type="bibr" target="#b47">[48]</ref>, which shares the same backbone network with us, our model is more advantageous in all aspects. This verifies our network is superior because of the effectiveness of APNB and AFNB rather than just having more parameters than previous works.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Performance Comparisons</head><p>Cityscapes. To compare the performance on the test set of Cityscapes with other methods, we directly train our asymmetric non-local neural network for 120K iterations with only the finely annotated data, including the training and validation sets. As shown in Tab. 3, our method outperforms the previous state-of-the-art methods, attaining the performance of 81.3%. We give several typical qualitative comparisons with other methods in <ref type="figure" target="#fig_14">Fig. 5</ref>. DeepLab-V3 <ref type="bibr" target="#b6">[7]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone mIoU (%) RefineNet <ref type="bibr" target="#b17">[18]</ref> ResNet-152 40.70 UperNet <ref type="bibr" target="#b34">[35]</ref> ResNet-101 42.65 DSSPN <ref type="bibr" target="#b16">[17]</ref> ResNet-101 43.68 PSANet <ref type="bibr" target="#b47">[48]</ref> ResNet-101 43.77 SAC <ref type="bibr" target="#b44">[45]</ref> ResNet-101 44.30 EncNet <ref type="bibr" target="#b42">[43]</ref> ResNet-101 44.65 PSPNet <ref type="bibr" target="#b46">[47]</ref> ResNet-101 43.29 PSPNet <ref type="bibr" target="#b46">[47]</ref> ResNet-269 44.94 Ours</p><p>ResNet-101 45.24 and PSPNet <ref type="bibr" target="#b46">[47]</ref> are somewhat troubled with local inconsistency on large objects like truck (first row), fence (second row) and building (third row) etc. while our method isn't. Besides, our method performs better for very slim objects like the pole (fourth row) as well.</p><p>ADE20K. As is known, ADE20K is challenging due to its various image sizes, lots of semantic categories and the gap between its training and validation set. Even under such circumstance, our method achieves better results than EncNet <ref type="bibr" target="#b42">[43]</ref>. It is noteworthy that our result is better than PSPNet <ref type="bibr" target="#b46">[47]</ref> even when it uses a deeper backbone ResNet-269.</p><p>PASCAL Context. We report the comparison with stateof-the-art methods in Tab. 5. It can be seen that our model achieves the state-of-the-art performance of 52.8%. This result firmly suggests the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we give extensive experiments to verify the efficacy of our main method. We also give several design choices and show their influences on the results. All the following experiments adopt ResNet-101 as the backbone, trained on the fine-annotated training set of Cityscapes for ResNet-101 47.8 CCL <ref type="bibr" target="#b10">[11]</ref> ResNet-101 51.6 EncNet <ref type="bibr" target="#b42">[43]</ref> ResNet-101 51.7 Ours</p><p>ResNet-101 52.8   <ref type="figure" target="#fig_13">(Conv(Stage4)</ref>)) to Baseline model, we also achieve a good improvement compared to the Baseline (75.8% → 76.5%). This phenomenon verifies the usefulness of the strategy that fuses the features from the last two stages. Replacing the common fusion module with our proposed Fusion Non-local Block (+ FNB), the performance is further boosted at 0.8% (76.5% → 77.3%). Likewise, changing FNB to AFNB (+ AFNB) reduces the computation considerably at the cost of a minor performance decrease (77.3% → 77.1%).</p><p>To study whether the fusion strategy could further boost the highly competitive + NB model, we add Common fusion to + NB model (+ Common fusion + NB) and achieve 0.6% performance improvement (78.4% → 79.0%). Using both the fusion non-local block and typical non-local block (+ FNB + NB) can improve the performance of 79.7%. Using the combination of APNB and AFNB, namely our asymmetric non-local neural network (+ AFNB + APNB (Full) in <ref type="figure" target="#fig_9">Fig. 3</ref>), achieves the best performance of 79.9%, demonstrating the efficacy of APNB and AFNB.</p><p>Selection of Sampling Methods. As discussed in Sec. 3.2, the selection of the sampling module has a great impact on the performance of APNB. Normal sampling strategies include: max, average and random. When integrated into spatial pyramid pooling, there goes another three strategies: pyramid max, pyramid average and pyramid random. We thereupon conduct several experiments to study their effects by combining them with APNB. As shown in Tab. 7, average sampling performs better than max and random sampling, which conforms to the conclusion drawn in <ref type="bibr" target="#b46">[47]</ref>. We reckon it is because the resulted sampling points are more informative by receiving the provided information of all the input locations inside the average sampling kernel, when compared to the other two. This explanation could also be transferred to pyramid settings. Comparing average sampling and pyramid sampling under the same number of anchor points (third row vs. the last row), we can surely find pyramid pooling is a very key factor that contributes to the significant performance boost.  <ref type="table">Table 7</ref>: Ablation study on the validation set of Cityscapes in terms of sampling methods and anchor point number. "n" column represents the output width/height of a pooling layer. Note when implementing random and pyramid random, we use the numpy.random.choice function to randomly sample n 2 anchor points from all possible locations. "S" column means the total number of the anchor points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an asymmetric non-local neural network for semantic segmentation. The core contribution of asymmetric non-local neural network is the asymmetric pyramid non-local block, which can dramatically improve the efficiency and decrease the memory consumption of non-local neural blocks without sacrificing the performance. Besides, we also propose asymmetric fusion non-local block to fuse features of different levels. The asymmetric fusion non-local block can explore the long range spatial relevance among features of different levels, which demonstrates a considerable performance improvement over a strong baseline. Comprehensive experimental results on the Cityscapes, ADE20K and PASCAL Context datasets show that our work achieves the new state-of-theart performance. In the future, we will apply asymmetric non-local neural networks to other vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative comparisons on COCO-Stuff-10K and NYUD-V2</head><p>Following the training and evaluation protocols of DeepLab-V2 <ref type="bibr" target="#b5">[6]</ref> and RefineNet <ref type="bibr" target="#b17">[18]</ref>, our method achieves competitive results on the two datasets using single scale whole image testing, as shown in Tab. 8. As NYUD-V2 <ref type="bibr" target="#b28">[29]</ref> is a small benchmark and COCO-Stuff-10K is quite challenging and large, these results further verify the effectiveness of our method on both small and large benchmarks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More ablation results</head><p>Qualitative comparisons. We also give the qualitative comparisons of our Full (+ AFNB + APNB) method with other variants of our model in <ref type="figure">Fig. 6</ref>. In summary, our Full method shows the best semantic consistency and the least inconsistency artifacts while +AFNB and +APNB fail in some cases. The results also indicate AFNB and APNB is complementary to each other and the combination of them is beneficial to improve the performance.</p><p>Selection of the fusing layers. Fusing features from multilevels is effective in many computer vision tasks. However, it still requires a lot of trials to find a good combination of the fusing layers. For semantic segmentation, the last several layers of the network contain plenty of features with semantic information, which is critical for better performance. Hence, we only combine the features from the shallow layers to the deep layers in a top-down manner. The responses are summed up if a certain layer receives more than two fusion invitations. The results are listed in Tab. 9. An obvious conclusion is that fusing only the features of Stage4 and Stage5 brings a considerable improvement while keep fusing more layers will only hurt the performance. We conclude a possible intuitive reason: features from the early stages are generic to most tasks, while the last two are task-specific. Therefore, merging the features of the last several stages is more effective for a specific task. In the supplementary materials, we compare the feature visualizations of the outputs of all 5 stages of network trained on segmentation benchmark Cityscapes <ref type="bibr" target="#b8">[9]</ref> and of that trained on classification benchmark ImageNet <ref type="bibr" target="#b9">[10]</ref>, using the same backbone network ResNet-101 to demonstrate our guess.</p><p>As can be seen in <ref type="figure">Fig. 7</ref>, we compare the feature visualizations of the outputs of all 5 stages of network trained on segmentation benchmark Cityscapes <ref type="bibr" target="#b8">[9]</ref> (Lower) and of that trained on classification benchmark ImageNet <ref type="bibr" target="#b9">[10]</ref> (Upper), using the same backbone network ResNet-101.</p><p>Comparing the two networks' feature visualizations of the same stage, the features are quite similar in the first three stages while differs hugely in the last two. This observation accord with our guess. Note our experiment results partially conforms to the conclusions in ExFuse <ref type="bibr" target="#b45">[46]</ref>, further validating the effectiveness of fusing only the last two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussions</head><p>We've conducted extensive experiments to summarize some experiences about semantic segmentation. As known, deep networks are fantastic tools but also need very careful tuning. The tuning process is extremely painful when it comes to semantic segmentation as there are many factors and the training process is very time and resources consuming. So we plan to share some experiences to the interested readers as references. But please do note these experiences are not as thorough as those in the main draft and it's very possible that some may not suit your cases.</p><p>• We found the types of graphic cards, pytorch versions, CUDA versions and NVIDIA driver versions may influence the performances. In our case, we found the same model trained on a workstation with 8 × Titan V is around 0.5 mIoU better than it trained on another machine with 8 × Titan Xp.</p><p>• The architecture of AFNB needs careful modifications to make AFNB work better. For example, we found adding a batch normalization layer after W o helps a lot when using AFNB alone while may not help on the Full model. We suspect there is a theoretical reason here and we're working to solve this issue. However, we found APNB is quite stable across a variaty of architectures.</p><p>• It seems that learning rate is important for segmentation models because we observe that the performance is boosted tremendously in the last 1/4 of training iterations. We also found that changing the initial learning rate has a great impact on the performance. We strongly suggest those, who are new to this task, refer to the learning rate configurations of the published methods that are mostly similar to your own method in architecture.   • Using larger training iterations while keeping other configurations identical doesn't always improve the performance. In fact, changing training iterations is related to change the learning rate during training as we mainly adopt poly learning decay method in semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 4 6 a I 1 t z Z q B z 9 o 5 c 5 Y y R 5 j d R S 0 V w = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 C A l 8 a L H g h e P L Z i 2 0 I a y 2 U 7 a t Z t N 2 N 0 I J f Q X e P G g i F d / k j f / j d s 2 B 2 1 9 Y e H h n R l 2 5 g 1 T w b V x 3 W + n t L G 5 t b 1 T 3 q 3 s 7 R 8 c H l W P T 9 o 6 y R R D n y U i U d 2 Q a h R c o m + 4 E d h N F d I 4 F N g J J 3 f z e u c J l e a J f D D T F I O Y j i S P O K P G W q 3 u o F p z 6 + 5 C Z B 2 8 A m p Q q D m o f v W H C c t i l I Y J q n X P c 1 M T 5 F Q Z z g T O K v 1 M Y 0 r Z h I 6 w Z 1 H S G H W Q L x a d k Q v r D E m U K P u k I Q v 3 9 0 R O Y 6 2 n c W g 7 Y 2 r G e r U 2 N / + r 9 T I T 3 Q Y 5 l 2 l m U L L l R 1 E m i E n I / G o y 5 A q Z E V M L l C l u d y V s T B V l x m Z T s S F 4 q y e v Q / u 6 7 l l u u b X G V R F H G c 7 g H C 7 B g x t o w D 0 0 w Q c G C M / w C m / O o / P i v D s f y 9 a S U 8 y c w h 8 5 n z + t 3 Y z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / f Q w N v q / s n N 9 8 N U w t 7 6 X Y 3 X p c + A = " &gt; A A A C B H i c d Z B N S 8 N A E I Y 3 9 a v W r 6 j H X h a L 0 F N J t G 3 q r d B L j x X s B z S l b L a b d u l m E 3 Y 3 Q g k 9 e P G v e P G g i F d / h D f / j Z s 2 g o o O D D y 8 M 8 P M v F 7 E q F S W 9 W H k N j a 3 t n f y u 4 W 9 / Y P D I / P 4 p C f D W G D S x S E L x c B D k j D K S V d R x c g g E g Q F H i N 9 b 9 5 K 6 / 1 bI i Q N + Y 1 a R G Q U o C m n P s V I a W l s F t 0 A q Z n n J y 1 X 0 Y B I 2 I Y Z 9 J d j s 2 R V a k 7 j s u Z A D Y 1 q t V b X U L d s 5 / I K 2 h V r F S W Q R W d s v r u T E M c B 4 Q o z J O X Q t i I 1 S p B Q F D O y L L i x J B H C c z Q l Q 4 0 c 6 T 2 j Z P X E E p 5 r Z Q L 9 U O j k C q 7 U 7 x M J C q R c B J 7 u T E + W v 2 u p + F d t G C u / M U o o j 2 J F O F 4 v 8 m M G V Q h T R + C E C o I V W 2 h A W F B 9 K 8 Q z J B B W 2 r e C N u H r U / g / 9 C 4 q t u Z r q 9 Q s Z 3 b k Q R G c g T K w g Q O a o A 0 6 o A s w u A M P 4 A k 8 G / f G o / F i v K 5 b c 0 Y 2 c w p + h P H 2 C Z 3 8 l / w = &lt; / la t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / f Q w N v q / s n N 9 8 N U w t 7 6 X Y 3 X p c + A = " &gt; A A A C B H i c d Z B N S 8 N A E I Y 3 9 a v W r 6 j H X h a L 0 F N J t G 3 q r d B L j x X s B z S l b L a b d u l m E 3 Y 3 Q g k 9 e P G v e P G g i F d / h D f / j Z s 2 g o o O D D y 8 M 8 P M v F 7 E q F S W 9 W H k N j a 3 t n f y u 4 W 9 / Y P D I / P 4 p C f D W G D S x S E L x c B D k j D K S V d R x c g g E g Q F H i N 9 b 9 5 K 6 / 1 b I i Q N + Y 1 a R G Q U o C m n P s V I a W l s F t 0 A q Z n n J y 1 X 0 Y B I 2 I Y Z 9 J d j s 2 R V a k 7 j s u Z A D Y 1 q t V b X U L d s 5 / I K 2 h V r F S W Q R W d s v r u T E M c B 4 Q o z J O X Q t i I 1 S p B Q F D O y L L i x J B H C c z Q l Q 4 0 c 6 T 2 j Z P X E E p 5 r Z Q L 9 U O j k C q 7 U 7 x M J C q R c B J 7 u T E + W v 2 u p + F d t G C u / M U o o j 2 J F O F 4 v 8 m M G V Q h T R + C E C o I V W 2 h A W F B 9 K 8 Q z J B B W 2 r e C N u H r U / g / 9 C 4 q t u Z r q 9 Q s Z 3 b k Q R G c g T K w g Q O a o A 0 6 o A s w u A M P 4 A k 8 G / f G o / F i v K 5 b c 0 Y 2 c w p + h P H 2 C Z 3 8 l / w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / f Q w N v q / s n N 9 8 N U w t 7 6 X Y 3 X p c + A = " &gt; A A A C B H i c d Z B N S 8 N A E I Y 3 9 a v W r 6 j H X h a L 0 F N J t G 3 q r d B L j x X s B z S l b L a b d u l m E 3 Y 3 Q g k 9 e P G v e P G g i F d / h D f / j Z s 2 g o o O D D y 8 M 8 P M v F 7 E q F S W 9 W H k N j a 3 t n f y u 4 W 9 / Y P D I / P 4 p C f D W G D S x S E L x c B D k j D K S V d R x c g g E g Q F H i N 9 b 9 5 K 6 / 1 b I i Q N + Y 1 a R G Q U o C m n P s V I a W l s F t 0 A q Z n n J y 1 X 0 Y B I 2 I Y Z 9 J d j s 2 R V a k 7 j s u Z A D Y 1 q t V b X U L d s 5 / I K 2 h V r F S W Q R W d s v r u T E M c B 4 Q o z J O X Q t i I 1 S p B Q F D O y L L i x J B H C c z Q l Q 4 0 c 6 T 2 j Z P X E E p 5 r Z Q L 9 U O j k C q 7 U 7 x M J C q R c B J 7 u T E + W v 2 u p + F d t G C u / M U o o j 2 J F O F 4 v 8 m M G V Q h T R + C E C o I V W 2 h A W F B 9 K 8 Q z J B B W 2 r e C N u H r U / g / 9 C 4 q t u Z r q 9 Q s Z 3 b k Q R G c g T K w g Q O a o A 0 6 o A s w u A M P 4 A k 8 G / f G o / F i v K 5 b c 0 Y 2 c w p + h P H 2 C Z 3 8 l / w = &lt; / l a t e x i t &gt; Query Key Value &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B u o 8 w f 9 B h Y B T l J b d a N Q P K S b O C E o = " &gt; A A A B / H i c d V D L S s N A F J 3 U V 6 2 v a J d u B o v Q V U n s I 3 V X 6 M a V V L A P a E q Z T C f t 0 M k k z E y E E O q v u H G h i F s / x J 1 / 4 6 S t o K I H B g 7 n 3 M s 9 c 7 y I U a k s 6 8 P I b W x u b e / k d w t 7 + w e H R + b x S U + G s c C k i 0 M W i o G H J G G U k 6 6 i i p F B J A g K P E b 6 3 r y d + f 0 7 I i Q N + a 1 K I j I K 0 J R T n 2 K k t D Q 2 i 2 6 A 1 M z z 0 2 v o K h o Q C d u L s V m y K n W n W a 0 7 U J N m r V Z v a N K w b K d 6 C e 2 K t U Q J r N E Z m + / u J M R x Q L j C D E k 5 t K 1 I j V I k F M W M L A p u L E m E 8 B x N y V B T j v S d U b o M v 4 D n W p l A P x T 6 c Q W X 6 v e N F A V S J o G n J 7 O o 8 r e X i X 9 5 w 1 j 5 z V F K e R Q r w v H q k B 8 z q E K Y N Q E n V B C s W K I J w o L q r B D P k E B Y 6 b 4 K u o S v n 8 L / S e + i Y m t + Y 5 V a 5 X U d e X A K z k A Z 2 M A B L X A F O q A L M E j A A 3 g C z 8 a 9 8 W i 8 G K + r 0 Z y x 3 i m C H z D e P g G 9 w p S 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B u o 8 w f 9 B h Y B T l J b d a N Q P K S b O C E o = " &gt; A A A B / H i c d V D L S s N A F J 3 U V 6 2 v a J d u B o v Q V U n s I 3 V X 6 M a V V L A P a E q Z T C f t 0 M k k z E y E E O q v u H G h i F s / x J 1 / 4 6 S t o K I H B g 7 n 3 M s 9 c 7 y I U a k s 6 8 P I b W x u b e / k d w t 7 + w e H R + b x S U + G s c C k i 0 M W i o G H J G G U k 6 6 i i p F B J A g K P E b 6 3 r y d + f 0 7 I i Q N + a 1 K I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>z 8 a 9 8 W i 8 G K + r 0 Z y x 3 i m C H z D e P g G 9 w p S 9 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o u 9 2 + + F J 4 x y / 6 x X G 4 i j q w m J Z 9 p Y = " &gt; A A A C D n i c d V D L S g M x F M 3 U V 6 2 v U Z d u g k X o q s z Y 1 t a d 0 I 1 L R a t C W 0 o m v d O G Z h 4 k d 8 Q y z B e 4 8 V f c u F D E r W t 3 / o 1 p r a C i B 0 I O 5 9 y b e 3 O 8 W A q N j v N u 5 e b m F x a X 8 s u F l d W 1 9 Q 1 7 c + t C R 4 n i 0 O K R j N S V x z R I E U I L B U q 4 i h W w w J N w 6 Y 2 a E / / y G p Q W U X i O 4 x i 6 A R u E w h e c o Z F 6 9 l 4 n Y D j 0 / L R J O y g C 0 O a C G 5 w + n H o y g S w 9 y 7 K e X X T K t X q j U q t T Q x r V a u 3 A k A P H r V c O q V t 2 p i i S G U 5 6 9 l u n H / E k g B C 5 Z F q 3 X S f G b s o U C i 4 h K 3 Q S D T H j I z a A t q E h M 5 O 7 6 X R q R v e M 0 q d + p M w J k U 7 V 7 x 0 p C 7 Q e B 5 6 p n C y v f 3 s T 8 S + v n a D f 6 K Y i j B O E k H 8 O 8 h N J M a K T b G h f K O A o x 4 Y w r o T Z l f I h U 4 y j S b B g Q v j 6 K f 2 f X O y X X c N P n e J R a R Z H n u y Q X V I i L q m T I 3 J M T k i L c H J L 7 s k j e b L u r A f r 2 X r 5 L M 1 Z s 5 5 t 8 g P W 6 w c J A p 1 O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a i 5 U k j + j / G g 8 Y V k A y r v 2 V y 4 3 v 1 k = " &gt; A A A C D n i c d V C 7 S g N B F J 3 1 b X x F L W 0 G Q y B V 2 N W 8 7 A I 2 l o r m A d k Q Z i d 3 4 + D s g 5 m 7 Y l j 2 C 2 z 8 F R s L R W y t 7 f w b J z G C i h 4 Y O J z 7 O H e O F 0 u h 0 b b f r b n 5 h c W l 5 Z X V 3 N r 6 x u Z W f n u n r a N E c W j x S E a q 6 z E N U o T Q Q o E S u r E C F n g S O t 7 V 8 a T e u Q a l R R R e 4 D i G f s B G o f A F Z 2 i k Q b 7 o B g w v P T 9 1 E W 5 w u i / 1 Z A J Z e p 5 R F 0 U A m h 5 n g 3 z B L l f r j c N q n R r S q F S q N U N q t l M / P K J O 2 Z 6 i Q G Y 4 H e T f 3 G H E k w B C 5 J J p 3 X P s G P s p U y i 4 h C z n J h p i x q / Y C H q G h s z 4 9 N O p f U a L R h l S P 1 L m h U i n 6 v e J l A V a j w P P d E 6 O 1 7 9 r E / G v W i 9 B v 9 F P R R g n C C H / N P I T S T G i k 2 z o U C j g K M e G M K 6 E u Z X y S 6 Y Y R 5 N g z o T w 9 V P 6 P 2 k f l B 3 D z + x C s z S L Y 4 X s k X 1 S I g 6 p k y Y 5 I a e k R T i 5 J f f k k T x Z d 9 a D 9 W y 9 f L b O W b O Z X f I D 1 u s H G h i d T g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + s 9 S 2 r K 5 z C h V 1 Z 0 M K a 9 b r 9 A U o C 0 = " &gt; A A A C D n i c d V D L S g M x F M 3 4 r P U 1 6 t J N s A i u y o z 2 5 a 7 g x p V U t K 3 Q l p J J 7 2 g w 8 y C 5 I 5 Z h v s C N v + L G h S J u X b v z b 0 w f g o o e C D m c c 2 / u z f F i K T Q 6 z o c 1 M z s 3 v 7 C Y W 8 o v r 6 y u r d s b m y 0 d J Y p D k 0 c y U h c e 0 y B FC E 0 U K O E i V s A C T 0 L b u z 4 a + e 0 b U F p E 4 T k O Y + g F 7 D I U v u A M j d S 3 d 7 s B w y v P T 0 9 o F 0 U A 2 l x w i + O H U 0 8 m k K V n W d a 3 C 0 6 x X K 0 d l K v U k F q p V K 4 Y U n H c 6 s E h d Y v O G A U y R a N v v 3 c H E U 8 C C J F L p n X H d W L s p U y h 4 B K y f D f R E D N + z S 6 h Y 2 j I z O R e O p 6 a 0 V 2 j D K g f K X N C p G P 1 e 0 f K A q 2 H g W c q R 8 vr 3 9 5 I / M v r J O j X e q k I 4 w Q h 5 J N B f i I p R n S U D R 0 I B R z l 0 B D G l T C 7 U n 7 F F O N o E s y b E L 5 + S v 8 n r f 2 i a / i p U 6 j v T e P I k W 2 y Q / a I S 6 q k T o 5 J g z Q J J 3 f k g T y R Z + v e e r R e r N d J 6 Y w 1 7 d k i P 2 C 9 f Q I a 4 p 1 Z &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q 3 / c c m i f D D / E S g V O B z l E M k s v f q o = " &gt; A A A B 7 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I 3 u i y 4 c V n B P q A N Z T K 9 a Y d O J n H m R i i h P + H G h S J u / R 1 3 / o 3 T N g t t P T B w O O d c 5 t 4 T p l I Y 8 r x v p 7 S x u b W 9 U 9 6 t 7 O 0 f H B 5 V j 0 / a J s k 0 x x Z P Z K K 7 I T M o h c I W C Z L Y T T W y O J T Y C S e 3 c 7 / z h N q I R D 3 Q N M U g Z i M l I s E Z W a n b p z E S G z Q H 1 Z p X 9 x Z w 1 4 l f k B o U s P m v / j D h W Y y K u G T G 9 H w v p S B n m g S X O K v 0 M 4 M p 4 x M 2 w p 6 l i s V o g n y x 7 8 y 9 s M r Q j R J t n y J 3 o f 6 e y F l s z D Q O b T J m N D a r 3 l z 8 z + t l F N 0 E u V B p R q j 4 8 q M o k y 4 l 7 v x 4 d y g 0 c p J T S x j X w u 7 q 8 j H T j J O t q G J L 8 F d P X i f t q 7 p v + b 1 X a 3 h F H W U 4 g 3 O 4 B B + u o Q F 3 0 I Q W c J D w D K / w 5 j w 6 L 8 6 7 8 7 G M l p x i 5 h T + w P n 8 A f W 7 j 9 U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E W s 1 3 f J h g a b e D s p D c + j 3 8 i u T J U Q = " &gt; A A A B 7 3 i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s y 4 0 W X B j c s K 9 g H t U O 6 k m T Y 0 y Y x J R i h D f 8 K N C 0 X c + j v u / B v T d h b a e i B w O O d c c u + J U s G N 9 f 1 v r 7 S x u b W 9 U 9 6 t 7 O 0 f H B 5 V j 0 / a J s k 0 Z S 2 a i E R 3 I z R M c M V a l l v B u q l m K C P B O t H k d u 5 3 n p g 2 P F E P d p q y U O J I 8 Z h T t E 7 q 9 k c o J Q 6 a g 2 r N r / s L k H U S F K Q G B V z + q z 9 M a C a Z s l S g M b 3 A T 2 2 Y o 7 a c C j a r 9 D P D U q Q T H L G e o w o l M 2 G + 2 H d G L p w y J H G i 3 V O W L N T f E z l K Y 6 Y y c k m J d m x W v b n 4 n 9 f L b H w T 5 l y l m W W K L j + K M 0 F s Q u b H k y H X j F o x d Q S p 5 m 5 X Q s e o k V p X U c W V E K y e v E 7 a V / X A 8 X u / 1 v C L O s p w B u d w C Q F c Q w P u o A k t o C D g G V 7 h z X v 0 X r x 3 7 2 M Z L X n F z C n 8 g f f 5 A 9 i J j 8 I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y T 3 n r J P / 0 8 m J C 0 L f c R 9 + L p e R m 5 8 = " &gt; A A A B 8 H i c b V D L S g M x F L 3 j s 9 Z X 1 a W b Y B F c l R k 3 u i y 4 c V n B P q Q d S i a 9 0 4 Y m m S H J F M r Q r 3 D j Q h G 3 f o 4 7 / 8 a 0 n Y W 2 H g g c z j m X 3 H u i V H B j f f / b 2 9 j c 2 t 7 Z L e 2 V 9 w 8 O j 4 4 r J 6 c t k 2 S a Y Z M l I t G d i B o U X G H T c i u w k 2 q k M h L Y j s Z 3 c 7 8 9 Q W 1 4 o h 7 t N M V Q 0 q H i M W f U O u m p N 0 G W t 2 b 9 R r 9 S 9 W v + A m S d B A W p Q g G X / + o N E p Z J V J Y J a k w 3 8 F M b 5 l R b z g T O y r 3 M Y E r Z m A 6 x 6 6 i i E k 2 Y L x a e k U u n D E i c a P e U J Q v 1 9 0 R O p T F T G b m k p H Z k V r 2 5 + J / X z W x 8 G + Z c p Z l F x Z Y f x Z k g N i H z 6 8 m A a 2 R W T B 2 h T H O 3 K 2 E j q i m z r q O y K y F Y P X m d t K 5 r g e M P f r X u F 3 W U 4 B w u 4 A o C u I E 6 3 E M D m s B A w j O 8 w p u n v R f v 3 f t Y R j e 8 Y u Y M / s D 7 / A H Q 7 Z B V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q Y q W m p D u b q X c 9 i T b u 9 a g j o Z Z Q f I = " &gt; A A A B 7 n i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 F S y X v R Y 8 O K x g v 2 A d i n Z d L Y N z W a X J F s o S 3 + E F w + K e P X 3 e P P f m L Z 7 0 N Y X A g / v z J C Z N 0 y l M J b S b 6 + 0 t b 2 z u 1 f e r x w c H h 2 f V E / P 2 i b J N M c W T 2 S i u y E z K I X C l h V W Y j f V y O J Q Y i e c 3 C / q n S l q I x L 1 Z G c p B j E b K R E J z q y z O v 0 p 8 r w 9 H 1 R r t E 6 X I p v g F 1 C D Q s 1 B 9 a s / T H g W o 7 J c M m N 6 P k 1 t k D N t B Z c 4 r / Q z g y n j E z b C n k P F Y j R B v l x 3 T q 6 c M y R R o t 1 T l i z d 3 x M 5 i 4 2 Z x a H r j J k d m / X a w v y v 1 s t s d B f k Q q W Z R c V X H 0 W Z J D Y h i 9 v J U G j k V s 4 c M K 6 F 2 5 X w M d O M W 5 d Q x Y X g r 5 + 8 C e 2 b u u / 4 k d Y a t I i j D B d w C d f g w y 0 0 4 A G a 0 A I O E 3 i G V 3 j z U u / F e / c + V q 0 l r 5 g 5 h z / y P n 8 A d 5 G P k g = = &lt; / l a t e x i t &gt; Sample Sample (a) Non-local Block (b) Asymmetric Non-local Block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of a standard non-local block (a) and the asymmetric non-local block (b). N = H · W while S N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>GPU time ( ≥ 1 ms) comparison of different operations between a generic non-local block and our APNB. The last bin denotes the sum of all the time costs. The size of the inputs for these two blocks is 256 × 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " 0 8 D 4 T l l T t q L m 1 J v G W k k h 5 x 9 A l 5 c = " &gt; A A A B 6 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g Q k r i R p c F N + 6 s a B / Q h j K Z T t q h k 0 m Y u R F K 6 C e 4 c a G I W 7 / I n X / j N M 1 C W w 8 M H M 4 5 l 7 n 3 B I k U B l 3 3 2 y m t r W 9 s b p W 3 K z u 7 e / s H 1 c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>0 r 6 s e 5 b f u 7 X G R V F H G U 7 g F M 7 B g y t o w C 0 0 o Q U M R v A M r / D m S O f F e X c + F t G S U 8 w c w x 8 4 n z / 0 S 4 1 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I 4 f o L d O 7 9 P R Q I 0 D + a 3 R n A n Z T 1 H g = " &gt; A A A B 6 n i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 C B l 1 4 s e C 1 4 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " G T y 0 u + m p a I v 3 k s o 5 b + I D n J R d 9 F Q = " &gt; A A A B 6 n i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 C B l 1 4 s e C 1 4 8 V r S 1 0 C 4 l m 8 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>M 7 z C m y e 8 F + / d + 1 i 0 l r x i 5 h j + y P v 8 A S Z w j Z 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c x x i 7 a 9 w c Y m I P K B z u S d a b g v 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>h B M 4 h X P w o A Z 1 u I U G N I H B E J 7 h F d 4 c 6 b w 4 7 8 7 H o r X g 5 D P H 8 E f O 5 w / l I 4 1 y &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " h g 8 V u r t e c Q J G a n Z z o d a 3 o F u p L x g = " &gt; A A A B 6 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 C A l 8 V K P B U E 8 V r Q f 0 o a y 2 U 7 a p Z t N 2 N 0 I J f Q n e P G g i F d / k T f / j d s 2 B 2 1 9 Y e H h n R l 2 5 g 0 S w b V x 3 W + n s L a + s b l V 3 C 7 t 7 O 7 t H 5 Q P j 1 o 6 T h X D J o t F r D o B 1 S i 4 x K b h R m A n U U i j Q G A 7 G F / P 6 u 0 n V J r H 8 s F M E v Q j O p Q 8 5 I w a a 9 0 / 9 m / 6 5 Y p b d e c i q + D l U I F c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d i 5 J G q P 1 s v u q U n F l n Q M J Y 2 S c N m b u / J z I a a T 2 J A t s Z U T P S y 7 W Z + V + t m 5 r w y s + 4 T F K D k i 0 + C l N B T E x m d 5 M B V 8 i M m F i g T H G 7 K 2 E j q i g z N p 2 S D c F b P n k V W p d V z / K d W 6 l f 5 H E U 4 Q R O 4 R w 8 q E E d b q E B T W A w h G d 4 h T d H O C / O u / O x a C 0 4 + c w x / J H z + Q P 0 X 4 1 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h g 8 V u r t e c Q J G a n Z z o d a 3 o F u p L x g = " &gt; A A A B 6 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 C A l 8 V K P B U E 8 V r Q f 0 o a y 2 U 7 a p Z t N 2 N 0 I J f Q n e P G g i F d / k T f / j d s 2 B 2 1 9 Y e H h n R l 2 5 g 0 S w b V x 3 W + n s L a + s b l V 3 C 7 t 7 O 7 t H 5 Q P j 1 o 6 T h X D J o t F r D o B 1 S i 4 x K b h R m A n U U i j Q G A 7 G F / P 6 u 0 n V J r H 8 s F M E v Q j O p Q 8 5 I w a a 9 0 / 9 m / 6 5 Y p b d e c i q + D l U I F c j X 7 5 q z e I W R q h N E x Q r b u e m x g / o 8 p w J n B a 6 q U a E 8 r G d I h d i 5 J G q P 1 s v u q U n F l n Q M J Y 2 S c N m b u / J z I a a T 2 J A t s Z U T P S y 7 W Z + V + t m 5 r w y s + 4 T F K D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>x a C 0 4 + c w x / J H z + Q P 0 X 4 1 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g a C 9 S C o w O 3 7 6 p D 1 E O V 4 6 4 0 Z k q D 8 = " &gt; A A A B 6 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g Q k r i R p c F N y 4 r 2 o e 0 o U y m N + 3 Q y S T M T I Q Q + g l u X C j i 1 i 9 y 5 9 8 4 b b P Q 1 g M D h 3 P O Z e 4 9 Q S K 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R W 8 e p Y t h i s Y h V N 6 A a B Z f Y M t w I 7 C Y K a R Q I 7 A S T m 5 n f e U K l e S w f T J a g H 9 G R 5 C F n 1 F j p / n H Q H F R r b t 2 d g 6 w S r y A 1 K G D z X / 1 h z N I I p W G C a t 3 z 3 M T 4 O V W G M 4 H T S j / V m F A 2 o S P s W S p p h N r P 5 6 t O y Z l V h i S M l X 3 S k L n 6 e y K n k d Z Z F N h k R M 1 Y L 3 s z 8 T + v l 5 r w 2 s + 5 T F K D k i 0 + C l N B T E x m d 5 M h V 8 i M y C y h T H G 7 K 2 F j q i g z t p 2 K L c F b P n m V t C / r n u V 3 b q 1 x U d R R h h M 4 h X P w 4 A o a c A t N a A G D E T z D K 7 w 5 w n l x 3 p 2 P R b T k F D P H 8 A f O 5 w 8 D l o 2 G &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I 4 f o L d O 7 9 P R Q I 0 D + a 3 R n A n Z T 1 H g = " &gt; A A A B 6 n i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 C B l 1 4 s e C 1 4 8 V r S 1 0 C 4 l m 2 b b 0 G y y J L N C W f o T v H h Q x K u / y J v / x r T d g 7 a + E H h 4 Z 4 b M v F E q h U X f / / Z K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w q G 1 1 Z h h v M S 2 1 6 U T U c i k U b 6 F A y T u p 4 T S J J H + M x j e z + u M T N 1 Z o 9 Y C T l I c J H S o R C 0 b R W f e d v u x X a 3 7 d n 4 u s Q l B A D Q o 1 + 9 W v 3 k C z L O E K m a T W d g M / x T C n B g W T f F r p Z Z a n l I 3 p k H c d K p p w G + b z V a f k z D k D E m v j n k I y d 3 9 P 5 D S x d p J E r j O h O L L L t Z n 5 X 6 2 b Y X w d 5 k K l G X L F F h / F m S S o y e x u M h C G M 5 Q T B 5 Q Z 4 X Y l b E Q N Z e j S q b g Q g u W T V 6 F 9 W Q 8 c 3 / m 1 x k U R R x l O 4 B T O I Y A r a M A t N K E F D I b w D K / w 5 k n v x X v 3 P h a t J a + Y O Y Y / 8 j 5 / A C y A j a E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G T y 0 u + m p a I v 3 k s o 5 b + I D n J R d 9 F Q = " &gt; A A A B 6 n i c b Z B N S w M x E I Z n 6 1 e t X 1 W P X o J F 8 C B l 1 4 s e C 1 4 8 V r S 1 0 C 4 l m 8 6 2o U l 2 S b J C W f o T v H h Q x K u / y J v / x r T d g 7 a + E H h 4 Z 4 b M v F E q u L G + / + 2 V 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U d s k m W b Y Y o l I d C e i B g V X 2 L L c C u y k G q m M B D 5 G 4 5 t Z / f E J t e G J e r C T F E N J h 4 r H n F H r r P t O f 9 S v 1 v y 6 P x d Z h a C A G h R q 9 q t f v U H C M o n K M k G N 6 Q Z + a s O c a s u Z w G m l l x l M K R v T I X Y d K i r R h P l 8 1 S k 5 c 8 6 A x I l 2 T 1 k y d 3 9 P 5 F Q a M 5 G R 6 5 T U j s x y b W b + V + t m N r 4 O c 6 7 S z K J i i 4 / i T B C b k N n d Z M A 1 M i s m D i j T 3 O 1 K 2 I h q y q x L p + J C C J Z P X o X 2 Z T 1 w f O f X G h d F H G U 4 g V M 4 h w C u oA G 3 0 I Q W M B j C M 7 z C m y e 8 F + / d + 1 i 0 l r x i 5 h j + y P v 8 A S Z w j Z 0 = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " H E W 3 X K / 8 Z q C U W / 0 j J / N X R h v V X x Y = " &gt; A A A B 8 X i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I z G x I r c 2 W h J t L D E R M M K F 7 C 1 7 s G F v 7 7 I 7 Z 0 I u / A s b C 4 2 x 9 d / Y + W 9 c 4 A o F X z L J y 3 s z m Z k X p k p a 9 P 1 v r 7 S 2 v r G 5 V d 6 u 7 O z u 7 R 9 U D 4 / a N s k M F y 2 e q M Q 8 h M w K J b V o o U Q l H l I j W B w q 0 Q n H N z O / 8 y S M l Y m + x 0 k q g p g N t Y w k Z + i k R 0 p 6 K G N h C e 1 X a 3 7 d n 4 O s E l q Q G h R o 9 q t f v U H C s 1 h o 5 I p Z 2 6 V + i k H O D E q u x L T S y 6 x I G R + z o e g 6 q p l b E + T z i 6 f k z C k D E i X G l U Y y V 3 9 P 5 C y 2 d h K H r j N m O L L L 3 k z 8 z + t m G F 0 F u d R p h k L z x a I o U w Q T M n u f D K Q R H N X E E c a N d L c S P m K G c X Q h V V w I d P n l V d K + q F P H 7 / x a 4 7 q I o w w n c A r n Q O E S G n A L T W g B B w 3 P 8 A p v n v V e v H f v Y 9 F a 8 o q Z Y / g D 7 / M H R i y P + g = = &lt; / l a t e x i t &gt; Overview of the proposed Asymmetric Non-local Neural Network. In our implementation, the key branch and the value branch in APNB share the same 1 × 1 convolution and sampling module, which decreases the number of parameters and computation without sacrificing the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Motivation and Analysis. By inspecting the general computing flow of a non-local block, one could clearly find that Eq. (2) and Eq. (4) dominate the computation. The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " D E T D l N c N E V 2 k x s W X t G c p q A 9 B i f Q = " &gt; A A A B 6 H i c b Z D L S g M x F I b P 1 F u t t 6 p L N 8 E i u C o z b n R Z c O O y B X u B d i i Z 9 E w b m 2 S G J C O U o U / g x o U i b n 0 S n 8 G d b 2 N 6 W W j r D 4 G P / 5 x w z v m j V H B j f f / b K 2 x s b m 3 v F H d L e / s H h 0 f l 4 5 O W S T L N s M k S k e h O R A 0 K r r B p u R X Y S T V S G Q l s R + P b W b 3 9 i N r w R N 3 b S Y q h p E P F Y 8 6 o d V a j 0 y 9 X/ K o / F 1 m H Y A k V W K r e L 3 / 1 B g n L J C r L B D W m G / i p D X O q L W c C p 6 V e Z j C l b E y H 2 H W o q E Q T 5 v N F p + T C O Q M S J 9 o 9 Z c n c / f 0 j p 9 K Y i Y x c p 6 R 2 Z F Z r M / O / W j e z 8 U 2 Y c 5 V m F h V b D I o z Q W x C Z l e T Ad f I r J g 4 o E x z t y t h I 6 o p s y 6 b k g s h W D 1 5 H V p X 1 c B x w 6 / U o s 9 F H E U 4 g 3 O 4 h A C u o Q Z 3 U I c m M E B 4 g h d 4 9 R 6 8 Z + / N e 1 + 0 F r x l h K f w R 9 7 H D / c b j b g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T x / c a O J a t F Q s h 4 x H u Y w O 7 h N l 7 d s = " &gt; A A A B 6 H i c b Z D L S g M x F I b P 1 F u t t 6 p L N 8 E i u C o z b n R Z c O O y B X u B d i i Z 9 E w b m 2 S G J C O U o U / g x o U i b n 0 S n 8 G d b 2 N 6 W W j r D 4 G P / 5 x w z v m j V H B j f f / b K 2 x s b m 3 v F H d L e / s H h 0 f l 4 5 O W S T L N s M k S k e h O R A 0 K r r B p u R X Y S T V S G Q l s R + P b W b 3 9 i N r w R N 3 b S Y q h p E P F Y 8 6 o d V a j 3 S 9 X / K o / F 1 m H Y A k V W K r e L 3 / 1 B g n L J C r L B D W m G / i p D X O q L W c C p 6 V e Z j C l b E y H 2 H W o q E Q T 5 v N F p + T C O Q M S J 9 o 9 Z c n c / f 0 j p 9 K Y i Y x c p 6 R 2 Z F Z r M / O / W j e z 8 U 2 Y c 5 V m F h V b D I o z Q W x C Z l e T A d f I r J g 4 o E x z t y t h I 6 o p s y 6 b k g s h W D 1 5 H V p X 1 c B x w 6 / U o s 9 F H E U 4 g 3 O 4 h A C u o Q Z 3 U I c m M E B 4 g h d 4 9 R 6 8 Z + / N e 1 + 0 F r x l h K f w R 9 7 H D / W X j b c = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>A 3 u / B s 7 j 4 U m F j Q c 7 q 2 m q m 6 U K W n J 9 7 + 9 t f W N z a 3 t w k 5 x d 2 / / 4 L B 0 d N y w a W 4 E 1 k W q U t O K u E U l N d Z J k s J W Z p A n k c J m N L y Z + s 1 H N F a m + p 5 G G Y Y J 7 2 s Z S 8 H J S Q 8 B 6 5 B M 0 L K g W y r 7 F X 9 W b B W C B Z R h U b V u 6 a v T S 0 W e o C a h u L X t w M 8 o H H N D U i i c F D u 5 x Y y L I e 9 j 2 6 H m b k w 4 n m 0 8 Y e d O 6 b E 4 N e 5 p Y j P 1 9 4 8 x T 6 w d J Z H r T D g N 7 L I 3 F f / z 2 j n F 1 + F Y 6 i w n 1 G I + K M 4 V o 5 R N z 2 c 9 a V C Q G j n g w k i 3 K x M D b r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>&lt;Figure 4 :</head><label>4</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " 2 6 f 9 4 f R 2 M A 3 v B N z 0 V 6 7 y z 0 R M V 1 8 = " &gt; A A A B 8 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J U Z N 3 Z Z c O O y g r 1 g O 5 R M m m l D M 5 k h O S O U o W / h x o U i b n 0 J n 8 G d b 2 N 6 W W j r g c D H / 5 9 w z v n D V E m L n v d N C h u b W 9 s 7 x d 3 S 3 v 7 B 4 V H 5 + K R l k 8 x w 0 e S J S k w n Z F Y o q U U T J S r R S Y 1 g c a h E O x z f z P z 2 o z B W J v o e J 6 k I Y j b U M p K c o Z M e a r S H M h a W 1 v r l i l f 1 5 k X X w V 9 C B Z b V 6 J e / e o O E Z 7 H Q y B W z t u t 7 K Q Y 5 M y i 5 E t N S L 7 M i Z X z M h q L r U D M 3 J s j n G 0 / p h V M G N E q M e x r p X P 3 9 I 2 e x t Z M 4 d J 0 x w 5 F d 9 W b i f 1 4 3 w 6 g W 5 F K n G Q r N F 4 O i T F F M 6 O x 8 O p B G c F Q T B 4 w b 6 X a l f M Q M 4 + h C K r k Q / N W T 1 6 F 1 V f U d 3 3 m V e v i 5 i K M I Z 3 A O l + D D N d T h F h r Q B A 4 a n u A F X o k l z + S N v C 9 a C 2 Q Z 4 S n 8 K f L x A 5 5 D k O g = &lt; / l a t e x i t &gt; Demonstration of the pyramid max or average sampling process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparisons with DeepLab-V3<ref type="bibr" target="#b6">[7]</ref> and PSPNet<ref type="bibr" target="#b46">[47]</ref>. The red circles mark where our model is particularly superior to other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Qualitative comparisons among Full model and other variants of our model. The red circles indicate where Full model is superior to other model variants. Feature visualization of different stages on ResNet-101. The Upper row represents visualizations from network trained on classification dataset ImageNet<ref type="bibr" target="#b9">[10]</ref>. The Lower row represents visualizations from network trained on scene segmentation dataset Cityscapes<ref type="bibr" target="#b8">[9]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Matmul Convolution Softmax Batchnorm Pool Total Time (ms) Non-local Ours</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>178.068</cell><cell></cell></row><row><cell>118.772</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.875</cell><cell>15.839</cell><cell>15.936</cell><cell>43.103</cell><cell>0.185</cell><cell>0.355</cell><cell>0.728</cell><cell>0.000</cell><cell>12.174</cell><cell>29.898</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). In our network, non-local block/APNB receives a 96 × 96 (1/8 of the 769 × 769 input image patch) feature map during training while 256×128 (1/8 of the 2048×1024 input image) during</figDesc><table><row><cell>Method</cell><cell>Input size</cell><cell>GFLOPs</cell><cell>GPU memory</cell><cell>GPU time</cell></row><row><cell>NB</cell><cell>96 × 96</cell><cell>58.0</cell><cell>609</cell><cell>19.5</cell></row><row><cell>APNB</cell><cell>96 × 96</cell><cell>15.5 (↓ 42.5)</cell><cell>150 (↓ 459)</cell><cell>12.4 (↓ 7.1)</cell></row><row><cell>NB</cell><cell>256 × 128</cell><cell>601.4</cell><cell>7797</cell><cell>179.4</cell></row><row><cell>APNB</cell><cell>256 × 128</cell><cell>43.5 (↓ 557.9)</cell><cell>277 (↓ 7520)</cell><cell>30.8 (↓ 148.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Computation and memory statistics comparisons between nonlocal block and our APNB. The channel numbers of the input feature maps X is C = 2048 and of the embeddings φ, φ P etc. isĈ = 256, respectively. Batch size is 1. The lower values, the better.</figDesc><table><row><cell>single scale testing for Cityscapes dataset. Hence, we give</cell></row><row><cell>relevant statistics of the two sizes. The testing environment</cell></row><row><cell>is identical for these two blocks, that is, a Titan Xp GPU</cell></row><row><cell>under CUDA 9.0 without other ongoing programs. Note</cell></row><row><cell>our APNB has four extra adaptive average pooling layers to</cell></row><row><cell>count as opposed to the non-local block while other parts</cell></row><row><cell>are entirely identical. The comparison results are given in</cell></row><row><cell>Tab. 2. Our APNB is superior to the non-local block in all</cell></row><row><cell>aspects. Enlarging the input size will give a further edge</cell></row><row><cell>to our APNB because in Eq. (13), N grows quadratically</cell></row><row><cell>while S remains unchanged.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Time, parameter and GPU memory comparisons based on the whole networks. Inf. time, Mem., # Param mean inference time, GPU memory occupation and number of parameters, respectively. Results are averaged from feeding ten 2048 × 1024 images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparisons on the test set of Cityscapes with the state-of-the-art methods. Note that the Val column indicates whether including the finely annotated validation set data of Cityscapes for training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on the validation set of ADE20K with the state-ofthe-art methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparisons on the validation set of PASCAL Context with the state-of-the-art methods.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell></row><row><cell>Baseline</cell><cell>75.8</cell></row><row><cell>+ NB</cell><cell>78.4</cell></row><row><cell>+ APNB</cell><cell>78.6</cell></row><row><cell>+ Common fusion</cell><cell>76.5</cell></row><row><cell>+ FNB</cell><cell>77.3</cell></row><row><cell>+ AFNB</cell><cell>77.1</cell></row><row><cell>+ Common fusion + NB</cell><cell>79.0</cell></row><row><cell>+ FNB + NB</cell><cell>79.7</cell></row><row><cell>+ AFNB + APNB (Full)</cell><cell>79.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the validation set of Cityscapes about APNB and AFNB. "+ M odule" means add "M odule" to the Baseline model.Efficacy of the APNB and AFNB. Our network has two prominent components: APNB and AFNB. The following will evaluate the efficacy of each and the integration of both. The Baseline network is basically a FCN-like ResNet-101 network with a deep supervision branch. By adding a nonlocal block (+ NB) before the classifier to the Baseline model, the performance is improved by 2.6% (75.8% → 78.4%), as shown in Tab. 6. By replacing the normal nonlocal block with our APNB (+ APNB), the performance is slightly better (78.4% → 78.6%).</figDesc><table><row><cell>60K iterations.</cell></row><row><cell>When adding a common fusion module (+ Com-</cell></row><row><cell>mon Fusion) from Stage4 to Stage5: Stage5 +</cell></row><row><cell>ReLU(BatchNorm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Anchor Points Numbers. In our case, the output sizes of the pyramid pooling layers determine the total number of anchor points, which influence the efficacy of APNB. To investigate the influence, we perform the fol-</figDesc><table><row><cell>Sampling method</cell><cell>n</cell><cell>S</cell><cell>mIoU (%)</cell></row><row><cell>random</cell><cell>15</cell><cell>225</cell><cell>78.2</cell></row><row><cell>max</cell><cell>15</cell><cell>225</cell><cell>78.1</cell></row><row><cell>average</cell><cell>15</cell><cell>225</cell><cell>78.4</cell></row><row><cell>pyramid random</cell><cell>1,2,3,6</cell><cell>50</cell><cell>78.8</cell></row><row><cell>pyramid max</cell><cell>1,2,3,6</cell><cell>50</cell><cell>79.1</cell></row><row><cell>pyramid average</cell><cell>1,2,3,6</cell><cell>50</cell><cell>79.3</cell></row><row><cell>pyramid average</cell><cell cols="2">1,3,6,8 110</cell><cell>79.9</cell></row><row><cell>pyramid average</cell><cell cols="2">1,4,8,12 225</cell><cell>80.1</cell></row><row><cell>lowing experiments by altering the pyramid average pool-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ing output sizes: (1, 2, 3, 6), (1, 3, 6, 8) and (1, 4, 8, 12). As</cell><cell></cell><cell></cell><cell></cell></row><row><cell>shown in Tab. 7, it is clear that more anchor points improve</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the performance with the cost of increasing computation.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Considering this trade-off between efficacy and efficiency,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>we opt to choose (1, 3, 6, 8) as our default setting.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparisons on COCO-Stuff-10K (Left) and NYUD-V2 (Right) datasets. Results of the competing methods are taken from their papers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Ablation study on the validation set of Cityscapes in terms of the selection of layers to be fused. "4 &amp; 5" means fusing the features of Stage4 and Stage5. Others likewise.</figDesc><table><row><cell>Images</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by NSFC 61573160, to Dr. Xiang Bai by the National Program for Support of Top-notch Young Professionals and the Program for HUST Academic Frontier Youth Team 2017QYTD08. We sincerely thank Huawei EI Cloud for their generous grant of GPU use for our paper. We genuinely thank Ansheng You for his kind help and suggestions throughout the project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="402" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multiscale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="605" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamicstructured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better. CoRR, abs/1506.04579</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large kernel matters -improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep semantic segmentation for automated driving: Taxonomy, roadmap and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Elkerdawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jägersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ITSC</title>
		<meeting>ITSC</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparative study of real-time semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moemen</forename><surname>Abdel-Razek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jägersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gaussian conditional random field network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3224" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1611.10080</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="432" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Torchcv: A pytorch-based framework for deep learning in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<ptr target="https://github.com/donnyyou/torchcv,2019.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing. CoRR, abs/1809.00916</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2050" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Psanet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Similarity fusion for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longin Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="363" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Prior-aware neural network for partially-supervised multi-organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semi-supervised 3d abdominal multi-organ segmentation via deep multi-planar cotraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="121" to="140" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
