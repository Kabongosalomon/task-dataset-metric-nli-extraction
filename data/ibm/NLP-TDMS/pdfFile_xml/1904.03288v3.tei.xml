<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jasper: An End-to-End Convolutional Neural Acoustic Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Lavrukhin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Leary</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huyen</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Teja</forename><surname>Gadde</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jasper: An End-to-End Convolutional Neural Acoustic Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech recognition</term>
					<term>convolutional networks</term>
					<term>time-delay neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95% WER using a beam-search decoder with an external neural language model and 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on Wall Street Journal and the Hub5'00 conversational evaluation datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Conventional automatic speech recognition (ASR) systems typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Newer approaches such as end-to-end (E2E) systems reduce the overall complexity of the final system.</p><p>Our research builds on prior work that has explored using time-delay neural networks (TDNN), other forms of convolutional neural networks, and Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. We took inspiration from wav2letter <ref type="bibr" target="#b6">[7]</ref>, which uses 1D-convolution layers. Liptchinsky et al. <ref type="bibr" target="#b7">[8]</ref> improved wav2letter by increasing the model depth to 19 convolutional layers and adding Gated Linear Units (GLU) <ref type="bibr" target="#b8">[9]</ref>, weight normalization <ref type="bibr" target="#b9">[10]</ref> and dropout.</p><p>By building a deeper and larger capacity network, we aim to demonstrate that we can match or outperform non end-toend models on the LibriSpeech and 2000hr Fisher+Switchboard tasks. Like wav2letter, our architecture, Jasper, uses a stack of 1D-convolution layers, but with ReLU and batch normalization <ref type="bibr" target="#b10">[11]</ref>. We find that ReLU and batch normalization outperform other activation and normalization schemes that we tested for convolutional ASR. As a result, Jasper's architecture contains only 1D convolution, batch normalization, ReLU, and dropout layers -operators highly optimized for training and inference on GPUs. <ref type="bibr" target="#b1">2</ref> Work was conducted while the author was at NVIDIA It is possible to increase the capacity of the Jasper model by stacking these operations. Our largest version uses 54 convolutional layers (333M parameters), while our smaller model uses 34 (201M parameters). We use residual connections to enable this level of depth. We investigate a number of residual options and propose a new residual connection topology we call Dense Residual (DR).</p><p>Integrating our best acoustic model with a Transformer-XL <ref type="bibr" target="#b11">[12]</ref> language model allows us to obtain new state-of-the-art (SOTA) results on LibriSpeech <ref type="bibr" target="#b12">[13]</ref> test-clean of 2.95% WER and SOTA results among end-to-end models 1 on LibriSpeech test-other. We show competitive results on Wall Street Journal (WSJ), and 2000hr Fisher+Switchboard (F+S). Using only greedy decoding without a language model we achieve 3.86% WER on LibriSpeech test-clean. This paper makes the following contributions:</p><p>1. We present a computationally efficient end-to-end convolutional neural network acoustic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>We show ReLU and batch norm outperform other combinations for regularization and normalization, and residual connections are necessary for training to converge.</p><p>3. We introduce NovoGrad, a variant of the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with a smaller memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We improve the SOTA WER on LibriSpeech test-clean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Jasper Architecture</head><p>Jasper is a family of end-to-end ASR models that replace acoustic and pronunciation models with a convolutional neural network. Jasper uses mel-filterbank features calculated from 20ms windows with a 10ms overlap, and outputs a probability distribution over characters per frame 2 . Jasper has a block architecture: a Jasper BxR model has B blocks, each with R subblocks. Each sub-block applies the following operations: a 1Dconvolution, batch norm, ReLU, and dropout. All sub-blocks in a block have the same number of output channels. Each block input is connected directly into the last subblock via a residual connection. The residual connection is first projected through a 1x1 convolution to account for different numbers of input and output channels, then through a batch norm layer. The output of this batch norm layer is added to the output of the batch norm layer in the last sub-block. The result of this sum is passed through the activation function and dropout to produce the output of the current block. The sub-block architecture of Jasper was designed to facilitate fast GPU inference. Each sub-block can be fused into a single GPU kernel: dropout is not used at inference-time and is eliminated, batch norm can be fused with the preceding convolution, ReLU clamps the result, and residual summation can be treated as a modified bias term in this fused operation.</p><p>All Jasper models have four additional convolutional blocks: one pre-processing and three post-processing. See <ref type="table" target="#tab_0">Figure 1 and Table 1</ref> for details. We also build a variant of Jasper, Jasper Dense Residual (DR). Jasper DR follows DenseNet <ref type="bibr" target="#b15">[16]</ref> and DenseRNet <ref type="bibr" target="#b16">[17]</ref>, but instead of having dense connections within a block, the output of a convolution block is added to the inputs of all the following blocks. While DenseNet and DenseRNet concatenates the outputs of different layers, Jasper DR adds them in the same way that residuals are added in ResNet. As explained below, we find addition to be as effective as concatenation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Normalization and Activation</head><p>In our study, we evaluate performance of models with:</p><p>• 3 types of normalization: batch norm <ref type="bibr" target="#b10">[11]</ref>, weight norm <ref type="bibr" target="#b9">[10]</ref>, and layer norm <ref type="bibr" target="#b17">[18]</ref> • 3 types of rectified linear units: ReLU, clipped ReLU (cReLU), and leaky ReLU (lReLU)</p><p>• 2 types of gated units: gated linear units (GLU) <ref type="bibr" target="#b8">[9]</ref>, and gated activation units (GAU) <ref type="bibr" target="#b18">[19]</ref> All experiment results are shown in <ref type="table" target="#tab_1">Table 2</ref>. We first experimented with a smaller Jasper5x3 3 model to pick the top 3 settings before training on larger Jasper models. We found that layer norm with GAU performed the best on the smaller model. Layer norm with ReLU and batch norm with ReLU came second and third in our tests. Using these 3, we conducted further experiments on a larger Jasper10x4. For larger models, we noticed that batch norm with ReLU outperformed other choices. Thus, leading us to decide on batch normalization and ReLU for our architecture. During batching, all sequences are padded to match the longest sequence. These padded values caused issues when using layer norm. We applied a sequence mask to exclude padding values from the mean and variance calculation. Further, we computed mean and variance over both the time dimension and channels similar to the sequence-wise normalization proposed by Laurent et al. <ref type="bibr" target="#b19">[20]</ref>. In addition to masking layer norm, we additionally applied masking prior to the convolution operation, and masking the mean and variance calculations in batch norm. These results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Interestingly, we found that while masking before convolution gives a lower WER, using masks for both convolutions and batch norm results in worse performance.</p><p>As a final note, we found that training with weight norm was very unstable leading to exploding activations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Residual Connections</head><p>For models deeper than Jasper 5x3, we observe consistently that residual connections are necessary for training to converge. In addition to the simple residual and dense residual model described above, we investigated DenseNet <ref type="bibr" target="#b15">[16]</ref> and DenseR-Net <ref type="bibr" target="#b16">[17]</ref> variants of Jasper. Both connect the outputs of each sub-block to the inputs of following sub-blocks within a block. DenseRNet, similar to Dense Residual, connects the output of each block to the input of all following blocks. DenseNet and DenseRNet combine residual connections using concatenation whereas Residual and Dense Residual use addition. We found that Dense Residual and DenseRNet perform similarly with each performing better on specific subsets of LibriSpeech. We decided to use Dense Residual for subsequent experiments. The main reason is that due to concatenation, the growth factor for DenseNet and DenseRNet requires tuning for deeper models whereas Dense Residual does not have a growth factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Language Model</head><p>A language model (LM) is a probability distribution over arbitrary symbol sequences P (w1, ..., wn) such that more likely sequences are assigned higher probabilities. LMs are frequently used to condition beam search. During decoding, candidates are evaluated using both acoustic scores and LM scores. Traditional N-gram LMs have been augmented with neural LMs in recent work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. We experiment with statistical N-gram language models <ref type="bibr" target="#b23">[24]</ref> and neural Transformer-XL <ref type="bibr" target="#b11">[12]</ref> models. Our best results use acoustic and word-level N-gram language models to generate a candidate list using beam search with a width of 2048. Next, an external Transformer-XL LM rescores the final list. All LMs were trained on datasets independently from acoustic models. We show results with the neural LM in our Results section. We observed a strong correlation between the quality of the neural LM (measured by perplexity) and WER as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">NovoGrad</head><p>For training, we use either Stochastic Gradient Descent (SGD) with momentum or our own NovoGrad, an optimizer similar to Adam <ref type="bibr" target="#b14">[15]</ref>, except that its second moments are computed per layer instead of per weight. Compared to Adam, it reduces memory consumption and we find it to be more numerically stable.</p><p>At each step t, NovoGrad computes the stochastic gradient g l t following the regular forward-backward pass. Then the second-order moment v l t is computed for each layer l similar to ND-Adam <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_0">v l t = β2 · v l t−1 + (1 − β2) · ||g l t || 2<label>(1)</label></formula><p>The second-order moment v l t is used to re-scale gradients g l t before calculating the first-order moment m l t : If L2-regularization is used, a weight decay d · wt is added to the re-scaled gradient (as in AdamW <ref type="bibr" target="#b29">[30]</ref>):</p><formula xml:id="formula_1">m l t = β1 · m l t−1 + g l t v l t +<label>(2)</label></formula><formula xml:id="formula_2">m l t = β1 · m l t−1 + g l t v l t + + d · wt<label>(3)</label></formula><p>Finally, new weights are computed using the learning rate αt:</p><formula xml:id="formula_3">wt+1 = wt − αt · mt<label>(4)</label></formula><p>Using NovoGrad instead of SGD with momentum, we decreased the WER on dev-clean LibriSpeech from 4.00% to 3.64%, a relative improvement of 9% for Jasper DR 10x5. For more details and experiment results with NovoGrad, see <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We evaluate Jasper across a number of datasets in various domains.</p><p>In all experiments, we use dropout and weight decay as regularization. At training time, we use 3-fold speed perturbation with fixed +/-10% <ref type="bibr" target="#b31">[32]</ref> for Lib-riSpeech. For WSJ and Hub5'00, we use a random speed perturbation factor between [-10%, 10%] as each utterance is fed into the model. All models have been trained on NVIDIA DGX-1 in mixed precision <ref type="bibr" target="#b32">[33]</ref> using OpenSeq2Seq <ref type="bibr" target="#b33">[34]</ref>. Pretrained models and training configurations are available from "https://nvidia.github.io/OpenSeq2Seq/html/speechrecognition.html".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Read Speech</head><p>We evaluated the performance of Jasper on two read speech datasets: LibriSpeech and Wall Street Journal (WSJ). For Lib-riSpeech, we trained Jasper DR 10x5 using our NovoGrad optimizer for 400 epochs. We achieve SOTA performance on the test-clean subset and SOTA among end-to-end speech recognition models on test-other.</p><p>We trained a smaller Jasper 10x3 model using the SGD with momentum optimizer for 400 epochs on a combined WSJ dataset (80 hours): LDC93S6A (WSJ0) and LDC94S13A (WSJ1). The results are provided in <ref type="table" target="#tab_5">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Conversational Speech</head><p>We also evaluate the Jasper model's performance on a conversational English corpus. The Hub5 Year 2000 (Hub5'00) evaluation (LDC2002S09, LDC2002T43) is widely used in academia. It is divided into two subsets: Switchboard (SWB) and Callhome (CHM). The training data for both the acoustic and language models consisted of the 2000hr Fisher+Switchboard training data (LDC2004S13, LDC2005S13, LDC97S62). Jasper DR 10x5 was trained using SGD with momentum for 50 epochs. We compare to other models trained using the same data and report Hub5'00 results in <ref type="table" target="#tab_6">Table 7</ref>. We obtain good results for SWB. However, there is work to be done to improve WER on harder tasks such as CHM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We have presented a new family of neural architectures for endto-end speech recognition. Inspired by wav2letter's convolutional approach, we build a deep and scalable model, which requires a well-designed residual topology, effective regularization, and a strong optimizer. As our architecture studies demonstrated, a combination of standard components leads to SOTA results on LibriSpeech and competitive results on other benchmarks. Our Jasper architecture is highly efficient for training and inference, and serves as a good baseline approach on top of which to explore more sophisticated regularization, data augmentation, loss functions, language models, and optimization strategies. We are interested to see if our approach can continue to scale to deeper models and larger datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Jasper BxR model: B -number of blocks, R -number of sub-blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Jasper Dense Residual</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>LM perplexity vs WER. LibriSpeech dev-other. Varying perplexity is achieved by taking earlier or later snapshots during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Jasper 10x5: 10 blocks, each consisting of 5 1Dconvolutional sub-blocks, plus 4 additional blocks.</figDesc><table><row><cell cols="2"># Blocks Block</cell><cell>Kernel</cell><cell># Output Channels</cell><cell>Dropout</cell><cell># Sub Blocks</cell></row><row><cell>1</cell><cell>Conv1</cell><cell>11 stride=2</cell><cell>256</cell><cell>0.2</cell><cell>1</cell></row><row><cell>2</cell><cell>B1</cell><cell>11</cell><cell>256</cell><cell>0.2</cell><cell>5</cell></row><row><cell>2</cell><cell>B2</cell><cell>13</cell><cell>384</cell><cell>0.2</cell><cell>5</cell></row><row><cell>2</cell><cell>B3</cell><cell>17</cell><cell>512</cell><cell>0.2</cell><cell>5</cell></row><row><cell>2</cell><cell>B4</cell><cell>21</cell><cell>640</cell><cell>0.3</cell><cell>5</cell></row><row><cell>2</cell><cell>B5</cell><cell>25</cell><cell>768</cell><cell>0.3</cell><cell>5</cell></row><row><cell>1</cell><cell>Conv2</cell><cell>29 dilation=2</cell><cell>896</cell><cell>0.4</cell><cell>1</cell></row><row><cell>1</cell><cell>Conv3</cell><cell>1</cell><cell>1024</cell><cell>0.4</cell><cell>1</cell></row><row><cell>1</cell><cell>Conv4</cell><cell>1</cell><cell># graphemes</cell><cell>0</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Normalization and Activation: Greedy WER, Lib-riSpeech after 50 epochs</figDesc><table><row><cell>Model</cell><cell cols="2">Normalization Activation</cell><cell cols="2">Dev Clean Other</cell></row><row><cell></cell><cell></cell><cell>ReLU</cell><cell>8.82</cell><cell>23.26</cell></row><row><cell></cell><cell></cell><cell>cReLU</cell><cell>8.89</cell><cell>23.02</cell></row><row><cell></cell><cell>Batch Norm</cell><cell>lReLU</cell><cell>11.31</cell><cell>26.90</cell></row><row><cell></cell><cell></cell><cell>GLU</cell><cell>9.46</cell><cell>24.30</cell></row><row><cell></cell><cell></cell><cell>GAU</cell><cell>9.41</cell><cell>24.65</cell></row><row><cell></cell><cell></cell><cell>ReLU</cell><cell>8.82</cell><cell>22.83</cell></row><row><cell>Jasper 5x3</cell><cell>Layer Norm</cell><cell>cReLU lReLU</cell><cell>9.14 11.29</cell><cell>23.26 26.35</cell></row><row><cell></cell><cell>(masked)</cell><cell>GLU</cell><cell>12.62</cell><cell>29.22</cell></row><row><cell></cell><cell></cell><cell>GAU</cell><cell>8.35</cell><cell>23.07</cell></row><row><cell></cell><cell></cell><cell>ReLU</cell><cell>9.98</cell><cell>24.87</cell></row><row><cell></cell><cell></cell><cell>cReLU</cell><cell>11.25</cell><cell>26.87</cell></row><row><cell></cell><cell>Weight Norm</cell><cell>lReLU</cell><cell>11.87</cell><cell>27.54</cell></row><row><cell></cell><cell></cell><cell>GLU</cell><cell>11.05</cell><cell>27.10</cell></row><row><cell></cell><cell></cell><cell>GAU</cell><cell>11.25</cell><cell>27.70</cell></row><row><cell></cell><cell>Batch Norm</cell><cell>ReLU</cell><cell>6.15</cell><cell>17.58</cell></row><row><cell>Jasper 10x4</cell><cell>Layer Norm</cell><cell>ReLU</cell><cell>6.56</cell><cell>18.48</cell></row><row><cell></cell><cell>(Masked)</cell><cell>GAU</cell><cell>7.14</cell><cell>19.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Sequence Masking: Greedy WER, LibriSpeech for Jasper 10x4 after 50 epochs</figDesc><table><row><cell>Model</cell><cell>Masking</cell><cell cols="2">Dev Clean Other</cell></row><row><cell>Jasper DR 10x4</cell><cell>None</cell><cell>5.88</cell><cell>17.62</cell></row><row><cell>Jasper DR 10x4</cell><cell>BN Mask</cell><cell>5.92</cell><cell>17.63</cell></row><row><cell>Jasper DR 10x4</cell><cell>Conv Mask</cell><cell>5.66</cell><cell>16.77</cell></row><row><cell cols="2">Jasper DR 10x4 Conv+BN Mask</cell><cell>5.80</cell><cell>16.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Residual Connections: Greedy WER, LibriSpeech for Jasper 10x3 after 400 epochs. All models sized to have roughly the same parameter count.</figDesc><table><row><cell>Model</cell><cell>#params, M</cell><cell cols="2">Dev Clean Other</cell></row><row><cell>Residual</cell><cell>201</cell><cell>4.65</cell><cell>14.36</cell></row><row><cell>Dense Residual</cell><cell>211</cell><cell>4.51</cell><cell>14.15</cell></row><row><cell>DenseNet</cell><cell>205</cell><cell>4.77</cell><cell>14.55</cell></row><row><cell>DenseRNet</cell><cell>211</cell><cell>4.32</cell><cell>14.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>LibriSpeech, WER (%)</figDesc><table><row><cell>Model</cell><cell>E2E</cell><cell>LM</cell><cell cols="4">dev-clean dev-other test-clean test-other</cell></row><row><cell>CAPIO (single) [23]</cell><cell>N</cell><cell>RNN</cell><cell>3.02</cell><cell>8.28</cell><cell>3.56</cell><cell>8.58</cell></row><row><cell>pFSMN-Chain [25]</cell><cell>N</cell><cell>RNN</cell><cell>2.56</cell><cell>7.47</cell><cell>2.97</cell><cell>7.5</cell></row><row><cell>DeepSpeech2 [26]</cell><cell>Y</cell><cell>5-gram</cell><cell>-</cell><cell>-</cell><cell>5.33</cell><cell>13.25</cell></row><row><cell>Deep bLSTM w/ attention [21]</cell><cell>Y</cell><cell>LSTM</cell><cell>3.54</cell><cell>11.52</cell><cell>3.82</cell><cell>12.76</cell></row><row><cell>wav2letter++ [27]</cell><cell>Y</cell><cell>ConvLM</cell><cell>3.16</cell><cell>10.05</cell><cell>3.44</cell><cell>11.24</cell></row><row><cell>LAS + SpecAugment 4 [28]</cell><cell>Y</cell><cell>RNN</cell><cell>-</cell><cell>-</cell><cell>2.5</cell><cell>5.8</cell></row><row><cell>Jasper DR 10x5</cell><cell>Y</cell><cell>-</cell><cell>3.64</cell><cell>11.89</cell><cell>3.86</cell><cell>11.95</cell></row><row><cell>Jasper DR 10x5</cell><cell>Y</cell><cell>6-gram</cell><cell>2.89</cell><cell>9.53</cell><cell>3.34</cell><cell>9.62</cell></row><row><cell>Jasper DR 10x5</cell><cell>Y</cell><cell>Transformer-XL</cell><cell>2.68</cell><cell>8.62</cell><cell>2.95</cell><cell>8.79</cell></row><row><cell>Jasper DR 10x5 + Time/Freq Masks 4</cell><cell>Y</cell><cell>Transformer-XL</cell><cell>2.62</cell><cell>7.61</cell><cell>2.84</cell><cell>7.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>WSJ End-to-End Models, WER (%)</figDesc><table><row><cell>Model</cell><cell>LM</cell><cell cols="2">nov93 nov92</cell></row><row><cell>seq2seq + deep conv [35]</cell><cell>-</cell><cell>-</cell><cell>10.5</cell></row><row><cell>wav2letter++ [27]</cell><cell>4-gram</cell><cell>9.5</cell><cell>5.6</cell></row><row><cell>wav2letter++ [27]</cell><cell>ConvLM</cell><cell>7.5</cell><cell>4.1</cell></row><row><cell>E2E LF-MMI [14]</cell><cell>3-gram</cell><cell>-</cell><cell>4.1</cell></row><row><cell>Jasper 10x3</cell><cell>-</cell><cell>16.1</cell><cell>13.3</cell></row><row><cell>Jasper 10x3</cell><cell>4-gram</cell><cell>9.9</cell><cell>7.1</cell></row><row><cell>Jasper 10x3</cell><cell>Transformer-XL</cell><cell>9.3</cell><cell>6.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Hub5'00, WER (%)</figDesc><table><row><cell>Model</cell><cell>E2E</cell><cell>LM</cell><cell cols="2">SWB CHM</cell></row><row><cell>LF-MMI [14]</cell><cell>N</cell><cell>RNN</cell><cell>7.3</cell><cell>14.2</cell></row><row><cell>Attention Seq2Seq [36]</cell><cell>Y</cell><cell>-</cell><cell>8.3</cell><cell>15.5</cell></row><row><cell>RNN-T [37]</cell><cell>Y</cell><cell>4-gram</cell><cell>8.1</cell><cell>17.5</cell></row><row><cell>Char E2E LF-MMI [14]</cell><cell>Y</cell><cell>RNN</cell><cell>8.0</cell><cell>17.6</cell></row><row><cell>Phone E2E LF-MMI [14]</cell><cell>Y</cell><cell>RNN</cell><cell>7.5</cell><cell>14.6</cell></row><row><cell>CTC + Gram-CTC</cell><cell>Y</cell><cell>N-gram</cell><cell>7.3</cell><cell>14.7</cell></row><row><cell>Jasper DR 10x5</cell><cell>Y</cell><cell>4-gram</cell><cell>8.3</cell><cell>19.3</cell></row><row><cell>Jasper DR 10x5</cell><cell>Y</cell><cell>Transformer-XL</cell><cell>7.8</cell><cell>16.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We follow Hadian et. al's definition of end-to-end<ref type="bibr" target="#b13">[14]</ref>: "flatstart training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees."<ref type="bibr" target="#b1">2</ref> We use 40 features for WSJ and 64 for LibriSpeech and F+S.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Jasper 5x3 models contain one block of each B1 to B5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We include the latest SOTA which was achieved by Park et al.<ref type="bibr" target="#b27">[28]</ref> after our initial submission. We add results for Jasper with time and frequency masks similar to SpecAugment. We use 1 continuous time mask of size T ∼ U (0, 99) time steps, and 1 continuous frequency mask of size F ∼ U (0, 26) frequency bands.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A time-delay neural network architecture for isolated word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Global optimization of a neural network-hidden markov model hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Flammia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">252259</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Towards end-to-end speech recognition with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="410" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Wav2letter: an endto-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Letterbased speech recognition with gated convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transformer-xl: Language modeling with longer-term dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<idno>abs/1901.02860</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition using lattice-free mmi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sameti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acoustic modeling with densely connected residual network for multichannel speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1783" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalized recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="2657" to="2661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-orthogonal low-rank matrix factorization for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The CAPIO 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrashekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Lane</surname></persName>
		</author>
		<idno>abs/1801.00059</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth workshop on statistical machine translation</title>
		<meeting>the sixth workshop on statistical machine translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel pyramidal-fsmn architecture with lattice-free MMI for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1810.11352</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>ser. ICML&apos;16. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<idno>abs/1812.06864</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Normalized directionpreserving adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04546</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stochastic Gradient Methods with Layerwise Adaptive Moments for Training of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vijayaditya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sanjeev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Openseq2seq: extensible toolkit for distributed and mixed precision training of sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving attention based sequence-to-sequence models for end-to-end english conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="761" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring neural transducers for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
