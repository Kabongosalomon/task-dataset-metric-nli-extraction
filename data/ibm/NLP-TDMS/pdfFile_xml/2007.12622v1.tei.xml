<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BMBC:Bilateral Motion Estimation with Bilateral Cost Volume for Video Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-17">17 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Multimedia Engineering</orgName>
								<orgName type="institution">Dongguk University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BMBC:Bilateral Motion Estimation with Bilateral Cost Volume for Video Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-17">17 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video interpolation</term>
					<term>bilateral motion</term>
					<term>bilateral cost volume</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Junheum Park 1[0000−0002−9094−128X] , Keunsoo Ko 1[0000−0003−0203−4530] , Chul Lee 2[0000−0001−9329−7365] , and Chang-Su Kim 1[0000−0002−4276−1831]</p><p>Abstract. Video interpolation increases the temporal resolution of a video sequence by synthesizing intermediate frames between two consecutive frames. We propose a novel deep-learning-based video interpolation algorithm based on bilateral motion estimation. First, we develop the bilateral motion network with the bilateral cost volume to estimate bilateral motions accurately. Then, we approximate bi-directional motions to predict a different kind of bilateral motions. We then warp the two input frames using the estimated bilateral motions. Next, we develop the dynamic filter generation network to yield dynamic blending filters. Finally, we combine the warped frames using the dynamic blending filters to generate intermediate frames. Experimental results show that the proposed algorithm outperforms the state-of-the-art video interpolation algorithms on several benchmark datasets. The source codes and pretrained models are available at https://github.com/JunHeum/BMBC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A low temporal resolution causes aliasing, yields abrupt motion artifacts, and degrades the video quality. In other words, the temporal resolution is an important factor affecting video quality. To enhance temporal resolutions, many video interpolation algorithms <ref type="bibr">[2-4, 12, 14, 16-18, 20-22]</ref> have been proposed, which synthesize intermediate frames between two actual frames. These algorithms are widely used in applications, including visual quality enhancement <ref type="bibr" target="#b31">[32]</ref>, video compression <ref type="bibr" target="#b6">[7]</ref>, slow-motion video generation <ref type="bibr" target="#b13">[14]</ref>, and view synthesis <ref type="bibr" target="#b5">[6]</ref>. However, video interpolation is challenging due to diverse factors, such as large and nonlinear motions, occlusions, and variations in lighting conditions. Especially, to generate a high-quality intermediate frame, it is important to estimate motions or optical flow vectors accurately.</p><p>Recently, with the advance of deep-learning-based optical flow methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>, flow-based video interpolation algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref> have been developed, yielding reliable interpolation results. Niklaus et al. <ref type="bibr" target="#b19">[20]</ref> generated intermediate frames based on the forward warping. However, the forward warping may cause interpolation artifacts because of the hole and overlapped pixel problems. To overcome this, other approaches leverage the backward warping. To use the backward warping, intermediate motions should be obtained. Various video interpolation algorithms <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32]</ref> based on the bilateral motion estimation approximate these intermediate motions from optical flows between two input frames. However, this approximation may degrade video interpolation results.</p><p>In this work, we propose a novel video interpolation network, which consists of the bilateral motion network and the dynamic filter generation network. First, we predict six bilateral motions: two from the bilateral motion network and the other four through optical flow approximation. In the bilateral motion network, we develop the bilateral cost volume to facilitate the matching process. Second, we extract context maps to exploit rich contextual information. We then warp the two input frames and the corresponding context maps using the six bilateral motions, resulting in six pairs of warped frame and context map. Next, these pairs are used to generate dynamic blending filters. Finally, the six warped frames are superposed by the blending filters to generate an intermediate frame.</p><p>Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art video interpolation algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref> meaningfully on various benchmark datasets.</p><p>This work has the following major contributions:</p><p>-We develop a novel deep-learning-based video interpolation algorithm based on the bilateral motion estimation.</p><p>-We propose the bilateral motion network with the bilateral cost volume to estimate intermediate motions accurately.</p><p>-The proposed algorithm performs better than the state-of-the-art algorithms on various benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep-learning-based video interpolation</head><p>The objective of video interpolation is to enhance a low temporal resolution by synthesizing intermediate frames between two actual frames. With the great success of CNNs in various image processing and computer vision tasks, many deep-learning-based video interpolation techniques have been developed. Long et al. <ref type="bibr" target="#b17">[18]</ref> developed a CNN, which takes a pair of frames as input and then directly generates an intermediate frame. However, their algorithm yields severe blurring since it does not use a motion model. In <ref type="bibr" target="#b18">[19]</ref>, PhaseNet was proposed using the phase-based motion representation. Although it yields robust results to lightning changes or motion blur, it may fail to faithfully reconstruct detailed texture. In <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, Niklaus et al. proposed kernel-based methods that estimate an adaptive convolutional kernel for each pixel. The kernel-based methods produce reasonable results, but they cannot handle motions larger than a kernel's size.</p><p>To exploit motion information explicitly, flow-based algorithms have been developed. Niklaus and Liu <ref type="bibr" target="#b19">[20]</ref> generated an intermediate frame from two consecutive frames using the forward warping. However, the forward warping suffers from holes and overlapped pixels. Therefore, most flow-based algorithms are based on backward warping. In order to use backward warping, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cost volume</head><p>A cost volume records similarity scores between two data. For example, in pixelwise matching between two images, the similarity is computed between each pixel pair: one in a reference image and the other in a target image. Then, for each reference pixel, the target pixel with the highest similarity score becomes the matched pixel. The cost volume facilitates this matching process. Thus, the optical flow estimation techniques in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> are implemented using cost volumes. In <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>, a cost volume is computed using various features of two video frames, and optical flow is estimated using the similarity information in the cost volume through a CNN. Sun et al. <ref type="bibr" target="#b29">[30]</ref> proposed a partial cost volume to significantly reduce the memory requirement while improving the motion estimation accuracy based on a reduced search region. In this work, we develop a novel cost volume, called bilateral cost volume, which is different from the conventional volumes in that its reference is an intermediate frame to be interpolated, instead of one of the two input frames.</p><p>3 Proposed Algorithm <ref type="figure">Fig. 1</ref> is an overview of the proposed algorithm that takes successive frames I 0 and I 1 as input and synthesizes an intermediate frame I t at t ∈ (0, 1) as output. First, we estimate two 'bilateral' motions V t→0 and V 0→t between the input frames. Second, we estimate 'bi-directional' motions V 0→1 and V 1→0 between I 0 and I 1 and then use these motions to approximate four further bilateral motions. Third, the pixel-wise context maps C 0 and C 1 are extracted from I 0 and I 1 . Then, the input frames and corresponding context maps are warped using the six bilateral motions. Note that, since the warped frames become multiple </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bilateral motion estimation</head><p>Given the two input frames I 0 and I 1 , the goal is to predict the intermediate frame I t using motion information. However, it is impossible to directly estimate the intermediate motion between the intermediate frame I t and one of the input frames I 0 or I 1 because there is no image information of I t . To address this issue, we assume linear motion between successive frames. Specifically, we attempt to estimate the backward and forward motion vectors V t→0 (x) and V t→1 (x) at x, respectively, where x is a pixel location in I t . Based on the linear assumption,</p><formula xml:id="formula_0">we have V t→0 (x) = − t 1−t × V t→1 (x)</formula><p>. We develop a CNN to estimate bilateral motions V t→0 and V t→1 using I 0 and I 1 . To this end, we adopt an optical flow network, PWC-Net <ref type="bibr" target="#b29">[30]</ref>, and extend it for the bilateral motion estimation. <ref type="figure">Fig. 2</ref> shows the key components of the modified PWC-Net. Let us describe each component subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Warping layer:</head><p>The original PWC-Net uses the previous frame I 0 as a reference and the following frame I 1 as a target. On the other hand, the bilateral motion estimation uses the intermediate frame I t as a reference, and the input frames I 0 and I 1 as the target. Thus, whereas the original PWC-Net warps the feature c l 1 of I 1 toward the feature c l 0 of I 0 , we warp both features c l 0 and c l 1 toward the intermediate frame, leading to c l 0→t and c l 1→t , respectively. We employ the spatial transformer networks <ref type="bibr" target="#b10">[11]</ref> to achieve the warping. Specifically, a target feature map c tgt is warped into a reference feature map c ref using a motion vector field by Warping Layer <ref type="figure">Fig. 2</ref>. The architecture of the bilateral motion network: The feature maps c l 0 and c l 1 of the previous and following frames I0 and I1 at the lth level and the up-sampled motion fields V l t→0 and V l t→1 estimated at the (l − 1)th level are fed into the CNN to generate the motion fields V l t→0 and V l t→1 at the lth level. where V ref→tgt is the motion vector field from the reference to the target.</p><formula xml:id="formula_1">c w ref (x) = c tgt x + V ref→tgt (x) (1) →1 →0 1 0 ෨ →1 ෨ →0 Warping Layer Bilateral Cost Volume Layer Optical</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilateral cost volume layer:</head><p>A cost volume has been used to store the matching costs associating with a pixel in a reference frame with its corresponding pixels in a single target frame <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. However, in the bilateral motion estimation, because the reference frame does not exist and should be predicted from two target frames, the conventional cost volume cannot be used. Thus, we develop a new cost volume for the bilateral motion estimation, which we refer to as the bilateral cost volume. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates the proposed bilateral cost volume generation that takes the features c l 0 and c l 1 of the two input frames and the up-sampled bilateral motion fields V l t→0 and V l t→1 estimated at the (l − 1)th level. Let x denote a pixel location in the intermediate frame I l t . Then, we define the matching cost as the bilateral correlation between features c l 0 and c l 1 , indexed by the bilateral motion vector that passes through x, given by</p><formula xml:id="formula_2">BC l t (x, d) = c l 0 (x + V l t→0 (x) − 2t × d) T c l 1 (x + V l t→1 (x) + 2(1 − t) × d) (2)</formula><p>where d denotes the displacement vector within the search window</p><formula xml:id="formula_3">D = [−d, d]× [−d, d].</formula><p>Note that we compute only |D| = D 2 bilateral correlations to construct a partial cost volume, where D = 2d + 1. In the L-level pyramid architecture, a one-pixel motion at the coarsest level corresponds to 2 L−1 pixels at the finest resolution. Thus, the search range D of the bilateral cost volume can be set to a small value to reduce the memory usage. The dimension of the bilateral cost volume at the lth level is D 2 × H l × W l , where H l and W l denote the height and width of the lth level features, respectively. Also, the up-sampled bilateral motions V l t→0 and V l t→1 are set to zero at the coarsest level. Most conventional video interpolation algorithms generate a single intermediate frame at the middle of two input frames, i.e. t = 0.5. Thus, they cannot yield output videos with arbitrary frame rates. A few recent algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> attempt to interpolate intermediate frames at arbitrary time instances t ∈ (0, 1). However, because their approaches are based on the approximation, as the time instance gets far from either of the input frames, the quality of the interpolated frame gets worse. On the other hand, the proposed algorithm takes into account the time instance t ∈ [0, 1] during the computation of the bilateral cost volume in <ref type="bibr" target="#b1">(2)</ref>. Also, after we train the bilateral motion network with the bilateral cost volume, we can use the shared weights to estimate the bilateral motions at an arbitrary t ∈ [0, 1]. In the extreme cases t = 0 or t = 1, the bilateral cost volume becomes identical to the conventional cost volume in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, which is used to estimate the bi-directional motions V 0→1 and V 1→0 between input frames.</p><formula xml:id="formula_4">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motion approximation</head><p>Although the proposed bilateral motion network effectively estimates motion fields V t→0 and V t→1 from the intermediate frame at t to the previous and following frames, it may fail to find accurate motions, especially at occluded regions. For example, <ref type="figure" target="#fig_2">Fig. 4(d)</ref> and (e) show that the interpolated regions, reconstructed by the bilateral motion estimation, contain visual artifacts. To address this issue and improve the quality of an interpolated frame, in addition to the bilateral motion estimation, we develop an approximation scheme to predict a different kind of bilateral motions V t→0 and V t→1 using the bi-directional motions V 0→1 and V 1→0 between the two input frames.   <ref type="figure" target="#fig_3">5</ref> illustrates this motion approximation, in which each column represents a frame at a time instance and a dot corresponds to a pixel in the frame. In particular, in <ref type="figure" target="#fig_3">Fig. 5(a)</ref>, an occluded pixel in I 0 is depicted by a green dot. To complement the inaccuracy of the bilateral motion at pixel x in I t , we use two bi-directional motions V 0→1 (x) and V 1→0 (x), which are depicted by green and red lines, respectively. We approximate two forward bilateral motions V f w t→1 and V f w t→0 in <ref type="figure" target="#fig_3">Fig. 5(b)</ref> using V 0→1 . Specifically, for pixel x in I t , depicted by an orange dot, we approximate a motion vector V f w t→1 (x) by scaling V 0→1 (x) with a factor (1−t), assuming that the motion vector field is locally smooth. Since the bilateral motion estimation is based on the assumption that a motion trajectory between consecutive frames is linear, two approximate motions V f w t→1 (x) and V f w t→0 (x) should be symmetric with respect to x in I t . Thus, we obtain an additional approximate vector V f w t→0 (x) by reversing the direction of the vector V f w t→1 (x). In other words, we approximate the forward bilateral motions by</p><formula xml:id="formula_5">V f w t→1 (x) = (1 − t) × V 0→1 (x),<label>(3)</label></formula><formula xml:id="formula_6">V f w t→0 (x) = (−t) × V 0→1 (x).<label>(4)</label></formula><p>Similarly, we approximate the backward bilateral motions by</p><formula xml:id="formula_7">V bw t→0 (x) = t × V 1→0 (x),<label>(5)</label></formula><formula xml:id="formula_8">V bw t→1 (x) = −(1 − t) × V 1→0 (x),<label>(6)</label></formula><p>as illustrated in <ref type="figure" target="#fig_3">Fig. 5(c)</ref>. Note that Jiang et al. <ref type="bibr" target="#b13">[14]</ref> also used these equations (3)∼(6), but derived only two motion candidates: V t→1 (x) by combining <ref type="formula" target="#formula_5">(3)</ref> and <ref type="formula" target="#formula_8">(6)</ref> and V t→0 (x) by combining <ref type="formula" target="#formula_6">(4)</ref> and <ref type="bibr" target="#b4">(5)</ref>. Thus, if an approximated motion in (3)∼(6) is unreliable, the combined one is also degraded. In contrast, we use all four candidates in (3)∼(6) directly to choose reliable motions in Section 3.3. <ref type="figure" target="#fig_2">Fig. 4</ref> shows that, whereas the bilateral motion estimation provides visual artifacts in (d), the motion approximation provides results without noticeable artifacts in (f). On the other hand, the bilateral motion estimation is more effective than the motion approximation in the cases of (e) and (g). Thus, the two schemes are complementary to each other. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Frame synthesis</head><p>We interpolate an intermediate frame by combining the six intermediate candidates, which are warped by the warping layers in <ref type="figure">Fig. 1</ref>. If we consider only color information, rich contextual information in the input frames may be lost during the synthesis <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>, degrading the interpolation performance. Hence, as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref>, we further exploit contextual information in the input frames, called context maps. Specifically, we extract the output of the conv1 layer of ResNet-18 <ref type="bibr" target="#b7">[8]</ref> as a context map, which is done by the context extractor in <ref type="figure">Fig. 1</ref>.</p><p>By warping the two input frames and the corresponding context maps, we obtain six pairs of a warped frame and its context map: two pairs are reconstructed using the bilateral motion estimation, and four pairs using the motion approximation. <ref type="figure">Fig. 1</ref> shows these six pairs. Since these six warped pairs have different characteristics, they are used as complementary candidates of the intermediate frame. Recent video interpolation algorithms employ synthesis neural networks, which take warped frames as input and yield final interpolation results or residuals to refine pixel-wise blended results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref>. However, these synthesis networks may cause artifacts if motions are inaccurately estimated. To alleviate these artifacts, instead, we develop a dynamic filter network <ref type="bibr" target="#b12">[13]</ref> that takes the aforementioned six pairs of candidates as input and outputs local blending filters, which are then used to process the warped frames to yield the intermediate frame. These local blending filters compensate for motion inaccuracies, by considering spatiotemporal neighboring pixels in the stack of warped frames. The frame synthesis layer performs this synthesis in <ref type="figure">Fig. 1</ref>.</p><p>Dynamic local blending filters: <ref type="figure" target="#fig_5">Fig. 6</ref> shows the proposed synthesis network using dynamic local blending filters. The coefficients of the filters are learned from the images and contextual information through a dynamic blending filter network <ref type="bibr" target="#b12">[13]</ref>. We employ the residual dense network <ref type="bibr" target="#b33">[34]</ref> as the backbone for the filter generation. In <ref type="figure" target="#fig_5">Fig. 6, the</ref>   </p><p>For each x, the sum of all coefficients in the six filters are normalized to 1. Then, the intermediate frame is synthesized via the dynamic local convolution. More specifically, the intermediate frame is obtained by filtering the intermediate candidates, given by</p><formula xml:id="formula_10">I t (x, y) = 6 c=1 2 i=−2 2 j=−2 F x,y t (i, j, c)I c t (x + i, y + j).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>The proposed algorithm includes two neural networks: the bilateral motion network and the dynamic filter generation network. We found that separate training of these two networks is more efficient than the end-to-end training in training time and memory space. Thus, we first train the bilateral motion network. Then, after fixing it, we train the dynamic filter generation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilateral motion network:</head><p>To train the proposed bilateral motion network, we define the bilateral loss L b as</p><formula xml:id="formula_11">L b = L p + L s<label>(9)</label></formula><p>where L p and L s are the photometric loss <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref> and the smoothness loss <ref type="bibr" target="#b16">[17]</ref>. For the photometric loss, we compute the sum of differences between a ground-truth frame I l t and two warped frames I l 0→t and I l 1→t using the bilateral motion fields V l t→0 and V l t→1 , respectively, at all pyramid levels,</p><formula xml:id="formula_12">L p = L l=1 α l x ρ(I l 0→t (x) − I l t (x)) + ρ(I l 1→t (x) − I l t (x))<label>(10)</label></formula><p>where ρ(x) = √ x 2 + 2 is the Charbonnier function <ref type="bibr" target="#b22">[23]</ref>. The parameters α l and are set to 0.01 × 2 l and 10 −6 , respectively. Also, we compute the smoothness loss to constrain neighboring pixels to have similar motions, given by</p><formula xml:id="formula_13">L s = ∇V t→0 1 + ∇V t→1 1 .<label>(11)</label></formula><p>We use the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with a learning rate of η = 10 −4 and shrink it via η ← 0.5η at every 0.5M iterations. We use a batch size of 4 for 2.5M iterations and augment the training dataset by randomly cropping 256 × 256 patches with random flipping and rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic filter generation network:</head><p>We define the dynamic filter loss L d as the Charbonnier loss between I t and its synthesized versionÎ t , given by</p><formula xml:id="formula_14">L d = x ρ(Î t (x) − I t (x)).<label>(12)</label></formula><p>Similarly to the bilateral motion network, we use the Adam optimizer with η = 10 −4 and shrink it via η ← 0.5η at 0.5M, 0.75M, and 1M iterations. We use a batch size of 4 for 1.25M iterations. Also, we use the same augmentation technique as that for the bilateral motion network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We use the Vimeo90K dataset <ref type="bibr" target="#b31">[32]</ref> to train the proposed networks. The training set in Vimeo90K is composed of 51,312 triplets with a resolution of 448 × 256. We train the bilateral motion network with t = 0.5 at the first 1M iterations and then with t ∈ {0, 0.5, 1} for fine-tuning. Next, we train the dynamic filter generation network with t = 0.5. However, notice that both networks are capable of handling any t ∈ (0, 1) using the bilateral cost volume in (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate the performances of the proposed video interpolation algorithm on the Middlebury <ref type="bibr" target="#b0">[1]</ref>, Vimeo90K <ref type="bibr" target="#b31">[32]</ref>, UCF101 <ref type="bibr" target="#b27">[28]</ref>, and Adobe240-fps <ref type="bibr" target="#b28">[29]</ref> datasets. We compare the proposed algorithm with state-of-the-art algorithms. Then, we conduct ablation studies to analyze the contributions of the proposed bilateral motion network and dynamic filter generation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Middlebury: The Middlebury benchmark <ref type="bibr" target="#b0">[1]</ref>, the most commonly used benchmark for video interpolation, provides two sets: Other and Evaluation. 'Other' contains the ground-truth for fine-tuning, while 'Evaluation' provides two frames selected from each of 8 sequences for evaluation.</p><p>Vimeo90K: The test set in Vimeo90K <ref type="bibr" target="#b31">[32]</ref> contains 3,782 triplets of spatial resolution 256 × 448. It is not used to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF101</head><p>: The UCF101 dataset <ref type="bibr" target="#b27">[28]</ref> contains human action videos of resolution 256 × 256. Liu et al. <ref type="bibr" target="#b16">[17]</ref> constructed the test set by selecting 379 triplets.</p><p>Adobe240-fps: Adobe240-fps <ref type="bibr" target="#b28">[29]</ref> consists of high frame-rate videos. To assess the interpolation performance, we selected a test set of 254 sequences, each of which consists of nine frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with the state-of-the-arts</head><p>We assess the interpolation performances of the proposed algorithm in comparison with the conventional video interpolation algorithms: MIND <ref type="bibr" target="#b17">[18]</ref>, DVF <ref type="bibr" target="#b16">[17]</ref>, SpyNet <ref type="bibr" target="#b24">[25]</ref>, SepConv <ref type="bibr" target="#b21">[22]</ref>, CtxSyn <ref type="bibr" target="#b19">[20]</ref>, ToFlow <ref type="bibr" target="#b31">[32]</ref>, SuperSloMo <ref type="bibr" target="#b13">[14]</ref>, MEMC-Net <ref type="bibr" target="#b2">[3]</ref>, CyclicGen <ref type="bibr" target="#b15">[16]</ref>, and DAIN <ref type="bibr" target="#b1">[2]</ref>. For SpyNet, we generated intermediate frames using the Baker et al.'s algorithm <ref type="bibr" target="#b0">[1]</ref>. <ref type="table">Table 1</ref> shows the comparisons on the Middlebury Evaluation set <ref type="bibr" target="#b0">[1]</ref>, which are also available on the Middlebury website. We compare the average interpolation error (IE) and normalized interpolation error (NIE). A lower IE or NIE indicates better performance. The proposed algorithm outperforms all the state-of-the-art algorithms in terms of average IE and NIE scores. <ref type="figure" target="#fig_7">Fig. 7</ref> visually compares interpolation results. SepConv-L 1 <ref type="bibr" target="#b21">[22]</ref>, ToFlow <ref type="bibr" target="#b31">[32]</ref> SuperSlomo <ref type="bibr" target="#b13">[14]</ref>, <ref type="table">Table 1</ref>. Quantitative comparisons on the Middlebury Evaluation set. For each metric, the numbers in red and blue denote the best and the second best results, respectively.  CtxSyn <ref type="bibr" target="#b19">[20]</ref>, MEMC-Net <ref type="bibr" target="#b2">[3]</ref>, and DAIN <ref type="bibr" target="#b1">[2]</ref> yield blurring artifacts around the balls, losing texture details. On the contrary, the proposed algorithm reconstructs the clear shapes of the balls, preserving the details faithfully.</p><formula xml:id="formula_15">(a) (b) (c) (d) (e) (f) (g) (h)</formula><p>In <ref type="table" target="#tab_4">Table 2</ref>, we provide quantitative comparisons on the UCF101 <ref type="bibr" target="#b27">[28]</ref> and Vimeo90K <ref type="bibr" target="#b31">[32]</ref> datasets. We compute the average PSNR and SSIM scores. The proposed algorithm outperforms the conventional algorithms by significant margins. Especially, the proposed algorithm provides 0.3 dB higher PSNR than DAIN [2] on Vimeo90K. <ref type="figure">Fig. 8</ref> compares interpolation results qualitatively. Because the cable moves rapidly and the background branches make the motion estimation difficult, all the conventional algorithms fail to reconstruct the cable properly. In contrast, the proposed algorithm faithfully interpolates the intermediate frame, providing fine details.</p><p>The proposed algorithm can interpolate an intermediate frame at any time instance t ∈ (0, 1). To demonstrate this capability, we assess the ×2, ×4, and ×8 frame interpolation performance on the Adobe240-fps dataset <ref type="bibr" target="#b28">[29]</ref>. Because the conventional algorithms in <ref type="table" target="#tab_6">Table 3</ref>, except for DAIN <ref type="bibr" target="#b1">[2]</ref>, can generate only intermediate frames at t = 0.5, we recursively apply those algorithms to interpolate intermediate frames at other t's. <ref type="table" target="#tab_6">Table 3</ref> shows that the proposed algorithm outperforms all the state-of-the-art algorithms. As the frame rate increases, the performance gain of the proposed algorithm against conventional algorithms gets larger. In <ref type="figure">Fig. 9</ref>, the proposed algorithm reconstructs the details in the rotating wheel more faithfully than the conventional algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model analysis</head><p>We conduct ablation studies to analyze the contributions of the three key components in the proposed algorithm:  -BM+Appx2: In addition to BM, two more candidates obtained using approximated bilateral motions V f w t→0 in (4) and V bw t→1 in (6) are used. -BM+Appx4: Six intermediate candidates are used as well (proposed model). <ref type="table">Table 4</ref> compares these models quantitatively. First, Appx4 shows the worst performance, while it is still comparable to the state-of-the-art algorithms. Second, BM provides better performance than Appx4 as well as the state-of-the-art algorithms, which confirms the superiority of the proposed BM to the approximation. Third, we can achieve even higher interpolation performance with more intermediate candidates obtained through the motion approximation.</p><p>Dynamic blending filters We analyze the optimal kernel size and the input to the dynamic filter generation network. <ref type="table">Table 5</ref> compares the PSNR performances of different settings. First, the kernel size has insignificant impacts, although the computational complexity is proportional to the kernel size. Next, when additional information (input frames and context maps) is fed into the dynamic filter generation network, the interpolation performance is improved. More specifically, using input frames improves PSNRs by 0.10 and 0.15 dB on the UCF101 and Vimeo90K datasets. Also, using context maps further improves the performances by 0.07 and 0.05dB. This is because the input frames and context maps help restore geometric structure and exploit rich contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We developed a deep-learning-based video interpolation algorithm based on the bilateral motion estimation, which consists of the bilateral motion network and the dynamic filter generation network. In the bilateral motion network, we developed the bilateral cost volume to estimate accurate bilateral motions. In the dynamic filter generation network, we warped the two input frames using the estimated bilateral motions and fed them to learn filter coefficients. Finally, we synthesized the intermediate frame by superposing the warped frames with the generated blending filters. Experimental results showed that the proposed algorithm outperforms the state-of-the-art video interpolation algorithms on four benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the bilateral cost volume layer for a specific time t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of interpolation results obtained by the bilinear motion estimation and the motion approximation: (a) ground-truth intermediate frame; (b), (c) enlarged parts for the green and yellow squares in (a); (d), (e) interpolation results using the bilateral motion estimation; (f), (g) those using the motion approximation. The red squares in (d) and (g) contain visual artifacts caused by motion inaccuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Motion approximation: bi-directional motions in (a) are used to approximate forward bilateral motions in (b) and backward bilateral motions in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.</head><label></label><figDesc>Fig. 5 illustrates this motion approximation, in which each column represents a frame at a time instance and a dot corresponds to a pixel in the frame. In particular, in Fig. 5(a), an occluded pixel in I 0 is depicted by a green dot. To complement the inaccuracy of the bilateral motion at pixel x in I t , we use two bi-directional motions V 0→1 (x) and V 1→0 (x), which are depicted by green and red lines, respectively. We approximate two forward bilateral motions V f w t→1 and V f w t→0 in Fig. 5(b) using V 0→1 . Specifically, for pixel x in I t , depicted by an orange dot, we approximate a motion vector V f w t→1 (x) by scaling V 0→1 (x) with a factor (1−t), assuming that the motion vector field is locally smooth. Since the bilateral motion estimation is based on the assumption that a motion trajectory between consecutive frames is linear, two approximate motions V f w t→1 (x) and V f w t→0 (x) should be symmetric with respect to x in I t . Thus, we obtain an additional approximate vector V f w t→0 (x) by reversing the direction of the vector V f w t→1 (x). In other words, we approximate the forward bilateral motions by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Frame synthesis using dynamic local blending filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Visual comparison on the Middlebury Evaluation set. (a) Input, (b) SepConv-L1 [22], (c) ToFlow [32], (d) SuperSlomo [14], (e) CtxSyn [20], (f) MEMC-Net* [3], (g) DAIN [2], and (h) BMBC (Ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>intermediate motions (i.e. motion vectors of intermediate frames) should be estimated. Jiang et al. [14] estimated optical flows and performed bilateral motion approximation to predict intermediate motions from the optical flows. Bao et al. [3] approximated intermediate motions based on the flow projection. However, large errors may occur when two flows are projected onto the same pixel. In [2], Bao et al. proposed an advanced projection method using the depth information. However, the resultant intermediate motions are sensitive to the depth estimation performance. To summarize, although the backward warping yields reasonable video interpolation results, its performance degrades severely when intermediate motions are unreliable or erroneous. To solve this problem, we propose the bilateral motion network to estimate intermediate motions directly.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>generation network takes the input frames I 0 and I 1 and the intermediate candidates {I 1:6 t } with the corresponding context maps {C 1:6 t } as input. Then, for each pixel x = (x, y), we generate six blending filters to fuse the six intermediate candidates, given by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons on the UCF101 and Vimeo90K datasets.</figDesc><table><row><cell></cell><cell></cell><cell>Runtime</cell><cell>#Parameters</cell><cell cols="2">UCF101 [17]</cell><cell cols="2">Vimeo90K [32]</cell></row><row><cell></cell><cell></cell><cell>(seconds)</cell><cell>(million)</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>SpyNet [25]</cell><cell></cell><cell>0.11</cell><cell>1.20</cell><cell>33.67</cell><cell>0.9633</cell><cell>31.95</cell><cell>0.9601</cell></row><row><cell>MIND [18]</cell><cell></cell><cell>0.01</cell><cell>7.60</cell><cell>33.93</cell><cell>0.9661</cell><cell>33.50</cell><cell>0.9429</cell></row><row><cell>DVF [17]</cell><cell></cell><cell>0.47</cell><cell>1.60</cell><cell>34.12</cell><cell>0.9631</cell><cell>31.54</cell><cell>0.9462</cell></row><row><cell>ToFlow [32]</cell><cell></cell><cell>0.43</cell><cell>1.07</cell><cell>34.58</cell><cell>0.9667</cell><cell>33.73</cell><cell>0.9682</cell></row><row><cell cols="2">SepConv-L f [22]</cell><cell>0.20</cell><cell>21.6</cell><cell>34.69</cell><cell>0.9655</cell><cell>33.45</cell><cell>0.9674</cell></row><row><cell cols="2">SepConv-L1 [22]</cell><cell>0.20</cell><cell>21.6</cell><cell>34.78</cell><cell>0.9669</cell><cell>33.79</cell><cell>0.9702</cell></row><row><cell cols="2">MEMC-Net [3]</cell><cell>0.12</cell><cell>70.3</cell><cell>34.96</cell><cell>0.9682</cell><cell>34.29</cell><cell>0.9739</cell></row><row><cell cols="2">CyclicGen [16]</cell><cell>0.09</cell><cell>3.04</cell><cell>35.11</cell><cell>0.9684</cell><cell>32.09</cell><cell>0.9490</cell></row><row><cell cols="2">CyclicGen large [16]</cell><cell>-</cell><cell>19.8</cell><cell>34.69</cell><cell>0.9658</cell><cell>31.46</cell><cell>0.9395</cell></row><row><cell>DAIN [2]</cell><cell></cell><cell>0.13</cell><cell>24.0</cell><cell>34.99</cell><cell>0.9683</cell><cell>34.71</cell><cell>0.9756</cell></row><row><cell cols="2">BMBC (Ours)</cell><cell>0.77</cell><cell>11.0</cell><cell cols="4">35.15 0.9689 35.01 0.9764</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell><cell></cell><cell>(f)</cell><cell>(g)</cell></row><row><cell cols="8">Fig. 8. Visual comparison on the Viemo90K test set. (a) Ground-truth, (b)</cell></row><row><cell cols="8">ToFlow [32], (c) SepConv-L f [22], (d) CyclicGen [16], (e) MEMC-Net* [3], (f) DAIN [2],</cell></row><row><cell cols="2">and (g) BMBC (Ours).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>bilateral cost volume, intermediate motion approximation, and dynamic filter generation network. By comparing various combinations of intermediate candidates, we analyze the efficacy of the bilateral cost volume and the intermediate motion approximation jointly. To analyze the effectiveness of the bilateral motion estimation and the intermediate motion approximation, we train the proposed networks to synthesize intermediate frames using the following combinations:</figDesc><table><row><cell>Intermediate candidates:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparisons on the Adobe240-fps dataset for ×2, ×4, and ×8 frame interpolation.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">×2</cell><cell cols="2">×4</cell><cell cols="2">×8</cell></row><row><cell></cell><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell cols="2">ToFlow [32]</cell><cell>28.51</cell><cell>0.8731</cell><cell>29.20</cell><cell>0.8807</cell><cell>28.93</cell><cell>0.8812</cell></row><row><cell cols="2">SepConv-L f [22]</cell><cell>29.14</cell><cell>0.8784</cell><cell>29.75</cell><cell>0.8907</cell><cell>30.07</cell><cell>0.8956</cell></row><row><cell cols="2">SepConv-L1 [22]</cell><cell>29.31</cell><cell>0.8815</cell><cell>29.91</cell><cell>0.8935</cell><cell>30.23</cell><cell>0.8985</cell></row><row><cell cols="2">CyclicGen [16]</cell><cell>29.39</cell><cell>0.8787</cell><cell>29.72</cell><cell>0.8889</cell><cell>30.18</cell><cell>0.8972</cell></row><row><cell cols="3">CyclicGen large [16] 28.90</cell><cell>0.8682</cell><cell>29.70</cell><cell>0.8866</cell><cell>30.24</cell><cell>0.8955</cell></row><row><cell cols="2">DAIN [2]</cell><cell>29.35</cell><cell>0.8820</cell><cell>29.73</cell><cell>0.8925</cell><cell>30.03</cell><cell>0.8983</cell></row><row><cell cols="2">BMBC (Ours)</cell><cell cols="6">29.49 0.8832 30.18 0.8964 30.60 0.9029</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell></cell><cell>(e)</cell><cell></cell><cell>(f)</cell><cell>(g)</cell></row><row><cell cols="8">Fig. 9. Visual comparison on the Adobe240-fps dataset. (a) Ground-truth, (b)</cell></row><row><cell cols="8">ToFlow [32], (c) SepConv-L f [22], (d) SepConv-L1 [22], (e) CyclicGen [16], (f) DAIN [2],</cell></row><row><cell cols="2">and (g) BMBC (Ours).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">-Appx4: Four intermediate candidates, obtained using approximated bilateral</cell></row><row><cell cols="4">motions in (3)∼(6), are combined.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">-BM: Two intermediate candidates, obtained using bilateral motions, are</cell></row><row><cell>combined.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>PSNR comparison of combination of the intermediate candidates. Analysis of the dynamic filter generation network. In all settings, the six warped frames are input to the network.</figDesc><table><row><cell></cell><cell cols="2">Intermediate candidates</cell><cell>UCF101 [28]</cell><cell>Vimeo90K [32]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell>PSNR</cell></row><row><cell></cell><cell>Appx4</cell><cell></cell><cell>34.99</cell><cell>34.72</cell></row><row><cell></cell><cell>BM</cell><cell></cell><cell>35.12</cell><cell>34.93</cell></row><row><cell></cell><cell>BM+Appx2</cell><cell></cell><cell>35.14</cell><cell>34.95</cell></row><row><cell></cell><cell>BM+Appx4</cell><cell></cell><cell>35.15</cell><cell>35.01</cell></row><row><cell>Kernel size</cell><cell cols="3">Input to filter generation network</cell><cell>UCF101 [28]</cell><cell>Vimeo90K [32]</cell></row><row><cell></cell><cell>Input frames</cell><cell cols="2">Context maps</cell><cell>PSNR</cell><cell>PSNR</cell></row><row><cell>5 × 5</cell><cell></cell><cell></cell><cell></cell><cell>34.98</cell><cell>34.81</cell></row><row><cell>3 × 3</cell><cell></cell><cell></cell><cell></cell><cell>35.09</cell><cell>34.90</cell></row><row><cell>5 × 5</cell><cell></cell><cell></cell><cell></cell><cell>35.08</cell><cell>34.96</cell></row><row><cell>7 × 7</cell><cell></cell><cell></cell><cell></cell><cell>35.02</cell><cell>34.98</cell></row><row><cell>5 × 5</cell><cell></cell><cell></cell><cell></cell><cell>35.15</cell><cell>35.01</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<ptr target="http://vision.middlebury.edu/flow/eval/10" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MEMC-Net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019-09-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Motion-compensated frame interpolation using bilateral motion estimation and adaptive overlapped block motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="407" to="416" />
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepStereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Novel integration of frame rate up conversion and hevc coding based on rate-distortion optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="504" to="511" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motion-compensated frame interpolation based on multihypothesis motion estimation and texture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4497" to="4509" />
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">Dynamic filter networks. NIPS pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Super SloMo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep video frame interpolation using cyclic frame generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019-01-01" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PhaseNet for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two deterministic halfquadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="1994-11" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Qifeng Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017-02" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Medical Image Comput. Comput.-Assisted Intervention</title>
		<meeting>Int. Conf. Medical Image Comput. Comput.-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015-10" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Video enhancement with task-oriented flow. IJCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
