<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECON: Relation Extraction using Knowledge Graph Context in a Graph Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anson</forename><surname>Bastos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerotha</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aachen</forename><surname>Rwth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Germany</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaiah</forename><forename type="middle">Onando</forename><surname>Mulang&amp;apos;</surname></persName>
							<email>mulang@iai.uni-bonn.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
							<email>johannes.hoffart@gs.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goldman</forename><surname>Sachs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germany</forename><forename type="middle">Manohar</forename><surname>Kaul</surname></persName>
							<email>mkaul@iith.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anson</forename><surname>Bastos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
							<email>abhishek22596@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
							<email>kuldeep.singh1@cerence.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaiah</forename><forename type="middle">Onando</forename><surname>Mulang&amp;apos;</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IIT, Hyderabad and Zerotha Research India Abhishek Nadgeri</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Zerotha Research and Cerence GmbH</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Dayton</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RECON: Relation Extraction using Knowledge Graph Context in a Graph Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Zerotha Research and Fraunhofer IAIS, Germany Saeedeh Shekarpour Saeedeh Shekarpour, Johannes Hoffart, and Manohar Kaul. 2020. RECON: Relation Extraction using Knowledge Graph Context in a Graph Neural Net-work. In WWW &apos;21: The Web Conference, April 19-23, 2021, Ljubljana, Slove-nia. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn. nnnnnnn</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel method named RECON, that automatically identifies relations in a sentence (sentential relation extraction) and aligns to a knowledge graph (KG). RECON uses a graph neural network to learn representations of both the sentence as well as facts stored in a KG, improving the overall extraction quality. These facts, including entity attributes (label, alias, description, instance-of) and factual triples, have not been collectively used in the state of the art methods. We evaluate the effect of various forms of representing the KG context on the performance of RECON. The empirical evaluation on two standard relation extraction datasets shows that RECON significantly outperforms all state of the art methods on NYT Freebase and Wikidata datasets. ACM Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The publicly available Web-scale knowledge graphs (KGs) (e.g., DBpedia <ref type="bibr" target="#b0">[1]</ref>, Freebase <ref type="bibr" target="#b1">[2]</ref>, and Wikidata <ref type="bibr" target="#b24">[25]</ref>) find wide usage in many real world applications such as question answering, fact checking, voice assistants, and search engines <ref type="bibr" target="#b4">[5]</ref>. Despite the success and popularity, these KGs are not exhaustive. Hence there is a need for approaches that automatically extract knowledge from unstructured text into the KGs <ref type="bibr" target="#b11">[12]</ref>. Distantly supervised relation extraction (RE) is one of the knowledge graph completion tasks aiming at determining the entailed relation between two given entities annotated on the text to a background KG <ref type="bibr" target="#b28">[29]</ref>. For example, given the sentence "Bocelli also took part in the Christmas in Washington special on Dec 12, in the presence of president Barack Obama and the first lady" with annotated entities-wdt:Q76 (Barack Obama) <ref type="bibr" target="#b0">1</ref> and wdt:Q13133(Michelle Obama); the RE task aims to infer the semantic relationship. Here wdt:P26 (spouse) is the target relation. In this example, one can immediately see the impact of background knowledge: the correct target relation spouse is not explicitly stated in the sentence, but given background knowledge about the first lady and her marital status, the correct relation can be inferred by the model. In cases having no relations, the label "NA" is predicted.</p><p>Existing RE approaches have mainly relied on the multi-instance and distant learning paradigms <ref type="bibr" target="#b19">[20]</ref>. Given a bag of sentences (or instances), the multi-instance RE considers all previous occurrences of a given entity pair while predicting the target relation <ref type="bibr" target="#b22">[23]</ref>. However, incorporating contextual signals from the previous occurrences of entity pair in the neural models add some noise in the training data, resulting in a negative impact on the overall performance <ref type="bibr" target="#b13">[14]</ref>. Several approaches (e.g., based on attention mechanism <ref type="bibr" target="#b30">[31]</ref>, neural noise converter <ref type="bibr" target="#b28">[29]</ref>) have been proposed to alleviate the noise from the previous sentences for improving overall relation extraction. Additionally, to mitigate the noise in multi-instance setting, there are few approaches that not only use background KGs as a source of target relation but exploit specific properties of KGs as additional contextual features for augmenting the learning model. Earlier work by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> utilizes entity descriptions and entity/relation aliases from the underlying KG as complementary features. Work in <ref type="bibr" target="#b16">[17]</ref> employs attention-based embeddings of KG triples to feed in a graph attention network for capturing the context. Overall, the knowledge captured from KG complements the context derived from the text.</p><p>In contrast, the sentential RE <ref type="bibr" target="#b20">[21]</ref> ignores any other occurrence of the given entity pair, thereby making the target relation predictions on the sentence level. However, the existing approaches for sentential RE <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref> rely on local features/context present in the sentence and do not incorporate any external features. In this paper, we study the effect of KG context on sentential RE task by proposing a novel method RECON. Our approach focuses on an effective representation of the knowledge derived from the KG induced in a graph neural network (GNN). The proposed approach has three building blocks illustrated in the <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, <ref type="figure" target="#fig_0">Figure 1</ref>: RECON has three building blocks: i) entity attribute context (EAC) encodes context from entity attributes ii) triple context learner independently learns relation and entity embeddings of the KG triples in separate vector spaces iii) a context aggregator (a GNN model) used for consolidating the KG contexts to predict target relation.</p><p>RECON harnesses the following three novel insights to outperform existing sentential and multi-instance RE methods:</p><p>• Entity Attribute Context: we propose a recurrent neural network based module that learns representations of the given entities expanded from the KG using entity attributes (properties) such as entity label, entity alias, entity description and entity Instance of (entity type). • Triple Context Learner: we aim to utilize a graph attention mechanism to capture both entity and relation features in a given entity's multi-hop neighborhood. By doing so, our hypothesis is to supplement the context derived from the previous module with the additional neighborhood KG triple context. For the same, the second module of RECON independently yet effectively learns entity and relation embeddings of the 1&amp;2-hop triples of entities using a graph attention network (GAT) <ref type="bibr" target="#b23">[24]</ref>. • Context Aggregator: our idea is to exploit the message passing capabilities of a graph neural network <ref type="bibr" target="#b31">[32]</ref> to learn representations of both the sentence and facts stored in a KG. Hence, in the third module of RECON, we employ an aggregator consisting of a GNN and a classifier. It receives as input the sentence embeddings, entity attribute context embeddings, and the triple context embeddings. The aggregator then obtains a homogeneous representation, passed into a classifier to predict the correct relation.</p><p>We perform exhaustive evaluation to understand the efficacy of RECON in capturing the KG context. Our work has following contributions:</p><p>• RECON: a sentential RE approach that utilizes entity attributes and triple context derived from the Web scale knowledge graphs, induced in a GNN, thereby significantly outperforming the existing baselines on two standard datasets.</p><p>• We augment two datasets: Wikidata dataset <ref type="bibr" target="#b20">[21]</ref> and NYT dataset for Freebase <ref type="bibr" target="#b17">[18]</ref> with KG context. Our implementation and datasets are publicly available 2 . The structure of the paper is follows: Section 2 reviews the related work. Section 3 formalizes the problem and the proposed approach. Section 4 describes experiment setup. Our results and ablation studies are illustrated in Section 5. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multi-instance RE: The recent success in RE can attribute to the availability of vast training data curated using distant supervision <ref type="bibr" target="#b14">[15]</ref>. Methods for distant supervision assume that if two entities have a relationship in a KG, then all sentences containing those entities express the same relation, this may sometimes lead to noise in the data. To overcome the challenges, researchers in <ref type="bibr" target="#b17">[18]</ref> initiated the multi-instance learning followed by <ref type="bibr" target="#b6">[7]</ref> which extracted relation from a bag of sentences. Researchers <ref type="bibr" target="#b9">[10]</ref> attained improved performance by introducing entity descriptions as KG context to supplement the task. The RESIDE approach <ref type="bibr" target="#b22">[23]</ref> ignores entity descriptions but utilize entity type along with relation and entity aliases. RELE approach <ref type="bibr" target="#b8">[9]</ref> jointly learned embeddings of structural information from KGs and textual data from entity descriptions to improve multi-instance RE. Unlike existing approaches where one or other entity attributes are considered, in this work, we combined four typical properties of KG entities for building what we refer as entity attribute context. Learning information from KG Triples: The survey <ref type="bibr" target="#b25">[26]</ref> provides holistic overview of available KG embedding techniques and their application in entity oriented tasks. TransE <ref type="bibr" target="#b2">[3]</ref> studied knowledge base completion task using entity and relation embeddings learned in the same vector space. It lacks ability to determine oneto-many, many-to-one, and many-to-many relations. TransH <ref type="bibr" target="#b26">[27]</ref> has tried to address this problem by learning embeddings on different hyperplanes per relation. However, the entity and relation embeddings are still learned in the same space. TransR <ref type="bibr" target="#b11">[12]</ref> represents entity and relation embeddings in separate vector spaces, which works better on the task of relation prediction and triple classification. They perform a linear transformation from entity to relation embedding vector space. Work by <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b5">[6]</ref> are few attempts for jointly learning different representations from text and facts in an existing knowledge graph. Furthermore, graph attention network (GAT) has been proposed to learn embeddings for graphstructured data <ref type="bibr" target="#b23">[24]</ref>. KBGAT is an extension of GAT that embeds KG triples by training entities and relations in same vector spaces specifically for relation prediction <ref type="bibr" target="#b16">[17]</ref>. However, we argue that entity and relation embedding space should be separated. Moreover, the transformation from entity to relation space should be nonlinear and distinct for every relation. This setting allows the embeddings to be more expressive (section 5). Sentential RE: There exists a little work on the sentential RE task. The work in <ref type="bibr" target="#b20">[21]</ref> established an LSTM-based baseline that learns context from other relations in the sentence when predicting the target relation. <ref type="bibr" target="#b31">[32]</ref> generate the parameters of graph neural networks (GP-GNN) according to natural language sentences for multi-hop relation reasoning for the entity pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM STATEMENT AND APPROACH 3.1 Problem Statement</head><p>We define a KG as a tuple = (E, R, T + ) where E denotes the set of entities (vertices), R is the set of relations (edges), and T + ⊆ E × R × E is a set of all triples. A triple τ = ( ℎ , , ) ∈ T + indicates that, for the relation ∈ R, ℎ is the head entity (origin of the relation) while is the tail entity. Since is a multigraph; ℎ = may hold and |{ ℎ , }| ≥ 0 for any two entities. We define the tuple ( , ) = ( ) obtained from a context retrieval function , that returns, for any given entity , two sets: , a set of all attributes and ⊂ T + the set of all triples with head at .</p><p>A sentence W = ( 1 , 2 , ..., ) is a sequence of words. The set of entities in a sentence is denoted by M = { 1 , 2 , ..., } where every = ( , ..., ) is a segment of the sentence W. Each mention is annotated by an entity from KG as [ : ] where ∈ E. Two annotated entities form a pair = ⟨ , ⟩ when there exists a relationship between them in the sentence (in case no corresponding relation in the KG -label N/A).</p><p>The RE Task predicts the target relation ∈ R for a given pair of entities ⟨ , ⟩ within the sentence W. If no relation is inferred, it returns 'NA' label. We attempt the sentential RE task which posits that the sentence within which a given pair of entities occurs is the only visible sentence from the bag of sentences. All other sentences in the bag are not considered while predicting the correct relation . Similar to other researchers <ref type="bibr" target="#b20">[21]</ref>, we view RE as a classification task. However, we aim to model KG contextual information to improve the classification. This is achieved by learning representations of the sets , , and W as described in section 3.2. The EAC module (sec. 3.2.1) takes each entity of the sentence and enrich the entity embeddings with corresponding contextual representation from the KG using entity properties such as aliases, label, description, and instance-of. The Triple context learner module (sec. 3.2.2) learns a representation of entities and relations in a given entity's 2-hop neighborhood. A Graph Neural Network is finally used to aggregate the entity attribute, KG triple, and sentence contexts with a relation classification layer generating the final output (sec. 3.2.3). We now present our approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RECON Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Entity Attribute Context (EAC).</head><p>The entity attribute context is built from commonly available properties of a KG entity <ref type="bibr" target="#b7">[8]</ref>: entity labels, entity alias, entity description, and entity Instance of. We extract this information for each entity from the public dump of Freebase <ref type="bibr" target="#b1">[2]</ref>, and Wikidata <ref type="bibr" target="#b24">[25]</ref>) depending on the underlying KG (cf. section 4). To formulate our input, we consider the literals of the retrieved entity attributes. For each of these attributes, we concatenate the word and character embeddings and pass them through a bidirectional-LSTM encoder <ref type="bibr" target="#b18">[19]</ref>. The final outputs from the BiLSTM network are stacked and given to a one dimensional convolution network (CNN) described in the <ref type="figure" target="#fig_1">Figure 2</ref> and formalized in equation 1. The reasons behind choosing CNN are i) to enable a dynamic number of contexts using the max pooling ii) to keep the model invariant of the order in which the context is fed.</p><formula xml:id="formula_0">ℎ = 1D_CNN( ∥ =0 [BiLSTM( )])<label>(1)</label></formula><p>where each is attribute of given entity and ∥ is the concatenation. is an extension of KBGAT <ref type="bibr" target="#b16">[17]</ref> that retains the capability to capture context from neighboring triples in the KG. In addition, our idea is to learn the entity and relation embeddings of the triples in separate vector spaces to capture more expressive representations. This is because each entity might be engaged in several relations in various contexts, and different aspects of the entity may participate in representing each relation <ref type="bibr" target="#b11">[12]</ref>. Let ì ℎ and ì be the initial entity vectors and ì be a initial relation vector between them representing the triple τ ℎ , is the weight metric, then the vector representation of triple is</p><formula xml:id="formula_1">ì τ ℎ = [ ì ℎ ∥ ì ∥ ì ]<label>(2)</label></formula><p>where we concatenate the head and tail entity embeddings and relation embedding vector. The importance of each triple (i.e. attention values) is represented by ℎ and is computed as in equation <ref type="bibr" target="#b2">3</ref> where LeakyReLU is an activation function:</p><formula xml:id="formula_2">ℎ = ( 2 ì τ ℎ )<label>(3)</label></formula><p>To get the relative attention values over the neighboring triples, a softmax is applied to equation 3</p><formula xml:id="formula_3">ℎ = ( ℎ ) = ( ℎ ) ∈ ℎ ∈ ℎ ( ℎ )<label>(4)</label></formula><p>ℎ denotes the neighborhood of entity ℎ and ℎ denotes the set of relations between entities ℎ and . The new embedding for the entity ℎ is now the weighted sum of the triple embeddings using equations 2 and 4. In order to stabilize the learning and encapsulate more information, X independent attention heads have been used and the final embedding is the concatenation of the embedding from each head:</p><formula xml:id="formula_4">ì ′ ℎ = =1 ∑︁ ∈ ℎ ∑︁ ∈ ℎ ℎ ì τ ℎ<label>(5)</label></formula><p>The original entity embedding ì ℎ after a transformation, using matrix , is added to the equation 5 to preserve the initial entity embedding information.</p><formula xml:id="formula_5">ì ′′ ℎ = ì ′ ℎ + ì ℎ<label>(6)</label></formula><p>For relation embeddings, a linear transformation is performed on the initial embedding vector, using matrix , to match the entity vector's dimension in equation 6</p><formula xml:id="formula_6">ì ′′ = ì<label>(7)</label></formula><p>Traditionally, the training objective for learning embeddings in same vector spaces are borrowed from <ref type="bibr" target="#b2">[3]</ref>. The embeddings here are learned such that, for a valid triple τ ℎ = ( ℎ , , ) the following equation holds where ì ′′ is embeddings in entity space.</p><formula xml:id="formula_7">ì ′′ ℎ + ì ′′ = ì ′′<label>(8)</label></formula><p>The optimization process tries to satisfy equation 8 and the vectors are learned in same vector space. Contrary to the previous equation, we keep entities and relation embeddings in separate spaces. With that, we now need to transform entities from entity spaces to the relation space. We achieve this by applying a nonlinear transformation: (cf. the theoretical foundation is in the section 7.1).</p><formula xml:id="formula_8">ì = ì ′′ ℎ<label>(9)</label></formula><p>here ì (where = {ℎ, }) is the relation specific entity vector in the relation embedding space, is the relation specific transformation matrix and ì ′′ ℎ is the corresponding embedding in the entity space from equation 6. We presume that such separation helps to capture a comprehensive representations for relations and entities. Equation 8 is now modified as</p><formula xml:id="formula_9">ì ℎ + ì ′′ = ì<label>(10)</label></formula><p>We define a distance metric ℎ for a relation ì ′′ , representing the triple τ ℎ as</p><formula xml:id="formula_10">τ ℎ = ì ℎ + ì ′′ − ì<label>(11)</label></formula><p>A margin ranking loss minimizes the following expression</p><formula xml:id="formula_11">(Ω) = ∑︁ τ ℎ ∈T ∑︁ τ ′ ℎ ∈T { τ ′ ℎ − τ ℎ + , 0}<label>(12)</label></formula><p>where T is the set of valid triples, T is the set of invalid triples and is a margin parameter. We consider the actual triples present in the dataset as positive (valid) triples and the rest of the triples, which are not in the dataset as invalid. For example, as we do RE, if in the KG, entities Barack Obama and Michelle Obama have one valid relation "spouse," then the valid triple is &lt;Barack Obama, spouse, Michelle Obama&gt;. The invalid triples will contain relations that do not exist between these two entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Aggregating KG Context.</head><p>For aggregating context from previous two steps, we adapt and modify generated parameter graph neural network (GP-GNN) <ref type="bibr" target="#b31">[32]</ref> due to its proven ability to enable message passing between nodes. It consists of an encoder module, a propagation module and a classification module. The encoder takes as input the word vectors concatenated to the position vectors from the sentence.</p><formula xml:id="formula_12">( , ) = ∥ ,<label>(13)</label></formula><p>where denotes the position embedding of word spot "s" in the sentence relative to the entity pair's position , and is the word embedding. Position vectors are basically to mark whether the token belongs to head or tail entity or none of them. We use position embedding scheme from <ref type="bibr" target="#b31">[32]</ref>. We use concatenated word embeddings in a biLSTM followed by a fully connected network for generating transition matrix given as:</p><formula xml:id="formula_13">( , ) = [ MLP( −1 BiLSTM =0 ( ( , ) =1 ) ]<label>(14)</label></formula><p>Here [.] denotes conversion of vectors into a matrix, is the layer of biLSTM, is the index of word in sentence and is the length of the sentence. For each layer (n) of the propagation module we learn a matrix ( ) , using equation 14. Then, the propagation module learns representations of entity nodes (layer wise) according to the following equation</p><formula xml:id="formula_14">ℎ ( +1) = ∑︁ ∈ ( ) ( ( ) , ℎ ( ) )<label>(15)</label></formula><p>( ) represents the neighborhood of . Here ℎ 0 is the initial entity embedding which is taken from equation 1. In classification module, the vectors learned by each layer in the propagation module are concatenated and used for linking the relation:</p><formula xml:id="formula_15">, = ∥ =1 [ℎ ( ) ⊙ ℎ ( ) ] ⊤<label>(16)</label></formula><p>where ⊙ denotes element wise multiplication. We concatenate the entity embeddings learned from the triples context in equation 6 to , obtained from 16 and feed into classification layer to get the probability of each relation</p><formula xml:id="formula_16">( | ℎ, , ) = (MLP([ , ∥ ì ′′ ℎ ∥ ì ′′ ]))<label>(17)</label></formula><p>where ì ′′ ℎ and ì ′′ are the entity embeddings learned from previous module in equation <ref type="bibr" target="#b5">6</ref>. Aggregating the separate space embeddings: The probability in equation 17 uses the embeddings learned in the same vector space. For the embeddings learned in separate vector spaces, we compute the similarity of the logits with the corresponding relation vector i.e. we use the embedding learned in equation 9 to find the probability of a triple exhibiting a valid relation. For the same, we concatenate the entity embeddings from equation 9 with the Equation <ref type="bibr" target="#b15">16</ref>. This is then transformed as below:</p><formula xml:id="formula_17">ℎ = [ ℎ , ∥ ì ℎ ∥ ì ]<label>(18)</label></formula><p>Where ℎ is a vector obtained by applying a non-linear function on the final representation in the aggregator. We then compute the distance between this embedding and the relation vector ì (aka ì ′′ ) obtained in the equation 7 to get the probability of the relation existing between the two entities.</p><formula xml:id="formula_18">( | ℎ, , W, , ) = ì ⊤ ℎ<label>(19)</label></formula><p>where ℎ, are the head and tail entities, W is the sentence, is the context, and is the computed graph. Optimizing equation 19 using binary cross entropy loss with negative sampling on the invalid triples is computationally expensive. Hence, we take the translation of this entity pair and compare with every relation. Specifically, obtain the norm of the distance metric as in 11 and concatenate these norms for every relation to get a translation vector.</p><formula xml:id="formula_19">τ ℎ = ì ℎ + ì ′′ − ì (20) ì ℎ = ∥ =1 τ ℎ<label>(21)</label></formula><p>ì ℎ is translation vector of the entity pair ℎ and which represents the closeness with each relation and is the number of relations in the KG. This is concatenated with the vectors learned from the propagation stage and entity embeddings to classify the target relation. </p><formula xml:id="formula_20">( | ℎ, , ) = (MLP([ , ∥ ì ℎ ∥ ì ∥ ì ℎ ))<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparative Models</head><p>We consider the recent state-of-the-art approaches for our comparison study as follows:</p><p>KBGAT <ref type="bibr" target="#b16">[17]</ref>: this open-source implementation is compared with our KGGAT-SEP for evaluating the effectiveness of our approach in learning the KG triple context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyperparameters and Metric</head><p>The EAC module (section 3.2.1) uses a biLSTM with one hidden layer of size 50. The convolution filter is of width one, and the output size is 8. In KGGAT-SEP (section 3.2.2), the initial entity and relation embedding size is 50, number of heads are two with two GAT layers, and the final entity and relation embedding size is 200. For the context aggregator module, we adapt the parameters provided in GP-GNN <ref type="bibr" target="#b31">[32]</ref>. The word embedding dimension is 50 initialized from the Glove embeddings. The position embedding is also kept at 50 dimensions. Encoder uses a layer of bidirectional LSTM of size 256. We use three propagation layers with the entity embedding dimension set at 8. For brevity, complete training details and validation results are in the public Github. Metric and Optimization: Similar to baseline, we ignore probability predicted for the NA relation during testing on both datasets. We  Dataset. We observe similar behavior as <ref type="figure" target="#fig_2">Figure 3</ref>, where RE-CON and its configurations consistently maintain a higher precision (against the baselines) over entire recall range.</p><p>use different metrics depending on the dataset as per the respective baselines for fair comparison. On Wikidata dataset, we adapt (micro and macro) precision (P), recall (R), and F-score (F1) from <ref type="bibr" target="#b20">[21]</ref>. For NYT Freebase dataset, we follow the work by <ref type="bibr" target="#b29">[30]</ref> that uses (micro) P@10 and P@30. An ablation is performed to measure effectiveness of KGGAT-SEP in learning entity and relation embeddings. For this, we use the hits@N, average rank, and average reciprocal rank in similar to <ref type="bibr" target="#b16">[17]</ref>. Our work employs the Adam optimizer <ref type="bibr" target="#b10">[11]</ref> with categorical cross entropy loss where each model is run three times on the whole training set. For the P/R curves, we select the results from the first run of each model. Our experiment settings are borrowed from the baselines: GP-GNN <ref type="bibr" target="#b31">[32]</ref> for the sentential RE and HRERE <ref type="bibr" target="#b29">[30]</ref> for the multi-instance RE.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We study following research questions: "RQ1: How effective is RECON in capturing the KG context-induced in a graph neural network for the sentential RE?" The research question is further divided into two sub-research questions: RQ1.1: what is the useful contribution of each entity attribute context (alias, instance-of, type, label in RECON-EAC) for sentential RE? RQ1.2: How effective is separation of entity and relation embedding spaces (RECON-KGGAT-SEP) in capturing the triple context from neighborhood 1&amp;2 hop triples for the given entities? RQ2: Is the addition of the KG context statistically significant? Each of our experiments systematically studies the research questions in different settings. Performance on Wikidata dataset: <ref type="table" target="#tab_3">Table 1</ref> summarizes the performance of RECON and its configurations against other sentential RE models. It can be observed that by adding the entity attribute context (RECON-EAC), we surpass the baseline results. The RECON-EAC-KGGAT values indicate that when we further add context from KG triples, there is an improvement. However, the final configuration RECON achieves the best results. It validates our hypothesis that RECON is able to capture the KG context effectively. The P/R curves are illustrated in the <ref type="figure" target="#fig_2">Figure 3</ref>. RECON steadily achieves higher precision over the entire recall range compared to other models. In running example (cf. <ref type="figure" target="#fig_0">Figure 1)</ref>, RECON could predict the correct relation wdt:P26 (spouse) between wdt:Q76 (Barack Obama) and wdt:Q13133 (Michelle Obama), while, the other two baselines wrongly predicted the relation wdt:P155 (follows). Performance on NYT Freebase Dataset: RECON and its configurations outperforms the sentential RE baselines (cf. <ref type="table" target="#tab_5">Table 2</ref>). Hence, independent of underlying KG, RECON can still capture sufficient context collectively from entity attributes and factual triples. We also compare the performance of sentential RE models, including RECON and its configurations against multi-instance RE baselines. It can be deducted from <ref type="table" target="#tab_5">Table 2</ref> that RECON supersedes the performance of multi-instance baselines. Furthermore, the RE-CON's P/R curve for the NYT Freebase dataset shown in <ref type="figure" target="#fig_3">Figure  4</ref> maintains a higher precision over the entire recall range. The observation can be interpreted as follows: adding context from the knowledge graphs instead of the bag of sentences for the entity pairs keeps the precision higher over a more extended recall range. Hence, we conclude that RECON is effectively capturing the KG context across KGs, thereby answering the first research question RQ1 successfully.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Studies</head><p>Effectiveness of EAC: We separately studied each entity attribute's effect on the performance of the RECON-EAC. <ref type="table" target="#tab_8">Table 4</ref> and <ref type="table">Table  5</ref> summarize the contribution of the four entity attributes when independently added to the model. The entity type (Instance-of) contributes the least across both datasets. We see that the entity descriptions significantly impact RECON's performance on the Wikidata dataset, while descriptions have not provided much gain on Freebase. The Freebase entity descriptions are the first paragraph from the Wikipedia entity web page, whereas, for Wikidata, descriptions are a human-curated concise form of the text. Mulang' et al. <ref type="bibr" target="#b15">[16]</ref> also observed that when the Wikipedia descriptions are replaced with the entity descriptions derived from the Wikidata KG, the performance of an entity disambiguation model increases.</p><p>The reported study on the EAC module's effectiveness answers our first sub-research question (RQ1.1). We conclude that the contribution of entity attributes in the EAC context varies per underlying KG. Nevertheless, once we induce cumulative context from all entity attributes, we attain a significant jump in the RECON-EAC performance (cf.   <ref type="table">Table 5</ref>: RECON-EAC performance on NYT Freebase Dataset. The rows comprise of the configuration when context from each entity attribute is added in isolation. We report P@10 and P@30, similar to other NYT dataset experiments. (Best score in bold)</p><p>Understanding the KG triple Context: To understand the effect of relying on one single embedding space or two separate spaces, we conducted an ablation study for the triple classification task. We performed a ranking of all the triples for a given entity pair and obtained hits@N, average rank, and Mean Reciprocal Rank (MRR). Hits@10 denotes the fraction of the actual triples that are returned in the top 10 predicted triples. <ref type="table">Table 7</ref> illustrates that the KGGAT-SEP (separate spaces) exceeds KBGAT (single space) by a large margin on the triple classification task. Training in separate vector spaces facilitates learning more expressive embeddings   <ref type="table">Table 7</ref>: Comparing KGGAT-SEP and KBGAT for triple classification task on both Datasets. We conclude that separating the entity and relation embedding space has been beneficial for the triple classification task, hence, contributing positively to RECON performance. (cf. <ref type="table" target="#tab_3">Table 1</ref> and 2).</p><p>of the entities and relations in the triple classification task. The positive results validate the effectiveness of the KGGAT-SEP module and answers the research question RQ1.2. However, when we trained entity and relation embeddings of KG triples in separate spaces, improvements are marginal for the sentential RE task (cf. <ref type="table" target="#tab_3">Table 1</ref>). We could interpret this behavior as : the model may have already learned relevant information from the sentence and the triple context before we separate vector spaces. Also, in our case, the computed graph is sparse for sentential RE, i.e., few relations per entity prevents effective learning of good representation <ref type="bibr" target="#b16">[17]</ref>. We believe sparseness of the computed graph has hindered effective learning of the entity embeddings. It requires further investigation, and we plan it for our future work.</p><p>Statistical Significance of RECON: The McNemar's test for statistical significance has been used to find if the reduction in error at each of the incremental stages in RECON are significant. The test is primarily used to compare two supervised classification models <ref type="bibr" target="#b3">[4]</ref>. The results are shown in <ref type="table" target="#tab_6">Table 3</ref>. For the column "contingency <ref type="table" target="#tab_5">table" (2x2 contingency table)</ref>, the values of the first row and second column ( ) represent the number of instances that model 1 predicted correctly and model 2 incorrectly. Similarly the values of the second row and first column gives the number of instances that model 2 predicted correctly and model 1 predicted incorrectly (</p><p>). The statistic here is</p><formula xml:id="formula_21">( − ) 2 +</formula><p>The differences in the models are said to be statistically significant if the − &lt; 0.05 <ref type="bibr" target="#b3">[4]</ref>. On both datasets, for all RECON configurations, the results are statistically significant, illustrating our approach's robustness (also answering second research question RQ2). In the contingency table, the ( ) values provide an exciting insight. For example, in the first row of the <ref type="table" target="#tab_6">Table 3</ref>, there are 40882 sentences for which adding the RECON-EAC context has negatively resulted in the performance compared to GP-GNN. This opens up a new research question that how can one intelligently select the KG context based on the sentence before feeding it into the model. We leave the detailed exploration for future work. Performance on Human Annotation Dataset: To provide a comprehensive ablation study, <ref type="bibr" target="#b31">[32]</ref> provided a human evaluation setting and reports Micro P, R, and F1 values. Following the same setting, we asked five annotators 5 to annotate randomly selected sentences from Wikidata dataset <ref type="bibr" target="#b20">[21]</ref>. The task was to see whether a distantly supervised dataset is right for every pair of entities. Sentences accepted by all annotators are part of the human-annotated dataset. There are 500 sentences in this test set. <ref type="table" target="#tab_13">Table 9</ref> reports RECON performance against the sentential baselines. We could see that RECON and its configurations continue to outperform other sentential RE baselines. The results further re-assure the robustness of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Case Studies.</head><p>We conducted three case studies. For the first case study, <ref type="table" target="#tab_10">Table 6</ref> demonstrates RECON's performance against two sentential baselines: Context-Aware LSTM <ref type="bibr" target="#b20">[21]</ref> and GP-GNN <ref type="bibr" target="#b31">[32]</ref> on a few randomly selected sentences from the Wikidata dataset. We can see that these sentences don't directly contain much information regarding the potential relationship between two entities (the relations are implicitly coded in the text). For example, in the first sentence, the relation between the entities rapper and Eminem is "occupation. " The baselines predicted "Instance of" as the target relation considering sentential context is limited. However, the Wikidata description of the entity Q8589(Eminem) is "American rapper, producer and actor". Once we feed description in our model as context for this sentence, RECON predicts the correct relation.   Sorokin et al. <ref type="bibr" target="#b20">[21]</ref> provided a study to analyze the impact of their approach on top relations (acc. to the number of occurrences) in Wikidata dataset. Hence, in the second case study, we compare the performance of RECON against sentential RE baselines for the top relations in Wikidata dataset (cf. <ref type="table" target="#tab_12">Table 8</ref>). We conclude that the KG context has positively impacted all top relation categories and appears to be especially useful for taxonomy relations (INSTANCE OF, SUBCLASS OF, PART OF).</p><p>The third case study focuses on the scalability of Triple Context Learner (KGGAT-SEP) on both datasets. We incrementally add a fraction of entity nodes in the KB to capture the neighboring triples' context. Our idea here is to study how training times scale with the size of the considered KB. <ref type="figure">Figure 5</ref> illustrates that when we systematically add entity nodes in the KB, the time increases by a polynomial factor, which is expected since we consider the 2 hop neighborhood of the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE DIRECTIONS</head><p>This paper presents RECON, a sentential RE approach that integrates sufficient context from a background KG. Our empirical study shows that KG context provides valuable additional signals when the context of the RE task is limited to a single sentence. Gleaning from our evaluations, we conclude three significant findings: i) the simplest form of KG context like entity description already provide ample signals to improve the performance of GNNs. We also see that proper encoding of combined entity attributes (labels, descriptions, instance of, and aliases) results in more impacting knowledge representation. ii) Although graph attention networks provide one of the best avenue to encode KG triples, more expressive embeddings can be achieved when entity and relation embeddings are learned in separate vector spaces. iii) Finally, due to the proposed KG context and encoding thereof, RECON transcends the SOTA in sentential RE while also achieving SOTA results against multi-instance RE models. The Multi-instance setting, which adds context from the previous sentences of the bag is a widely used practice in the research community since 2012 <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. We submit that sentential RE models induced with effectively learned KG context could be a good trade-off compared to the multi-instance setting. We expect the research community to look deeper into this potential trade-off for relation extraction.</p><p>Based on our findings, exhaustive evaluations, and gained insights in this paper, we point readers with the following future research directions: 1) Results reported in <ref type="table" target="#tab_6">Table 3</ref> illustrate that there exist several sentences for which KG context offered minimal or negative impact. Hence, it remains an open question of how an approach intelligently selects a specific form of the context based on the input sentence. 2) We suggest further investigation on optimizing the training of embeddings in separate vector spaces for RE. We also found that combining the triple context with the entity attribute context offered minimal gain to the model. Hence, we recommend jointly training the entity attribute and triple context as a viable path for future work. 3) The applicability of RECON in an industrial scale setting was out of the paper's scope. The researchers with access to the industrial research ecosystem can study how RECON and other sentential RE baselines can be applied to industrial applications. 4) The data quality of the derived KG context directly impacts the performance of knowledge-intense information extraction methods <ref type="bibr" target="#b27">[28]</ref>. The effect of data quality of the KG context on RECON is not studied in this paper's scope and is a viable next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">APPENDIX 7.1 Theoretical Motivation</head><p>We define a set of theorems that motivated our approach RECON and provided a theoretical foundation. Lemma 7.1. If entity and relation embeddings are expressed in the same vector space, there cannot be more than one distinct relation per entity pair Proof. Consider two entities ì 1 and ì 2 . Consider a relation ì 1 between them. We want to have these vectors satisfy the triangle law of vector addition as below</p><formula xml:id="formula_22">ì 1 + ì 1 = ì 2<label>(23)</label></formula><p>Now assume another relation ì 2 between ì 1 and ì 2 (where ì 1 is the subject). Thus we have,</p><formula xml:id="formula_23">ì 1 + ì 2 = ì 2<label>(24)</label></formula><p>From lemmas 23 and 24 we get: ì 1 = ì 2 □ Lemma 7.2. If entity and relation embeddings are expressed in the same vector space, there can not exist a single common relation between an entity and two different, directly connected entities Proof. Consider ì 1 and ì 2 to have relation ì 1 . Consider ì 1 and ì 3 to have the same relation ì 1 . Then,</p><formula xml:id="formula_24">ì 1 + ì 1 = ì 2 ; ì 1 + ì 1 = ì 3 ; ∴ ì 2 − ì 3 = ì 0; ì 2 = ì 3<label>(25)</label></formula><p>We call this problem a mode collapse as the two separate entity embeddings collapse into a single vector. □ Lemma 7.3. If entity and relation embeddings are expressed in the same vector space, no entity is sharing a common relation between two indirectly related entities Proof. Consider ì 1 and ì 2 to have a relation ì 1 . Consider ì 1 and ì 3 to have a relation ì 3 . Let ì 1 and ì 3 be inverse relations. Assume ì 1 , ì 2 ≠ 0</p><formula xml:id="formula_25">ì 1 = −ì 2 ; ì 1 = −ì 2 ì 1 + ì 2 = ì 3 ; ì 2 − ì 3 = 2ì 1<label>(26)</label></formula><p>Now consider ì 4 to have a common relation with ì 2 and ì 3 . Let this relation be ì 3 .</p><formula xml:id="formula_26">ì 2 + ì 3 = ì 4 ; ì 3 + ì 3 = ì 4 ì 2 − ì 3 = ì 0; ì 1 = ì 0<label>(27)</label></formula><p>Which contradicts the assumption □ Lemma 7.4. If is an invertible and distributive function/transform for a relation ì, then for an entity sharing a common relation between two other distinct entities, this function causes the embeddings of the two entities to be merged into one Proof. Let's assume a transformation function that transforms from the entity to the relation space. Assuming the triangle law holds we have, (ì 1 ) + ì 1 = (ì 2 ) and</p><formula xml:id="formula_27">(ì 1 ) + ì 1 = (ì 3 ) ∴ (ì 2 ) − (ì 1 ) = (ì 3 ) − (ì 1 ) (ì 2 − ì 1 ) = (ì 3 − ì 1 ) ...since is distributive −1 * (ì 2 − ì 1 ) = −1 * ( 3 − 1)</formula><p>..since is invertible</p><formula xml:id="formula_28">ì 2 − ì 1 = ì 3 = ì 1 ì 2 = ì 3</formula><p>However we may want to have ì 2 separate from ì 3 . □</p><p>The affine transform as used by TransR <ref type="bibr" target="#b11">[12]</ref> belongs to this class of transform. Hence we propose adding a non-linear transform.</p><p>Lemma 7.5. If T is the set of triples learned under a common transform and T is the set of triples learned under a transform which is distinct per relation then T ⊊ T i.e. T is a strict subset of T Proof. We prove this lemma in two parts. First we show that T ⊆ T then we show that T ⊈ T . 1. The first part is straightforward as we can set = and make T ⊆ T 2. For showing the second part we consider the following system of triples Consider relations ì 1 and ì 2 between entities ì 1 and ì 2 and ì 1 ≠ ì 2 We define a common transform such that (ì 1 ) + ì 1 = (ì 2 ) and</p><formula xml:id="formula_29">(ì 1 ) + ì 2 = (ì 2 ) ∴ ì 1 = ì 2</formula><p>For the per relation transform we can define a function 1 for 1 and 2 for 2 such that 1 (ì 1 ) + ì 1 = 1 (ì 2 ) and 2 (ì 1 ) + ì 2 = 2 (ì 2 ) such that ì 1 ≠ ì 2 Thus T ⊈ T , and hence the proof. □ Lemma 7.6. If T is the set of triples that can be learned under a global context aware transform and T is the set of transforms learned under a local context aware transform then T ⊊ T . By context here, we mean the KG triples, global context refers to all the triples in the KG the current entities are a part of, and local context indicates the triple under consideration.</p><p>Proof. We proceed similar to lemma 7.5. 1. We can make = by ignoring the global context and thus T ⊆ T 2. We define a globally context aware transform as below:</p><p>(ì 1 ) = (ì 1 )</p><formula xml:id="formula_30">(ì 2 ) = ∑︁ ∈ ( ì 1 ) * ( )</formula><p>Where is the attention value learned for the triple &lt; ì 1 , ì, ì &gt; In a simple setting we can have = 1 and learn</p><formula xml:id="formula_31">ì = (ì 2 ) − (ì 1 ) = (ì 3 ) − (ì 1 )</formula><p>With ì 2 ≠ ì 3 However in a local context aware transform we have,</p><formula xml:id="formula_32">(ì 1 ) + ì = (ì 2 ) (ì 1 ) + ì = (ì 3 )</formula><p>From lemma 7.4 ì 2 = ì 3 and thus we can not have both &lt; ì 1 , ì, ì 2 &gt; and &lt; ì 1 , ì, ì 3 &gt; in Thus T ⊈ T and hence the proof □ Theorem 7.1. Global context aware transform that is distinct for every relation for learning relation and entity embeddings in separate vector spaces is strictly more expressive than i) Learning the same embedding space ii) Using a common transform for every relation iii) Using local context only Proof. Follows from lemma 7.1 to 7.6 □ Theorem 7.2. There exists an optimum point for the ranking loss between the triplet vector additions of positive and negative triples, which can be traversed with decreasing loss at each step of the optimization from any point in the embedding space, and as such, an optimum optimization algorithm should be able to find such a point Proof. Let us define the framework of the ranking loss as below. Consider a positive triple ( 1 , , 2 ) and a negative triple <ref type="bibr">( 3 , , 4</ref> ). The vector addition for the first triple would give 1 = (ì 1 + ì − ì 2 ) and for the second would give 2 = (ì 3 + ì − ì 4 ). The margin loss would then be defined as (0, − ( 2 − 1 )). If we take the margin to be zero and ignore the term 2 we get = (0, 1 ). Since the norm has to be &gt;= 0, 1 &gt;= 0, hence, the loss becomes minimum when 1 = 0. Removing the trivial case of all entity embeddings= ì 0, we define the loss space as follows. Without loss of generality we take the relation vectors to be fixed. For a triple (ì 1 , ì, ì 2 ) we take the difference 2 − 1 . The loss for this triple then becomes − ( 2 − 1 ). For all triples, we get</p><formula xml:id="formula_33">= ∑︁ ∈T − ( 2 − 1 ) = ∑︁ ∈T − ∑︁ ∈T 2 − 1<label>(28)</label></formula><p>Now we define the point in vector space represented by ∈T ( 2 − 1 ) to be the current point in the optimization and plot the loss concerning it, which is the norm of the loss in the equation 28. Since there could be multiple configurations of the entity embeddings for each such point, we assume the loss to be an optimum loss given a configuration of entity embeddings. I.e., the relation vectors to be modified such that each difference term − ( 2 − 1 ) is always greater than or equal to 0. Let = ∈ and = ∈ ( 2 − 1 ), then =| − | represents a cone. Now if we consider all the possible relation vector configurations and take all losses so that at each point in the vector space the minimum of each contribution is taken we get a piece-wise continuous function with conical regions and hyperbolic intersection of the cones as in <ref type="figure" target="#fig_5">figure 6</ref>. For a path to exist between the start and an optimum global point under gradient descent, two conditions must hold (1) The function must be continuous.</p><p>(2) At no point in the function must there be a point such that there exists no point in it's neighborhood with a lesser value. The derived function satisfies both the above properties. □</p><p>The above theorem proves convergence when all entities are updated simultaneously. However, this may not be possible in practice as the number of entities could be very large, causing memory errors. We introduce a simple modification to train the entities batch-wise, i.e., to update via gradient descent only a sample of the entities, thus reducing memory requirements. We shall see in the next theorem that this approach also converges.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>describes the RECON architecture. The sentence embedding module retrieves the static embeddings of the input sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Entity Attribute Context Module 3.2.2 Triple Context Learner (KGGAT-SEP). The KG triple context learner (KGGAT-SEP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The P-R curves for Sentential RE approaches on Wikidata Dataset. RECON and its configurations maintain a higher precision (against the baselines) over entire recall range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The P-R curves for RE approaches on NYT Freebase</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 7 . 3 .</head><label>73</label><figDesc>The entity vectors could be updated batch wise to monotonically reduce the loss till optimum is reached Consider a set of vectors ì 1 , ì 2 ...ì and the resultant ì. ì = ì 1 + ì 2 + ... + ì</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :•.</head><label>6</label><figDesc>Loss function topology under the 1 norm of the difference between the sum of relation vectors and entity vectors, demonstrating that convergence is possible from any starting point Algorithm 1: Algorithm for learning entity embeddings batchwise using the margin ranking loss Initialize the relation and entity embeddings randomly;while not converged doProof.• Select a subset of entities{ 1 , 2 ... } ⊆ • Select the subset of 1-hop &amp; 2-hop triples T ℎ ⊆ T | ∈ τ ∧ τ ∈ T ℎ ∧ ∈ { 1 , 2 ... } • Input T toKGGAT-SEP model and compute a forward pass to get the new entity embeddings for the entities in the current batch keeping the other entity embeddings fixed. • Compute the loss according to (Ω) = τ ℎ ∈T τ Back propagate using gradient descent to update { 1 , 2 ... } ⊆ end Also consider another set of entities ì ′ The difference between ì and the sum of new set of vectors is ì = ì − (ì ′ that such an update exists and performing it recursively for other entity vectors till optimum is possible under the given framework. Algorithm 1 details the batch wise learning. □</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We use two standard datasets for our experiment. (i) Wikidata dataset<ref type="bibr" target="#b20">[21]</ref> created in a distantly-supervised manner by linking the Wikipedia English Corpus to Wikidata and includes sentences with multiple relations. It has 353 unique relations, 372,059 sentences in training, and 360,334 for testing. (ii) NYT Freebase dataset which was annotated by linking New York Times articles with Freebase KG<ref type="bibr" target="#b17">[18]</ref>. This dataset has 53 relations (including no relation "NA"). The number of sentences in training and test set are 455,771 and 172,448 respectively. We augment both datasets with our proposed context. For EAC, we used dumps of Wikidata 3 and Freebase 4 to retrieve entity properties. In addition, the 1&amp;2 hop triples are retrieved from the local KG associated with each dataset. KGGAT-SEP: this implementation encompasses only KGGAT-SEP module of RECON (cf. section 3.2.2) which learns triple context. This is for comparing against<ref type="bibr" target="#b16">[17]</ref>. RECON-EAC: induces encoded entity attribute context (from section 3.2.1) along with sentence embeddings into the propagation layer of context aggregator module. RECON-EAC-KGGAT: along with sentence embeddings, it consumes both types of context i.e., entity context and triple context (cf. section 3.2.2) where relation and entity embeddings from the triples are trained on same vector space. RECON: similar to RECON-EAC-KGGAT, except entity and relation embeddings for triple context learner are trained in different vector spaces.</figDesc><table><row><cell>22)</cell></row><row><cell>4 EXPERIMENTAL SETUP</cell></row><row><cell>4.1 Datasets</cell></row><row><cell>4.2 RECON Configurations</cell></row><row><cell>We configure RECON model applying various contextual input</cell></row><row><cell>vectors detailed below:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison of RECON and sentential RE models on the Wikidata dataset. Best values are in bold. Each time a KG context is added in a graph neural network, the per- formance has increased, resulting in a significant RECON outperformance against all sentential RE baselines.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Compared Models</cell><cell cols="3">Contingency Statistic p-value</cell><cell>Dataset</cell></row><row><cell>GP-GNN Vs</cell><cell cols="3">568469 40882 4978.84 0.0</cell><cell>Wikidata</cell></row><row><cell>RECON-EAC</cell><cell cols="2">63702 67713</cell></row><row><cell>RECON-EAC Vs</cell><cell cols="2">599135 33036 862.38</cell><cell>1.5 * 10 −189 Wikidata</cell></row><row><cell>RECON-EAC-KGGAT</cell><cell cols="2">41029 67566</cell></row><row><cell>RECON-EAC-KGGAT</cell><cell cols="2">608442 31722 455.29</cell><cell>5.1 * 10 −101 Wikidata</cell></row><row><cell>Vs RECON</cell><cell cols="2">37330 63272</cell></row><row><cell>GP-GNN Vs</cell><cell cols="2">158426 4936 15.72</cell><cell>7.3  *  10 −5 Freebase</cell></row><row><cell>RECON-EAC</cell><cell cols="2">53392 3699</cell></row><row><cell>RECON-EAC Vs</cell><cell cols="2">160227 3538 59.44</cell><cell>1.2  *  10 −14 Freebase</cell></row><row><cell>RECON-EAC-KGGAT</cell><cell>4218</cell><cell>4417</cell></row><row><cell>RECON-EAC-KGGAT</cell><cell cols="2">161012 3433 54.88</cell><cell>1.3  *  10 −13 Freebase</cell></row><row><cell>Vs RECON</cell><cell>4076</cell><cell>3879</cell></row></table><note>Comparison of RECON against baselines (sentential and multi-instance) on the NYT Freebase dataset. Best val- ues are in bold. RECON continues to significantly outper- form sentential RE baselines and also surpasses the perfor- mance of state of the art multi-instance RE approach.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>The McNemar's test for statistical significance on the results of both datasets. It can be observed that each of the improvement in the RECON configurations is statisti- cally significant independent of the underlying KG.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 and</head><label>1</label><figDesc>Table 2).</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>RECON-EAC(Instance of)</cell><cell>76.33</cell><cell>76.32</cell><cell>76.32</cell></row><row><cell>RECON-EAC(label)</cell><cell>78.64</cell><cell>78.70</cell><cell>78.67</cell></row><row><cell>RECON-EAC(Alias)</cell><cell>81.58</cell><cell>81.56</cell><cell>81.57</cell></row><row><cell>RECON-EAC(Description)</cell><cell cols="3">83.16 83.18 83.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>RECON-EAC performance on Wikidata Dataset.</figDesc><table><row><cell cols="3">The rows comprise of the configuration when context from</cell></row><row><cell cols="3">each entity attribute is added in isolation. We report micro</cell></row><row><cell cols="2">P, R, and F scores. (Best score in bold)</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">P@10 P@30</cell></row><row><cell>RECON-EAC(Instance of)</cell><cell>71.83</cell><cell>57.52</cell></row><row><cell>RECON-EAC(label)</cell><cell>78.14</cell><cell>66.34</cell></row><row><cell>RECON-EAC(Alias)</cell><cell cols="2">80.60 67.13</cell></row><row><cell cols="2">RECON-EAC(Description) 72.40</cell><cell>67.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Specifically , the rapper listed Suzanne Vega , Led Zeppelin , Talking Heads , Eminem , and Spice Girls.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Context-</cell><cell></cell><cell></cell></row><row><cell>Sentence</cell><cell>Entities</cell><cell></cell><cell></cell><cell>Correct</cell><cell>Aware</cell><cell>GP-</cell><cell>RECON</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GNN[32]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Relation</cell><cell>LSTM[21]</cell><cell></cell><cell></cell></row><row><cell cols="4">1. Q5608 : Eminem</cell><cell>P106</cell><cell>P31</cell><cell>P31</cell><cell>P106</cell></row><row><cell></cell><cell cols="3">Q2252262 : rapper</cell><cell cols="2">Occupation Instance</cell><cell>Instance</cell><cell>Occupation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>of</cell><cell>Of</cell><cell></cell></row><row><cell>2. Bocelli also took part in the Christmas in Washington special on Dec</cell><cell cols="3">Q76 : Barack Obama</cell><cell>P26</cell><cell>P155</cell><cell>P155</cell><cell>P26</cell></row><row><cell>12, in the presence of president Barack Obama and the first lady</cell><cell>Q13133</cell><cell>:</cell><cell>Michelle</cell><cell>spouse</cell><cell>follows</cell><cell>follows</cell><cell>spouse</cell></row><row><cell></cell><cell>Obama</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Q15862 : Queen</cell><cell>P175</cell><cell>P50</cell><cell>P50</cell><cell>P175</cell></row><row><cell>3. It was kept from number one by Queen's Bohemian Rhapsody</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Q187745 : Bohemian</cell><cell cols="2">performer author</cell><cell>author</cell><cell>performer</cell></row><row><cell></cell><cell>Rhapsody</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Sample sentence examples from the Wikidata dataset. RECON is able to predict the relations which are not explicitly observable from the sentence itself.</figDesc><table><row><cell>Model</cell><cell cols="2">%Hits @10 MR</cell><cell cols="2">MRR Dataset</cell></row><row><cell>KBGAT</cell><cell>65.8</cell><cell>35.2</cell><cell>0.36</cell><cell>Wikidata</cell></row><row><cell>KGGAT-SEP</cell><cell>72.6</cell><cell>29</cell><cell>0.38</cell><cell>Wikidata</cell></row><row><cell>KBGAT</cell><cell>85.8</cell><cell>7.48</cell><cell>21.6</cell><cell>NYT Freebase</cell></row><row><cell>KGGAT-SEP</cell><cell>88.4</cell><cell>5.42</cell><cell>32.3</cell><cell>NYT Freebase</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Precision and Recall of the top relations (as per number of occurrences) in the Wikidata dataset. Induction of KG context in RECON and configurations demonstrate the most improvement on precision across all relation categories.</figDesc><table><row><cell cols="4">Figure 5: Scalability of Triple Context Learner (KGGAT-SEP)</cell></row><row><cell cols="4">on Wikidata and NYT Freebase datasets. When we incremen-</cell></row><row><cell cols="4">tally added entity nodes in the KB to capture the triple con-</cell></row><row><cell cols="4">text, the training time increases by a polynomial factor.</cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Context Aware LSTM [21]</cell><cell>77.77</cell><cell>78.69</cell><cell>78.23</cell></row><row><cell>GP-GNN [32]</cell><cell>81.99</cell><cell>82.31</cell><cell>82.15</cell></row><row><cell>RECON-EAC</cell><cell>86.10</cell><cell>86.58</cell><cell>86.33</cell></row><row><cell>RECON-KBGAT</cell><cell>86.93</cell><cell>87.16</cell><cell>87.04</cell></row><row><cell>RECON</cell><cell cols="3">87.34 87.55 87.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Sentential RE performance on Human Annotation Dataset. RECON again outperforms the baselines. We report Micro P,R, and F1 values. (Best score in bold)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">wdt: binds to https://www.wikidata.org/wiki/ arXiv:2009.08694v2 [cs.CL] 17 Jan 2021 WWW '21, April 19-23, 2021, Ljubljana, Slovenia Bastos, et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ansonb/RECON</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://dumps.wikimedia.org/wikidatawiki/entities/ 4 https://developers.google.com/freebase</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Annotators were well-educated university students.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank Satish Suggala for additional server access and anonymous reviewers for very constructive reviews.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A Shared Database of Structured General Human Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Tufts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Why We Need Knowledge Graphs: Applications. In Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fensel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umutcan</forename><surname>Şimşek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Angele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elwin</forename><surname>Huaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Kärle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandra</forename><surname>Panasiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioan</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Umbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wahler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="95" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural Knowledge Acquisition via Mutual Attention Between Knowledge Graph and Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>AAAI-18. 4832-4839</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Blomqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>et al. 2020. Knowledge graphs.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving Distantly-Supervised Relation Extraction with Joint Label Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3819" to="3827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015-01-25" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A soft-label method for noise-tolerant distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1790" to="1795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2009, Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating the Impact of Knowledge Graph Context on Entity Disambiguation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Isaiah Onando Mulang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitali</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CIKM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling Relations and Their Mentions without Labeled Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6323</biblScope>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation extraction using distant supervision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisa</forename><surname>Smirnova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-Aware Representations for Knowledge Base Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-instance Multi-label Learning for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). ACL</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiranjib</forename><surname>Sai Suman Prayaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1257" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wikidata: a new platform for collaborative data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st World Wide Web Conference</title>
		<meeting>the 21st World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW 2012 (Companion Volume</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1063" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining and leveraging background knowledge for improving named entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Weichselbraun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kuntschik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian Mp</forename><surname>Braşoveanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics</title>
		<meeting>the 8th International Conference on Web Intelligence, Mining and Semantics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving Distantly Supervised Relation Extraction with Neural Noise Converter and Conditional Optimal Selector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7273" to="7280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Connecting Language and Knowledge with Heterogeneous Representations for Neural Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019</title>
		<meeting>NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3201" to="3206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Zhi-Xiu Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2810" to="2819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph Neural Networks with Generated Parameters for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1331" to="1339" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Maosong Sun</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
