<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Learning with Auxiliary Speaker Identification for Conversational Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of New Media and Communication</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Task Learning with Auxiliary Speaker Identification for Conversational Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational emotion recognition (CER) has attracted increasing interests in the natural language processing (NLP) community. Different from the vanilla emotion recognition, effective speakersensitive utterance representation is one major challenge for CER. In this paper, we exploit speaker identification (SI) as an auxiliary task to enhance the utterance representation in conversations. By this method, we can learn better speaker-aware contextual representations from the additional SI corpus. Experiments on two benchmark datasets demonstrate that the proposed architecture is highly effective for CER, obtaining new state-of-the-art results on two datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion recognition has been one hot topic in natural language processing (NLP) which aims to detect emotions in texts <ref type="bibr" target="#b5">[Wen and Wan, 2014;</ref><ref type="bibr" target="#b2">Li et al., 2015]</ref>. Recently, emotion recognition in conversions has been received increasing attentions <ref type="bibr" target="#b5">Zhong et al., 2019;</ref><ref type="bibr" target="#b1">Ghosal et al., 2019]</ref>. Given a sequence of utterances by multiple speakers, conversational emotion recognition (CER) aims to recognize emotion for each utterance. CER is a typical sequence labeling problem, and end-to-end neural sequence labeling models have achieved state-of-the-art performance <ref type="bibr" target="#b4">[Poria et al., 2017;</ref><ref type="bibr" target="#b1">Jiao et al., 2019]</ref>.</p><p>Intuitively, speaker information can be greatly helpful for CER. For example, the last utterance of a same speaker could be severed as an important clue for the current utterance. Thus, how to effectively represent the speaker-sensitive utterances in conversions is critical for CER models. Previous studies, e.g., <ref type="bibr">ConGCN [Zhang et al., 2019]</ref> and <ref type="bibr">Dia-logueGCN [Ghosal et al., 2019]</ref>, build graphical structures over the input utterance sequences by speaker information and then exploit graph neural networks to model the dependencies, leading to better performance for CER.</p><p>The above models just adopt speakers as indicators to build connections over sequential utterances, using only CER training corpus to learn speaker-aware representations, which Figure 1: Illustration of multi-task learning of our method. For CER, the emotion of every utterance in conversation is predicted. For SI, several pairs of utterances will be selected for binary classification, where 1 denotes the same speaker and 0 otherwise. could be insufficient for speaker exploration. In most cases, we can have much larger scale corpora of raw conversions, where no emotion information is annotated. These corpora could be potentially useful to learn speaker-aware contextual representations, since the utterances as well as the corresponding speaker identities are offered jointly in them. Speaker identification (SI) could be one good alternative for this purpose. As shown in <ref type="figure">Figure 1</ref>, we can learn speakeraware contextual representations through the raw conversions by judging whether two utterances are from the same user.</p><p>In this work, we propose to use SI as one auxiliary task in order to obtain better speaker-aware contextual representations of the conversational utterances. We exploit a multi-task learning (MTL) framework to achieve our final goal to enhance CER. For CER and SI, we use the same network structure for utterance encoding, but with different set of model parameters. We adopt BERT as the basic representation to make our baseline strong, and hierarchical bidirectional gated recurrent neural networks (Bi-GRU) are exploited at the utterance-level and conversation-level to enhance the contextual representations. Further, we unite the two tasks with two bridging network structures by using an attention network <ref type="bibr" target="#b0">[Bahdanau et al., 2014]</ref> and a gate mechanism  at the utterance-level and conversation-level for full mutual interaction, respectively .</p><p>We conduct experiments on two benchmark datasets to verify our framework, which are respectively EmoryNLP and MELD by name, both are sourced from the TV show of Friends. The results show that our baseline system is highly strong, achieving better performance than previous state-ofthe-arts. Our final model can lead to significant improvements on the two datasets both, which demonstrates the ef-fectiveness of our proposed method. In addition, we conduct extensive analysis work to examine the model in depth, for better understanding the advantages of our model. All codes and experimental settings will be released publicly available on https://github.com/ThdLee/CER for research purpose under Apache License 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Emotion recognition in conversational texts is generally treated as a sequence labelling problem in the literature. Traditional approaches often use lexicon-based and acoustic features to detect emotions <ref type="bibr">[Forbes-Riley and Litman, 2004;</ref><ref type="bibr">Devillers and Vidrascu, 2006]</ref>. Recently, deep learning based recurrent neural networks (RNN) and transformer can bring state-of-the-art performance <ref type="bibr" target="#b4">[Poria et al., 2017;</ref><ref type="bibr">Tzirakis et al., 2017;</ref><ref type="bibr" target="#b5">Zhong et al., 2019]</ref>, which is able to capture contextual utterance representations effectively. Our baseline CER model follow these settings, adopting a sophisticated RNN for contextualized representation learning.</p><p>Several studies attempt to integrate speaker information in the conversational utterances, as it can affect the final CER performance much <ref type="bibr">[Hazarika et al., 2018b;</ref><ref type="bibr">Hazarika et al., 2018a]</ref>.  propose a recurrent model to detect emotion by tracking party state and global state dynamically. Graph convolutional networks (GCN) have been demonstrated stronger in modeling context-sensitive and speaker-sensitive dependence in conversation <ref type="bibr" target="#b1">Ghosal et al., 2019]</ref>. In this work, we propose an improved approach to better utilize speaker information by multi-task learning with a closely-related auxiliary task.</p><p>Multi-task learning aims to model multiple relevant tasks simultaneously, which is capable of exploiting potential shared features across tasks effectively. There have been a number of successful studies in the NLP community <ref type="bibr" target="#b4">[Liu et al., 2017;</ref><ref type="bibr" target="#b4">Ma et al., 2018;</ref><ref type="bibr" target="#b5">Xiao et al., 2018;</ref><ref type="bibr" target="#b4">Pentyala et al., 2019;</ref>. <ref type="bibr" target="#b6">Zhou et al. [2019]</ref> exploit language modeling as an auxiliary task to assist question generation task by multi-task learning, which motivates our work greatly. Similar to the language modeling task, speaker identification has rich training corpus, which can be collected automatically, and we exploit it to assist CER mainly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Suppose we have a conversation with N consecutive utterances u 1 , u 2 , · · · , u N and M speakers p 1 , p 2 , · · · , p M . Each utterance u i is uttered by one speaker p S(ui) , where the function S maps the index of the utterance into its corresponding speaker. The objective of CER is to predict the emotion label of each utterance, and the objective the auxiliary task SI is to classify whether two given utterances u p , u q in a conversation are from the same speaker. In this section, we will introduce our specific-task model and multi-task learning framework in detail. <ref type="figure" target="#fig_2">Figure 2</ref> shows the architecture of our baseline model, which exploits attention-based hierarchical network as encoder. <ref type="figure" target="#fig_4">Figure 3</ref> depicts the multi-task learning framework of our proposed model. </p><formula xml:id="formula_0">V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 k P Z 5 v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k l a t 6 l 1 U a / e X l f p N H k c R T u A U z s G D K 6 j D H T S g C Q y G 8 A y v 8 O Y I 5 8 V 5 d z 4 W r Q U n n z m G P 3 A + f w B d T I 3 Z &lt; / l a t e x i t &gt; BiGRU</formula><p>BiGRU BiGRU  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Encoder Encoder</head><formula xml:id="formula_1">V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l h 7 R f 6 5 c r b t W d g 6 w S L y c V y N H o l 7 9 6 g 5 i l E V f I J D W m 6 7 k J + h n V K J j k 0 1 I v N T y h b E y H v G u p o h E 3 f j Y / d U r O r D I g Y a x t K S R z 9 f d E R i N j J l F g O y O K I 7 P s z c T / v G 6 K 4 b W f C Z W k y B V b L A p T S T A m s 7 / J Q G j O U E 4 s o U w L e y t h I 6 o p Q 5 t O y Y b g L b + 8 S l q 1 q n d R r d 1 f V u o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w j O 8 w p s j n R f n 3 f l Y t B a c f O Y Y / s D 5 / A E J 8 I 2 i &lt; / l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conversational Emotion Recognition</head><p>For the baseline CER model, first a hierarchy encoder is built on the input conversion utterance sequence, resulting in one feature vector for each utterance, then a standard emotion classification is performed based on the encoder sequence. Our hierarchy encoder consists of two components: individual utterance encoder and contextual utterance encoder, where the individual utterance encoder is an attention-based network with Bi-GRU, as shown by the right part of <ref type="figure" target="#fig_2">Figure  2</ref>, and the contextual utterance encoder is a Bi-GRU over the output sequence of individual utterance encoder, aiming to capture the conversation-level context. 1 Individual Utterance Encoder The input of our model is a sequence of utterances u 1 , · · · , u N . Assuming that the ith utterance u i is denoted by w i,1 , · · · , w i,L(i) , where L(i) is the length of u i . We exploit BERT as the basic encoder because it has achieved state-of-the-art performances for a number of NLP tasks <ref type="bibr" target="#b1">[Devlin et al., 2019]</ref>, obtaining the word-level output for u i :</p><formula xml:id="formula_2">{x i,1 , · · · , x i,L(i) }.</formula><p>Further, we adopt a single-layer Bi-GRU to further enhance the contextualized word representation at the utterance level, which can be formulated as:</p><formula xml:id="formula_3">{h i,1 , · · · , h i,L(i) } = Bi-GRU w (x i,1 , · · · , x i,L(i) ) (1)</formula><p>where h i, * denotes the Bi-GRU output for word w i, * .</p><p>The goal of individual utterance encoder is to derive a single feature vector for each utterance based on the covered words. Next, we need to aggregate the word-level outputs into a single vector for the utterance. We exploit an attentionbased aggregation to accomplish the goal. Formally, the utterance representation is defined as follows:</p><formula xml:id="formula_4">z i,j = tanh(W a h i,j + b a ) α i,j = exp(v a z i,j ) k exp(v a z i,k ) u i = j α i,j h i,j ,<label>(2)</label></formula><p>where W a , b a and v a are model parameters, indicates vector transposition, u e i is the vector representation for utterance  u i . Intuitively, α i,j is the importance of word w i,i in u i , and the weighted sum is adopted for the utterance representation. Contextual Utterance Encoder After the individual utterance encoder, we obtain a sequence of utterance-level representations {u 1 , · · · , u N }. These representations are solely sourced from their internal words. In order to encode utterance-level contextualized information, we build a second Bi-GRU as follows:</p><formula xml:id="formula_5">V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l h 7 R f 6 5 c r b t W d g 6 w S L y c V y N H o l 7 9 6 g 5 i l E V f I J D W m 6 7 k J + h n V K J j k 0 1 I v N T y h b E y H v G u p o h E 3 f j Y / d U r O r D I g Y a x t K S R z 9 f d E R i N j J l F g O y O K I 7 P s z c T / v G 6 K 4 b W f C Z W k y B V b L A p T S T A m s 7 / J Q G j O U E 4 s o U w L e y t h I 6 o p Q 5 t O y Y b g L b + 8 S l q 1 q n d R r d 1 f V u o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w j O 8 w p s j n R f n 3 f l Y t B a c f</formula><formula xml:id="formula_6">{f i , · · · , f N } = Bi-GRU u (u 1 , · · · , u N ),<label>(3)</label></formula><p>where {f 1 , · · · , f N } is the final utterance presentations for prediction, which can capture surrounding contextual information in conversations.</p><p>Output Layer When the final contextualized utterance feature representation {f 1 , · · · , f N } is ready, we can calculate the output probability of each candidate emotion labels by a linear transformation followed with a softmax operation:</p><formula xml:id="formula_7">p CER i = softmax(W CER f i ), s.t. i ∈ [1, N ]<label>(4)</label></formula><p>where W CER is one model parameter, and p CER i denotes the output distribution of emotion labels for utterance u i . Training We optimize the CER model by minimizing the cross-entropy between the predicted emotion distribution and the true distribution. For a single conversation, the obejctive function is defined as follows:</p><formula xml:id="formula_8">L CER = − 1 N N i=1 K j y CER i,j log p CER i,j ,<label>(5)</label></formula><p>where N and K are the number of utterances in a conversation and emotion labels, respectively, y CER i is the one-hot vector of the ground truth emotion for utterance u i , and p CER i is the predicted distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Speaker Identification</head><p>As discussed before, speaker information plays an important role in CER. Here we use the same hierarchical network as CER to encoder utterances for SI. Similarly, we obtain the first stage utterance representations {u 1 , · · · , u N } by individual utterance encoder, and then achieve the second stage utterance representations {f 1 , · · · , f N } by contextual utterance encoder.</p><p>The goal of SI is to determine whether two selected utterances are from the same speaker, which is a binary classification problem. To reach this goal, we randomly sample T pairs of utterances for classification. Note that we do not extract all utterance pairs in a conversion for a balance with CER. In addition, we do not really recognize the utterance speakers, as the classification category could be extremely large in some real scenarios, which makes the training very difficult. Classification Given a sampled pair of utterance representations f i , f j from a single conversation, we adopt four sources of features for SI classification: (1)f i , (2) f j , <ref type="formula" target="#formula_6">(3)</ref> f i − f j , and (4) f i f j ( denotes element-wise multiplication). We concatenate them, apply a nonlinear MLP layer to reach the final feature vector f SI i,j , and then perform binary classification based on it. The overall process can be formalized as follows:</p><formula xml:id="formula_9">δ i,j = f i ⊕ f j ⊕ ( f i − f j ) ⊕ (f i f j ) f SI i,j = ReLU(W f δ i,j + b f ) p SI i,j = softmax(W SI f SI i,j )<label>(6)</label></formula><p>where W f , W SI and b f are model parameters, ⊕ denotes vector concatenation, and p SI i,j is a two-dimensional vector with one dim indicates the probability of the same speaker and the other dim denotes the probability of different speakers. Training For SI, we also adopt the cross-entropy loss between the ground truth and the predicted distribution as the training objective:</p><formula xml:id="formula_10">L SI = − 1 T T t=1 1 k=0 y SI i,j,k log p SI i,j,k ,<label>(7)</label></formula><p>where y SI i,j is a two-dimensional vector for the ground-truth answer, and T is one hyperparameter. We randomly sample T times for the utterance pair (u i , u j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Task Learning</head><p>The two above models for CER and SI have the same encoder network structure, which brings convenience in unite them under a multi-task learning framework. In this work, we keep the task-specific encoders, and exploit two bridging network structures for mutual interaction of the hierarchical encoders, where one network is designed for the utterancelevel individual utterance encoder, and the other is targeted to the conversation-level contextual utterance encoder.</p><p>Individual Utterance Encoder For the utterance-level individual utterance encoder, we exploit a shared-private structure to connect the two tasks. Concretely, as shown by the right part of <ref type="figure" target="#fig_4">Figure 3</ref>, we add a shared Bi-GRU module to unite the two tasks. Given the BERT outputs {x i,1 , · · · , x i,L(i) } (h i,j (i ∈ [1, N ] and j ∈ [1, L(i)]), we compute shared hidden vectors by Bi-GRU first:</p><formula xml:id="formula_11">{h sh i,1 , · · · , h sh i,L(i) } = Bi-GRU sh (x i,1 , · · · , x i,L(i) ),<label>(8)</label></formula><p>and then design a gated mechanism to dynamically incorporate shared feature h sh i,j into task-specific features. The network is mostly inspired by Wu et al. <ref type="bibr">[2019]</ref>. The updated contextual word representations are computed as follows:</p><formula xml:id="formula_12">g i,j = σ(W g h i,j + b g ) h i,j = g i,j h sh i,j + (1 − g i,j ) h i,j<label>(9)</label></formula><p>where g i,j is a gate to control the portion of information flowing from the shared Bi-GRU layer, σ is a sigmoid function. For the SI part, we only need to substitute h i,j into h i,j , and h i,j changes toĥ i,j correspondingly. Finally, we useĥ i,j andĥ i,j as the individual utterance encoder outputs for CER and SI, respectively.</p><p>Contextual Utterance Encoder For the union of contextual utterance encoder, we adopt a cross attention mechanism to augment the utterance representation from the task of the apart side. Taken the target CER model as an example, we obtain one kind of extra features from the SI contextual utterance encoder. Concretely, assuming that the contextual utterance representations of CER and SI are {f 1 , · · · , f N } and {f 1 , · · · , f N }, respectively, then we compute the additional feature for f i by the following equations:</p><formula xml:id="formula_13">c i,j = f i W c f j β i,j = exp(c i,j ) k exp(c i,k ) f i = f i ⊕ ( j β i,j f j ),<label>(10)</label></formula><p>where W c is one model parameter. The attention mechanism is mainly motivated by Bahdanau et al. <ref type="bibr">[2014]</ref> for feature selection from the apart side. When the SI model is the target task, the mechanism is just performed at an opposite direction, andf i is obtained. Finally, we use {f 1 , · · · ,f N } and {f 1 , · · · ,f N } instead for CER and SI decoding.</p><p>Multi-Task Training For the multi-task learning of the two tasks, we simply sum the losses of the two individual tasks together as the joint objective: </p><formula xml:id="formula_14">L Multi = L CER + L SI<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>We adopt Adam as the optimizer with the batch size of 4 to train our models, where the learning rates to fine-tune BERT and the other parameters are 1e−6 and 2.5e−4, respectively. Dropout rate with 0.5 is applied to avoid overfitting. The dimension sizes of the hidden states of all the BiGRUs is set to 200 on EmoryNLP and 150 on MELD. For evaluation, we exploit the standard weighted macro-F1 score as the major metric to measure all models, following Zhong et al. <ref type="bibr">[2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Models</head><p>For a comprehensive evaluation, we compare our model with the following baselines as well: CNN <ref type="bibr" target="#b2">[Kim, 2014]</ref> A convolutional neural network for utterance-level classification without using contextual information at the conversation level.   ConGCN  A multi-modal model for CER, which also exploits GCN to model the context-sensitive and speaker-sensitive dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Developmental Results</head><p>We conduct experiments on the developmental datasets of EmoryNLP and MELD to examine our proposed models. The Influence of T T represents the number of utterance pairs sampled from a conversation for SI, aiming to leverage the combined loss of the two tasks. We investigate the influence of T by ranging it from 1 to 5. <ref type="figure" target="#fig_6">Figure 4</ref> shows the results. As shown, we can obtain the best results when T = 3 on EmoryNLP and T = 2 on MELD, respectively, demonstrating the importance of sampling. In addition, the optimum T of different datasets may vary, which could be possibly due to the different averaged conversation length. The Influence of BERT Fine-Tuning The utilizing of BERT should be carefully studied. When BERT parameters are frozen, we can save the resource cost greatly, for example, the memory of GPU. However, it may lead to significant performance decrease. Here we study the gap between BERT fine-tuning and frozen. <ref type="figure" target="#fig_7">Figure 5</ref> shows the comparison results, where the baseline model and our final model are both investigated. As shown, we can see that by freezing the BERT parameters, drops of over 2% and 4% can be resulted   on EmoryNLP and MELD, respectively, demonstrating that fine-tuning is a necessary for CER.</p><p>Ablation Study To comprehensively study the effectiveness of the two bridging neural network structures at the different levels for mutual interaction, we conduct ablation experiments in detail. The results are offered in <ref type="table" target="#tab_2">Table 2</ref>. We can see that both the two networks can bring improved performance for CER. By excluding the bridging network at IUE, our final model falls by 0.19% on EmoryNLP and 0.50% on MELD, respectively. Similarly, the model shows 0.45% and 0.80% declines on the two datasets by eliminating the bridging network at CUE, respectively. When both network structures are removed, drops by 0.72% and 1.43% points on the two datasets are resulted, respectively.  EmoryNLP and MELD datasets are 34.39% and 59.40%, respectively. Our BERT-based baseline gives F-scores of 34.76% and 61.31%, respectively, which are both higher than the previous state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Final Results</head><p>According to the results, our models with MTL can achieve better performance on two datasets as compared to their corresponding baselines. On the two datasets, the F1-score improvements based on the pretrained golve embeddings are 1.95% and 1.02%, respectively. When the contextualized ELMO representations are exploited, the improvements over the baseline are 1.30% and 0.76%, respectively. For our baseline based on BERT, the final MTL enhanced model leads to F1 increases of 1.16% and 0.59% on the two datasets, respectively. All the improvements by MTL are significant (p-value below 0.0001 by using pair-wised t-test). Interestingly, we can also find that the improvements become smaller as the baseline becomes stronger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization Analysis</head><p>For comprehensive understanding of our proposed models, we visualize the attention matrices by a case study, which is selected from the MELD test dataset. <ref type="figure" target="#fig_8">Figure 6</ref> shows the example, where both salient words of individual utterance encoder and the key utterances of cross attention neural structure both are offered.</p><p>First, we examine the difference in salient words for individual utterance encoder. As shown, the final model treats gimme it, she, Rachel and Pheebs as strong clues for CER, which are speaker related, while misses the punctuation such as comma and period, which are mostly objective. The observation indicates that the shared Bi-GRU module can help to identify speaker-related words such as speaker names and attributes, highlighting them for further feature representation, and meanwhile can effectively exclude the unimportant ob-jective words. In addition, the comparison further demonstrates the importance of the speaker information because of the better performance of our final model.</p><p>At the conversation-level, cross attention mechanism is used to identify closely-related utterances for a given utterance. Here we show the indexes of the related utterances to study the learned information by MTL with SI. As shown by the rightmost part of <ref type="figure" target="#fig_8">Figure 6</ref>, we can see that the cross attention mechanism can help to associate the utterances with the same speaker and the next utterances of the targeted speakers for a specific utterance. For example, for the 9th utterance, the speaker is Phoebe, and the targeted speaker is Joey. By MTL, the model connects the 7th and 11th utterances from the same speaker, and meanwhile connects the 6th utterance from the target speaker. Intuitively, these utterances could be potential evidences to recognize the current emotion, which is demonstrated by our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a multi-task learning network for CER with the assistance of SI, aiming to better capture speaker-related information, which has been demonstrated important for CER. We exploited a strong baseline with BERT as backend, and then presented two neural network structures to bridge the two tasks for mutual interaction. We conducted on two benchmark datasets to verify the effectiveness of the proposed method. Results showed that our baseline is very strong, achieving the best performance compared with the previous state-of-the-art. Further, the MTL based method can boost the performance significantly, leading to a new state-of-the-art in the literature. Detailed experiments showed that our suggested components for MTL are both important. In addition, we analyzed the proposed model in depth for comprehensive understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " B G I v v 1 Q u e 1 a I S V w + 1 p G E u T 4 u C 1 M = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C m h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4 F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q n n 9 U p l t + L O Q J a J l 5 M y 5 K j 3 S l / d f s z S i C t k k h r T 8 d w E / Y x q F E z y S b G b G p 5 Q N q I D 3 r F U 0 Y g b P 5 u d O i G n V u m T M N a 2 F J K Z + n s i o 5 E x 4 y i w n R H F o V n 0 p u J / X i f F 8 M r P h E p S 5 I r N F 4 W p J B i T 6 d + k L z R n K M e W U K a F v Z W w I d W U o U 2 n a E P w F l 9 e J s 1 q x T u v V O 8 u y r X r P I 4 C H M M J n I E H l 1 C D W 6 h D A x g M 4 B l e 4 c 2 R z o v z 7 n z M W 1 e c f O Y I / s D 5 / A E L e I 2 j &lt; / l a t e x i t &gt; w 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k 9 T H 6 J R V G z z n x l g 0 B H K 2 A h K 6 D h 8 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R C m h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 K 6 t r 6 x W d g q b u / s 7 u 2 X D g 6 b J k 4 1 4 w 0 W y 1 i 3 A 2 q 4F I o 3 U K D k 7 U R z G g W S t 4 L R z d R v P X J t R K w e c J x w P 6 I D J U L B K F r p / q l X 7 Z X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E V f I J D W m 4 7 k J + h n V K J j k k 2 I 3 N T y h b E Q H v G O p o h E 3 f j Y 7 d U J O r d I n Y a x t K S Q z 9 f d E R i N j x l F g O y O K Q 7 P o T c X / v E 6 K 4 Z W f C Z W k y B W b L w p T S T A m 0 7 9 J X 2 j O U I 4 t o U w L e y t h Q 6 o p Q 5 t O 0 Y b g L b 6 8 T J r V i n d e q d 5 d l G v X e R w F O I Y T O A M P L q E G t 1 C H B j A Y w D O8 w p s j n R f n 3 f m Y t 6 4 4 + c w R / I H z + Q M M / I 2 k &lt; / l a t e x i t &gt; w3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J q a p 7 p i I c W W 5 u p 2 + v 3 A T n 2 Y 9 l B E = " &gt; A A A B 6 n i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H b B R I 9 E L x 4 x y i O B D Z k d e m H C 7 O x m Z l Z D C J / g x Y P G e P W L v P k 3 D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3 v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R z c x v P a L S P J Y P Z p y g H 9 G B 5 C F n 1 F j p / q l X 7 R V L b t m d g 6 w S L y M l y F D v F b + 6 / Z i l E U r D B N W 6 4 7 m J 8 S d U G c 4 E T g v d V G N C 2 Y g O s G O p p B F q f z I / d U r O r N I n Y a x s S U P m 6 u + J C Y 2 0 H k e B 7 Y y o G e p l b y b + 5 3 V S E 1 7 5 E y 6 T 1 K B k i 0 V h K o i J y e x v 0 u c K m R F j S y h T 3 N 5 K 2 J A q y o x N p 2 B D 8 J Z f X i X N S t m r l i t 3 F 6 X a d R Z H H k 7 g F M 7 B g 0 u o w S 3 U o Q E M B v A M r / D m C O f F e X c + F q 0 5 J 5 s 5 h j 9 w P n 8 A D o C N p Q = = &lt; / l a t e x i t &gt; w 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z 4 V s f m + P U 1 V 5 L I A y 0 e W 8 W R t O 4 2 0 = " &gt; A A A B 6 n i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H a R R I 9 E L x 4 x y i O B D Z k d e m H C 7 O x m Z l Z D C J / g x Y P G e P W L v P k 3 D r A H B S v p p F L V n e 6 u I B F c G 9 f 9 d n J r 6 x u b W / n t w s 7 u 3 v 5 B 8 f C o q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R z c x v P a L S P J Y P Z p y g H 9 G B 5 C F n 1 F j p / q l X 7 R V L b t m d g 6 w S L y M l y F D v F b + 6 / Z i l E U r D B N W 6 4 7 m J 8 S d U G c 4 E T g v d V G N C 2 Y g O s G O p p B F q f z I / d U r O r N I n Y a x s S U P m 6 u + J C Y 2 0 H k e B 7 Y y o G e p l b y b + 5 3 V S E 1 7 5 E y 6 T 1 K B k i 0 V h K o i J y e x v 0 u c K m R F j S y h T 3 N 5 K 2 J A q y o x N p 2 B D 8 J Z f X i X N S t m 7 K F f u q q X a d R Z H H k 7 g F M 7 B g 0 u o w S 3 U o Q E M B v A M r / D m C O f F e X c + F q 0 5 J 5 s 5 h j 9 w P n 8 A E A S N p g = = &lt; / l a t e x i t &gt; w5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + 8 r m s z E C I 9 E V 3 G o x P s 4 y 4 O 5 9 c p 8 = " &gt; A A A B 6 n i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H Z R o 0 e i F 4 8 Y 5 Z H A h s w O D U y Y n d 3 M z G r I h k / w 4 k F j v P p F 3 v w b B 9 i D g p V 0 U q n q T n d X E A u u j e t + O 7 m V 1 b X 1 j f x m Y W t 7 Z 3 e v u H / Q 0 F G i G N Z Z J C L V C q h G w S X W D T c C W 7 F C G g Y C m 8 H o Z u o 3 H 1 F p H s k H M 4 7 R D + l A 8 j 5 n 1 F j p / q l 7 0 S 2 W 3 L I 7 A 1 k m X k Z K k K H W L X 5 1 e h F L Q p S G C a p 1 2 3 N j 4 6 d U G c 4 E T g q d R G N M 2 Y g O s G 2 p p C F q P 5 2 d O i E n V u m R f q R s S U N m 6 u + J l I Z a j 8 P A d o b U D P W i N x X / 8 9 q J 6 V / 5 K Z d x Y l C y + a J + I o i J y P R v 0 u M K m R F j S y h T 3 N 5 K 2 J A q y o x N p 2 B D 8 B Z f X i a N S t k 7 K 1 f u z k v V 6 y y O P B z B M Z y C B 5 d Q h V u o Q R 0 Y D O A Z X u H N E c 6 L 8 + 5 8 z F t z T j Z z C H / g f P 4 A E Y i N p w = = &lt; / l a t e x i t &gt; u i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K K D U Q i l C Q P f Q s y r s 0 G Q P I Y J d B f E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " o w U 6 1 2 s s 0 k t / b e W s T Z 0 W W x k + P T U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 k P a 9 f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P Y z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 1 a 1 b u o 1 u 4 v K / W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g A I b I 2 h &lt; / l a t e x i t &gt; u 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 K K 0 0 9 f 1 v F r l g P w s t 9 m s Q x + M a a Y = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 Q e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q l e P C j i 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 6 p 5 O g k g U 1 N m g k 2 L e R / f w K Z C 7 Q k c = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 k k q 2 g 9 o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0 s F h U 8 e p Y t h g s Y h V O 6 A a B Z f Y M N w I b C c K a R Q I b A W j m 6 n f e k K l e S w f z T h B P 6 I D y U P O q L H S Q 9 q 7 6 5 X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E U r D B N W 6 4 7 m J 8 T O q D G c C J 8 V u q j G h b E Q H 2 L F U 0 g i 1 n 8 1 O n Z B T q / R J G C t b 0 p C Z + n s i o 5 H W 4 y i w n R E 1 Q 7 3 o T c X / v E 5 q w i s / 4 z J J D U o 2 X x S m g p i Y T P 8 m f a 6 Q G T G 2 h D L F 7 a 2 E D a m i z N h 0 i j Y E b / H l Z d K s V r z z S v X + o l y 7 z u M o w D G c w B l 4 c A k 1 u I U 6 N I D B A J 7 h F d 4 c 4 b w 4 7 8 7 H v H X F y W e O 4 A + c z x 8 0 Y I 2 + &lt; / l a t e x i t &gt; The overall architecture of our individual models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " o w U 6 1 2 s s 0 k t / b e W s T Z 0 W W x k + P T U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 k P a 9 f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P Y z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 1 a 1 b u o 1 u 4 v K / W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g A I b I 2 h &lt; / l a t e x i t &gt; u 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 K K 0 0 9 f 1 v F r l g P w s t 9 m s Q x + M a a Y = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 Q e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q l e P C j i 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>O Y Y / s D 5 / A E J 8 I 2 i &lt; / l a t e x i t &gt; u N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 p 5 O g k g U 1 N m g k 2 L e R / f w K Z C 7 Q k c = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 k k q 2 g 9 o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0 s F h U 8 e p Y t h g s Y h V O 6 A a B Z f Y M N w I b C c K a R Q I b A W j m 6 n f e k K l e S w f z T h B P 6 I D y U P O q L H S Q 9 q 7 6 5 X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E U r D B N W 6 4 7 m J 8 T O q D G c C J 8 V u q j G h b E Q H 2 L F U 0 g i 1 n 8 1 O n Z B T q / R J G C t b 0 p C Z + n s i o 5 H W 4 y i w n R E 1 Q 7 3 o T c X / v E 5 q w i s / 4 z J J D U o 2 X x S m g p i Y T P 8 m f a 6 Q G T G 2 h D L F 7 a 2 E D a m i z N h 0 i j Y E b / H l Z d K s V r z z S v X + o l y 7 z u M o w D G c w B l 4 c A k 1 u I U 6 N I D B A J 7 h F d 4 c 4 b w 4 7 8 7 H v H X F y W e O 4 A + c z x 8 0 Y I 2 + &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " K K D U Q i l C Q P f Q s y r s 0 G Q P I Y J d B f E = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 k P Z 5 v 1 x x q + 4 c Z J V 4 O a l A j k a / / N U b x C y N U B o m q N Z d z 0 2 M n 1 F l O B M 4 L f V S j Q l l Y z r E r q W S R q j 9 b H 7 q l J x Z Z U D C W N m S h s z V 3 x M Z j b S e R I H t j K g Z 6 W V v J v 7 n d V M T X v s Z l 0 l q U L L F o j A V x M R k 9 j c Z c I X M i I k l l C l u b y V s R B V l x q Z T s i F 4 y y + v k l a t 6 l 1 U a / e X l f p N H k c R T u A U z s G D K 6 j D H T S g C Q y G 8 A y v 8 O Y I 5 8 V 5 d z 4 W r Q U n n z m G P 3 A + f w B d T I 3 Z &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " N 8 k z r V X + 8 j k 4 R b r P E e W / T j D / 6 w w = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 k J J U Q Y 9 F L x 4 r 2 A 9 o Q 9 l s J + 3 S z S b s b p Q S + i O 8 e F D E q 7 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0s F h U 8 e p Y t h g s Y h V O 6 A a B Z f Y M N w I b C c K a R Q I b A W j 2 6 n f e k S l e S w f z D h B P 6 I D y U P O q L F S 6 6 m X 8 X N v 0 i u V 3 Y o 7 A 1 k m X k 7 K k K P e K 3 1 1 + z F L I 5 S G C a p 1 x 3 M T 4 2 d U G c 4 E T o r d V G N C 2 Y g O s G O p p B F q P 5 u d O y G n V u m T M F a 2 p C E z 9 f d E R i O t x 1 F g O y N q h n r R m 4 r / e Z 3 U h N d + x m W S G p R s v i h M B T E x m f 5 O + l w h M 2 J s C W W K 2 1 s J G 1 J F m b E J F W 0 I 3 u L L y 6 R Z r X g X l e r 9 Z b l 2 k 8 d R g G M 4 g T P w 4 A p q c A d 1 a A C D E T z D K 7 w 5 i f P i v D s f 8 9 Y V J 5 8 5 g j 9 w P n 8 A A A 2 P W A = = &lt; / l a t e x i t &gt; w i,2 &lt; l a te x i t s h a 1 _ b a s e 6 4 = " L K y / W n R B j i t x A H V / b f A 8 k 0 z H n 0 M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 k J J U Q Y 9 F L x 4 r 2 A 9 o Q 9 l s N + 3 S z S b s T p Q S + i O 8 e F D E q 7 / H m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O y u r a + s b m 4 W t 4 v b O 7 t 5 + 6 e C w a e J U M 9 5 g s Y x 1 O 6 C G S 6 F 4 A w V K 3 k 4 0 p 1 E g e S s Y 3 U 7 9 1 i P X R s T q A c c J 9 y M 6 U C I U j K K V W k + 9 T J x X J 7 1 S 2 a 2 4 M 5 B l 4 u W k D D n q v d J X t x + z N O I K m a T G d D w 3 Q T + j G g W T f F L s p o Y n l I 3 o g H c s V T T i x s 9 m 5 0 7 I q V X 6 J I y 1 L Y V k p v 6 e y G h k z D g K b G d E c W g W v a n 4 n 9 d J M b z 2 M 6 G S F L l i 8 0 V h K g n G Z P o 7 6 Q v N G c q x J Z R p Y W 8 l b E g 1 Z W g T K t o Q v M W X l 0 m z W v E u K t X 7 y 3 L t J o + j A M d w A m f g w R X U 4 A 7 q 0 A A G I 3 i G V 3 h z E u f F e X c + 5 q 0 r T j 5 z B H / g f P 4 A A Z K P W Q = = &lt; / l a t e x i t &gt; w i,3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r V t H 9 V 4 k p H O y 6 i 0 L w D L z X A e 7 Z + o = " &gt; A A A B 7 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g Q c J u I u g x 6 M V j B P O A Z A m z k 0 k y Z H Z 2 m e l V w p K P 8 O J B E a 9 + j z f / x k m y B 0 0 s a C i q u u n u C m I p D L r u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H T R M l m v E G i 2 S k 2 w E 1 X A r F G y h Q 8 n a s O Q 0 D y V v B + H b m t x 6 5 N i J S D z i J u R / S o R I D w S h a q f X U S 8 V F d d o r l t y y O w d Z J V 5 G S p C h 3 i t + d f s R S 0 K u k E l q T M d z Y / R T q l E w y a e F b m J 4 T N m Y D n n H U k V D b v x 0 f u 6 U n F m l T w a R t q W Q z N X f E y k N j Z m E g e 0 M K Y 7 M s j c T / / M 6 C Q 6 u / V S o O E G u 2 G L R I J E E I z L 7 n f S F 5 g z l x B L K t L C 3 E j a i m j K 0 C R V s C N 7 y y 6 u k W S l 7 1 X L l / r J U u 8 n i y M M J n M I 5 e H A F N b i D O j S A w R i e 4 R X e n N h 5 c d 6 d j 0 V r z s l m j u E P n M 8 f A x e P W g = = &lt; / l a t e x i t &gt; w i,4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 x B r s s G 7 Q r B Z S P M z y k d 7 Z 4 R O n o Y = " &gt; A A A B 7 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g Q c J u D O g x 6 M V j B P O A Z A m z k 0 k y Z H Z 2 m e l V w p K P 8 O J B E a 9 + j z f / x k m y B 0 0 s a C i q u u n u C m I p D L r u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H T R M l m v E G i 2 S k 2 w E 1 X A r F G y h Q 8 n a s O Q 0 D y V v B + H b m t x 6 5 N i J S D z i J u R / S o R I D w S h a q f X U S 8 V F d d o r l t y y O w d Z J V 5 G S p C h 3 i t + d f s R S 0 K u k E l q T M d z Y / R T q l E w y a e F b m J 4 T N m Y D n n H U k V D b v x 0 f u 6 U n F m l T w a R t q W Q z N X f E y k N j Z m E g e 0 M K Y 7 M s j c T / / M 6 C Q 6 u / V S o O E G u 2 G L R I J E E I z L 7 n f S F 5 g z l x B L K t L C 3 E j a i m j K 0 C R V s C N 7 y y 6 u k W S l 7 l + X K f b V U u 8 n i y M M J n M I 5 e H A F N b i D O j S A w R i e 4 R X e n N h 5 c d 6 d j 0 V r z s l m j u E P n M 8 f B J y P W w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " o w U 6 1 2 s s 0 k t / b e W s T Z 0 W W x k + P T U = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 Q e 0 o W y 2 k 3 b p Z h N 2 N 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s t H M 0 n Q j + h Q 8 p A z a q z 0 k P a 9 f r n i V t 0 5 y C r x c l K B H I 1 + + a s 3 i F k a o T R M U K 2 7 n p s Y P 6 P K c C Z w W u q l G h P K x n S I X U s l j V D 7 2 f z U K T m z y o C E s b I l D Z m r v y c y G m k 9 i Q L b G V E z 0 s v e T P z P 6 6 Y m v P Y z L p P U o G S L R W E q i I n J 7 G 8 y 4 A q Z E R N L K F P c 3 k r Y i C r K j E 2 n Z E P w l l 9 e J a 1 a 1 b u o 1 u 4 v K / W b P I 4 i n M A p n I M H V 1 C H O 2 h A E x g M 4 R l e 4 c 0 R z o v z 7 n w s W g t O P n M M f + B 8 / g A I b I 2 h &lt; / l a t e x i t &gt; u 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 K K 0 0 9 f 1 v F r l g P w s t 9 m s Q x + M a a Y = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V 7 Q e 0 o W y 2 m 3 b p Z h N 2 J 0 I J / Q l e P C j i 1 V / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 x E n C / Y g O l Q g F o 2 i l h 7 R f 6 5 c r b t W d g 6 w S L y c V y N H o l 7 9 6 g 5 i l E V f I J D W m 6 7 k J + h n V K J j k 0 1 I v N T y h b E y H v G u p o h E 3 f j Y / d U r O r D I g Y a x t K S R z 9 f d E R i N j J l F g O y O K I 7 P s z c T / v G 6 K 4 b W f C Z W k y B V b L A p T S T A m s 7 / J Q G j O U E 4 s o U w L e y t h I 6 o p Q 5 t O y Y b g L b + 8 S l q 1 q n d R r d 1 f V u o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w j O 8 w p s j n R f n 3 f l Y t B a c f O Y Y / s D 5 / A E J 8 I 2 i &lt; / l a t e x i t &gt; u N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 p 5 O g k g U 1 N m g k 2 L e R / f w K Z C 7 Q k c = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j 0 4 k k q 2 g 9 o Q 9 l s J + 3 S z S b s b o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d l Z W 1 9 Y 3 N g t b x e 2 d 3 b 3 9 0 s F h U 8 e p Y t h g s Y h V O 6 A a B Z f Y M N w I b C c K a R Q I b A W j m 6 n f e k K l e S w f z T h B P 6 I D y U P O q L H S Q 9 q 7 6 5 X K b s W d g S w T L y d l y F H v l b 6 6 / Z i l E U r D B N W 6 4 7 m J 8 T O q D G c C J 8 V u q j G h b E Q H 2 L F U 0 g i 1 n 8 1 O n Z B T q / R J G C t b 0 p C Z + n s i o 5 H W 4 y i w n R E 1 Q 7 3 o T c X / v E 5 q w i s / 4 z J J D U o 2 X x S m g p i Y T P 8 m f a 6 Q G T G 2 h D L F 7 a 2 E D a m i z N h 0 i j Y E b / H l Z d K s V r z z S v X + o l y 7 z u M o w D G c w B l 4 c A k 1 u I U 6 N I D B A J 7 h F d 4 c 4 b w 4 7 8 7 H v H X F y W e O 4 A + c z x 8 0 Y I 2 + &lt; / l a t e x i t &gt; Illustration of our proposed model for CER and SI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>c-LSTM [Poria et al., 2017] A hierarchical classification model based on LSTM-RNN model, where contextual utterance-level features are adopted. DialogueRNN [Majumder et al., 2019] A sophisticated RNN-based model based on three GRUs, which are used to model speakers, global contexts and historical emotions. KET [Zhong et al., 2019] The state-of-the-art model in the literature which exploits external commonsense knowledge to enhance the contextual utterance representation. DialogueGCN [Ghosal et al., 2019] A GCN-based model aiming to better representing inter-speaker dependence, where GRU is used as the basic feature composition modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Developmental experimental results of our model on EmoryNLP by varying the values of T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>The influence of BERT Fine-Tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the attentions, where the thresholds are 0.1 and 0.2 for words and utterances by their attention values, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablated performance on EmoryNLP and MELD, where IUE and CUE denote individual and contextual utterance encoders, respectively.</figDesc><table><row><cell>Method</cell><cell>EmoryNLP</cell><cell>MELD</cell></row><row><cell>Our (GloVe)</cell><cell>32.59</cell><cell>59.67</cell></row><row><cell>+MTL</cell><cell>34.54</cell><cell>60.69</cell></row><row><cell>Our (ELMo)</cell><cell>33.55</cell><cell>61.10</cell></row><row><cell>+MTL</cell><cell>34.85</cell><cell>61.86</cell></row><row><cell>Our (BERT)</cell><cell>34.76</cell><cell>61.31</cell></row><row><cell>+MTL</cell><cell>35.92</cell><cell>61.90</cell></row><row><cell>CNN</cell><cell>32.59</cell><cell>55.02</cell></row><row><cell>cLSTM</cell><cell>32.89</cell><cell>56.44</cell></row><row><cell>DialogueRNN</cell><cell>31.70</cell><cell>57.03</cell></row><row><cell>DialogueGCN</cell><cell>-</cell><cell>58.10</cell></row><row><cell>KET</cell><cell>34.39</cell><cell>58.18</cell></row><row><cell>ConGCN</cell><cell>-</cell><cell>59.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Final results on the test datasets of EmoryNLP, MELD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>shows the performance of various models on the test sections of the two datasets, respectively. We report the performance of our baseline model with three kinds of word representations, namely the pretrained glove word embeddings [Pennington et al., 2014] 2 , ELMO [Peters et al., 2018] 3 and BERT 4 . We can see that all our baseline results are strong and can achieve comparable performance with the previous state-of-the-art systems. The best-reported numbers on the Phoebe: Hey everybody , Rachel was so good today . She didn t gossip at all . Even when I found out umm , all right , well let s just say I found something out something about someone and let s just say she s gonna keep it . Hey everybody , Rachel was so good today . She didn t gossip at all . Even when I found out umm , all right , well let s just say I found something out something about someone and let s just say she s gonna keep it .</figDesc><table><row><cell>Rachel: I didn t !</cell></row><row><cell>Rachel: Joey: Hey , Pheebs ! Check check this out .</cell></row><row><cell>Phoebe: Ooh , you nailed the Old Lady !</cell></row><row><cell>Joey: Yeah listen so , I thought I was getting</cell></row><row><cell>better , so on my way home today I stopped</cell></row><row><cell>by this guitar store and</cell></row><row><cell>Phoebe: Did you , did you</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Bi-GRU is used as the basic RNN operation here by considering both efficiency and effectiveness.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://nlp.stanford.edu/data/glove.6B.zip, 300d 3 https://allennlp.org/elmo, original, 5.5B 4 https://github.com/google-research/bert, BERT-Base, Uncased</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">ICSLP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Neural machine translation by jointly learning to align and translate</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DialogueGCN: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<editor>Soujanya Poria, Amir Zadeh, Erik Cambria, Louis-Philippe Morency, and Roger Zimmermann</editor>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentence-level emotion classification with label and context dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim ; Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1045" to="1053" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder, Amir Zadeh, and Louis-Philippe Morency. Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
	</analytic>
	<monogr>
		<title level="m">Modeling task relationships in multi-task learning with multi-gate mixtureof-experts. In KDD</title>
		<meeting><address><addrLine>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,; Gautam Naik, Erik Cambria, and Rada Mihalcea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>J-STSP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan ; Shiyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<editor>Dong Zhang, Liangqing Wu, Changlong Sun, Shoushan Li, Qiaoming Zhu, and Guodong Zhou</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
	<note>EMNLP-IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task learning with language modeling for question generation</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3385" to="3390" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
