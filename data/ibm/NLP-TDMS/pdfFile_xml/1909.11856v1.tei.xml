<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight Image Super-Resolution with Information Multi-distillation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 21-25, 2019. October 21-25, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
							<email>zheng_hui@aliyun.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
							<email>xbgao@mail.xidian.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchu</forename><surname>Yang</surname></persName>
							<email>yc_yang@aliyun.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
							<email>wangxm@xidian.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchu</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University Xi&apos;an</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering</orgName>
								<address>
									<addrLine>Xidian University Xi&apos;an</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Electronic Engineering</orgName>
								<address>
									<addrLine>Xidian University Xi&apos;an</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Electronic Engineering</orgName>
								<address>
									<addrLine>Xidian University Xi&apos;an</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lightweight Image Super-Resolution with Information Multi-distillation Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 27th ACM International Conference on Multimedia (MM&apos;19)</title>
						<meeting>the 27th ACM International Conference on Multimedia (MM&apos;19) <address><addrLine>Nice, France; Nice</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 21-25, 2019. October 21-25, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3343031.3351084</idno>
					<note>, France. ACM, New York, NY, USA, 9 pages. https: //</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Computational photography</term>
					<term>Reconstruction</term>
					<term>Image processing * Corresponding author KEYWORDS image super-resolution</term>
					<term>lightweight network</term>
					<term>information multi- distillation</term>
					<term>contrast-aware channel attention</term>
					<term>adaptive cropping strategy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, single image super-resolution (SISR) methods using deep convolution neural network (CNN) have achieved impressive results. Thanks to the powerful representation capabilities of the deep networks, numerous previous ways can learn the complex non-linear mapping between low-resolution (LR) image patches and their high-resolution (HR) versions. However, excessive convolutions will limit the application of super-resolution technology in low computing power devices. Besides, super-resolution of any arbitrary scale factor is a critical issue in practical applications, which has not been well solved in the previous approaches. To address these issues, we propose a lightweight information multi-distillation network (IMDN) by constructing the cascaded information multidistillation blocks (IMDB), which contains distillation and selective fusion parts. Specifically, the distillation module extracts hierarchical features step-by-step, and fusion module aggregates them according to the importance of candidate features, which is evaluated by the proposed contrast-aware channel attention mechanism. To process real images with any sizes, we develop an adaptive cropping strategy (ACS) to super-resolve block-wise image patches using the same well-trained model. Extensive experiments suggest that the proposed method performs favorably against the state-of-the-art SR algorithms in term of visual quality, memory footprint, and inference time. Code is available at https://github.com/Zheng222/IMDN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Single image super-resolution (SISR) aims at reconstructing a highresolution (HR) image from its low-resolution (LR) observation, which is inherently ill-posed because many HR images that can be downsampled to an identical LR image. To address this problem, numerous image SR methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> based on deep neural architectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref> have been proposed and shown prominent performance.</p><p>Dong et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> first developed a three-layer network (SRCNN) to establish a direct relationship between LR and HR. Then, Wang et al. <ref type="bibr" target="#b30">[31]</ref> proposed a neural network according to the conventional sparse coding framework and further designed a progressive upsampling style to produce better SR results at the large scale factor (e.g., ×4). Inspired by VGG model <ref type="bibr" target="#b22">[23]</ref> that used for ImageNet classification, Kim et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> first pushed the depth of SR network to 20 and their model outperformed SRCNN by a large margin. This indicates a deeper model is instructive to enhance the quality of generated images. To accelerate the training of deep network, the authors introduced global residual learning with a high initial learning rate. At the same time, they also presented a deeply-recursive convolutional network (DRCN), which applied recursive learning to SR problem. This way can significantly reduce the model parameters. Similarly, Tai et al. proposed two novel networks, and one is a deep recursive residual network (DRRN) <ref type="bibr" target="#b23">[24]</ref>, another is a persistent memory network (MemNet) <ref type="bibr" target="#b24">[25]</ref>. The former mainly utilized recursive learning to reach the goal of economizing parameters. The latter model tackled the long-term dependency problem existed in the previous CNN architecture by several memory blocks that stacked with a densely connected structure <ref type="bibr" target="#b8">[9]</ref>. However, these two arXiv:1909.11856v1 [eess.IV] <ref type="bibr" target="#b25">26</ref> Sep 2019 algorithms required a long time and huge graphics memory consumption both in the training and testing phases. The primary reason is the inputs sent to these two models are interpolation version of LR images and the networks have not adopted any downsampling operations. This scheme will bring about a huge computational cost. To increase testing speed and shorten the testing time, Shi et al. <ref type="bibr" target="#b21">[22]</ref> first performed most of the mappings in low-dimensional space and designed an efficient sub-pixel convolution to upsample the resolutions of feature maps at the end of SR models.</p><p>To the same end, Dong et al. proposed fast SRCNN (FSRCNN) <ref type="bibr" target="#b5">[6]</ref>, which employed a learnable upsampling layer (transposed convolution) to accomplish post-upsampling SR. Afterward, Lai et al. presented the Laplacian pyramid super-resolution network (Lap-SRN) <ref type="bibr" target="#b13">[14]</ref> to progressively reconstruct higher-resolution images. Some other work such as MS-LapSRN <ref type="bibr" target="#b14">[15]</ref> and progressive SR (ProSR) <ref type="bibr" target="#b28">[29]</ref> also adopt this progressive upsampling SR framework and achieve relatively high performance. EDSR <ref type="bibr" target="#b17">[18]</ref> made a significant breakthrough in term of SR performance, which won the competition of NTIRE 2017 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref>. The authors removed some unnecessary modules (e.g., Batch Normalization) of the SRResNet <ref type="bibr" target="#b15">[16]</ref> to obtain better results. Based on EDSR, Zhang et al. incorporated densely connected block <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref> into residual block <ref type="bibr" target="#b6">[7]</ref> to construct a residual dense network (RDN). Soon they exploited the residualin-residual architecture for the very deep model and introduced channel attention mechanism <ref type="bibr" target="#b7">[8]</ref> to form the very deep residual attention networks (RCAN) <ref type="bibr" target="#b35">[36]</ref>. More recently, Zhang et al. also introduced spatial attention (non-local module) into the residual block and then constructed residual non-local attention network (RNAN) <ref type="bibr" target="#b36">[37]</ref> for various image restoration tasks.</p><p>The major trend of these algorithms is increasing more convolution layers to improve performance that measured by PSNR and SSIM <ref type="bibr" target="#b29">[30]</ref>. As a result, most of them suffered from large model parameters, huge memory footprints, and slow training and testing speeds. For instance, EDSR <ref type="bibr" target="#b17">[18]</ref> has about 43M parameters, 69 layers, and RDN <ref type="bibr" target="#b37">[38]</ref> achieved comparable performance, which has about 22M parameters, over 128 layers. Another typical network is RCAN <ref type="bibr" target="#b35">[36]</ref>, its depth up to 400 but the parameters are about 15.59M. However, these methods are still not suitable for resourceconstrained equipment. For the mobile devices, the desired practice should be to pursuing higher SR performance as much as possible when the available memory and inference time are constrained in a certain range. Many cases require not only the performance but also high execution speed, such as video applications, edge devices, and smartphones. Accordingly, it is significant to devise a lightweight but efficient model for meeting such demands.</p><p>Concerning the reduction of the parameters, many approaches adopted the recursive manner or parameter sharing strategy, such as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Although these methods did reduce the size of the model, they increased the depth or the width of the network to make up for the performance loss caused by the recursive module. This will lead to spending a great lot of calculating time when performing SR processing. To address this issue, the better way is to design the lightweight and efficient network structures that avoid using recursive paradigm. Ahn et al. developed CARN-M <ref type="bibr" target="#b1">[2]</ref> for mobile scenario through a cascading network architecture, but it is at the cost of a substantial reduction on PSNR. Hui et al. <ref type="bibr" target="#b10">[11]</ref> proposed an information distillation network (IDN) that explicitly divided the preceding extracted features into two parts, one was retained and another was further processed. Through this way, IDN achieved good performance at a moderate size. But there is still room for improvement in term of performance.</p><p>Another factor that affects the inference speed is the depth of the network. In the testing phase, the previous layer and the next layer have dependencies. Simply, conducting the computation of the current layer must wait for the previous calculation is completed. But multiple convolutional operations at each layer can be processed in parallel. Therefore, the depth of model architecture is an essential factor affecting time performance. This point will be verified in Section 4.</p><p>As to solving the different scale factors (×2, ×3, ×4) SR problem using a single model, previous solutions pretreated an image to the desired size and using the fully convolutional network without any downsampling operations. This way will inevitably lead to a substantial increase in the amount of calculation.</p><p>To address the above issues, we propose a lightweight information multi-distillation network (IMDN) for better balancing performance against applicability. Unlike most previous small parameters models that use recursive structure, we elaborately design an information multi-distillation block (IMDB) inspired by <ref type="bibr" target="#b10">[11]</ref>. The proposed IMDB extracts features at a granular level, which retains partial information and further treats other features at each step (layer) as illustrated in <ref type="figure">Figure 2</ref>. For aggregating features distilled by all steps, we devise a contrast-aware channel attention layer, specifically related to the low-level vision tasks, to enhance collected various refined information. Concretely, we exploit more useful features (edges, corners, textures, et al. ) for image restoration. In order to handle SR of any arbitrary scale factor with a single model, we need to scale the input image to the target size, and then employ the proposed adaptive cropping strategy (see in <ref type="figure" target="#fig_2">Figure 4</ref>) to obtain image patches of appropriate size for lightweight SR model with downsampling layers.</p><p>The contributions of this paper can be summarized as follows:</p><p>• We propose a lightweight information multi-distillation network (IMDN) for fast and accurate image super-resolution. Thanks to our information multi-distillation block (IMDB) with contrast-aware attention (CCA) layer, we achieve competitive results with a modest number of parameters (refer to <ref type="figure">Figure 6</ref>). • We propose the adaptive cropping strategy (ACS), which allows the network included downsampling operations (e.g., convolution layer with a stride of 2) to process images of any arbitrary size. By adopting this scheme, the computational cost, memory occupation, and inference time can dramatically reduce in the case of treating indefinite magnification SR. • We explore factors affecting actual inference time through experiments and find the depth of the network is related to the execution speed. It can be a guideline for guiding a lightweight network design. And our model achieves an excellent balance among visual quality, inference speed, and memory occupation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single image super-resolution</head><p>With the rapid development of deep learning, numerous methods based on convolutional neural network (CNN) have been the mainstream in SISR. The pioneering work of SR is proposed by Dong et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> named SRCNN. The SRCNN upscaled the LR image with bicubic interpolation before feeding into the network, which would cause substantial unnecessary computational cost. To address this issue, the authors removed this pre-processing and upscaled the image at the end of the net to reduce the computation in <ref type="bibr" target="#b5">[6]</ref>. Lim et al. <ref type="bibr" target="#b17">[18]</ref> modified SRResNet <ref type="bibr" target="#b15">[16]</ref> to construct a more in-depth and broader residual network denoted as EDSR. With the smart topology structure and a significantly large number of learnable parameters, EDSR dramatically advanced the SR performance. Zhang et al. <ref type="bibr" target="#b37">[38]</ref> introduced channel attention <ref type="bibr" target="#b7">[8]</ref> into the residual block to further boost very deep network (more than 400 layers without considering the depth of channel attention modules). Liu <ref type="bibr" target="#b18">[19]</ref> explored the effectiveness of non-local module applied to image restoration. Similarly, Zhang et al. <ref type="bibr" target="#b36">[37]</ref> utilized non-local attention to better guide feature extraction in their trunk branch for reaching better performance. Very recently, Li et al. <ref type="bibr" target="#b16">[17]</ref> exploited feedback mechanism that enhancing low-level representation with high-level ones. For lightweight networks, Hui et al. <ref type="bibr" target="#b10">[11]</ref> developed the information distillation network for better exploiting hierarchical features by separation processing of the current feature maps. And Ahn <ref type="bibr" target="#b1">[2]</ref> designed an architecture that implemented a cascading mechanism on a residual network to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention model</head><p>Attention model, aiming at concentrating on more useful information in features, has been widely used in various computer vision tasks. Hu et al. <ref type="bibr" target="#b7">[8]</ref> introduced squeeze-and-excitation (SE) block that models channel-wise relationships in a computationally efficient manner and enhances the representational ability of the network, showing its effectiveness on image classification. CBAM <ref type="bibr" target="#b31">[32]</ref> modified the SE block to exploit both spatial and channel-wise attention. Wang et al. <ref type="bibr" target="#b27">[28]</ref> proposed the non-local module to generate the wide attention map by calculating the correlation matrix between each spatial point in the feature map, then the attention map guided dense contextual information aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 Framework</head><p>In this section, we describe our proposed information multi-distillation network (IMDN) in detail, its graphical depiction is shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a). The upsampler (see <ref type="figure" target="#fig_0">Figure 1</ref>(b)) includes one 3 × 3 convolution with 3 × s 2 output channels and a sub-pixel convolution. Given an input LR image I LR , its corresponding target HR image I H R . The super-resolved image I S R can be generated by</p><formula xml:id="formula_0">I S R = H I M D N I LR ,<label>(1)</label></formula><p>where H I M D N (·) is our IMDN. It is optimized with mean absolute error (MAE) loss followed most of previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. Given a training set</p><formula xml:id="formula_1">I LR i , I H R i N i=1</formula><p>that has N LR-HR pairs. Thus, the loss function of our IMDN can be expressed by</p><formula xml:id="formula_2">L (Θ) = 1 N N i=1 H I M D N I LR i − I H R i 1 ,<label>(2)</label></formula><p>where Θ indicates the updateable parameters of our model and ∥·∥ 1 is l 1 norm. Then we give more details about the entire framework. We first conduct LR feature extraction implemented by one 3 × 3 convolution with 64 output channels. Then, the key component of our network utilizes multiple stacked information multi-distillation blocks (IMDBs) and assembles all intermediate features to fusing by a 1 × 1 convolution layer. This scheme, intermediate information collection (IIC), is beneficial to guarantee the integrity of the collected information and can further boost the SR performance by increasing very few parameters. The final upsampler only consists of one learnable layer and a non-parametric operation (sub-pixel convolution) for saving parameters as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information multi-distillation block</head><p>As depicted in <ref type="figure">Figure 2</ref>, our information multi-distillation block (IMDB) is constructed by progressive refinement module, contrastaware channel attention (CCA) layer, and a 1 × 1 convolution that is used to reduce the number of feature channels. The whole block adopts residual connection. The main idea of this block is extracting useful features little by little like DenseNet <ref type="bibr" target="#b8">[9]</ref>. Then we give more details to these modules. 3.2.1 Progressive refinement module. As labeled with the gray box in <ref type="figure">Figure 2</ref>, the progressive refinement module (PRM) first adopts the 3 × 3 convolution layer to extract input features for multiple subsequent distillation (refinement) steps. For each step, we employ channel split operation on the preceding features, which will produce two-part features. One is preserved and the other portion is fed into the next calculation unit. The retained part can be regarded as the refined features. Given the input features F in , this procedure in the n-th IMDB can be described as</p><formula xml:id="formula_3">F n r ef ined _1 , F n coar se_1 = Split n 1 CL n 1 F n in , F n r ef ined _2 , F n coar se_2 = Split n 2 CL n 2 F n coar se_1 , F n r ef ined _3 , F n coar se_3 = Split n 3 CL n 3 F n coar se_2 , F n r ef ined _4 = CL n 4 F n coar se_3 ,<label>(3)</label></formula><p>where CL n j denotes the j-th convolution layer (including Leaky ReLU) of the n-th IMDB, Split n j indicates the j-th channel split layer   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrast</head><p>Conv-3 LR 64 <ref type="figure">Figure 2</ref>: The architecture of our proposed information multi-distillation block (IMDB). Here, 64, 48, and 16 all represent the output channels of the convolution layer. "Conv-3" denotes the 3 × 3 convolutional layer, and "CCA Layer" indicates the proposed contrast-aware channel attention (CCA) that is depicted in <ref type="figure" target="#fig_1">Figure 3</ref>. Each convolution followed by a Leaky ReLU activation function except for the last 1 × 1 convolution. We omit them for concise.</p><p>of the n-th IMDB, F n r e f ined_j represents the j-th refined features (preserved), and F n coar se_j is the j-th coarse features to be further processed. The hyperparameter of PRM architecture is shown in <ref type="table" target="#tab_0">Table 1</ref>. The following stage is concatenating refined features from each step. It can be expressed by F n dist ill ed = Concat F n r e f ined _1 , F n r e f ined_2 , F n r e f ined_3 , F n r e f ined _4 ,</p><p>where Concat denotes concatenation operation along the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Contrast-aware channel attention layer. The initial channel attention is employed in image classification task and is well-known as the squeeze-and-excitation (SE) module. In the high-level field, the importance of a feature map depends on activated high-value areas, since these regions in favor of classification or detection. Accordingly, global average/maximum pooling is utilized to capture <ref type="bibr" target="#b15">16</ref> Channel  the global information in these high-level or mid-level vision. Although the average pooling can indeed improve the PSNR value, it lacks the information about structures, textures, and edges that are propitious to enhance image details (related to SSIM). As depicted in <ref type="figure" target="#fig_1">Figure 3</ref>, the contrast-aware channel attention module is special to low-level vision, e.g., image super-resolution, and enhancement. Specifically, we replace global average pooling with the summation of standard deviation and mean (evaluating the contrast degree of a feature map). Let's denote X = [x 1 , . . . , x c , . . . , x C ] as the input, which has C feature maps with spatial size of H × W . Therefore, the contrast information value can be calculated by</p><note type="other">Split Conv-3 Concat Conv-1 64 16 48 Channel Split Conv-</note><formula xml:id="formula_5">z c = H GC (x c ) = 1 HW (i, j)∈x c x i, j c − 1 HW (i, j)∈x c x i, j c 2 + 1 HW (i, j)∈x c x i, j c ,<label>(5)</label></formula><p>where z c is the c-th element of output. H GC (·) indicates the global contrast (GC) information evaluation function. With the assistance of the CCA module, our network can steadily improve the accuracy of SISR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive cropping strategy</head><p>The adaptive cropping strategy (ACS) is special to image of any arbitrary size super-resolving. Meanwhile, it can also deal with the SR problem of any scale factor with a single model (see <ref type="bibr">Figure 5)</ref>. We slightly modify the original IMDN by introducing two downsampling layer and construct the current IMDN_AS (IMDN for any scales). Here, the LR and HR images have the same spatial size (height and width). To handle images whose height and width are not divisible by 4, we first cut the entire images into 4 parts and then feed them into our IMDN_AS. As illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>, we can obtain 4 overlapped image patches through ACS. Take the first patch in the upper left corner as an example, and we give the    details about ACS. This image patch must satisfy</p><formula xml:id="formula_6">H 2 + ∆l H %4 = 0, W 2 + ∆l W %4 = 0,<label>(6)</label></formula><p>where ∆l H , ∆l W are extra increments of height and width, respectively. They can be computed by</p><formula xml:id="formula_7">∆l H = paddinд H − H 2 + paddinд H %4, ∆l W = paddinд W − W 2 + paddinд W %4,<label>(7)</label></formula><p>where paddinд H , paddinд W are preset additional lengths. In general, their values are setting by</p><formula xml:id="formula_8">paddinд H = paddinд W = 4k, k ≥ 1.<label>(8)</label></formula><p>Here, k is an integer greater than or equal to 1. These four patches can be processed in parallel (they have the same sizes), after which the outputs are pasted to their original location, and the extra increments (∆l H and ∆l W ) are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and metrics</head><p>In our experiments, we use the DIV2K dataset <ref type="bibr" target="#b0">[1]</ref>, which contains 800 high-quality RGB training images and widely used in image restoration tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. For evaluation, we use five widely used benchmark datasets: Set5 <ref type="bibr" target="#b2">[3]</ref>, Set14 <ref type="bibr" target="#b32">[33]</ref>, BSD100 <ref type="bibr" target="#b19">[20]</ref>, Ur-ban100 <ref type="bibr" target="#b9">[10]</ref>, and Manga109 <ref type="bibr" target="#b20">[21]</ref>. We evaluate the performance of the super-resolved images using two metrics, including peak signalto-noise ratio (PSNR) and structure similarity index (SSIM) <ref type="bibr" target="#b29">[30]</ref>. As with existing works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>, we calculate the values on the luminance channel (i.e., Y channel of the YCbCr channels converted from the RGB channels). Additionally, for any/unknown scale factor experiments, we use RealSR dataset from NTIRE2019 Real Super-Resolution Challenge 1 . It is a novel dataset of real low and high resolution paired images. The training data consists of 60 real low, and high resolution paired images, and the validation data contains 20 LR-HR pairs. It is noteworthy that the LR and HR have the same size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>To obtain LR DIV2K training images, we downscale HR images with the scaling factors (×2, ×3, and ×4) using bicubic interpolation in MATLAB R2017a. The HR image patches with a size of 192 × 192 are randomly cropped from HR images as the input of our model, and the mini-batch size is set to 16. For data augmentation, we perform randomly horizontal flip and 90 degree rotation. Our model is trained by ADAM optimizer with the momentum parameter β 1 = 0.9. The initial learning rate is set to 2 × 10 −4 and halved at every 2 × 10 5 iterations. We set the number of IMDB to 6 in our IMDN and IMDN_AS. We apply PyTorch framework to implement the proposed network on the desktop computer with 4.2GHz Intel i7-7700K CPU, 64G RAM, and NVIDIA TITAN Xp GPU (12G memory).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model analysis</head><p>In this subsection, we investigate model parameters, the effectiveness of IMDB, the intermediate information collection scheme, and adaptive cropping strategy.  <ref type="figure">Figure 6</ref>: Trade-off between performance and number of parameters on Set5 ×4 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Model parameters.</head><p>To construct a lightweight SR model, the parameters of the network is vital. From <ref type="table" target="#tab_10">Table 5</ref>, we can observe that our IMDN with fewer parameters achieves comparative or better performance when comparing with other state-of-the-art methods, such as EDSR-baseline (CVPRW'17), IDN (CVPR'18), SR-MDNF (CVPR'18), and CARN (ECCV <ref type="bibr">'18)</ref>. We also visualize the trade-off analysis between performance and model size in <ref type="figure">Figure 6</ref>.</p><p>We can see that our IMDN achieves a better trade-off between the performance and model size.   <ref type="figure">Figure 7</ref>. From <ref type="table" target="#tab_6">Table 2</ref>, we can find out that the CCA module leads to performance improvement (PSNR: +0.09dB, SSIM: +0.0012 for ×4 Manga109) only by increasing 2K parameters (which is an increase of 0.4%). The results compared with the CA module are placed in <ref type="table" target="#tab_7">Table 3</ref>. To study the efficiency of PRM in IMDB, we replace it with three cascaded 3 × 3 convolution layers (64 channels) and remove the final 1 × 1 convolution (used for fusion). The compared results are given in <ref type="table" target="#tab_6">Table 2</ref>. Although this network has more parameters (510K), its performance is much lower than our IMDN_basic_B4 (480K) especially on Urban100 and Manga109 datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with state-of-the-arts</head><p>We compare our IMDN with 11 state-of-the-art methods: SRCNN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, FSRCNN <ref type="bibr" target="#b5">[6]</ref>, VDSR <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b12">[13]</ref>, LapSRN <ref type="bibr" target="#b13">[14]</ref>, DRRN <ref type="bibr" target="#b23">[24]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, IDN <ref type="bibr" target="#b10">[11]</ref>, EDSR-baseline <ref type="bibr" target="#b17">[18]</ref>, SRMDNF <ref type="bibr" target="#b33">[34]</ref>, and CARN <ref type="bibr" target="#b1">[2]</ref>. <ref type="table" target="#tab_10">Table 5</ref> shows quantitative comparisons for ×2, ×3, and ×4 SR. It can find out that our IMDN performs favorably against other compared approaches on most datasets, especially at the scaling factor of ×2. <ref type="figure" target="#fig_6">Figure 8</ref> shows ×2, ×3 and ×4 visual comparisons on Set5 and Urban100 datasets. For "img_67" image from Urban100, we can see that grid structure is recovered better than others. It also demonstrates the effectiveness of our IMDN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Running time</head><p>where l is the layer index, L denotes the total number of layers, and f represents the spatial size of the filters. The number of convolutional kernels belong to l-th layer is n l , and its input channels are n l −1 . Suppose that the spatial size of output feature maps is m l × m l , the time complexity can be roughly calculated by</p><formula xml:id="formula_10">O L l =1 n l −1 · n l · f 2 l · m 2 l .<label>(10)</label></formula><p>We assume that the size of the HR image is m × m and then the computational costs can be calculated by Equation 10 (see <ref type="table" target="#tab_12">Table 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Running Time.</head><p>We use official codes of the compared methods to test their running time in a feed-forward process. From      Our IMDN achieves dominant performance in term of memory usage and time consumption.</p><p>For more intuitive comparisons with other approaches, we provide the trade-off between the running time and performance on Set5 dataset for ×4 SR in the <ref type="figure" target="#fig_7">Figure 9</ref>. It shows our IMDN gains comparable execution time and best PSNR value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose an information multi-distillation network for lightweight and accurate single image super-resolution. We construct a progressive refinement module to extract hierarchical feature step-by-step. By cooperating with the proposed contrastaware channel attention module, the SR performance is significantly and steadily improved. Additionally, we present the adaptive cropping strategy to solve the SR problem of an arbitrary scale factor, which is critical for the application of SR algorithms in the actual scenes. Numerous experiments have shown that the proposed method achieves a commendable balance between factors affecting practical use, including visual quality, execution speed, and memory consumption. In the future, this approach will be explored to facilitate other image restoration tasks such as image denoising and enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of information multi-distillation network (IMDN). (a) The orange box represents Leaky ReLU activation function and the details of IMDB is shown in Figure 2. (b) s represents the upscale factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Contrast-aware channel attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The diagrammatic sketch of adaptive cropping strategy (ACS). The cropped image patches in the green dotted boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The network structure of our IMDN_AS. "s2" represents the stride of 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 5 . 1</head><label>51</label><figDesc>Complexity analysis. As the proposed IMDN mainly consists of convolutions, the total number of parameters can be computed as Params = L l =1 n l −1 · n l · f 2 l conv + n l bias ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Visual comparisons of IMDN with other SR methods on Set5 and Urban100 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Trade-off between performance and running time on Set5 ×4 dataset. VDSR, DRCN, and LapSRN were implemented by MatConvNet, while DRRN, and IDN employed Caffe package. The rest EDSR-baseline, CARN, and our IMDN utilized PyTorch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PRM architecture. The columns represent layer, kernel-size, stride, input channels, and output channels. The symbols, C, and L denote a convolution layer, and Leaky ReLU (α = 0.05).</figDesc><table><row><cell cols="5">Layer Kernel Stride Input_channel Output_channel</cell></row><row><cell>CL</cell><cell>3</cell><cell>1</cell><cell>64</cell><cell>64</cell></row><row><cell>CL</cell><cell>3</cell><cell>1</cell><cell>48</cell><cell>64</cell></row><row><cell>CL</cell><cell>3</cell><cell>1</cell><cell>48</cell><cell>64</cell></row><row><cell>CL</cell><cell>3</cell><cell>1</cell><cell>48</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Information Multiple Distillations Network (IMDN)</figDesc><table><row><cell>Conv-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">nnel Split Conv-3 Channel Split</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LR</cell><cell>Conv-3</cell><cell>IMDB</cell><cell>IMDB</cell><cell>IMDB</cell><cell>IMDB</cell><cell>Conv-1</cell><cell>Conv-3</cell><cell>Upsampler</cell></row><row><cell></cell><cell>Conv-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell>64</cell><cell>64</cell></row><row><cell></cell><cell cols="2">Channel Split</cell><cell></cell><cell></cell><cell>Conv-3</cell><cell>progressive refinement</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Conv-3</cell><cell></cell><cell></cell><cell>Channel Split</cell><cell>module (PRM)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Channel Split</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Concat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Conv-3 Channel Split Conv-3</cell><cell></cell><cell></cell><cell>Contrast</cell><cell>Conv-1</cell><cell>Conv-1</cell><cell>sigmod</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Concat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>64</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CCA Layer</cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv-1</cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Conv-1</cell><cell>Conv-1</cell><cell>sigmod</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 H     l    </cell><cell>H</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2 W         l W</cell></row><row><cell>IMDB</cell><cell>IMDB</cell><cell>IMDB</cell><cell></cell><cell>64 Conv-1</cell><cell>64 Conv-3</cell><cell>Upsampler</cell><cell>SR</cell><cell></cell><cell></cell><cell></cell><cell>Contrast Mean</cell><cell>Conv-1 Conv-1</cell><cell>Conv-1 Conv-1</cell><cell>sigmod</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Investigations of CCA module and IIC scheme. 499K 32.11 / 0.8934 28.52 / 0.7797 27.53 / 0.7342 25.90 / 0.7797 30.28 / 0.9054</figDesc><table><row><cell>Set5 PSNR / SSIM # # # 510K 31.86 / 0.8901 28.43 / 0.7775 27.45 / 0.7320 25.63 / 0.7711 29.92 / 0.9003 Set14 BSD100 Urban100 Manga109 Scale PRM CCA IIC Params PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM ×4 ! # # 480K 32.01 / 0.8927 28.49 / 0.7792 27.50 / 0.7338 25.81 / 0.7773 30.16 / 0.9038 ! ! # 482K 32.10 / 0.8934 28.51 / 0.7794 27.52 / 0.7341 25.89 / 0.7793 30.25 / 0.9050 ! ! !</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparison with original channel attention (CA) and the presented contrast-aware channel attention (CCA).</figDesc><table><row><cell>IMDB</cell><cell>IMDB</cell><cell></cell><cell>64 64 Conv-3 Conv-1</cell><cell>Upsampler</cell><cell>SR</cell><cell cols="2">Conv-3 2 3 s </cell><cell>Sub-pixel</cell><cell></cell><cell></cell></row><row><cell>sigmod</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Module</cell><cell></cell><cell></cell><cell>Set5</cell><cell></cell><cell>Set14</cell><cell></cell><cell cols="2">BSD100 Urban100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">IMDN_basic_B4 + CA</cell><cell cols="2">32.0821</cell><cell cols="2">28.5086</cell><cell>27.5124</cell><cell>25.8829</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">IMDN_basic_B4 + CCA 32.0964 28.5118 27.5185</cell><cell>25.8916</cell></row><row><cell cols="2">2 H     l    </cell><cell>H</cell><cell>LR</cell><cell>Conv-3</cell><cell>IMDB</cell><cell>IMDB</cell><cell>IMDB</cell><cell>IMDB</cell><cell>Conv-3</cell><cell>Upsampler</cell><cell>SR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">Figure 7: The structure of IMDN_basic_B4.</cell></row><row><cell>Mean</cell><cell>Conv-1</cell><cell>Conv-1</cell><cell cols="9">Mean 4.3.2 Ablation studies of CCA module and IIC scheme. To quickly Conv-1 Conv-1 validate the effectiveness of the contrast-aware attention (CCA) module and intermediate information collection (IIC) scheme, we</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">adopt 4 IMDBs to conduct the following ablation study experi-</cell></row><row><cell>Contrast</cell><cell>Conv-1</cell><cell>Conv-1</cell><cell cols="9">Contrast ment, named IMDN_B4. When removing the CCA module and IIC Conv-1 Conv-1 sigmod sigmod scheme, the IMDN_B4 becomes IMDN_basic_B4 as illustrated in</cell></row><row><cell></cell><cell cols="2">4 64</cell><cell></cell><cell></cell><cell></cell><cell cols="2">4 64</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Quantitative evaluation of VDSR and our IMDN_AS in PSNR, SSIM, LPIPS, running time, and memory occupation.</figDesc><table><row><cell>Method</cell><cell>PSNR</cell><cell>SSIM</cell><cell>LPIPS [35]</cell><cell>Time</cell><cell>Memory</cell></row><row><cell cols="2">VDSR [12] 28.75</cell><cell>0.8439</cell><cell>0.2417</cell><cell>0.0290</cell><cell>7,855M</cell></row><row><cell cols="3">IMDN_AS 29.35 0.8595</cell><cell>0.2147</cell><cell cols="2">0.0041 3,597M</cell></row><row><cell cols="6">4.3.3 Investigation of ACS. To verify the efficiency of the proposed</cell></row><row><cell cols="6">adaptive cropping strategy (ACS), we use RealSR training images</cell></row><row><cell cols="6">to train VDSR [12] and our IMDN_AS. The results, evaluated on</cell></row><row><cell cols="6">RealSR RGB validation dataset, are illustrated in Table 4 and we</cell></row><row><cell cols="3">1 http://www.vision.ee.ethz.ch/ntire19/</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 ,</head><label>6</label><figDesc>we can be informed of actual execution time is related to the depth of networks. Although EDSR has a large number of parameters (43M), it runs very fast. The only drawback is that it takes up more graphics memory. The main reason should be the convolution computation for each layer are parallel. And RCAN has only 16M parameters, its depth is up to 415 and results in very slow inference speed. Compared with CARN<ref type="bibr" target="#b1">[2]</ref> and EDSR-baseline<ref type="bibr" target="#b17">[18]</ref>,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Average PSNR/SSIM for scale factor ×2, ×3 and ×4 on datasets Set5, Set14, BSD100, Urban100, and Manga109. Best and second best results are highlighted and underlined. 0.9270 30.32 / 0.8417 29.09 / 0.8046 28.17 / 0.8519 33.61 / 0.9445 Bicubic .8937 28.60 / 0.7806 27.58 / 0.7349 26.07 / 0.7837 30.47 / 0.9084</figDesc><table><row><cell>Method</cell><cell>Scale Params</cell><cell>Set5 PSNR / SSIM</cell><cell>Set14 PSNR / SSIM</cell><cell>BSD100 PSNR / SSIM</cell><cell>Urban100 PSNR / SSIM</cell><cell>Manga109 PSNR / SSIM</cell></row><row><cell>Bicubic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Memory Consumption (MB) and average inference time (second).</figDesc><table><row><cell>Method</cell><cell>Scale Params Depth</cell><cell>BSD100 Memory / Time Memory / Time Memory / Time Urban100 Manga109</cell></row><row><cell>EDSR-baseline [18]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :PSNR (dB) Execution time (sec)</head><label>7</label><figDesc>The computational costs. For representing concisely, we omit m 2 . Least and second least computational costs are highlighted and underlined.</figDesc><table><row><cell cols="6">Scale LapSRN [14] IDN [11] EDSR-b [18] CARN [2] IMDN</cell></row><row><cell>×2</cell><cell>112K</cell><cell>175K</cell><cell>341K</cell><cell>157K</cell><cell>173K</cell></row><row><cell>×3</cell><cell>76K</cell><cell>75K</cell><cell>172K</cell><cell>90K</cell><cell>78K</cell></row><row><cell>×4</cell><cell>76K</cell><cell>51K</cell><cell>122K</cell><cell>76K</cell><cell>45K</cell></row><row><cell>32.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>IMDN</cell><cell></cell></row><row><cell>32.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>32.1</cell><cell></cell><cell></cell><cell></cell><cell>CARN</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>EDSR-baseline</cell><cell></cell></row><row><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>31.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>31.8</cell><cell></cell><cell></cell><cell>IDN</cell><cell></cell><cell></cell></row><row><cell>31.7</cell><cell>DRRN_B1U9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>31.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>31.5</cell><cell>DRCN</cell><cell></cell><cell>LapSRN</cell><cell></cell><cell></cell></row><row><cell>31.4</cell><cell></cell><cell>VDSR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>31.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>0.1</cell><cell>0.01</cell><cell cols="2">0.001</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the National Natural Science </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Challenge on Single Image Super-Resolution: Dataset and Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="252" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie Line Alberi-Morel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and Accurate Single Image Super-Resolution via Information Distillation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-Realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feedback Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-Local Recurrent Network for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1680" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuma</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ferenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MemNet: A Persistent Memory Network for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Challenge on Single Image Super-Resolution: Methods and Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="965" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Dense Skip Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiejie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4799" to="4807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
	<note>Non-local Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Fully Progressive Approach to Single-Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkin-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional Block Attention Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>Joon-Young Lee, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Curves and Surfaces (ICCS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a Single Convolutional Super-Resolution Network for Multiple Degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Very Deep Residual Channel Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Residual Non-local Attention Networks for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Residual Dense Network for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
