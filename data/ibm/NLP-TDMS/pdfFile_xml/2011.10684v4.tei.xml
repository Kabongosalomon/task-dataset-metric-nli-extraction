<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SHOT-VAE: Semi-supervised Deep Generative Models With Label-aware ELBO Approximations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Zhe</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
							<email>kong@cs.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
							<email>minghaochen01@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
							<email>minfeng.zhu@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SHOT-VAE: Semi-supervised Deep Generative Models With Label-aware ELBO Approximations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised variational autoencoders (VAEs) have obtained strong results, but have also encountered the challenge that good ELBO values do not always imply accurate inference results. In this paper, we investigate and propose two causes of this problem: (1) The ELBO objective cannot utilize the label information directly. (2) A bottleneck value exists, and continuing to optimize ELBO after this value will not improve inference accuracy. On the basis of the experiment results, we propose SHOT-VAE to address these problems without introducing additional prior knowledge. The SHOT-VAE offers two contributions: (1) A new ELBO approximation named smooth-ELBO that integrates the label predictive loss into ELBO. (2) An approximation based on optimal interpolation that breaks the ELBO value bottleneck by reducing the margin between ELBO and the data likelihood. The SHOT-VAE achieves good performance with 25.30% error rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11% on CIFAR-10 with 4k labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Most deep learning models are trained with large labeled datasets via supervised learning. However, in many scenarios, although acquiring a large amount of original data is easy, obtaining corresponding labels is often costly or even infeasible. Thus, semi-supervised variational autoencoder (VAE) <ref type="bibr" target="#b13">(Kingma et al. 2014</ref>) is proposed to address this problem by training classifiers with multiple unlabeled data and a small fraction of labeled data.</p><p>Based on the latent variable assumption <ref type="bibr" target="#b6">(Doersch 2016</ref>), semi-supervised VAE models combine the evidence lower bound (ELBO) and the classification loss as objective, so that it can not only learn the required classification representations from labeled data, but also capture the disentangled factors which could be used for data generation. Although semi-supervised VAE models have obtained strong empirical results on many benchmark datasets (e.g. MNIST, SVHN, Yale B) <ref type="bibr" target="#b19">(Narayanaswamy et al. 2017)</ref>, it still encounters one common problem that good ELBO values do not always imply accurate inference results Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="bibr" target="#b29">(Zhao et al. 2017)</ref>. To address this problem, existing works introduce prior knowledge that needs to be set manually, e.g., the stacked VAE structure (M1+M2, <ref type="bibr" target="#b13">Kingma et al. 2014;</ref><ref type="bibr" target="#b5">Davidson et al. 2018)</ref>, the prior domain knowledge <ref type="bibr" target="#b16">(Louizos et al. 2016;</ref><ref type="bibr" target="#b10">Ilse et al. 2019</ref>) and mutual information bounds <ref type="bibr" target="#b7">(Dupont 2018)</ref>.</p><p>In this study, we investigate the training process of semisupervised VAE with extensive experiments and propose two possible causes of the problem. (1) First, the ELBO cannot utilize label information directly. In the semi-supervised VAE framework <ref type="bibr" target="#b13">(Kingma et al. 2014)</ref>, the classification loss and ELBO learn from the labels and unlabeled data separately, making it difficult to improve the inference accuracy with ELBO. (2) Second, an "ELBO bottleneck" exists, and continuing to optimize the ELBO after a certain bottleneck value will not improve inference accuracy. Thus, we propose SmootH-ELBO Optimal InTerpolation VAE (SHOT-VAE) to solve the "good ELBO, bad performance" problem without requiring additional prior knowledge, which offers the following contributions:</p><p>• The smooth-ELBO objective that integrates the classification loss into ELBO. We derive a new ELBO approximation named smooth-ELBO with the label-smoothing technique <ref type="bibr" target="#b18">(Müller et al. 2019)</ref>. Theoretically, we prove that the smooth-ELBO integrates the classification loss into ELBO. Then, we empirically show that a better inference accuracy can be achieved with smooth-ELBO.</p><p>• The margin approximation that breaks the ELBO bottleneck.</p><p>We propose an approximation of the margin between the real data distribution and the one from ELBO. The approximation is based on the optimal interpolation in data space and latent space. In practice, we show this optimal interpolation approximation (OT-approximation) can break the "ELBO bottleneck" and achieve a better inference accuracy.</p><p>• Good semi-supervised performance.</p><p>We evaluate SHOT-VAE on 4 benchmark datasets and the results show that our model achieves good performance with 25.30% error rate on CIFAR-100 with 10k labels arXiv:2011.10684v4 <ref type="bibr">[cs.</ref>LG] 8 Dec 2020</p><p>and reduces the error rate to 6.11% on CIFAR-10 with 4k labels. Moreover, we find it can get strong results even with fewer labels and smaller models, for example obtaining a 14.27% error rate on CIFAR-10 with 500 labels and 1.5M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised VAE</head><p>In supervised learning, we are facing with training data that appears as input-label pairs (X, y) sampled from the labeled dataset D L . While in semi-supervised learning, we can obtain an extra collection of unlabeled data X denoted by D U . We hope to leverage the data from both D L and D U to achieve a more accurate model than only using D L . Semi-supervised VAEs <ref type="bibr" target="#b13">(Kingma et al. 2014</ref>) solve the problem by constructing a probabilistic model to disentangle the data into continuous variables z and label variables y. It consists of a generation process and an inference process parameterized by θ and φ respectively. The generation process assumes the posterior distribution of X given the latent variables z and y as p θ (X|z, y) = N (X; f θ (z, y), σ 2 ).</p><p>(1) The inference process assumes the posterior distribution of z and y given X as</p><formula xml:id="formula_0">q φ (z|X) = N (z|µ φ (X), σ 2 φ (X)); q φ (y|X) = Cat(y|π φ (X)).</formula><p>( <ref type="formula" target="#formula_24">2)</ref> where Cat(y|π) is the multinomial distribution of the label, π φ (X) is a probability vector, and the functions f θ , µ φ , σ φ and π φ are represented as deep neural networks. To make the model learn disentangled representations, the independent assumptions <ref type="bibr" target="#b13">(Kingma et al. 2014;</ref><ref type="bibr" target="#b7">Dupont 2018)</ref> are also widely used as p(z, y) = p(z)p(y); q φ (z, y|X) = q φ (z|X)q φ (y|X).</p><p>(</p><p>For the unlabeled dataset D U , VAE models want to learn the disentangled representation of q φ (z|X) and q φ (y|X) by maximizing the evidence lower bound of log p(X) as</p><formula xml:id="formula_2">ELBO D U (X) = E q φ (z,y|X) [log p θ (X|z, y)] −D KL (q φ (z|X) p(z)) − D KL (q φ (y|X) p(y)).<label>(4)</label></formula><p>For the labeled dataset D L , the labels y are treated as latent variables and the related ELBO becomes</p><formula xml:id="formula_3">ELBO D L (X, y) = E q φ (z|X,y) [log p θ (X|z, y)] − D KL (q φ (z|X, y) p(z)).<label>(5)</label></formula><p>Considering the label prediction q φ (y|X) contributes only to the unlabeled data in (4), which is an undesirable property as we wish the semi-supervised model can also learn from the given labels, <ref type="bibr" target="#b13">Kingma et al. (2014)</ref> proposes to add a cross-entropy (CE) loss as a solution and the extended target is as follows: min</p><formula xml:id="formula_4">θ,φ E X∼D U [−ELBO D U (X)]+ E (X,y)∼D L [−ELBO D L (X, y) + α · CE(q φ (y|X), y)]<label>(6)</label></formula><p>where α is a hyper-parameter controlling the loss weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good ELBO, Bad Inference</head><p>However, a frequent phenomenon is that good ELBO values do not always imply accurate inference results <ref type="bibr" target="#b29">(Zhao et al. 2017)</ref>, which often occurs on realistic datasets with high variance, such as CIFAR-10 and CIFAR-100. In this paper, we investigate the training process of semi-supervised VAE models on the above two datasets and propose two possible causes of the "good ELBO, bad inference" problem.</p><p>The ELBO cannot utilize the label information. As mentioned in equation <ref type="formula" target="#formula_4">(6)</ref>, the label prediction q φ (y|X) only contributes to the unlabeled loss −ELBO D U (X), which indicates that the labeled loss −ELBO D L (X, y) can not utilize the label information directly. We assume this problem will make the ELBO value irrelevant to the final inference accuracy. To evaluate our assumption, we compare the semisupervised VAE (M2) models <ref type="bibr" target="#b13">(Kingma et al. 2014</ref>) with the same model but removing the ELBO D L in (6). As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the results indicate that ELBO D L can accelerate the learning process of q φ (y|X), but it fails to achieve a better inference accuracy than the one removing ELBO D L . The "ELBO bottleneck" effect. Another possible cause is the "ELBO bottleneck", that is, continuing to optimize ELBO after a certain bottleneck value will not improve the inference accuracy. <ref type="figure">Figure 2</ref> shows that the inference accuracy raises rapidly and peaks at the bottleneck value. After that, the optimization of ELBO value does not affect the inference accuracy.</p><p>(a) CIFAR-10 (b) CIFAR-100 <ref type="figure">Figure 2</ref>: Comparison between the negative ELBO value and accuracy for semi-supervised VAE. Results indicate that a ELBO bottleneck exists, and continuing to optimize ELBO after this bottleneck will not improve the inference accuracy.</p><p>Existing works introduce prior knowledge and specific structures to address these problems. <ref type="bibr" target="#b13">Kingma et al. (2014)</ref> and <ref type="bibr" target="#b5">Davidson et al. (2018)</ref> propose the stacked VAE structure (M1+M2), which forces the model to utilize the repre-sentations learned from ELBO D L to inference the q φ (y|X). <ref type="bibr" target="#b16">Louizos et al. (2016)</ref> and <ref type="bibr" target="#b10">Ilse et al. (2019)</ref> incorporate domain knowledge into models, making the ELBO representations relevant to the label prediction. <ref type="bibr" target="#b29">Zhao et al. (2017)</ref> and <ref type="bibr" target="#b7">Dupont (2018)</ref> utilize the ELBO decomposition technique <ref type="bibr" target="#b9">(Hoffman and Johnson 2016)</ref>, setting the mutual information bounds to perform feature selection. These methods have achieved great success on many benchmark datasets (e.g. MNIST, SVHN, Yale B). However, the related prior knowledge and structures need to be selected manually. Moreover, for some standard datasets with high variance such as CIFAR-10 and CIFAR-100, the semi-supervised performance of VAE is not satisfactory.</p><p>Instead of introducing additional prior knowledge, we propose a novel solution based on the ELBO approximations, SmootH-ELBO Optimal inTerpolation VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHOT-VAE</head><p>In this section, we derive the SHOT-VAE model by introducing its two improvements. First, for the labeled dataset D L , we derive a new ELBO approximation named smooth-ELBO that unifies the ELBO and the label predictive loss. Then, for the unlabeled dataset D U , we create the differentiable OT-approximation to break the ELBO value bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smooth-ELBO: integrating the classification loss into ELBO.</head><p>To overcome the problem that the ELBO D L cannot utilize the label information directly, we first perform an "ELBO surgery". Following previous works <ref type="bibr" target="#b6">(Doersch 2016)</ref>, the ELBO D L can be derived with Jensen-Inequality as:</p><formula xml:id="formula_5">log p(X, y) = log E q φ (z|X,y) p(X, y, z) q φ (z|X, y) ≥ E q φ (z|X,y) log p(X, y, z) q φ (z|X, y) = ELBO D L (X, y)<label>(7)</label></formula><p>Utilizing the independent assumptions in equation <ref type="formula" target="#formula_1">(3)</ref>, we have q φ (z|X, y) = q φ (z|X). In addition, the labels y are treated as latent variables directly in ELBO D L , which equals to obey the empirical degenerate distribution, i.e. p(y i |X i ) = 1, ∀(X i , y i ) ∈ D L . Substituting the above two conditions into (7), we have</p><formula xml:id="formula_6">ELBO D L (X, y) = E q φ (z|X),p(y|X) log p(X, y, z) q φ (z|X)p(y|X) = E q φ ,p log p(X|z, y) − D KL (q φ (z|X) p(z)) − D KL (p(y|X) p(y)).<label>(8)</label></formula><p>In equation <ref type="formula" target="#formula_6">(8)</ref>, the last objective D KL (p(y|X) p(y)) is irrelevant to the label prediction q φ (y|X), which causes the "good ELBO, bad inference" problem. Inspired by this, we derive a new ELBO approximation named smooth-ELBO.</p><p>The smooth-ELBO provides two improvements. First, we propose a more flexible assumption of the empirical distributionp(y|X). Instead of treatingp(y|X) as the degenerate distribution, we use the label smoothing technique <ref type="bibr" target="#b18">(Müller et al. 2019</ref>) and view the one-hot label 1 y as the parameters of the empirical distributionp(y|X), that is, ∀(X, y) ∈ D L p(y|X) = Cat(y|smooth(1 y ));</p><formula xml:id="formula_7">smooth(1 y ) i = 1 − if 1 y,i = 1, K−1 if 1 y,i = 0. ,<label>(9)</label></formula><p>where K is the number of classes and controls the smooth level. We use = 0.001 in all experiments. Then, we derive the following convergent approximation with the smoothedp(y|X):</p><formula xml:id="formula_8">D KL (p(y|X) q φ (y|X)) + D KL (q φ (y|X) p(y)) → D KL (p(y|X) p(y)) when q φ (y|X) →p(y|X).<label>(10)</label></formula><p>The proof can be found in Appendix A and we also point out the approximation (1) does not converge under the empirical degenerate distribution, which explains the importance of label smoothing. Combining equations <ref type="formula" target="#formula_6">(8)</ref>, <ref type="formula" target="#formula_7">(9)</ref> and <ref type="formula" target="#formula_23">(1)</ref>, we propose the smooth-ELBO objective for D L :</p><formula xml:id="formula_9">smooth-ELBO D L (X, y) = E q φ ,p log p(X|z, y) − D KL (q φ (z|X) p(z)) −D KL (q φ (y|X) p(y)) − D KL (p(y|X) q φ (y|X)).<label>(11)</label></formula><p>Theoretically, we demonstrate the following properties.</p><p>Smooth-ELBO integrates the classification loss into ELBO. Compared with the original ELBO D L , smooth-ELBO derives two extra components, D KL (q φ (y|X) p(y)) and D KL (p(y|X) q φ (y|X)). Utilizing the decomposition in <ref type="bibr" target="#b9">(Hoffman and Johnson 2016)</ref>, we can rewrite the first component into</p><formula xml:id="formula_10">E D L D KL (q φ (y|X) p(y)) = I q φ (X; y)+D KL (q φ (y) p(y))</formula><p>where I q φ (X; y) is the constant of mutual information between X and y, p(y) is the true marginal distribution for y which can be estimated withp(y|X) in D L and q φ (y) = 1 |D L | (X,y)∈D L q φ (y|X) is the estimation of marginal distribution. By optimizing D KL (q φ (y) p(y)), the first component can learn the marginal distribution p(y) from labels.</p><p>For the second component, with Pinsker's inequality, it's easy to prove that for all i = 1, 2, . . . , K</p><formula xml:id="formula_11">|π φ (X) − smooth(1 y )| i ≤ 1 2 D KL (p(y|X) q φ (y|X))</formula><p>The proof can be found in Appendix B, which indicates that q φ (y|X) converges top(y|X) in training process. Convergence analysis. As mentioned above, q φ (y|X) converge to the smoothedp(y|X) in training. Based on this property, we can assert that the smooth-ELBO converges to ELBO D L with the following equation:</p><formula xml:id="formula_12">|smooth-ELBO D L (X, y)−ELBO D L (X, y)| ≤ C 1 δ+C 2 δ 2 +∆(δ)</formula><p>The proof can be found in Appendix C. C 1 , C 2 are the constants related to class number K and δ = sup i |π φ (X) i − smooth(1 y ) i | is the distance between q φ (y|X) andp(y|X).</p><p>To summarize the above, smooth-ELBO can utilize the label information directly. Compared with the original −ELBO D L + αCE loss in <ref type="formula" target="#formula_4">(6)</ref>, smooth-ELBO has three advantages. First, it not only learns from single labels, but also learns from the marginal distribution p(y). Second, we do not need to manually set the loss weight α. Moreover, it also takes advantages of the ELBO D L , such as disentangled representations and convergence assurance. The extensive experiments will also show that a better model performance can be achieved with smooth-ELBO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OT-approximation: breaking the ELBO bottleneck</head><p>To overcome the ELBO bottleneck problem, we first analyze what the semi-supervised VAE model does after reaching the bottleneck, then we create the differentiable OTapproximation to break it, which is based on the optimal interpolation in latent space.</p><p>As mentioned in equation <ref type="formula" target="#formula_2">(4)</ref>, VAE aims to learn disentangled representations q φ (z|X) and q φ (y|X) by maximizing the lower bound of the likelihood of data as log p(X) ≥ ELBO(X), while the margin between log p(X) and ELBO(X) has the following closed form:</p><formula xml:id="formula_13">log p(X) − ELBO(X) = D KL (q φ (z|X)q φ (y|X) p(z, y|X))<label>(12)</label></formula><p>Ideally, the optimization process of ELBO will make the representation q φ (z|X) and q φ (y|X) converge to their ground truth p(z|X) and p(y|X). However, the unimproved inference accuracy of q φ (y|X) indicates that optimizing ELBO after the bottleneck will only contribute to the continuous representation q φ (z|X), while the q φ (y|X) seems to get stuck in the local minimum. Since the ground truth p(y|X) is not available for the unlabeled dataset D U , it is hard for the model to jump out by itself. Inspired by this, we create a differentiable approximation of D KL (q φ (y|X) p(y|X)) for D U to break the bottleneck.</p><p>Following previous works <ref type="bibr" target="#b15">(Lee 2013)</ref>, the approximation is usually constructed with two steps: creating the pseudo inputX with data augmentations and creating the pseudo distributionp(z|X) ofX. Recent advanced works use autoaugment <ref type="bibr" target="#b4">(Cubuk et al. 2019</ref>) and random mixmatch <ref type="bibr" target="#b2">(Berthelot et al. 2019b)</ref> to perform data augmentations. However, these strategies will greatly change the representation of continuous variable z, e.g., changing the image style and background. To overcome this, we propose the optimal interpolation based approximation.</p><p>The optimal interpolation consists of two steps. First, for each input X 0 in D U , we find the optimal match X 1 with the most similar continuous variable z, that is,</p><formula xml:id="formula_14">arg X1∈D U min D KL (q φ (z|X 0 ) q φ (z|X 1 )).</formula><p>Then, on purpose of jumping out the stuck point q φ (y|X), we take the widely-used mixup strategy <ref type="bibr" target="#b28">(Zhang et al. 2018</ref>) to create pseudo inputX as follows:</p><formula xml:id="formula_15">X = (1 − λ)X 0 + λX 1 ,<label>(13)</label></formula><p>where λ is sampled from the uniform distribution U(0, 1).</p><p>The mixup strategy can be understood as calculating the optimal interpolation between two points X 0 , X 1 in input Algorithm 1 SHOT-VAE training process with epoch t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Batch of labeled data (X L , y L ) ∈ D L ; Batch of unlabeled data X U ∈ D U ; Mixup λ ∼ U(0, 1); Loss weight w t ; Model parameters: θ (t−1) , φ (t−1) ; Model optimizer: SGD Output:</p><p>Updated parameters:</p><formula xml:id="formula_16">θ (t) , φ (t) 1: L D L = −smooth-ELBO D L (X L , y L ) 2: X 0 U , X 1 U = X U , OptimalMatch(X U ) 3: L D U = −ELBO D U (X U ) + w t · OT D U (X 0 U , X 1 U , λ) 4: L = L D L + L D U 5: θ (t) , φ (t) = SGD(θ (t−1) , φ (t−1) , ∂L ∂θ , ∂L ∂φ )</formula><p>space with the maximum likelihood:</p><formula xml:id="formula_17">max X (1 − λ) · log(p θ (X|z 0 , y 0 )) + λ · log(p θ (X|z 1 , y 1 )),</formula><p>where {z i , y i } i=0,1 is the latent variables for the data points X 0 , X 1 , and the proof can be found in Appendix D.</p><p>To create the pseudo distributionp(y|X) ofX, it is a natural thought that the optimal interpolation in data space could associate with the same in latent space with D KL distance used in ELBO. Inspired by this, we propose the optimal interpolation method to calculatep(y|X) as Proposition 1 The optimal interpolation derived from</p><formula xml:id="formula_18">D KL distance between q φ (y|π φ (X 0 )) and q φ (y|π φ (X 1 )) with λ ∈ [0, 1] can be written as miñ π (1 − λ) · D KL (π φ (X 0 ) π) + λ · D KL (π φ (X 1 ) π)</formula><p>and the solutionπ satisfying</p><formula xml:id="formula_19">π = (1 − λ)π φ (X 0 ) + λπ φ (X 1 ).<label>(14)</label></formula><p>The proof can be found in Appendix E. Combining the optimal interpolation in data space and latent space, we derive the optimal interpolation approximation (OT-approximation) for D U as</p><formula xml:id="formula_20">OT D U (X 0 , X 1 , λ) = D KL (q φ (y|X) p(y|X)) s.t.   X = (1 − λ)X 0 + λX 1 p(y|X) = Cat(y|π) π = (1 − λ)π φ (X 0 ) + λπ φ (X 1 ) .<label>(15)</label></formula><p>Notice that the OT-approximation does not require additional prior knowledge and is easy to implement. Moreover, although OT-approximation utilizes the mixup strategy to create pseudo inputX, our work has two main advantages over mixup-based methods <ref type="bibr" target="#b28">(Zhang et al. 2018;</ref><ref type="bibr" target="#b24">Verma et al. 2019)</ref>. First, mixup methods directly assume the pseudo labelỹ behaves linear in latent space without explanations. Instead, we derive the D KL from ELBO as the metric and utilize the optimal interpolation (15) to constructỹ. Second, mixup methods use · 2 2 loss between q φ (y|X) andp(y|X)), while we use the D KL loss and achieve better semi-supervised learning performance.   <ref type="formula" target="#formula_23">(1)</ref> SHOT-VAE surpasses other models with a large margin in all cases.</p><p>(2) both smooth-ELBO and OT-approximation contribute to the inference accuracy, reducing the error rate on 10% labels from 18.08% to 13.54% and from 13.54% to 8.51%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The implementation details of SHOT-VAE</head><p>The complete algorithm of SHOT-VAE can be obtained by combining the smooth-ELBO and the OT-approximation, as shown in Algorithm 1. In this section, we discuss some details in training process. First, the working condition for OT-approximation is that the ELBO has reached the bottleneck value. However, quantifying the ELBO bottleneck value is difficult. Therefore, we extend the warm-up strategy in β − VAE <ref type="bibr" target="#b8">(Higgins et al. 2017</ref>) to achieve the working condition. The main idea of warm-up is to make the weight w t for OT-approximation increase slowly at the beginning and most rapidly in the middle of the training, i.e. exponential schedule. The function of the exponential schedule is</p><formula xml:id="formula_21">w t = exp(−γ · (1 − t t max ) 2 ),<label>(16)</label></formula><p>where γ is the hyper-parameter controlling the increasing speed, and we use γ = 5 in all experiments. Second, the optimal match operation in equation <ref type="formula" target="#formula_1">(13)</ref> requires to find the most similar X 1 for each X 0 in D U , which consumes a lot of computation resources. To overcome this, we set a large batch-size (e.g., 512) and use the most similar X 1 in one mini-batch to perform optimal interpolation. Moreover, calculating the gradient of the expected loglikelihood E q φ (z,y|X) log p θ (X|z, y) is difficult. Therefore, we apply the reparameterization tricks <ref type="bibr" target="#b12">Jang et al. 2017</ref>) to obtain the gradients as follows:</p><formula xml:id="formula_22">∇ θ,φ E q φ (z|X) log p θ (X|z) ≈ ( i ∼ N (0, I)) 1 N N i=1 ∇ θ,φ log p θ (X|µ φ (X) + σ φ (X) i ). and ∇ θ,φ E q φ (y|X) log p θ (X|y) ≈ ( i ∼ Gumbel( ; 0, 1)) 1 N N i=1 ∇ θ,φ log p θ (X|Softmax( log π φ (X) + i τ )).</formula><p>Following previous works <ref type="bibr" target="#b7">(Dupont 2018)</ref>, we used N = 1 and τ = 0.67 in all experiments.</p><p>To make the VAE model learn the disentangled representations, we also take the widely-used β-VAE strategy <ref type="bibr" target="#b3">(Burgess et al. 2018)</ref> in training process and chose β = 0.01 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we evaluate the SHOT-VAE model with sufficient experiments on four benchmark datasets, i.e. MNIST, SVHN, CIFAR-10, and CIFAR-100. In all experiments, we apply stochastic gradient descent (SGD) as optimizer with momentum 0.9 and multiply the learning rate by 0.1 at regularly scheduled epochs. For each experiment, we create five D L -D U splits with different random seeds and the error rates are reported by the mean and variance across splits. Due to space limitations, we mainly show results on CIFAR-10 and CIFAR-100; more results on MNIST and SVHN as well as the robustness analysis of hyper-parameters are provided in Appendix F. The code, with which the most important results can be reproduced, is available at Github 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smooth-ELBO improves the inference accuracy</head><p>In the above sections, We propose smooth-ELBO as the alternative of −ELBO D L + CE loss in equation <ref type="formula" target="#formula_4">(6)</ref>, and analyze the convergence theoretically. Here we evaluate the inference accuracy and convergence speed of smooth-ELBO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Amount</head><p>Method  First, we compare the smooth-ELBO with other semisupervised VAE models under a varying label ratios from 1.25% to 25%. As baselines, we consider three advanced VAE models mentioned above: standard semi-supervised VAE (M2), stacked-VAE (M1+M2), and domain-VAE <ref type="bibr" target="#b10">(Ilse et al. 2019)</ref>.</p><note type="other">CIFAR10 (4k) CIFAR100 (4k) CIFAR100 (10k</note><p>As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, smooth-ELBO makes VAE model learn better representations from labels, reducing the error rates among all label ratios on CIFAR-10 and CIFAR-100 respectively. Moreover, smooth-ELBO also achieves competitive results to other VAE models without introducing additional domain knowledge or multi-stage structures.</p><p>Second, we analyze the convergence speed in training process. As mentioned above, the smooth-ELBO will converge to the real ELBO when q φ (y|X) →p(y|X). Moreover, we also descover that q φ (y|X) converges top(y|X) in training process. Here we evaluate the convergence speed in training process with the relative error between the smooth-ELBO and the real ELBO. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the relative error can be very low even at the early stage of training, that is, 0.71% on CIFAR-10 and 2.79% on CIFAR-100, which indicates that the smooth-ELBO converges rapidly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SHOT-VAE breaks the ELBO bottleneck</head><p>In the above sections, we make two assertions: (1) optimizing ELBO after the bottleneck will make q φ (y|X) get stuck in the local minimum. (2) The OT-approximation can break the ELBO bottleneck by making good estimation of D KL (q φ (y|X) p(y|X)) for X ∈ D U . Here we evaluate the assertions through two stage experiments.</p><p>First, to evaluate the "local minimum" assertion, we utilize the label of D U to estimate the empirical distribution p(y|X) and calculate D KL (q φ (y|X) p(y|X)) in training process as the metric. Notice these labels are only used to calculate the metric and do not contribute to the model. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, we compare the SHOT-VAE with the same model but removing the OT-approximation. The results indicate that optimizing ELBO itself without OTapproximation will make the gap D KL (q φ (y|X) p(y|X)) stuck into the local minimum, while the OT approximation helps the model jump out the local minimum, leading to a better inference of q φ (y|X). Then, we investigate the relation between the negative ELBO and inference accuracy for SHOT-VAE and M2 model. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, for M2 model, the inference accuracy stalls after the ELBO bottleneck. While for SHOT-VAE, optimizing ELBO contributes to the improvement of inference accuracy during the whole training process. Moreover, the SHOT-VAE achieves a much better accuracy than M2, which also indicates that SHOT-VAE breaks the "ELBO bottleneck".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised learning performance</head><p>We evaluate the effectiveness of the SHOT-VAE on two parts: evaluations under a varying number of labeled samples and evaluations under different parameter amounts of neural networks. First, we compare the SHOT-VAE with other advanced VAE models under a varying label ratios from 1.25% to 25%. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, both smooth-ELBO and OTapproximation contribute to the improvement of inference accuracy, reducing the error rate on 10% labels from 18.08% to 13.54% and from 13.54% to 8.51%, respectively. Furthermore, SHOT-VAE outperforms all other methods by a large margin, e.g., reaching an error rate of 14.27% on CIFAR-10 with the label ratio 2.5%. For reference, with the same backbone, fully supervised training on all 50000 samples achieves an error rate of 5.33%.</p><p>Then, we evaluate SHOT-VAE under different parameter amounts of neural networks, i.e. "WideResNet-28-2" with 1.5M parameters and "WideResNet-28-10" with 36.5M parameters. As baselines for comparison, we select six current best models from 4 categories: Virtual Adversarial Training <ref type="bibr">(VAT Miyato et al. (2019)</ref>) and MixMatch <ref type="bibr" target="#b2">(Berthelot et al. 2019b)</ref>   <ref type="table">Table 1</ref>. Besides, we also take the mixup-based method into consideration. In general, the SHOT-VAE model outperforms other methods among all experiments on CIFAR-100. Furthermore, our model is not sensitive to the parameter amount and reaches competitive results even with small networks (e.g., 1.5M parameters).</p><p>Moreover, our SHOT-VAE can easily combine other advanced semi-supervised methods, which further improves model performance. As shown in Appendix G, we combine SHOT-VAE with data augmentations and mean teacher separately and achieve much better results, i.e. 4.56% error rate on CIFAR-10 (4k) and 24.09% on CIFAR-100 (10k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disentangled representations</head><p>Among semi-supervised models, VAE based approaches have great advantages in interpretability by capturing semantics-disentangled latent variables. To demonstrate this property, we perform conditional generation experiments on MNIST and SVHN datasets. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, we pass the test image through the inference network to obtain the distribution of the latent variables z and y corresponding to this image. We then fix the inference q φ (z|X) of continuous variable z, vary y with different labels, and generate new samples. The generation results show that z and y have learned semantic-disentangled representations, as z represents the image style and y represents the classification contents. Moreover, by comparing the results through columns, we find that each dimension of the discrete variable y corresponds to one class label separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We investigate one challenge in semi-supervised VAEs that "good ELBO values do not imply accurate inference results". We propose two causes of this problem through reasonable experiments. Based on the experiment results, we propose SHOT-VAE to address the "good ELBO, bad inference" problem. With extensive experiments, We demonstrate that SHOT-VAE can break the ELBO value bottleneck without introducing additional prior knowledge. Results also show that our SHOT-VAE outperforms other advanced semisupervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethic statement</head><p>We address the SHOT-VAE to solve one common problem in semi-supervised generative models that good ELBO values do not always imply accurate inference results. We offer the possible influence of our work from three aspects: fundamental theory impact, machine learning application impact, and social impact.</p><p>For the fundamental theory impact, we propose two causes of the "good ELBO, bad inference" problem through extensive experiments. Based on the experiment results, we provide two contributions: (1) A new ELBO approximation named smooth-ELBO that integrates the label predictive loss into ELBO. (2) An approximation based on optimal interpolation that breaks the ELBO value bottleneck. The two approximations are all reasonable in theory and work in practice.</p><p>For the machine learning application impact. We evaluate the SHOT-VAE under realistic datasets with high variance and show it has great advantages in interpretability by capturing semantics-disentangled latent variables.</p><p>For the social impact, the SHOT-VAE can achieve good performance with a small fraction of labeled data. Therefore, it is friendly to the data providers and the governments that pay attention to the privacy-preserving policy, e.g., the general data protection regulation in the European Union.</p><formula xml:id="formula_23">D KL (p(y|X) q φ (y|X)) + D KL (q φ (y|X) p(y)) → D KL (p(y|X) p(y))<label>(1)</label></formula><p>proof :</p><p>To analyze the convergence of (1), we first decompose the D KL (p(y|X) p(y)) as follows</p><formula xml:id="formula_24">D KL (p(y|X) p(y)) = Ep (y|X) logp (y|X) p(y) = Ep (y|X) logp (y|X) q φ (y|X) + Ep (y|X) log q φ (y|X) p(y) = D KL (p(y|X) q φ (y|X)) + Ep (y|X) log q φ (y|X) p(y)<label>(2)</label></formula><p>Since D KL (q φ (y|X) p(y)) = −E q φ (y|X) log p(y) q φ (y|X) , the proof for (1) is equals to prove the following limitation:</p><formula xml:id="formula_25">lim q φ (y|X)→p(y|X) E q φ (y|X) log p(y) q φ (y|X) = Ep (y|X) log p(y) q φ (y|X) .</formula><p>The proof is as follows. First, we explicitly define the condition q φ (y|X) → p(y|X) with a closed form. That is, ∀ζ ≥ 0, there exists at least one δ satisfies</p><formula xml:id="formula_26">Ep (y|X) log p(y) q φ (y|X) − E q φ (y|X) log p(y) q φ (y|X) ≤ ζ when sup i |π φ (X) i − smooth(1 y ) i | ≤ δ<label>(3)</label></formula><p>We derive the following inequalities and use them to demonstrate (3).</p><formula xml:id="formula_27">|Ep (y|X) log p(y) q φ (y|X) − E q φ (y|X) log p(y) q φ (y|X) | = | K i=1 (smooth(1 y ) i −π φ (X) i )(log p(y) i − log π φ (X) i )| ≤ K · δ · M +K · δ · sup i | log π φ (X) i |<label>(4)</label></formula><p>where M is the upper bound of log p(y), that is</p><formula xml:id="formula_28">sup i | log p(y) i | ≤ M.</formula><p>Utilize the equation <ref type="formula" target="#formula_1">(3)</ref>, we have</p><formula xml:id="formula_29">sup i | π φ (X) i smooth(1 y ) i − 1| ≤ δ · K − 1<label>(5)</label></formula><p>When δ → 0, we have log(T δ + 1) = T δ + ∆(T δ). Combining (5), we have Combining <ref type="formula" target="#formula_2">(4)</ref> and <ref type="formula" target="#formula_4">(6)</ref>, we have</p><formula xml:id="formula_30">sup i | log π φ (X) i | ≤ δ · K − 1 + log 1 1 − + ∆(δ) (6)</formula><formula xml:id="formula_31">|Ep (y|X) log p(y) q φ (y|X) − E q φ (y|X) log p(y) q φ (y|X) | ≤ KM δ + K(K − 1) δ 2 + K log 1 1 − δ + ∆(δ)<label>(7)</label></formula><p>The bound (7) states that, when δ → 0, the ζ can be arbitrarily small, which prove the limitation (1). The convergence for degenerate distribution. In original ELBO D L , the labels y are treated as latent variables directly, which equals to obey the empirical degenerate distribution, i.e.p(y|X) = 1. In this situation, → 0, and the second component in (7) becomes</p><formula xml:id="formula_32">K(K − 1) δ 2 + K log 1 1 − δ → ∞</formula><p>which indicates that the limitation (1) may not converge under the degenerate distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B</head><p>Proposition 2 The inference result of discrete variable y satisfies the following inequality that ∀i = 1, · · · , K</p><formula xml:id="formula_33">|π φ (X) − smooth(1 y )| i ≤ 1 2 D KL (p(y|X) q φ (y|X))</formula><p>proof:</p><p>The widely used Pinsker's inequality states that, if P and Q are two probability distributions on a measurable space (X, Σ), then</p><formula xml:id="formula_34">δ(P, Q) ≤ 1 2 D KL (P Q) where δ(P, Q) = sup{|P (A)−Q(A)||A ∈ Σ is a measurable event.}</formula><p>In our situation, the discrete random variable y has the event set A ⊂ Σ = {1, . . . , K}, and the distribution P, Q satisfies</p><formula xml:id="formula_35">|P (A) − Q(A)| = | i∈A p(y = i) − q(y = i)|</formula><p>where p(y) = Cat(y|smooth(1 y )), q(y) = Cat(y|π φ (X)). For all i = 1, . . . , K, we have</p><formula xml:id="formula_36">|P (i) − Q(i)| ≤ δ(P, Q)</formula><p>and</p><formula xml:id="formula_37">|P (i) − Q(i)| = |π φ (X) − smooth(1 y )| i Then, with</formula><p>Pinsker's inequality, the proposition is easy to prove.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C</head><p>Proposition 3 smooth-ELBO converges to ELBO D L with the following equation</p><formula xml:id="formula_38">|smooth-ELBO D L (X, y)−ELBO D L (X, y)| ≤ C 1 δ+C 2 δ 2 +∆(δ)</formula><p>As mentioned in <ref type="formula" target="#formula_5">(7)</ref>, we have</p><formula xml:id="formula_39">|smooth-ELBO D L (X, y) − ELBO D L (X, y)| = |Ep (y|X) log p(y) q φ (y|X) − E q φ (y|X) log p(y) q φ (y|X) | ≤ KM δ + K(K − 1) δ 2 + K log 1 1 − δ + ∆(δ)<label>(8)</label></formula><p>then C 1 = KM + K log 1 1− and C 2 = K(K − 1). Furthermore, in the original paper, we derive the ELBO D L (X, y) with the independent assumptions as well as the empirical degenerate distribution. One problem is, whether the format also holds for other empirical estimation form ofp(y|X), e.g., the smoothedp(y|X). Utilizing the Jensen-Inequality, we can derive the following equivalent form:</p><formula xml:id="formula_40">log p(X) = log E q φ (z|X),p(y|X) p(X, z, y) q φ (z|X)p(y|X) ≥ E q φ (z|X),p(y|X) log p(X, z, y) q φ (z|X)p(y|X) = E q φ ,p log p(X|z, y) − D KL (q φ (z|X) p(z)) − D KL (p(y|X) p(y)) = ELBO D L (X, y).<label>(9)</label></formula><p>which explicitly proves that ELBO D L (X, y) holds for any reasonable empirical distribution form ofp(y|X).</p><p>Combining the equation <ref type="formula" target="#formula_7">(9)</ref> and <ref type="formula" target="#formula_6">(8)</ref>, the convergence of Proposition 3 is easy to prove.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D</head><p>Proposition 4 The mixup strategy under the disentangled VAE models can be understood as calculating the optimal interpolation between two points X 0 , X 1 in input space with the maximum likelihood: max X (1 − λ) · log(p θ (X|z 0 , y 0 )) + λ · log(p θ (X|z 1 , y 1 )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(10)</head><p>First, it is easy to prove that the mixup strategyX = (1 − λ)X 0 + λX 1 can be understood as calculating the optimal interpolation between two points X 0 , X 1 in data space with the norm-2 distance:</p><formula xml:id="formula_41">miñ X (1 − λ) · X − X 0 2 2 + λ · X − X 1 2 2 .<label>(11)</label></formula><p>In VAE models, with the generation process, we have</p><formula xml:id="formula_42">X 0 = f θ (z 0 , y 0 ); X 1 = f θ (z 1 , y 1 )<label>(12)</label></formula><p>then the distribution p θ (X|z, y) becomes</p><formula xml:id="formula_43">p θ (X|z, y) = C 0 (σ) · exp − X − f θ (z, y) 2 · C 1 (σ)<label>(13)</label></formula><p>where C 0 , C 1 are constants associated with the constant σ.</p><p>Substituting <ref type="formula" target="#formula_9">(11)</ref>, <ref type="formula" target="#formula_13">(12)</ref> into <ref type="formula" target="#formula_7">(9)</ref>, we can obtain the equivalence of (9) and (10), which proves the proposition.</p><p>Appendix E E.1: The margin between log p(X) and ELBO(X). Proposition 5 The margin between the true log-likelihood log p(X) and ELBO(X) under the independent assumptions is</p><formula xml:id="formula_44">log p(X) − ELBO = D KL (q φ (z|X)q φ (y|X) p(z, y|X) proof: log p(X) = z,y q φ (z|X)q φ (y|X) log p(X)dzdy = z,y q φ (z|X)q φ (y|X) log p(X, z, y) p(z, y|X) dzdy = E q φ (z|X)q φ (y|X) log p(X, z, y) q φ (z|X)q φ (y|X) + z,y q φ (z|X)q φ (y|X) log q φ (z|X)q φ (y|X) p(z, y|X) = ELBO + D KL (q φ (z|X)q φ (y|X) p(z, y|X)</formula><p>E.2: The optimal interpolation. Proposition 6 The optimal interpolation derived from D KL distance between q φ (y|π φ (X 0 )) and q φ (y|π φ (X 1 )) with λ ∈ [0, 1] can be written as </p><formula xml:id="formula_45">miñ π (1 − λ)·D KL (π φ (X 0 ) π) + λ · D KL (π φ (X 1 ) π) s.t. K i=1π i = 1;π i ≥ 0,</formula><formula xml:id="formula_46">π = (1 − λ)π φ (X 0 ) + λπ φ (X 1 ).<label>(14)</label></formula><p>proof: Denote π 0 = π φ (X 0 ) and π 1 = π φ (X 1 ). The Lagrange multiplier form of (14) in Proposition i satisfies:</p><formula xml:id="formula_47">L(π, t) = (1−λ)·D KL (π 0 π)+λ·D KL (π 1 π)+t * ( K i=1π i −1) and the related KKT conditions are ∂L(π, t) ∂π = t − (1 − λ) · π 0 + λ · π 1 π = 0 t * ( K i=1π i − 1) = 0</formula><p>Solve the above equations, the closed form ofπ is π = (1 − λ) · π 0 + λ · π 1 Appendix F F.1: The semi-supervised performance on MNIST and SVHN. We evaluate the inference accuracy of SHOT-VAE with experiments on two benchmark datasets, MNIST and SVHN. In experiments, we consider five advanced VAE models as baselines, i.e. the standard VAE (M2) <ref type="bibr" target="#b13">(Kingma et al. 2014</ref>  <ref type="table" target="#tab_3">Table 3</ref> show that our SHOT-VAE achieves competitive results to other VAE models without introducing additional domain knowledge or multi-stage structures.</p><p>F.2: Robustness analysis of hyper-parameters. We have introduced some hyper-parameters in training SHOT-VAE, which can be grouped into 2 categories: (1) Parameters to train a deep generative model, i.e. τ for the reparameterization tricks <ref type="bibr" target="#b12">Jang et al. 2017</ref>) and β for the beta-VAE model <ref type="bibr" target="#b3">(Burgess et al. 2018</ref>).</p><p>(2) Parameters  to improve the semi-supervised learning performance, i.e. in smooth-label and γ in the warm-up strategy. Following the previous works <ref type="bibr" target="#b12">(Jang et al. 2017;</ref><ref type="bibr" target="#b7">Dupont 2018;</ref><ref type="bibr" target="#b3">Burgess et al. 2018)</ref>, we set τ = 0.67 and β = 0.01 to make generative models work.</p><p>For parameters related to semi-supervised performance, we also simply use the default value in previous works <ref type="bibr" target="#b3">(Burgess et al. 2018;</ref><ref type="bibr" target="#b18">Müller et al. 2019</ref>), setting = 0.001 and γ = 5. Here we use the statistic hypothesis testing method one-way ANOVA <ref type="bibr" target="#b23">(Tabachnick and Fidell 2007)</ref> to test the null hypothesis of the above three hyper-parameters, that is, the semi-supervised learning performance is the same for different settings of parameters. For , as stated in equation <ref type="formula" target="#formula_5">(7)</ref>, it should not be too large or too small. If is too large, the smoothedp(y|X) is over flexible and becomes too far from the basic degenerated distribution. If too small, then the convergence speed ∝ 1/ may be too slow. Therefore, we set 5 value scales, i.e. [10 −5 , 10 −4 , 10 −3 , 10 −2 , 10 −1 ]. For γ, we also use 5 value scales, i.e. <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">20]</ref>. For each value, we performe 5 experiments with different random seeds to conduct ANOVA. The results in <ref type="table" target="#tab_4">Table 4</ref> do not reject the null hypothesis (all p-values &gt; 0.1), which proves that the semi-supervised performance is robust to the selection of hyper-parameters.</p><p>tive models and data augmentation, FixMatch <ref type="bibr" target="#b22">(Sohn et al. 2020)</ref>, ReMixMatch <ref type="bibr" target="#b1">(Berthelot et al. 2019a</ref>) and UDA <ref type="bibr" target="#b27">(Xie et al. 2019)</ref> combining data augmentation and consistency learning, and SWSA <ref type="bibr" target="#b0">(Athiwaratkun et al. 2019</ref>) combining Π−model and fast-SWA.</p><p>In our experiments, we choose the latest single models as baselines and claim that our model outperformed others. Moreover, our SHOT-VAE model can also easily combine other semi-supervised models and raise much better results. As shown in <ref type="table" target="#tab_5">Table 5</ref>, we combine SHOT-VAE with data augmentations (DA) and Mean Teacher (MT) separately and achieved competitive results in the leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Test accuracy of semi-supervised VAE (M2) model with and w/o ELBO D L . Results indicates that the ELBO D L fails to achieve a better inference accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Error rate comparison of SHOT-VAE to baseline methods on CIFAR-10 (left) and CIFAR-100 (right) for a varying number of labels. "Supervised" refers to training with all 50000 training samples and no unlabeled data. Results show that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The D KL (q φ (y|X) p(y|X)) in D U with and w/o OT-approximation. Results indicate that the OT approximation bridges the gap between q φ (y|X) andp(y|X) in D U , making q φ (y|X) jump out the local minimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison between the negative ELBO value and test accuracy for SHOT-VAE and M2 model. Results indicate that SHOT-VAE breaks the ELBO bottleneck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>which are based on data augmentation, Π-model (Laine and Aila 2017) and Mean Teacher (Tarvainen and Valpola 2017), based on model consistency regularization, Label Propagation (LP) (Iscen et al. 2019) based on pseudolabel and CT-GAN (Wei et al. 2018) based on generative models. The results are presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The conditional generation results of SHOT-VAE. The leftmost columns show images from the test set and the other columns show the conditional generation samples with the learned representation. It indicates that z and y have learned disentangled representations in latent space, as z represents the image style and y represents the digit label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>The schematic of SHOT-VAE. SHOT-VAE has great advantages in interpretability by capturing semanticsdisentangled latent variables as z represents the image style and y represents the image class. The smooth-ELBO proposes a more flexible assumption ofp(y|X) with labelsmoothing technique and the optimal interpolation performs data augmentation on the input pairs with the most similar continuous representations and breaks the ELBO bottleneck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>), stacked-VAE (M1+M2) (Kingma et al. 2014), disentangled-VAE (Narayanaswamy et al. 2017), hyperspherical-VAE (Davidson et al. 2018), and domain-VAE (Ilse et al. 2019). For fairness, the backbones are all 4-layer MLPs with the same amount of parameters (approximately 1M) and the latent dimensions of z are 10 for MNIST and 32 for SVHN. The results presented in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Relative error of smooth-ELBO.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Error rates for SHOT-VAE and other advanced VAE models on MNIST with 100 labels and SVHN with 1000 labels.</figDesc><table><row><cell>Method</cell><cell>MNIST</cell><cell>SVHN</cell></row><row><cell>M2</cell><cell cols="2">11.97(±1.71) 54.33(±0.11)</cell></row><row><cell>M1+M2</cell><cell>3.33(±0.14)</cell><cell>36.02(±0.10)</cell></row><row><cell>Disentangled-VAE</cell><cell>9.71(±0.91)</cell><cell>38.91(±1.06)</cell></row><row><cell>Hyperspherical-VAE</cell><cell>5.2(±0.20)</cell><cell>/</cell></row><row><cell>Domain-VAE</cell><cell>2.7(±1.30)</cell><cell>32.17(±1.20)</cell></row><row><cell>smooth-ELBO</cell><cell cols="2">3.14(±0.19) 29.38(±0.78)</cell></row><row><cell>SHOT-VAE</cell><cell cols="2">3.12(±0.22) 28.82(±0.49)</cell></row><row><cell>and the solutionπ satisfying</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: p-value of ANOVA.</cell></row><row><cell></cell><cell cols="2">CIFAR-10 CIFAR-100</cell></row><row><cell></cell><cell>0.47</cell><cell>0.38</cell></row><row><cell>γ</cell><cell>0.24</cell><cell>0.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The results of combined SHOT-VAE. VAE + DA 95.44 ± 0.27 75.91 ± 0.35 SHOT-VAE + MT 95.59 ± 0.24 76.79 ± 0.32</figDesc><table><row><cell>Method</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>EnAET</cell><cell>95.82</cell><cell>77.08</cell></row><row><cell>FixMatch</cell><cell>95.69</cell><cell>76.82</cell></row><row><cell>ReMixMatch</cell><cell>94.86</cell><cell>74.82</cell></row><row><cell>UDA</cell><cell>94.73</cell><cell>74.12</cell></row><row><cell>SWSA</cell><cell>95.00</cell><cell>72.11</cell></row><row><cell>Mean Teacher</cell><cell>93.72</cell><cell>72.29</cell></row><row><cell>SHOT-</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/PaperCodeSubmission/AAAI2021-260</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://paperswithcode.com/sota/semi-supervised-imageclassification-on-cifar</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Appendix A Proposition 1 The following limitations hold for the smoothed empirical distributionp(y|X) as defined in the Section smooth-ELBO, when q φ (y|X) →p(y|X):</p><p>Appendix G: Comparison with advanced SOTA methods in leaderboard.</p><p>Recent works on semi-supervised learning can be grouped into 3 categories: (1) data augmentation based models (e.g., VAT). (2) consistency learning (e.g., Mean Teacher and Π−model).</p><p>(3) generative models (e.g., GAN and VAE). As we know, combining different useful methods will improve the performance, and there are 5 combined methods in the leaderboard 2 that report better results than SHOT-VAE, i.e. EnAET <ref type="bibr" target="#b25">(Wang et al. 2019</ref>) combining genera-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgKBhA5Y7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CoRR abs/1911.09785</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MixMatch: A Holistic Approach to Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno>abs/1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Understanding disentangling in β-VAE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CoRR abs/1804.03599</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning Augmentation Strategies From Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hyperspherical Variational Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018</title>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018<address><addrLine>Monterey, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-06" />
			<biblScope unit="page" from="856" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tutorial on Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<idno>abs/1606.05908</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Disentangled Joint Continuous and Discrete Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="708" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Elbo surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop in Advances in Approximate Bayesian Inference, NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DIVA: Domain Invariant Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Generative Models for Highly Structured Data, ICLR 2019 Workshop</title>
		<meeting><address><addrLine>New Orleans, Louisiana, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning with Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal Ensembling for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Variational Fair Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno>doi:10.1109/ TPAMI.2018.2858821</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="5925" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno>abs/2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Tabachnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Fidell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brooks/Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Belmont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
	<note>Experimental designs using ANOVA</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Manifold Mixup: Better Representations by Interpolating Hidden States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<idno>abs/1911.09265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">InfoVAE: Information Maximizing Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>abs/1706.02262</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
