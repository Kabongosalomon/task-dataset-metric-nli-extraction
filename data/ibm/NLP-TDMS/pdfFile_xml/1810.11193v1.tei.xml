<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrating Transformer and Paraphrase Rules for Sentence Simplification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
							<email>sanqiang.zhao@pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics and Networked Systems</orgName>
								<orgName type="department" key="dep2">School of Computing and Information</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Meng</surname></persName>
							<email>rui.meng@pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics and Networked Systems</orgName>
								<orgName type="department" key="dep2">School of Computing and Information</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqing</forename><surname>He</surname></persName>
							<email>daqing@pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Informatics and Networked Systems</orgName>
								<orgName type="department" key="dep2">School of Computing and Information</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saptono</forename><surname>Andi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Health Information Management</orgName>
								<orgName type="department" key="dep2">School of Health and Rehabilitation Sciences</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parmanto</forename><surname>Bambang</surname></persName>
							<email>parmanto@pitt.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Health Information Management</orgName>
								<orgName type="department" key="dep2">School of Health and Rehabilitation Sciences</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Integrating Transformer and Paraphrase Rules for Sentence Simplification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from machine translation studies and implicitly learned simplification mapping rules from normalsimple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we propose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple stateof-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/ Sanqiang/text_simplification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. It can benefit individuals with lowliteracy skills <ref type="bibr" target="#b17">(Watanabe et al., 2009</ref>) including children, non-native speakers and individuals with language impairments such as dyslexia <ref type="bibr" target="#b13">(Rello et al., 2013)</ref>, aphasic <ref type="bibr" target="#b0">(Carroll et al., 1999)</ref>.</p><p>Most of the previous studies tackled this task in a way similar to machine translation <ref type="bibr" target="#b20">(Xu et al., 2015a;</ref><ref type="bibr" target="#b23">Zhang and Lapata, 2017)</ref>, in which models are trained on a large number of pairs of sentences, each consisting of a normal sentence and a simplified sentence. Statistical and neural network modeling are two major methods used for this task. The statistical models have the benefit of easily integrating with human-curated rules and features, thus they generally perform well even they are trained with a limited number of data. In contrast, neural network models could learn the simplifying rules automatically without the need for feature engineering, but at the cost of requiring a huge amount of training data. Even though models based on neural networks have outperformed the statistical methods in multiple Natural Language Processing (NLP) tasks, their performance in sentence simplification is still inferior to that of statistical models <ref type="bibr" target="#b20">(Xu et al., 2015a;</ref><ref type="bibr" target="#b23">Zhang and Lapata, 2017)</ref>. We speculate that current training datasets may not be large and broad enough to cover common simplification situations. However, humancreated resources do exist which can provide abundant knowledge for simplification. This motivates us to investigate if it is possible to train neural network models with these types of resources.</p><p>Another limitation to using existing neural network models for sentence simplification is that they are only able to capture frequent transformations; they have difficulty in learning rules that are not frequently observed despite their significance. This may be due to nature of neural networks <ref type="bibr" target="#b3">(Feng et al., 2017)</ref>: during training, a neural network tunes its parameters to learn how to simplify different aspects of the sentence, which means that all the simplification rules are actually contained in the shared parameters. Therefore, if one simplification rule appears more frequently than others, the model will be trained to be more focused on it than the infrequent ones. Meanwhile, models tend to treat infrequent rules as noise if they are merely trained using sentence pairs. If we can leverage an additional memory component to maintain simplification rules individually, it would prevent the model from forgetting low-frequency rules as well as help it to distinguish real rules from noise. Therefore, we propose the Deep Memory Augmented Sentence Simplification (DMASS) model. For comparison pur-pose, we also introduce another approach, Deep Critic Sentence Simplification (DCSS) model, to encourage applying the less frequently occurring rules by revising the loss function. It this way, simplification rules are encouraged to maintained internally in the shared parameters while avoiding the consumption of an unwieldy amount of additional memory.</p><p>In this study, we propose two improvements to the neural network models for sentence simplification. For the first improvement, we propose to use a multi-layer, multi-head attention architecture <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>. Compared to RNN/LSTM (Recurrent Neural Network / Long Short-term Memory), the multi-layer, multi-head attention model would be able to selectively choose the correct words in the normal sentence and simplify them more accurately.</p><p>Secondly, we propose two new approaches to integrate neural networks with human-curated simplification rules. Note that previous studies rarely tried to incorporate explicit human language knowledge into the encoder-decoder model. Our first approach, DMASS, maintains additional memory to recognize the context and output of each simplification rules. Our second approach, DCSS, follows a more traditional approach to encode the context and output of each simplification rules into the shared parameters.</p><p>Our empirical study demonstrates that our model outperforms all the previous sentence simplification models. They achieve both a good coverage of rules to be applied (recall) and a high accuracy gained by applying the correct rules (precision).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentence Simplification For statistical modeling, <ref type="bibr" target="#b24">Zhu et al. (2010)</ref> proposed a tree-based sentence simplification model drawing inspiration from statistical machine translation. <ref type="bibr" target="#b18">Woodsend and Lapata (2011)</ref> employed quasi-synchronous grammar and integer programming to score the simplification rules. <ref type="bibr" target="#b19">Wubben et al. (2012)</ref> proposed a two-stage model PBMT-R, where a standard phrase-based machine translation (PBMT) model was trained on normal-simple aligned sentence pairs, and several best generations from PBMT were re-ranked based how dissimilar they were to a normal sentence. Hybrid, a model proposed by <ref type="bibr" target="#b9">Narayan and Gardent (2014)</ref> was also a two-stage model combining a deep semantic analysis and machine translation framework. SBMT-SARI <ref type="bibr" target="#b22">(Xu et al., 2016)</ref> achieved state-of-the-art performance by employing an external knowledge base to promote simplification. In terms of neural network models, <ref type="bibr" target="#b23">Zhang and Lapata (2017)</ref> argued that the RNN/LSTM model generated sentences but it does not have the capability to simplify them. They proposed DRESS and DRESS-LS that employ reinforcement learning to reward simpler outputs. As they indicated, the performance is still inferior due to the lack of external knowledge. Our proposed model is designed to address the deficiency of current neural network models which are not able to integrate an external knowledge base.</p><p>Augmented Dynamic Memory Despite positive results obtained so far, a particular problem with the neural network approach is that it has a tendency towards favoring to frequent observations but overlooking special cases that are not frequently observed. This weakness with regard to infrequent cases has been noticed by a number of researchers who propose an augmented dynamic memory for multiple applications, such as language models <ref type="bibr" target="#b1">(Daniluk et al., 2017;</ref><ref type="bibr" target="#b5">Grave et al., 2016)</ref>, question answering <ref type="bibr">(Miller et al., 2016)</ref>, and machine translation <ref type="bibr" target="#b3">(Feng et al., 2017;</ref><ref type="bibr" target="#b15">Tu et al., 2017)</ref>. We find that current sentence simplification models suffer from a similar neglect of infrequent simplification rules, which inspires us to explore augmented dynamic memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Sentence Simplification Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Layer, Multi-Head Attention</head><p>Our basic neural network-based sentence simplification model utilizes a multi-layer and multi-head attention architecture <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our model based on the Transformer architecture works as follows: given a pair consisting a normal sentence I and a simple sentence O, the model learns the mapping from I to O.</p><p>The encoder part of the model (see the left part of <ref type="figure" target="#fig_0">Figure 1</ref>) encodes the normal sentence with a stack of L identical layers. Each layer has two sublayers: one layer is for multi-head self-attention and the other one is a fully connected feed-forward neural network for transformation. The multi-head self-attention layer encodes the output from the previous layer into hidden state e (s,l) (step s and layer l) as shown in Equation 1, where α enc (s ,l) indicates the attention distribution over the step s and layer l. Each hidden state summarizes the hidden states in the previous layer through the multi-head attention function a() <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> where H refers to the number of heads.</p><p>The right part of <ref type="figure" target="#fig_0">Figure 1</ref> denotes the decoder for generating the simplified sentence. The decoder also consists of a stack of L identical layers. In addition to the same two sub-layers as those in the encoder part, the decoder also inserts another multi-head attention layer aiming to attend on the encoder outputs. The bottom multi-head self-attention plays the same role as the one in the encoder, where the hidden state d <ref type="bibr">(s,l)</ref> is computed in the Equation 2. The upper multi-head attention layer is used to seek relevant information from encoder outputs. Through the same mechanism, context vector c (s,l) (step s and layer l) is computed in the Equation 3.</p><formula xml:id="formula_0">e (s,l) = s α enc (s ,l) e (s ,l−1) , α enc (s ,l) =a(e (s,l) , e (s ,l−1) , H) 1 (1) d (s,l) = s α dec (s ,l) d (s ,l−1) , α dec (s ,l) =a(d (s,l) , c (s ,l−1) , H) 2 (2) c (s,l) = s α dec2 (s ,l) e (s ,L) , α dec2 (s ,l) =a(d (s,l) , e (s ,L) , H)<label>(3)</label></formula><p>The model is trained to minimize the negative log-likelihood of the simple sentence, L seq = −logP (O|I, θ) where θ represents all the parameters in the current model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integrating with Simple PPDB</head><p>A previous study <ref type="bibr" target="#b22">(Xu et al., 2016)</ref> has demonstrated the benefits of using an external knowledge base in conjunction with a statistical simplification model. However, as far as we know, no efforts have been made to integrate neural network models with the knowledge base, and our study is the first to meet this goal.   refers to a paraphrase knowledge base for simplification. It is a refined version of another knowledge, PPDB <ref type="bibr" target="#b4">(Ganitkevitch et al., 2013)</ref>, which was originally designed to support paraphrase. Simple PPDB contains 4.5 million paraphrase rules, each of which provides the mapping from a normal phrase to a simplified phrase, the syntactic type of the normal phrase, and the simplification weight. <ref type="table">Table 1</ref> shows four examples, where "recipient" can be simplified to "winner" with a weight 0.75530 if "recipient" is a singular noun (NN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Deep Critic Sentence Simplification Model (DCSS)</head><p>The Simple PPDB offers guidance about whether a word needs to be simplified and how it should be simplified. The Deep Critic Sentence Simplification (DCSS) model is designed to apply rules identified by the Simple PPDB by introducing a new loss function. Different from the standard loss function that minimizes the distance away from the ground truth, the new loss function aims to maximize the likelihood of applying simplification rules. It also reweights the probability of generating each word by its simplification weight in order to relieve the problem of overlooking infrequent simplification rules. For example, given a normal sentence in the training set, "the recipient of the kate greenaway medal", the simplified sentence is "the winner of the kate greenaway medal.", where "recipient" is simplified to "winner", which is identified by Simple PPDB. The major goal of the loss functions is to support the model in generating the simplified word "winner" while deterring the model from generating the word "recipient". Specifically, for an applicable simplification rule, our new loss function maximizes the probability of generating the simplified form (word "winner") and meanwhile minimizes the probability of generating the original form (word "recipient"). As in Equation 4, where w rule indicates the weight of the simplification rule provided by the Simple PPDB, once the model generates "recipient", the model is criticized to generate word "winner"; when model predicts correctly with "winner", the model is trained to minimize the probability of "recipient". In this way, the model avoids selecting normal words and instead becomes inclined to choose the simplified words.</p><formula xml:id="formula_1">Lcritic =          −w rule logP (winner|I, θ) if model generates recipient w rule logP (recipient|I, θ)</formula><p>if model generates winner <ref type="formula">(4)</ref> The L critic merely focuses on the words identified by the Simple PPDB and L seq focuses on the entire vocabulary. So, the model is trained in an end-to-end fashion by minimizing L seq and L critic alternately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Deep Memory Augmented Sentence</head><p>Simplification Model (DMASS)</p><p>DCSS, similar to the majority of neural network models, uses a piece of shared memory, i.e. the parameters, as the media to store the learned rules from the data. As a result, it still focuses much more on rules that are frequently observed and ignores the rules observed infrequently. However, infrequent rules are still important, particularly when the training data is limited.</p><p>In order to make full use of the rules in the knowledge base, we introduce the Deep Memory Augmented Sentence Simplification (DMASS) model. DMASS has an augmented dynamic memory to record multiple key-value pairs for each rule in the Simple PPDB. The key vector stores a context vector that is computed based on the weighted average of encoder hidden states and the current decoder hidden states. The value vector stores the output vector.</p><p>Our DMASS model is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Given the same example normal sentence " the recipient of kate greenaway medal", Simple PPDB determines that the word "recipient" should be simplified to "winner". The encoder represents the normal sentence as a list of hidden states, [e (1,L) , e (2,L) , ...] where L indicates the final layer of encoder hidden states. When predicting the next word in the simplified sentence, the decoder of layer j represents the previous words as hidden <ref type="bibr">states [d (1,j)</ref> , ... ]. c <ref type="bibr">(1,j)</ref> refers to the current context vector following attention layer, which is the weighted average of [e (1,L) , e (2,L) , ...] based on d <ref type="bibr">(1,j)</ref> . A feed-forward fully connected neural network (FFN) combines the output of the decoder and the output from memory read module into the final output r winner . In addition to the word prediction, c (1,j) and r winner will be sent to memory update module.</p><p>In the remainder of this section, we will introduce the two modules of DMASS mentioned above: Memory Read Module and Memory Update Module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Read Module</head><p>The memory read module incorporates rules into prediction. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, current augmented memory contains three candidate rules for the word "recipient", which indicates that it can be simplified into "winner", "receiver" or "host", respectively. The current context vector c (1,j) is treated as a query to search for suitable rules by using Equation 5, where α r i denotes the weight for i th rule, which is computed through the dot product between current context vector c (1,j) and c i . Then using Equation 6, α r i weights each output vector to generate mem- </p><formula xml:id="formula_2">α r i = ei j ej ei = exp(c (1,j) · ci) (5) ro = α r i rr rr ∈ [rwinner, rreceiver, r host ] (6)</formula><p>Memory Update Module The task of the memory update module is to update the key and value vectors in the augmented memory. Once the model predicts the output vector r winner , both r winner and the current context vector c 1,j are sent to the memory update module. If the augmented memory does not contain the key-value pair for the rule, c 1,j and r winner are appended to the memory. If the augmented memory contains the key-value pair, the key vector is updated as the mean of current key vector and c 1,j . Similarly, the value vector is also updated as the mean of current value vector and r winner .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset We utilize the dataset WikiLarge (Zhang and Lapata, 2017) for training. It is the largest Wikipedia corpus, constructed by merging previously created simplification corpora. Specifically, the training dataset contains 296,402 normalsimple sentence pairs gathered from <ref type="bibr" target="#b24">(Zhu et al., 2010;</ref><ref type="bibr" target="#b18">Woodsend and Lapata, 2011;</ref><ref type="bibr" target="#b6">Kauchak, 2013)</ref>. For validation and testing, we use the dataset Turk created by <ref type="bibr" target="#b22">(Xu et al., 2016)</ref>. In this dataset, eight simplified reference sentences for each normal sentence are used as the ground-truth, all of which are generated by Amazon Mechanical Turk workers. The Turk dataset contains 2,000 data samples for validation and 356 samples for testing. We consider the Turk to be the most reliable data set because (1) it is human-generated and (2) it contains multiple simplification references for each normal sentence due to the existence of multiple equally good simplifications of each sentence. We also include the second test set Newsela, a corpus introduced by (Xu et al., 2015b) who argue that only using normal-simple sentence pairs from Wikipedia is suboptimal due to the automatic sentence alignment which unavoidably introduces errors, and the uniform writing style which leads to systems that generalize poorly. The test set contains 1,419 normal-simple sentence pairs 3 . To demonstrate that our models are able to perform well on a different style of corpus, we report the results of Newsela test set by using the models trained/tuned on Turk dataset. Following Zhang and Lapata (2017)'s way, we tag and anonymize name entities with a special token in the format of NE@N, where NE includes {P ER, LOC, ORG} and N indicates the N th distinct NE type of entity. We also replace those tokens occurring three times or less in the training set with a mark "UNK" as mentioned in <ref type="bibr" target="#b23">(Zhang and Lapata, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We report the results of the experiment with two metrics that are widely used in the literature: SARI <ref type="bibr" target="#b22">(Xu et al., 2016)</ref> and <ref type="bibr">FKGL (Kincaid et al., 1975)</ref>. FKGL computes the sentence length and word length as a way to measure the simplicity of a sentence. The lower value of FKGL indicates simpler sentence. FKGL measures the simplicity of a sentence without considering the ground truth simplification references and it correlates little with human judgment <ref type="bibr" target="#b22">(Xu et al., 2016)</ref>, so we also use another metric, SARI. SARI, which stands for "System output Against References and against the normal sentence", computes the arithmetic mean of Ngrams (N includes 1,2,3 and 4) F1-score of three rewrite operations: addition, deletion, and keeping. Specifically, it rewards addition operations where a word in the generated simplified sentence does not appear in the normal sentence but is mentioned in the reference sentences. It also rewards words kept or deleted in both the simplified sentence and the reference sentences. In our experiment, we also present the F1-score of three rewrite operations: addition, deletion, and keeping. <ref type="bibr" target="#b22">Xu et al. (2016)</ref> demonstrated that SARI correlates most closely to human judgments in sentence simplification tasks. Thus, we treated SARI as the most important measurement in our study. Because SARI rewards deleting and adding separately, we also include another metric to measure the correctness of lexical transformation, namely word simplification, verified by Simple PPDB. By comparing the normal sentence and ground truth simplified references, we collect rules that are correct to be used for simplifying each normal sentence. Then we calculate the precision, recall, and F1 score for using the correct rules. As a result, the recall expresses the coverage of rules to be applied, and the precision implies the accuracy gained by applying the correct rules.</p><p>Training Details We initialized the encoder and decoder word embedding lookup matrices with 300-dimensional Glove vectors <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref>. The word embedding dimensionality and the number of hidden units are set to 300. During the training, we regularize all layers with a dropout rate of 0.2 <ref type="bibr" target="#b14">(Srivastava et al., 2014)</ref>. For multilayer and multi-head architecture, 4 encoder and decoder layers (set L as 4) and 5 multi-attention heads (set H as 5) are used. We will discuss the trade-off between different layers and different heads in Sections 4.1. For DMASS, we use the context vector based on the first layer of the decoder (set j as 1). For optimization, we use Adagrad <ref type="bibr" target="#b2">(Duchi et al., 2011)</ref> with the learning rate set to 0.1. The gradient is truncated by 4 <ref type="bibr" target="#b10">(Pascanu et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Impacts of Multi-Layer, Multi-Head Attention Architecture</head><p>The reason to employ the Transformer architecture in the sentence simplification task is that we believe that its multi-layer, multi-head attention provides a better capability of modeling both the overall context and the important cues for sentence simplification. In this section, we examine the applicability of multi-layer, multi-head attention architecture to the sentence simplification task. We compare our results against the RNN/LSTMbased sentence simplification models. Note that the results of our models presented here have not been integrated with the Simple PPDB. <ref type="table" target="#tab_2">Table 2</ref> shows the experiment results where LxHy indicates a run with Transformer using x layers and y heads. When compared with results of RNN/LSTM, our Transformer-based model performed better in terms of SARI and FKGL values. In addition, with the increased number of layers or heads, the values of SARI and FKGL improve accordingly. In the remainder of this section, we analyze the insights of these results in detail.</p><p>In our tasks, FKGL measures the sentence length and the word length as two factors for evaluating a simplified sentence. Therefore, we include Wlen(Word Length) and Slen(Sentence Length) into our analysis. As shown in Table 2, models with higher numbers of layers and/or heads do generally reduce the average word length and the average sentence length, which indicates that the higher number of layers and/or heads in the model leads to simpler outcomes.</p><p>It has been found that SARI correlates most closely to human judgment <ref type="bibr" target="#b22">(Xu et al., 2016)</ref>. To further analyze the effects of SARI, we study the impacts of three rewrite operations in SARI: add, delete, and keep. As shown in <ref type="table" target="#tab_2">Table 2</ref>, we find that the improvement mostly results from correctly adding simplified words and deleting normal words, but not from keeping words. By analyzing the outputs, the increased number of layers or heads results in better capability to simplify the words. Specifically, models with the greater number of layers or heads tend to remove the normal words and add simplified words. However, they may introduce inaccurate simplified words, thereby driving down the F1 score for keeping words. We believe the Simple PPDB, which offers guidance about whether words need to be simplified and how they should be simplified, provides an ideal method to alleviate this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impacts of Integrating the Simple PPDB</head><p>In order to make comprehensive comparisons with the state-of-the-art models, we include multiple baselines from the literature, including <ref type="bibr">PBMT-R (Wubben et al., 2012)</ref>, Hybrid <ref type="bibr" target="#b9">(Narayan and Gardent, 2014)</ref>, and SBMT-SARI <ref type="bibr" target="#b22">(Xu et al., 2016)</ref>. We also include several strong baselines based on neural networks such as RNN/LSTM, DRESS, DRESS-LS <ref type="bibr" target="#b23">(Zhang and Lapata, 2017)</ref> as shown in <ref type="table" target="#tab_3">Tables 3 and 4</ref> We developed three models for this experiment.    subscript beam indicates the size of beam search. <ref type="table" target="#tab_3">Tables  3 and 4</ref>, Hybrid achieves the lowest (thus the best) FKGL score, and DRESS and DRESS-LS have the second best FKGL scores. All the other models including ours do not perform as well as these two. But FKGL measures the simplicity of a sentence without considering the ground truth simplification references, so high FKGL may be at the cost of losing information and readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results with FKGL Metric As shown in</head><p>To further analyze the FKGL results, we exam-ine the average sentence length and word length of the outcomes of the models and they are listed as WLen (Word Length) and SLen (Sentence Length) in <ref type="table" target="#tab_3">Tables 3 and 4</ref>. Hybrid, DRESS, and DRESS-LS are good at generating shorter sentences, but they are not as good at choosing shorter words. In contrast, SBMT-SARI, DCSS, and DMASS all generate shorter words. Therefore, we believe that, by optimizing language model as a goal for the reinforcement learning, DRESS and DRESS-LS are tuned to simplify sentences by shortening the sentence lengths. In contrast, with the help of an integrated external knowledge base, SBMT-SARI and our models have more capability to generate shorter words in order to simplify sentences. Therefore, these two sets of models complete sentence simplification tasks via different routes, and perhaps there should be an exploration of combining these two routes for even more successful sentence simplification.</p><p>Another interesting finding is that the larger beam search size increases average word length slightly. This is because the larger beam search size mitigates the issue of the inaccurate simplification so that fewer words are simplified. To measure the correctness of simplification, we analyze the SARI metric and Rule Utilization.</p><p>Results with SARI Metric SARI is the most reliable metric for the sentence simplification task <ref type="bibr" target="#b22">(Xu et al., 2016)</ref>, therefore we would like to present more detailed discussion regarding the SARI results. As shown in <ref type="table" target="#tab_3">Tables 3 and 4</ref>, DMASS+DCSS achieves the best SARI score, which demonstrates the effectiveness of integrating the knowledge base Simple PPDB for sentence simplification.</p><p>To further examine the impacts of the F1 scores for three operations in calculating the SARI scores, as shown in <ref type="table" target="#tab_3">Tables 3 and 4</ref>, DMASS+DCSS, as well as other models with high SARI performance benefit greatly by correctly adding and deleting words. We believe these benefits mostly result from the integration with the knowledge base, which provides reliable guidance about which words to modify. SBMT-SARI, which represents a previous state-of-the-art model that also integrates with knowledge bases, performs best in correctly adding new words but performs inferiorly in deleting/keeping words. By analyzing the outputs, SBMT-SARI acts aggressively to simplify as many words as possible. But it also results in incorrect simplification. DRESS and DRESS-LS are inclined to generate the shorter sentence, which leads to high F1 scores for deleting words, but it lags behind other models in adding/keeping words.</p><p>DMASS leverages an additional memory component to maintain the simplification rules; DCSS uses internal memory to store those rules. A large number of simplification rules might confuse the model with limited internal memory. This might be the reason why DMASS works better than DCSS. By taking a two-way advantage of both models, DMASS+DCSS takes a two-fisted approach to store the simplification rules in both additional and internal memory. As a result, DMASS+DCSS achieves the best performance in SARI.</p><p>Results with Rule Utilization In this section, we evaluate the models' capabilities for word transformation. The majority of previous approaches, except for the SBMT-SARI, perform poorly in recall. We believe the knowledge base Simple PPDB will reduce uncertainty in the word selection.</p><p>As before, SBMT-SARI acts aggressively to simplify every word in the sentence. Such an aggressive action leads to relatively high performance in recall. However, it does not achieve a strong performance in precision. DMASS performs better in terms of rule utilization as compared to DCSS by leveraging an additional memory. DMASS+DCSS takes advantage of both approaches that store the simplification rules in additional and internal memory. This combined model is guaranteed to apply more accurate rules.</p><p>As compared to the loose relationship between SARI and beam search size, we find that that beam search size correlates strongly with the performance in rule utilization. Thus, we believe larger beam search size contributes to good coverage of rules to be applied as well as accuracy in applying rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose two innovative approaches for sentence simplification based on neural networks. Both approaches are based on multilayer and multi-head attention architecture and integrated with the Simple PPDB, an external sentence simplification knowledge base, in different ways. By conducting a set of experiments, we demonstrate that the proposed models perform better than existing methods and achieve new state-of-the-art in sentence simplification. Our experiments firstly prove that the multi-layer and multi-head attention architecture has an excellent capability to understand the text by accurately selecting specific words in a normal sentence and then choosing right simplified words. Secondly, by integrating with the knowledge base, our models outperform multiple state-of-the-art baselines for sentence simplification. Compared to previous models which integrated with the knowledge base, our models, especially, DMASS+DCSS, provide both good coverage of rules to be applied and accuracy in applying the correct rules. In future, we would like to investigate deeper into the different effects of additional memory and internal memory.</p><p>6 Acknowledge</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of the Transformer architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Diagram of DMASS Model ory read output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of transformers with different layers and heads of attention on Turk dataset</figDesc><table><row><cell>Model</cell><cell>FKGL</cell><cell cols="2">Factors in FKGL WLen SLen</cell><cell>SARI</cell><cell cols="3">F1 for operations of SARI Add Delete Keep</cell><cell>Prec</cell><cell cols="3">Rule Utilization Recall F1</cell></row><row><cell>PBMT-R</cell><cell>8.35</cell><cell>1.30</cell><cell>22.08</cell><cell>38.56</cell><cell>5.73</cell><cell>36.93</cell><cell>73.02</cell><cell cols="3">14.60 22.29</cell><cell>15.01</cell></row><row><cell>Hybrid</cell><cell>4.71</cell><cell>1.28</cell><cell>13.38</cell><cell>31.40</cell><cell>5.49</cell><cell>45.48</cell><cell>46.86</cell><cell cols="3">10.62 7.61</cell><cell>7.62</cell></row><row><cell>SBMT-SARI</cell><cell>7.49</cell><cell>1.18</cell><cell>23.50</cell><cell>39.96</cell><cell>5.97</cell><cell>41.43</cell><cell>72.51</cell><cell cols="3">13.30 28.96</cell><cell>15.77</cell></row><row><cell>RNN/LSTM</cell><cell>8.67</cell><cell>1.34</cell><cell>21.68</cell><cell>35.66</cell><cell>3.00</cell><cell>28.95</cell><cell>75.03</cell><cell cols="3">13.67 14.83</cell><cell>11.65</cell></row><row><cell>DRESS</cell><cell>6.80</cell><cell>1.34</cell><cell>16.55</cell><cell>37.08</cell><cell>2.94</cell><cell>43.14</cell><cell>65.16</cell><cell cols="3">13.06 12.50</cell><cell>10.77</cell></row><row><cell>DRESS-LS</cell><cell>6.92</cell><cell>1.35</cell><cell>16.76</cell><cell>37.27</cell><cell>2.82</cell><cell>42.21</cell><cell>66.78</cell><cell cols="3">12.40 11.36</cell><cell>9.83</cell></row><row><cell>DMASS</cell><cell>7.41</cell><cell>1.29</cell><cell>20.00</cell><cell>39.81</cell><cell>5.04</cell><cell>41.94</cell><cell>72.46</cell><cell cols="3">17.97 25.54</cell><cell>18.12</cell></row><row><cell>DCSS</cell><cell>7.34</cell><cell>1.31</cell><cell>19.30</cell><cell>39.26</cell><cell>5.29</cell><cell>41.24</cell><cell>71.26</cell><cell cols="3">13.14 21.30</cell><cell>13.87</cell></row><row><cell>DMASS+DCSS</cell><cell>7.18</cell><cell>1.27</cell><cell>20.10</cell><cell>40.42</cell><cell>5.48</cell><cell>45.55</cell><cell>70.22</cell><cell cols="3">16.25 30.42</cell><cell>18.98</cell></row><row><cell>DMASS beam=4</cell><cell>8.20</cell><cell>1.30</cell><cell>21.66</cell><cell>39.16</cell><cell>4.90</cell><cell>38.41</cell><cell>74.18</cell><cell cols="3">18.53 25.46</cell><cell>18.40</cell></row><row><cell>DCSS beam=4</cell><cell>7.97</cell><cell>1.32</cell><cell>20.56</cell><cell>39.11</cell><cell>5.10</cell><cell>38.87</cell><cell>73.36</cell><cell cols="3">14.36 20.96</cell><cell>14.48</cell></row><row><cell>DMASS+DCSS beam=4</cell><cell>7.93</cell><cell>1.28</cell><cell>21.49</cell><cell>40.34</cell><cell>5.73</cell><cell>42.55</cell><cell>72.74</cell><cell cols="3">18.55 31.56</cell><cell>20.81</cell></row><row><cell>DMASS beam=8</cell><cell>8.23</cell><cell>1.30</cell><cell>21.68</cell><cell>39.15</cell><cell>4.95</cell><cell>37.80</cell><cell>74.69</cell><cell cols="3">18.44 25.34</cell><cell>18.32</cell></row><row><cell>DCSS beam=8</cell><cell>7.97</cell><cell>1.32</cell><cell>20.56</cell><cell>39.11</cell><cell>5.10</cell><cell>38.87</cell><cell>73.36</cell><cell cols="3">14.37 20.96</cell><cell>14.80</cell></row><row><cell>DMASS+DCSS beam=8</cell><cell>8.04</cell><cell>1.29</cell><cell>21.64</cell><cell>40.45</cell><cell>5.72</cell><cell>42.23</cell><cell>73.41</cell><cell cols="2">19.46</cell><cell>31.99</cell><cell>21.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of baselines and proposed models on the Turk dataset.</figDesc><table><row><cell>Model</cell><cell>FKGL</cell><cell cols="3">Factors in FKGL SARI WLen SLen</cell><cell cols="2">F1 for operations of SARI Add Delete Keep</cell><cell cols="2">Rule Utilization Prec Recall F1</cell></row><row><cell>RNN/LSTM</cell><cell>6.09</cell><cell>1.22</cell><cell>18.67</cell><cell cols="2">21.09 11.10 38.78</cell><cell>13.39</cell><cell>12.62 22.63</cell><cell>14.68</cell></row><row><cell>DRESS</cell><cell>4.96</cell><cell>1.23</cell><cell>15.27</cell><cell cols="2">25.70 10.65 52.59</cell><cell>13.86</cell><cell>12.56 17.88</cell><cell>13.28</cell></row><row><cell>DRESS-LS</cell><cell>5.07</cell><cell>1.24</cell><cell>15.47</cell><cell cols="2">24.91 11.21 49.74</cell><cell>13.76</cell><cell>12.61 17.50</cell><cell>13.42</cell></row><row><cell>DMASS</cell><cell>5.38</cell><cell>1.20</cell><cell>17.47</cell><cell cols="2">25.41 11.88 50.39</cell><cell>13.97</cell><cell>16.32 34.79</cell><cell>20.00</cell></row><row><cell>DCSS</cell><cell>5.64</cell><cell>1.22</cell><cell>17.58</cell><cell cols="2">24.31 13.52 45.60</cell><cell>13.81</cell><cell>15.20 30.38</cell><cell>18.39</cell></row><row><cell>DMASS+DCSS</cell><cell>5.17</cell><cell>1.18</cell><cell>17.60</cell><cell cols="2">27.28 11.56 56.10</cell><cell>14.19</cell><cell>15.98 40.64</cell><cell>20.98</cell></row><row><cell>DMASS beam=4</cell><cell>5.64</cell><cell>1.21</cell><cell>17.79</cell><cell cols="2">24.09 13.96 44.47</cell><cell>13.85</cell><cell>17.40 35.97</cell><cell>21.37</cell></row><row><cell>DCSS beam=4</cell><cell>5.80</cell><cell>1.22</cell><cell>17.85</cell><cell cols="2">23.28 15.28 40.76</cell><cell>13.81</cell><cell>16.77 31.81</cell><cell>20.06</cell></row><row><cell cols="2">DMASS+DCSS beam=4 5.42</cell><cell>1.19</cell><cell>17.81</cell><cell cols="2">26.39 13.92 51.13</cell><cell>14.13</cell><cell>18.71 43.36</cell><cell>24.23</cell></row><row><cell>DMASS beam=8</cell><cell>5.68</cell><cell>1.21</cell><cell>17.83</cell><cell cols="2">23.95 14.25 43.74</cell><cell>13.86</cell><cell>17.69 36.37</cell><cell>21.74</cell></row><row><cell>DCSS beam=8</cell><cell>5.77</cell><cell>1.22</cell><cell>17.76</cell><cell cols="2">23.18 15.65 40.08</cell><cell>13.82</cell><cell>17.18 32.18</cell><cell>20.50</cell></row><row><cell cols="2">DMASS+DCSS beam=8 5.43</cell><cell>1.19</cell><cell>17.83</cell><cell cols="2">26.29 14.08 50.62</cell><cell>14.17</cell><cell>18.89 43.54</cell><cell>24.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of baselines and proposed models on the Newsela dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The lowest hidden state e (:,0) is the word embedding.2  The lowest context vector c (:,0) is the word embedding.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Because the earlier publications don't provide preprocess details, we use our own script to pre-process the articles into sentence pairs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported in part by the University of Pittsburgh Center for Research Computing through the resources provided. The research is funded in part by grants the National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR) #90RE5018 and #90DP0064, and by Pittsburgh Health Data Alliance's "CARE" Project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simplifying text for language-impaired readers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siobhan</forename><surname>Canning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="269" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Frustratingly short attention spans in neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Daniluk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04521</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02005</idno>
		<title level="m">Memoryaugmented neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ppdb: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04426</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving text simplification language modeling using unsimplified text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1537" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert P Fishburne</forename><surname>Peter Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
		<respStmt>
			<orgName>Naval Technical Training Command Millington TN Research Branch</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<title level="m">Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid simplification using deep semantics and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple ppdb: A paraphrase database for simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 54th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">143</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dyswebxia 2.0!: more accessible text for people with dyslexia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luz</forename><surname>Rello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Bayarri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuki</forename><surname>Gòrriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurang</forename><surname>Kanvinde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Topac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility</title>
		<meeting>the 10th International Cross-Disciplinary Conference on Web Accessibility</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09367</idno>
		<title level="m">Learning to remember translation history with a continuous cache</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vinícius Rodriguez Uzêda, Renata Pontin de Mattos Fortes, Thiago Alexandre Salgueiro Pardo, and Sandra Maria Aluísio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaldo Candido</forename><surname>Willian Massami Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on Design of communication</title>
		<meeting>the 27th ACM international conference on Design of communication</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
	<note>Facilita: reading assistance for low-literacy readers</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to simplify sentences with quasi-synchronous grammar and integer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Problems in current text simplification research: New data can help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="283" to="297" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10931</idno>
		<title level="m">Sentence simplification with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A monolingual tree-based translation model for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on computational linguistics</title>
		<meeting>the 23rd international conference on computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1353" to="1361" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
