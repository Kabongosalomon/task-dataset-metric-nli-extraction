<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
							<email>luodezhao@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<email>liuchang615@mails.ucas.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
							<email>zhouyu@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
							<email>yangdongbao@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Ma</surname></persName>
							<email>macan@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<email>qxye@ucas.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
							<email>wangweiping@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel self-supervised method, referred to as Video Cloze Procedure (VCP), to learn rich spatial-temporal representations. VCP first generates "blanks" by withholding video clips and then creates "options" by applying spatiotemporal operations on the withheld clips. Finally, it fills the blanks with "options" and learns representations by predicting the categories of operations applied on the clips. VCP can act as either a proxy task or a target task in self-supervised learning. As a proxy task, it converts rich self-supervised representations into video clip operations (options), which enhances the flexibility and reduces the complexity of representation learning. As a target task, it can assess learned representation models in a uniform and interpretable manner. With VCP, we train spatial-temporal representation models (3D-CNNs) and apply such models on action recognition and video retrieval tasks. Experiments on commonly used benchmarks show that the trained models outperform the state-ofthe-art self-supervised models with significant margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, Convolutional Neural Networks (CNNs) have unprecedentedly advanced the field of computer vision. Generally, vision tasks are solved by training models on large-scale datasets with label annotations <ref type="bibr" target="#b15">(Kim, Cho, and Kweon 2019)</ref>. Typically, CNNs pre-trained on Im-ageNet <ref type="bibr" target="#b11">(Jia et al. 2009</ref>) incorporate rich representation capability and have been widely used as initial models.</p><p>Nevertheless, annotating large-scale datasets is costly and labor-intensive, particularly when facing tasks involving complex data (e.g., videos) and concepts (e.g., action analysis and video retrieval) <ref type="bibr" target="#b7">(Fernando et al. 2017;</ref><ref type="bibr" target="#b13">Kay et al. 2017)</ref>.</p><p>To conquer this issue, self-supervised representation learning, which leverages the information from unlabelled data to train desired models, has attracted increasing attention from the artificial intelligence community. For video data, existing approaches usually define an annotation-free Figure 1: VCP is a novel self-supervised method for spatialtemporal representation learning. It generates "blanks" by withholding video clips, creates "options" by applying spatial-temporal operations on the withheld clips, and completes cloze for feature learning.</p><p>proxy task, which provides special supervision for model learning by fulfilling the objective of the proxy task.</p><p>In the early research <ref type="bibr" target="#b6">(Doersch, Gupta, and Efros 2015;</ref><ref type="bibr" target="#b25">Xiaolong and Abhinav 2015)</ref>, relative location of the patches in images or the order of video frames were used as a supervisory signal. However, the learned features were merely on a frame-by-frame basis, which are implausible to video analytic tasks where spatio-temporal features are prevailing. Recently, <ref type="bibr" target="#b24">(Wang et al. 2019)</ref> proposed to learn representations by regressing motion and appearance statistics. In <ref type="bibr" target="#b7">(Fernando et al. 2017)</ref>, an odd-one-out network is proposed to identify the unrelated or odd video clips from a set of otherwise related clips. To find the odd video clip, the models have to learn spatio-temporal features that can discriminate video clips of minor differences.</p><p>Despite of the effectiveness, existing approaches are usually developed upon domain-knowledge and therefore are not capable to incorporate various spatial-temporal operations. This seriously restricts the representation capability of learned models. Furthermore, the lack of a model assessment approach strikingly limits the pertinence of selfsupervised representation learning.</p><p>In this paper, we propose a new self-supervised method called Video Cloze Procedure (VCP). In VCP, we withhold a video clip from a video sequence and apply multiple spatiotemporal operations on it. We train a 3D-CNN model to identify the category of operations, which drives learning rich feature representations. The motivation behinds VCP lies in that applying richer operations on video clips facilities exploring higher representation capability, <ref type="figure">Fig. 1</ref>.</p><p>VCP consists of three components including blank generation, option creation, and cloze completion. The first component generates blanks by withholding video clips from given clip sequences. The second component facilitates multiple spatial-temporal representation learning by applying spatial-temporal operations on the withheld clips. Finally, cloze completion fills the blanks with options and learns representations by predicting the category of operations.</p><p>VCP can act as either a proxy task or a target task in selfsupervised learning. As a proxy task, it converts rich selfsupervised representations into video clip operations, which enhances the flexibility and reduces the complexity of representation learning. As a target task, it can assess learned representation models in an interpretable manner.</p><p>The contributions of this work are summarized as follows:</p><p>• We propose Video Cloze Procedure (VCP), providing a simple-yet-effective framework for self-supervised spatio-temporal representation learning.</p><p>• We propose a new model assessment approach by designing VCP as a special target task, which improves the interpretability of self-supervised representation learning.</p><p>• VCP is applied on three kinds of 3D CNN models and two target tasks including action recognition and video retrieval, and improves the state-of-the-arts with significant margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Self-supervised learning leverages the information from unlabelled data to train target models. Existing approaches usually define an annotation-free proxy task which demands a network predicting information latent in unannotated videos. The learned models can be applied to target tasks (supervised or unsupervised) in a fine-tuning manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proxy Tasks</head><p>In a broad view of self-supervised learning, the proxy tasks can be constructed over multiple sensory data such as egomotion and sound <ref type="bibr" target="#b19">(Pulkit, João, and Jitendra 2015;</ref><ref type="bibr" target="#b4">Diederik and Max 2014;</ref><ref type="bibr" target="#b5">Dinesh and Kristen 2017;</ref><ref type="bibr" target="#b12">Jimmy et al. 2016;</ref><ref type="bibr" target="#b20">Relja and Andrew 2017)</ref>. In a special view of visual representation learning, proxy tasks can be categorized into: (1) Image property transform and (2) Video content transform.</p><p>Image Property Transform Spatial transforms applied on images can produce supervision signals for representation learning <ref type="bibr" target="#b16">(Larsson, Maire, and Shakhnarovich 2017)</ref>. As a representative research, <ref type="bibr" target="#b8">(Gidaris, Singh, and Komodakis 2018)</ref> proposed learning CNN features by rotating the images and predicting the rotated angles. <ref type="bibr" target="#b14">(Kim et al. 2018;</ref><ref type="bibr" target="#b6">Doersch, Gupta, and Efros 2015)</ref> proposed learning image representations by completing damaged Jigsaw puzzles. <ref type="bibr" target="#b2">(Deepak et al. 2016)</ref> proposed context inpainting, by training a CNN to generate the contents of a withheld image region conditioned on its surroundings. <ref type="bibr" target="#b26">(Xiaolong, Kaiming, and Abhinav 2017)</ref> proposed unsupervised correspondence, by training a representation model to match image patches of transform in-variance.</p><p>Video Content Transform A large number of video clips with rich motion information provide various selfsupervised signals. In <ref type="bibr" target="#b25">(Xiaolong and Abhinav 2015)</ref>, the order of video frames was used as a supervisory signal. In <ref type="bibr" target="#b18">(Misra, Zitnick, and Hebert 2016;</ref><ref type="bibr" target="#b17">Lee et al. 2017)</ref>, predicting the orders of frames or video clips drives learning spatiotemporal representation. In <ref type="bibr" target="#b7">(Fernando et al. 2017)</ref>, an oddone-out network was proposed to identify the unrelated or odd video clips from a set of otherwise related clips. To find the odd video clip, the models have to learn spatio-temporal features that can discriminate similar video clips. In <ref type="bibr" target="#b3">(Deepak et al. 2017)</ref>, unsupervised motion-based segmentation on videos was used to obtain segments, which performed as pseudo ground truth to train a CNN to segment objects. Early works usually learned features based on 2D CNN and merely on a frame-by-frame basis, which are implausible to video analytic tasks where spatio-temporal features are prevailing. Recently, <ref type="bibr" target="#b24">(Wang et al. 2019)</ref> proposed learning 3D representations by regressing motion and appearance statistics, <ref type="bibr" target="#b27">(Xu et al. 2019)</ref> proposed predicting the order of video clips. <ref type="bibr" target="#b15">(Kim, Cho, and Kweon 2019)</ref> proposed training 3D CNN by completing space-time cubic puzzles.</p><p>However, existing self-supervised learning methods are typically designed for specific target tasks, which restricts the capability of learned models. In addition, few of the proxy tasks are capable of assessing feature representation, which strikingly limits the pertinence of learned models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Target Tasks</head><p>In this work, the self-supervised representation models are applied to target tasks including video action recognition and video retrieval. In many recent works, <ref type="bibr" target="#b23">(Tran et al. 2018;</ref> investigated training 3D CNN models on a large scale supervised video database. Nevertheless, the models trained on specific self-supervised tasks lack general applicability, i.e., fine-tuning such models to various video tasks could produce sub-optimal results. To conquer these issues, we propose the novel VCP, which, by incorporating multiple self-supervised representations, improves the generality of the learned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blank Generation</head><formula xml:id="formula_0">Spatial Rotation (! " ) Temporal Remote Shuffling(# " ) Raw Video</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Option Creation</head><p>Option 3D ConvNets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D ConvNets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D ConvNets</head><p>Concat Fully Connected Layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cloze Completion</head><p>Original ($)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample and Delete</head><p>Spatial Permutation <ref type="figure">Figure 2</ref>: Illustration of the VCP framework. Given a video sequence, a sampled video clip is withheld and multiple spatiotemporal operations are applied on the withheld clip (up). A 3D-CNN model is applied to identify the category of operations, which drives learning rich feature representations. The motivation behinds VCP lies in that applying richer operations on the video clips facilities exploring richer feature representation (down).</p><formula xml:id="formula_1">(! % ) Temporal Adjacent Shuffling(# &amp; ) $ ! " ! % # " # &amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Video Cloze Procedure</head><p>Cloze Procedure was firstly introduced by Wilson Taylor in 1953 as a metric to evaluate the capability of human language learning. Specifically, it deletes words in a prose selection according to a word-count formula or various other criteria and evaluates the success a reader has in accurately supplying the deleted words <ref type="bibr" target="#b0">(Bickley, Ellington, and Bickley 1970</ref>). Motivated by the success of Cloze Procedure in the field of language learning, we design the Video Cloze Procedure.</p><p>In this section, we first describe the details of VCP which consists of three components, i.e., blank generation, option creation, and cloze completion. We then discuss the advantages of the VCP over state-of-the-art methods in three aspects, including complexity, flexibility, and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Blank Generation</head><p>Considering the spatial similarity and the temporal ambiguity among video frames, we take video clips <ref type="bibr" target="#b27">(Xu et al. 2019)</ref> as the smallest unit in VCP. Considering that semantic information of different videos is temporally non-uniform, we generate the blanks in VCP using every-nth-words manner <ref type="bibr" target="#b0">(Bickley, Ellington, and Bickley 1970)</ref>. Specifically, the blank generation component consists of two steps including clip sampling and clip deletion.</p><p>Clip Sampling The clips including k frames (with equal length) are sampled every l frames (with equal interval without overlap) from the raw video. In this way, the relevance of the low-level vision cues, such as texture and color, among clips is weakened compared to those in successive or overlapped clips. As a result, the learner is forced to focus on middle-and high-level spatio-temporal features.</p><p>Clip Deletion A video sequence of m successive clips is considered as a whole cloze item. We randomly delete one of the co-equal clips with the same probability in the cloze item to generate blanks. The removed clip is then utilized to create options. For clarity of description, we give an example of VCP by sampling three clips and deleting the middle one, as shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Option Creation</head><p>Aiming at training a model to distinguish the deleted clip from a heap of perplexing optional clips, we design spatial and temporal operations to create the optional clips (options). To learn richer representations, the operations should effectively confuse the learners, while reserving the spatialtemporal relevance. Under this principle, we design four operations including spatial rotation (S R ), spatial permutation (S P ), temporal remote shuffling (T R ), and temporal adjacent shuffling (T A ) for VCP.</p><p>Spatial Operation To provide options that focus on spatial representation learning, we introduce spatial rotation and spatial permutation. With spatial rotation (S R ), a video clip is rotated by 90, 180, and 270 degrees so that the model is forced to learn orientation related features. With spatial permutation (S P ), a video clip is divided into four tiles (2×2×1grids) and either two tiles are permuted to produce a new option. There are C 2 4 = 6 kinds of options produced . VCP can also act as a target task for model assessment, which can be used to evaluate self-supervised representation models.</p><p>in total. Permutation with two tiles produces options with spatial structure information partially remained, which prevents models from learning low-level statistics to distinguish spatial chaos.</p><p>Temporal Operation To provide options that focus on temporal features we further introduce two kinds of temporal operations. One operation is temporal remote shuffling (T R ), where the deleted clip is substituted with a clip that has large temporal distance forward or backward. As the background of frames with reasonable temporal distance is probably similar which means the discriminative difference lies in the foreground, T R drives the model to learn more temporal information related to the foreground. The other operation is temporal adjacent shuffling (T A ), where the original clip is divided into four sub-clips, and two of them are randomly shuffled once. Different from VCOP <ref type="bibr" target="#b27">(Xu et al. 2019</ref>), we do not shuffle all the sub-clips and reduce the difficulty by forcing the model to judge whether or not the clip is shuffled instead of predicting the exact orders. In this way, rich temporal representation can be easy to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cloze Completion</head><p>To complete cloze, we fill the blanks by randomly sampling the clip options with operation category labels. To predict the operation categories applied on the clips, we use three 3D CNNs as the backbones and concatenate their output features according to the order of the clips in the raw video as illustrated in <ref type="figure">Fig. 2</ref>. The three CNNs share parameters so that a single strong model can be learned. The concatenate feature is fed to a fully connected (FC) layer, which predicts the corresponding operation category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Self-supervised Representation Learning</head><p>We implement self-supervised representation learning and model assessment by treating VCP as a proxy task and a target task, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation Learning</head><p>As a proxy task, VCP can learn spatio-temporal representations using only the original labeled data for target tasks or using extra unlabeled data, <ref type="figure" target="#fig_0">Fig. 3</ref>. For the target task, deep models learn to extract features in a direct manner trying to minimize the training loss with the supervision of specific annotations, i.e., category labels. During the procedure, the task-specific representation capability of models can be enforced while the general representation capacity of models is unfortunately ignored. With spatio-temporal operations applied on the clips, VCP learns rich and general representations by pre-training the models, which enhances the performance of target tasks without extra labeling efforts required.</p><p>On the other hand, VCP can leverage massive unlabeled data to break the overhead of model representation capability. With VCP, we pre-train a representation model on an un-annotated dataset as a warm-up initialization and then fine-tune such model on the annotated target dataset. VCP has the potential to learn the general representation, e.g., spatial-temporal integrity and continuity, in spatio-temporal domain, which facilitates improving the representation capability of models in video-based vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Assessment</head><p>Beyond acting as a proxy task, VCP can also act as a target task, which offers a uniform and interpretable way to evaluate self-supervised representation models. In VCP, the classification accuracy of operations reflects what the models learn and how good they are. By simply replacing the head of the classification network with a fully connected layer to be fine-tuned while the parameters of the backbone network are fixed, operation category classification is implemented as a target task, <ref type="figure" target="#fig_0">Fig. 3</ref>.</p><p>In this way, the feature representative capability obtained from self-supervised proxy tasks is reserved. Meanwhile, corresponding features are utilized to train a classifier, the performance of which can be regarded as a metric to assess the representation models. With the hint dropped by VCP, we can not only elaborately assess models learned from different self-supervised proxy tasks but also can figure out how to improve a self-supervised method. This casts a new light on the significance of VCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>To analyze the advantages of VCP over existing selfsupervised methods, we contrast them from three aspects including complexity, flexibility, and interpretability.</p><p>Complexity Existing approaches that use spatio-temporal shuffling and order prediction <ref type="bibr" target="#b15">(Kim, Cho, and Kweon 2019;</ref><ref type="bibr" target="#b27">Xu et al. 2019;</ref><ref type="bibr" target="#b17">Lee et al. 2017)</ref> have O(n!) computational complexity, given n video frames/clips units. The high complexity is caused by the requirement to predict the exact order, which might be not necessary when learning representations. In contrast, VCP solely chooses n optional options to fill the blanks while predicting the operation category of the option. It thus has a O(n) computational complexity.</p><p>Flexibility For various target tasks, VCP can be adaptively applied by configuring the options (operations). For example, we can apply spatial permutation (S P ) to enhance spatial representation and apply temporal adjacent shuffling (T A ) to boost the temporal representation. In a flexible manner, VCP can incorporate special information in special spatial and/or temporal operations for different target tasks.</p><p>Interpretability In existing approaches, different proxy tasks learn different representation models. It requires an interpretive way to explore the relationship between representation models and target tasks. With well-designed options, VCP offers the opportunity to analyze the models by testing their classification accuracy on uniform options (operations), which has great potential to contrapuntally overcome the weakness of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct extensive experiments to evaluate VCP and its applications on target tasks. Firstly, we elaborate experimental settings for VCP. We then evaluate the representation learning of VCP with different option configurations and data strategies. We further conduct experiments on model assessment with VCP. Finally, we evaluate the performance of VCP applying on target tasks, i.e., action recognition and video retrieval, and compare it with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setting</head><p>Datasets The experiments are conducted on UCF101 <ref type="bibr" target="#b21">(Soomro, Zamir, and Shah 2012)</ref> and HMDB51 <ref type="bibr" target="#b10">(Jhuang et al. 2011)</ref> datasets. UCF101 contains 13320 videos over 101 action categories, exhibiting challenging problems include intra-class variance of actions, complex camera motions, and cluttered backgrounds. HMDB51 contains 6849 videos over 51 action categories. The videos are mainly collected from movies and websites including the Prelinger archive, YouTube, and Google videos.</p><p>Backbone Networks C3D <ref type="bibr" target="#b22">(Tran et al. 2015)</ref>, R3D and R(2+1)D <ref type="bibr" target="#b23">(Tran et al. 2018</ref>) are employed as backbones in VCP implementations. C3D extends the 2D convolution kernels to 3D kernels, so that it can model temporal information of videos. The size of convolution kernels is 3 × 3 × 3. R3D is an extension of ResNet <ref type="bibr" target="#b9">(He et al. 2016</ref>) with C3D. In R(2+1)D, 3D convolution kernels are decomposed. For spatial convolution, each kernel is set to be 1 × n × n where n = 3. For temporal convolution, it is set to be m × 1 × 1 where m = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>In the blank generation, to avoid trivial results, three successive 16-frame clips are sampled every 8 frames from the raw video as a whole cloze item. Each frame is resized to 128 × 171 and randomly cropped to 112 × 112. In the option generation, we define the clips sampled from 16 frames away to the cloze item as remote clips. We set the initial learning rate to be 0.01, momentum to be 0.9 and stop training after 300 epochs.  <ref type="table">Table 1</ref>: Accuracy of operation classification. "O" denotes original video clips, "S R " the spatial rotation, "S P " the spatial permutation , "T R " the temporal remote shuffling, and "T A " the temporal adjacent shuffling . </p><formula xml:id="formula_2">Method UCF101(%) random 62.0 S R -VCP 64.3 S P -VCP 63.4 S R,P -VCP 66.0 T R -VCP 67.8 T A -VCP 65.0 T R,A -VCP 68.0 S R,P T R,A -VCP 69.7</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Representation Learning</head><p>To validate what VCP learns, we first conduct ablation studies of VCP. We further conduct experiments with different data strategies to demonstrate the generality of the representations learned via VCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We firstly train a model to classify the categories of five options. <ref type="table">Table 1</ref> shows the results on UCF101, which are trained and evaluated on the first split. It can be seen that VCP achieves 78.42% overall accuracy, For spatial rotation (S R ), spatial permutation (S p ), and temporal adjacent shuffling (T A ), VCP respectively achieves 95.04%, 97.53% and 94.57% accuracy. The results show that the designed five operations are plausible.</p><p>To clearly show the effect of option creation for representation learning, we conduct ablation experiments on VCP with various options for action recognition, <ref type="table" target="#tab_1">Table 2</ref>. The experiments are conducted using C3D as the backbone. We pre-train VCP and then fine-tune the action recognition model on UCF101. The recognition accuracy is evaluated on the first test split.</p><p>It can be seen that when pre-training with a single spatial rotation (S R -VCP) or permutation (S P -VCP) operation, the accuracy of action recognition outperforms the baseline (random) by 2.3% or 1.4%. When using both spatial operations (S R,P -VCP), the performance further increased to 66.0%. Pre-training with a single temporal remote shuffling (T R -VCP) or adjacent shuffling (T A -VCP) operation improves the performance by 5.8% or 3.0%, where the performance is further improved to 68.0% when using both temporal operations (T R,A -VCP). Combining the spatial and temporal operations (S R,P T R,A -VCP) finally improves the performance to 69.7%, significantly outperforming the baseline by 7.7%. The experiments show that the options can be used in a flexible way including using standalone or combining with each other. VCP can learn more representative features by adding rich and complementary options.  <ref type="bibr" target="#b27">(Xu et al. 2019</ref>) and 3D Cubic puzzle <ref type="bibr" target="#b15">(Kim, Cho, and Kweon 2019)</ref>. "S-Puzzle" denotes spatial permutation, and "T-Puzzle" temporal permutation, "ST-Puzzle" spatial and temporal permutation.  Data Strategy To further validate the generality of VCP, we conduct experiments for VCP under different data strategies, with C3D as the backbone. Firstly, we pre-train VCP on UCF101 and HMDB51, and then respectively fine-tune each pre-trained model on UCF101 and HMDB51 for action recognition, <ref type="table" target="#tab_3">Table 3</ref>. Specially, the supervised action recognition task is directly trained on the target datasets, with random initialization. It can be seen that when pre-training and fine-tuning on UCF101, VCP outperforms VCOP by 2.9%; when pretraining and fine-tuning on HMDB51, VCP slightly outperforms VCOP, showing that the strategy used in VCP is better than that in VCOP. Note that using VCP as a pre-train model further significantly improves the performance of supervised methods by 6.7% (68.5% vs. 61.8%) on UCF101 and 6.8% (31.5% vs. 24.7%) on HMDB51, which shows that VCP is complementary to supervised model learning. After pre-training on UCF101 and fine-tuning models on HMDB51, VCP significantly outperforms VCOP by 4.1%. It is noteworthy that when pre-training on the smaller dataset HMDB51 but fine-tuning on the larger dataset UCF101, the performance of VCP also outperforms that of VCOP by 2.6%, which shows the generality of VCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Assessment</head><p>Regarding VCP as a target task, we only fine-tune the fully connected layer with the parameters of the self-supervised model fixed to get the operation classification accuracy curve, <ref type="figure" target="#fig_1">Fig. 4</ref>. We fine-tune the fully connected layer for 30 epochs and then output the test scores every 5 epochs.</p><p>It is obvious that the model trained with VCP can recognize the S R , S P , and T A operations with high accuracy (∼90%), <ref type="figure" target="#fig_1">Fig. 4</ref>(b)(c)(e). Nevertheless, it experiences diffi-Method UCF101(%) HMDB51(%) Jigsaw <ref type="bibr" target="#b19">(Noroozi and Favaro 2016)</ref> 51.5 22.5 OPN <ref type="bibr" target="#b17">(Lee et al. 2017)</ref> 56.3 22.1 Büchler <ref type="bibr" target="#b1">(Buchler, Brattoli, and Ommer 2018)</ref> 58.6 25.0 Mas <ref type="bibr" target="#b24">(Wang et al. 2019)</ref> 58.8 32.6 3D ST-puzzle <ref type="bibr" target="#b15">(Kim, Cho, and Kweon 2019)</ref> 65  culty when classifying the original clips and the remote shuffled clips, <ref type="figure" target="#fig_1">Fig. 4(a)(d)</ref>. It can be seen that the accuracy of O and T R is negatively correlated, which means the perplexity of them. In contrast, the accuracy of VCOP and 3D Cubic Puzzle is divergent, which implies they fail to classify the two categories. For spatial operation classification, <ref type="figure" target="#fig_1">Fig. 4(b)</ref>(c), ST-Puzzle and S-Puzzle outperform T-Puzze and VCOP, while for temporal operation classification, <ref type="figure" target="#fig_1">Fig. 4(d)</ref>(e), they underperform T-Puzze and VCOP. It shows that spatial representation learning is not consistent with temporal representation learning. Consequently, VCP benefits from integrating existing and newly designed spatial and temporal operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Action Recognition</head><p>Once a 3D CNN is pre-trained by VCP, we use it to initialize and fine-tune models for action recognition. For action recognition, we feed the features extracted by backbones to fully-connected layers for classification. During fine-tuning, we initialize the backbones from VCP while the fully-connected layers are randomly initialized. The hyperparameters and data pre-processing are the same as VCP training process. The fine-tune procedures are carried out for 150 epochs. During test, we follow the protocol of <ref type="bibr" target="#b23">(Tran et al. 2018</ref>) and sample 10 clips for each video. The predictions on the clips are averaged to obtain the video prediction.</p><p>The classification accuracy over 3 splits are averaged to obtain the final accuracy. As shown in <ref type="table" target="#tab_5">Table 4</ref>, with a C3D backbone, VCP (ours) outperforms the randomly initialized  C3D (random) by 6.7% and 7.8% on UCF101 and HMDB51 respectively. It also outperforms the state-of-the-art VCOP approach <ref type="bibr" target="#b27">(Xu et al. 2019</ref>) by 2.9% and 4.1%. With an R3D backbone, VCP has 11.5% (54.5% vs. 66%) and 9.8% (32.5% vs. 27.4%) performance gain over the random initialization (random) approach. It also outperforms the stateof-the-art VCOP <ref type="bibr" target="#b27">(Xu et al. 2019</ref>) with significant margins. The good performance validates that VCP can learn richer and more discriminative features than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Video Retrieval</head><p>VCP is also validated on the target task of nearest-neighbor video retrieval. As it does not require training data annotation, it largely relies on the pre-trained representation models. We conduct this experiment with the first split of UCF101, following the protocol in <ref type="bibr" target="#b27">(Xu et al. 2019)</ref>. The model trained by VCP is to used to extract convolutional (conv5) features for all samples (videos) in the training and test sets. Each video in the test set is used to query k nearest videos from the training set. If a video of the same category is matched, a correct retrieval is counted. It can be seen in <ref type="table" target="#tab_7">Table 5</ref> and 6 that VCP significantly outperforms the compared approaches on all evaluation metrics, i.e., top-1, top-5, top-10, top-20, and top-50 accuracy. In <ref type="figure">Fig. 5</ref>, qualitative results also shows superiority of VCP.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel self-supervised method, referred to as Video Cloze Procedure (VCP), to learn rich spatial-temporal representations. With VCP, we train spatialtemporal representation models (3D-CNNs) and apply such models on action recognition and video retrieval tasks. We also proposed a model assessment approach by designing VCP as a special target task, which improves the pertinence of self-supervised representation learning. Experimental results validated that VCP enhanced the representation capability and the interpretability of self-supervised models. The underlying fact is that VCP simulates the fashion of human language learning, which provides a fresh insight for selfsupervised learning tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>VCP can be utilized for representation learning with only the original labeled data (dataset L) for target tasks or using extra unlabeled data (dataset U)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Model assessment results of VCOP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of spatio-temporal operations. The figures refer to action recognition accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Performance (average on all test splits) comparison</cell></row><row><cell>under different data strategies. UCF101 (HMDB51) den-</cell></row><row><cell>toes the model is pre-trained on HMDB51 and fine-tuned</cell></row><row><cell>on UCF101.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of action recognition accuracy on UCF101 and HMDB51.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Video retrieval performance on UCF101.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Video retrieval performance on HMDB51.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is supported by the National Key R&amp;D Program of China (2017YFB1002400) and the Strategic Priority Research Program of Chinese Academy of Sciences (XDC02000000)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The cloze procedure: A conspectus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bickley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Ellington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Bickley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Reading Behavior</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="249" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving spatiotemporal self-supervision by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="770" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6024" to="6033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning image representations tied to egomotion from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="136" to="161" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-oneout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hmdb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li-Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look, listen and learn-A multimodal LSTM for speaker identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J R</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yongtao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu-Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wenxiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3581" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning image representations by completing damaged jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8545" to="8552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pulkit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jitendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Relja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4006" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiaolong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiaolong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abhinav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1338" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10334" to="10343" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
