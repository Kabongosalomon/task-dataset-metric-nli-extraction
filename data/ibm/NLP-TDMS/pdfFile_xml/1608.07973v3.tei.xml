<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linking Image and Text with 2-Way Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<email>wolf@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">The Blavatnik School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Linking Image and Text with 2-Way Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Linking two data sources is a basic building block in numerous computer vision problems. Canonical Correlation Analysis (CCA) achieves this by utilizing a linear optimizer in order to maximize the correlation between the two views. Recent work makes use of non-linear models, including deep learning techniques, that optimize the CCA loss in some feature space. In this paper, we introduce a novel, bi-directional neural network architecture for the task of matching vectors from two data sources. Our approach employs two tied neural network channels that project the two views into a common, maximally correlated space using the Euclidean loss. We show a direct link between the correlation-based loss and Euclidean loss, enabling the use of Euclidean loss for correlation maximization. To overcome common Euclidean regression optimization problems, we modify well-known techniques to our problem, including batch normalization and dropout. We show state of the art results on a number of computer vision matching tasks including MNIST image matching and sentence-image matching on the Flickr8k, Flickr30k and COCO datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computer vision emerged from its roots in image processing when researchers began to seek an understanding of the scene behind the image. Linking visual data X with an external data source Y is, therefore, the defining task of computer vision. When applying machine learning tools to solve such tasks, we often consider the outside source Y to be univariate, e.g., in image classification. A more general scenario is the one in which Y is also multidimensional. Examples of such view to view linking include matching between video and concurrent audio, matching an image with its textual description, matching images from two fixed views, etc.</p><p>The classical method of matching vectors between two different domains is Canonical Correlation Analysis (CCA). The algorithm has been generalized in many ways: regular-ization was added <ref type="bibr" target="#b29">[30]</ref>, kernels were introduced [2, <ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref>, versions for more than two sources were developed <ref type="bibr" target="#b41">[42]</ref> etc. Recently, with the advent of deep learning methods, deep versions were created and showed promise.</p><p>The current deep CCA methods optimize the CCA loss on top of a deep neural network architecture. In this work, an alternative is presented in which a network is built to map one source X to another source Y and back. This architecture, which bears similarities to the encoder-decoder framework <ref type="bibr" target="#b11">[12]</ref>, employs the Euclidean loss.</p><p>The Euclidean loss is hard to optimize for, when compared to classification losses such as the cross entropy loss. We, therefore, introduce a number of contributions that are critical to the success of our methods. These include: (i) a mid-way loss term that helps support the training of the hidden layers; (ii) a decorrelation regularization term that links the problem back to CCA; (iii) modified batch normalization layers; (iv) a regularization of the scale parameter that ensures that the variance does not diminish from one layer to the next; (v) a tied dropout method; and (vi) a method for dealing with high-dimensional data.</p><p>Taken together, we are able to present a general and robust method. In an extensive set of experiments, we present clear advantages over both the classical and recent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b14">[15]</ref> is a statistical method for computing a linear projection for two views into a common space which maximizes their correlation. CCA plays a crucial role in many computer vision applications including multiview analysis <ref type="bibr" target="#b0">[1]</ref>, multimodal human behavior analysis <ref type="bibr" target="#b39">[40]</ref>, action recognition <ref type="bibr" target="#b16">[17]</ref>, and linking text with images <ref type="bibr" target="#b18">[19]</ref>. There are a large number of CCA variants including: regularized CCA <ref type="bibr" target="#b44">[45]</ref>, Nonparametric canonical correlation analysis (NCCA) <ref type="bibr" target="#b31">[32]</ref>, and Kernel canonical correlation analysis (KCCA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref>, a method for producing non-linear, non-parametric projections using the kernel trick. Recently, randomized non-linear component analysis (RCCA) <ref type="bibr" target="#b32">[33]</ref> emerged as a low-rank approximation of KCCA.</p><p>While CCA is restricted to linear projections, KCCA is restricted to a fixed kernel. Both methods do not scale well with the size of the dataset and the size of the representations. A number of methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref> based on Deep Learning were recently proposed that aim to overcome these drawbacks. Deep canonical correlation analysis <ref type="bibr" target="#b3">[4]</ref> processes the pairs of inputs through two network pipelines and compares the results of each pipeline via the CCA loss.</p><p>[49] and <ref type="bibr" target="#b45">[46]</ref> extend <ref type="bibr" target="#b3">[4]</ref> to the task of images and text matching. The first employs the same model and training process of <ref type="bibr" target="#b3">[4]</ref> while the latter employs a different training scheme on the same architecture. Unlike <ref type="bibr" target="#b48">[49]</ref> and <ref type="bibr" target="#b45">[46]</ref> we present a novel deep model for matching images and text.</p><p>Other deep CCA methods, including ours, are inspired by a family of encoding/decoding unsupervised generative models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> that aim to capture a meaningful representation of input x by applying a non-linear encoding function E(x), decoding the encoded signal using a nonlinear decoding function D(x) and minimizing the squared L2 distance between the original input and the decoded output. Some of the auto-encoder based algorithms incorporate a noise on the input <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> or enforce a desired property using a regularization term <ref type="bibr" target="#b28">[29]</ref>.</p><p>Correlation Networks (CorrNet) <ref type="bibr" target="#b6">[7]</ref> and Deep canonically correlated autoencoders (DCCAE) <ref type="bibr" target="#b46">[47]</ref> expand the auto-encoder scheme by considering two input views and two output views. The encoding is shared between the two views (CorrNet) or the differences in the encodings are minimized (DCCAE). In both cases, it serves as a common bottleneck. Our model goes from one view to the other (in both directions) and not from each view to a reconstructed view.</p><p>The CCA loss is used by both CorrNet and DCCAE. The latter contribution explicitly states that the L2 loss is inferior to the CCA loss term <ref type="bibr" target="#b46">[47]</ref>. Our network, however, uses L2 successfully. This reinforces the need to apply the methods we propose in this work in order to enable effective training based on the L2 loss. For this end, we introduce innovative techniques based on common practices in deep learning, adapted to the problem at hand. These techniques include: dropout, batch normalization, and leaky ReLUs. While the latter is applied as is, the former two need to be carefully modified for our networks.</p><p>Dropout <ref type="bibr" target="#b40">[41]</ref> is a regularization method developed to reduce over-fitting in deep neural networks by zeroing a group of neurons at each training iteration. This stochastic elimination reduces the co-adaptation between neurons in the same layer and simulates the training of an ensemble of networks with shared weights.</p><p>Batch Normalization <ref type="bibr" target="#b37">[38]</ref> is used as a stabilizing mechanism for training a neural network by scaling the output of a hidden layer to zero norm and unit variance. This scaling lowers the change of distribution between neurons throughout the network and helps to speed up the training process.</p><p>Rectified Linear Unit (ReLU) <ref type="bibr" target="#b33">[34]</ref> is a non-linear activation function that does not suffer from the saturation phenomenon, which the classical sigmoids suffer from. Conventional ReLU zero negative activations, and as a result, no gradient is produced for many of the neurons. A few variants of ReLU were, therefore, proposed <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10]</ref> that reduce the effect of negative activations, but do not zero them completely. Similar to <ref type="bibr" target="#b26">[27]</ref> and unlike <ref type="bibr" target="#b9">[10]</ref>, we do not train the leakiness parameter and instead set it to a constant value.</p><p>As one of our contributions, we add a regularization term that removes the pairwise covariances of the learned features. A similar term was recently reported in work <ref type="bibr" target="#b7">[8]</ref> as part of a classification system (unrelated to modeling correlations between vectors). We adapt their terminology when describing our bi-directional term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Network Model</head><p>This section contains a detailed description of our proposed model, which we term the 2-way net 1 . The model utilizes the L2 loss in order to create a bi-directional mapping between two vector spaces. The absence of a correlation based loss (such as in DeepCCA <ref type="bibr" target="#b3">[4]</ref> and CorrNet <ref type="bibr" target="#b6">[7]</ref>) makes this model simpler. Like other regression problems, there are inherent challenges in obtaining meaningful solutions <ref type="bibr" target="#b8">[9]</ref>. These challenges are further amplified by the multivariate and layered structure of the performed regression. We, therefore, modify the problem in various ways, each contributing to the overall success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Architecture</head><p>Our proposed architecture is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. It contains two reconstruction channels. Both channels contains k hidden layers {h 1 , h 2 , ..., h k } and {ĥ 1 ,ĥ 2 , ...,ĥ k }. Lets define H i (x) andĤ i (y) as the output of each channel at layer i given network inputs x and y respectively, the model is optimized to minimize the Eucledean loss between botĥ H i (y) and x, and H i (x) and y. The two channels share weights and dropout function as explained in 3.5</p><p>The activations of each hidden layer are computed by a function h(</p><formula xml:id="formula_0">x) = Φ (W x + b 2 ) from R d1 to R d2 , where W ∈ R d2×d1 is the weight matrix, b 2 ∈ R d2</formula><p>is the bias vector and Φ is a non-linear function, which in our model is a leaky rectified linear unit <ref type="bibr" target="#b26">[27]</ref>. The tied layer is given aŝ h(y) = Φ W T y + b 1 , and employs the transpose of the matrix W and an untied bias term b 1 ∈ R d1 .</p><p>Given a pair of views x ∈ R dx and y ∈ R dy , two reconstructions are created:x ∈ R dx andỹ ∈ R dy by employing the two networks Loss is measured between x andx and y andỹ. Moreover, the Euclidean distance is also minimized directly on the desired representations. In order to do so, we select a mid-network position j = k/2 . We then add a loss term by considering the two networks:</p><formula xml:id="formula_1">H = h 1 • h 2 • ... • h k and H =ĥ k •ĥ x k−1 • ... •ĥ 1 , asx =Ĥ(y) andỹ = H(x).</formula><formula xml:id="formula_2">H j = h 1 • h 2 • ... • h j , andĤ j =ĥ k •ĥ x k−1 • ... •ĥ j+1 .</formula><p>A loss term is then added that compares H j (x) andĤ j (y).</p><p>The overall loss (sans regularization terms) is given by the three terms L x = x −x 2 , L y = y −ỹ 2 , and L h = H j (x) −Ĥ j (y) 2 . Note that minimizing Euclidean distances differs from maximizing the pairwise correlations as is done in CCA and its variants DeepCCA <ref type="bibr" target="#b3">[4]</ref> and RCCA <ref type="bibr" target="#b32">[33]</ref>.</p><p>In our experiments, in order to compare with previous work, we use the correlation as the success metric. As the Lemma below shows, there is a connection between the correlation of two vectors and their Euclidean distance, this connection also depends on the variance of the vectors. Lemma 1. Let x ∈ R n and y ∈ R n denote two paired lists of n matching samples from two random variables with zero mean and σ 2</p><p>x and σ 2 y variances. Then, the correlation between the two n dimensional samples x and y equals σx 2σy + σy 2σx − x−y 2 2nσxσy . Proof. Given two n-dimensional vectors x and y we consider the squared Euclidean distance</p><formula xml:id="formula_3">x − y 2 = n j=1 (x 2 j ) + n j=1 (y 2 j ) − 2 n j=1 (x j y j ) Thus: n j=1 (x j y j ) = nσ 2 x 2 + nσ 2 y 2 − x − y 2 2<label>(1)</label></formula><p>For zero mean variables, the correlation between x and y is given by c = 1 n n j=1 (xj yj ) σxσy</p><p>. Combining with 1 results in what had to be proven.</p><p>Given a batch of samples from views x and y, we measure the correlation between the outputs of two matching layers, {h j (x 1 ), ..., h j (x n )} and {ĥ j (y i ), ...,ĥ j (y n )} as the sum of correlations between the activations of each matching neuron. The Lemma below extends Lemma 1 and shows that the sum of correlations which we aim to maximize is bounded by a function of the Euclidean loss between the two representations. Lemma 2. Given two matching hidden layers, h j andĥ j with m neurons each. a k is the activation vector of neuron k from h j with standard deviation σ a k and b k is the activation vector of neuron k fromĥ j with standard deviation σ b k . Each vector is produced by feeding a batch of samples of size n from views x and y through channels H andĤ respectively. The sum of correlations C is bounded by:</p><formula xml:id="formula_4">m k=1 C k ≥ 1 2 m k=1 ( σ 2 a k + σ 2 b k σ a k σ b k ) − 1 2n m k=1 a k − b k 2 m k=1 σ −1 a k σ −1 b k<label>(2)</label></formula><p>Proof. From lemma 1, we get:</p><formula xml:id="formula_5">m k=1 C k = 1 2 m k=1 ( σ 2 a k + σ 2 b k σ a k σ b k ) − 1 2n m k=1 ( a k − b k 2 σ a k σ b k ) (3) We will define G m = m k=1 a k − b k 2 and f k = σ −1 a k σ −1 b k . Using Abel transform: m k=1 a k − b k 2 σ a k σ b k = f m G m − m−1 k=1 G k f k+1 + m−1 k=1 G k f k ≤ f m G m + m−1 k=1 G k f k ≤ f m G m + G m m−1 k=1 f k = G m m k=1 f k = m k=1 a k − b k 2 m k=1 σ −1 a k σ −1 b k<label>(4)</label></formula><p>Note that both σ a k σ b k and a k − b k 2 are positive for all k which makes the above inequalities valid. Inserting 4 in 3 results in what had to be proven.</p><p>From the above Lemma, we can conclude that by minimizing the L2 loss together with maximizing the variance of each neuron activation will result in maximization of the sum of correlations.</p><p>Solving this regression problem tends to eliminate the variance of the output representations. To overcome this limitation, we add two instruments. The first is batch normalization layer <ref type="bibr" target="#b37">[38]</ref> (BN) after each hidden layer. The settings of the batch normalization layer differ from the common settings to adapt to this model. Another instrument is regularizing the gamma parameter the batch normalization layer introduces. More details can be found below.</p><p>To the loss term, we add regularization terms. The first is weight decay R w = W 2 . A second regularization term is added in order to reduce the cross correlations between the network activations of the same layer. The property we encourage is inherent to CCAbased solutions where decorrelation is enforced. In our network solutions, we add a soft regularization term. During training, we consider the N samples of a single batch</p><formula xml:id="formula_6">{(x i , y i )} N i=1 and consider the set of mid-network activa- tions {(H j (x i ),Ĥ j (y i ))} N i=1 .</formula><p>The decorrelation regularization term is given by:</p><formula xml:id="formula_7">R decov = 1 2 C h 2 F − diag (C h ) 2 2 + 1 2 Cĥ 2 F − diag Cĥ 2 2 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">C h = 1 N i H j (x i ) H j (x i ) is the covariance es- timator for H j (x) and Cĥ = 1 N iĤ j (y i ) Ĥ j (y i )</formula><p>is the covariance estimator forĤ j (y). This regularization term is minimized when the off-diagonal coefficients of both C h and Cĥ are zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Batch normalization layers</head><p>As shown above, in order to maximize the correlation we need not only to minimize the Euclidean loss but also to increase the variance of each neuron's output. This is done by introducing a batch normalization layer <ref type="bibr" target="#b37">[38]</ref> customized to meet the model's needs.</p><p>Given a vector of activations a = [a 1 , . . . , a d ] produced by one of the network's hidden layers for a given batch of inputs, we normalize a to produce a = [a 1 , . . . , a d ], where a k = a k −µ k σ k and µ k and σ 2 k are the mean and variance of neuron k on the given batch. This is followed by scaling and shifting by learned parameters to produce a k = γ k a k + β k . The BN layer mitigates the loss of variance by enforcing unit variance and by removing the influence of the weights of the hidden layer on the output's variance.</p><p>BN layers are usually placed before the non-linearity or on the input of the layer as a preprocessing phase as shown in <ref type="bibr" target="#b10">[11]</ref>. This setting poses several problems. First, ReLU lowers the variance of the output which is counterproductive to our goal. Second, applying ReLU after BN has the effect of zeroing every k when a k is below the mean in a given batch plus the term β k /γ k . Typically, β k is initialized to zero and for a symmetric activation distribution, half of the activations are zeroed. When employing a bi-directional network, the zeroing effect occurs in both directions.</p><p>In order to estimate the magnitude of this effect, let us assume that we have a process that at time i outputs two vectors u i = H j (x i ) and v i =Ĥ j (y i ), both in R d , which are the hidden representation at layer j for a pair of samples (x i , y i ). Denote by ρ k the correlation between the activations at neuron k.</p><p>Let s i = {k|u i (k) &gt; µ k } be the group of indices of the values in u i that are larger than their population mean. Let s i = {k|v i (k) &gt;μ k } be the equivalent for the vectors v i . We observe the intersection s i ∩ŝ i , which is the group of active neurons, following a threshold at the mean value on both u i and v i .</p><p>As the Lemma below shows, even if the correlation ρ k is relatively high, the size of the intersection set s i ∩ŝ i is closer to the value d/4 obtained for randomly permuted vectors than to the maximal value of d/2. Lemma 3. Assume that u i and v i are drawn from a multivariate normal distribution with zero mean and the identity covariance matrix, such that the correlation between</p><formula xml:id="formula_9">u i (k) and v i (k) for all k is ρ k = ρ. Then, E (|s i ∩ŝ i |) = d 1 4 + sin −1 ρ 2π .</formula><p>Proof. To estimate the size of c, let us look at the quadrant probability p of u i (k) and v i (k) which is given analytically by <ref type="bibr" target="#b2">[3]</ref>,</p><formula xml:id="formula_10">p = P (u i (k) &gt; 0, v i (k) &gt; 0) = 1 4 + sin −1 ρ 2π</formula><p>Given that the variables in u i (k) and v i (k) are drawn independently, the probability of P (|c| = t) has a binomial distribution with probability p, thus the mean of the size of c is equal to E(|c|) = dp = d 1</p><formula xml:id="formula_11">4 + sin −1 ρ 2π .</formula><p>Even in the case of a correlation as high as 0.6, the intersection will include only about 35% of the neurons. For neurons k not in this intersection, either both sides u i (k) and v i (k) are zero, meaning that no backpropagation occurs, or only one neuron is active, in which case only that side is updated and the update is a simple shrinking effect, since the loss is the magnitude of the activation.</p><p>In order to break this symmetry, we choose to employ the BN after the non-linearity. This allows the network to choose weights that result in mostly positive activations, which remain positive after the ReLU activation units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Highly leaky ReLU</head><p>Another method to prevent the harmful effects of zeroing is by using leaky ReLU as our non-linear function. Leaky ReLU was first introduced by <ref type="bibr" target="#b26">[27]</ref> in order to overcome the difficulties that arise from the elimination of the gradients from neurons with negative activation. In the 2-Way network, this effect is amplified, and we find leaky ReLU units to be extremely important. Formally, a leaky ReLU is defined as:</p><formula xml:id="formula_12">y i = x i if x ≥ 0 ax i if x &lt; 0</formula><p>where a &lt; 1 is the leakiness coefficient and is fixed during both training and testing. In all of our experiments, we use a leakiness coefficient of 0.3. This value was selected on the validation set of the Flickr8k experiment described in Section 4 and is used for all experiments.</p><p>Using leaky ReLU helps to reduce the effect discussed in Section 3.2 but does not replace the need for performing BN after the non-linearity. As Lemma 3 shows, more than half of the neurons will be multiplied by the leakiness coefficient while their matching neuron will not. This asymmetric scaling adds an artificial distance between the matching neurons, which, in turn, increases the L2 loss and reduces the training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Variance injection</head><p>Applying BN on the output of each hidden layer is not enough. The variance can still vanish during training. The problem is that the γ factor introduced by each BN layer can be arbitrary and can diminish during training, resulting in low variance. To encourage high variance, we introduce a novel regularization term of the form R γ = j,k (1/γ jk ) 2 , where γ jk is the scaling parameter for neuron k in layer j.</p><p>This regularization term is enough to force the network to avoid solutions with low variance and to seek more infor-mative output. This is demonstrated experimentally in the ablation study of Section 4.</p><p>The compound loss term we employ is of the form:</p><formula xml:id="formula_13">L = L x + L y + L h + λ w R w + λ decov R decov + λ γ R γ</formula><p>Where λ w ,λ decov , and λ γ are the regularization coefficients. While it seems that three regularization tradeoff hyperparameters would make selecting the parameter values difficult, the converse is true: in all of our varied set of experiments λ γ = λ w , and λ decov is either set to a very high value of 1/2 or, for small datasets, to 1/20 (see <ref type="bibr">Section 4)</ref>. Moreover, by adding these terms, the network is much less sensitive to the selection of λ w and allows us to learn with a much higher learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Tied dropout</head><p>Dropout <ref type="bibr" target="#b40">[41]</ref> is a form of regularization method that simulates the training of multiple networks with shared weights. Dropout zeros neurons by element-wise multiplying the output of a hidden layer consisting of d neurons for a batch of n samples with a random matrix B of size d × n. Each element of B is drawn independently from a Bernoulli distribution with a parameter p.</p><p>Since dropout eliminates random neurons, it prevents coadaptation of neurons, which is a desirable property for correlation analysis. However, using dropout, as is, in our proposed model is harmful. This is because the 2-Way network aims to enhance correlations between parallel layers h j and h j . The elimination of neurons independently in the hidden layers creates an artificial loss, even for a perfect matching.</p><p>Let p be the dropout parameter for layer j, assume that the same parameter is applied on both directions. In probability (1 − p) 2 , a pair of matching neurons is active on both sides and learning occurs with the true gradient. In probability p 2 , the pair of matching neurons is silent on both sides and no learning occurs. In probability 2p(1 − p), only one neuron is active resulting in a shrinking effect on the other neuron. Here, too, shrinking of activations is can be damaging since it might lead to a state of constant representation.</p><p>For a dropout probability of p = 0.5, half of the gradients would stem from a match which is silent on exactly one side, and the harmful effect is clearly seen in Section 4.</p><p>To overcome this problem, we introduce a tied dropout layer, in which the same random matrix B j is applied to pairs of matching hidden layers: h j andĥ j , j = 1..K. This sharing eliminates the artifacts introduced by the conventional dropout while preserving the benefits of the stochastic process and helps avoid over-fitting.</p><p>Using tied dropout layer changes the distribution of the activations. In order to match the distribution at test time, we incorporate a scaling factor at train time.</p><p>Assume that the activations of a single neuron are zerocentered. As discussed below, most post BN activations are almost exactly centered. In this case, the variance of the neuron activations is simply the sum of the squared activations. During training, only a ratio 1 − p of the activations contributes to the variance. Therefore, we divide the activations, at train time, by √ 1 − p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training high dimensional inputs</head><p>Some of the experiments shown below contain high dimensional data. High dimensional input directly increases the number of parameters and can cause over-fitting as well as an increase in training time and memory usage. To lower the number of parameters, we introduce a new type of layer we term locally dense layer. Such layer of size n is composed of m different dense layersh 1 , ...,h m of size n m each. Input x of size d x is divided into m different parts of size dx m and each part x i is connected into one of the dense layers h i . The outputs of all inner hidden layers are concatenated, thus producing the locally dense layer's output. To the output, we add a regular bias term b of size n. Using this layer reduces the number of parameters by a factor of m comparing to a conventional dense layer. In the experiments below, when dealing with high dimensional input, we use a locally dense layer with two inner dense layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first present a detailed analysis of the two datasets most commonly used in the literature for examining recent CCA variants: MNIST half matching and X-Ray Microbeam Speech data (XRMB). We then provide additional experiments on the problem of image to sentence matching, showing state of the art results on the Flickr8k, Flickr30k and COCO datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with published results</head><p>We follow the conventional way of evaluating the performance of CCA variants and compute the sum of the correlations of the top c shared (canonical) representation variables found. The datasets used for this comparison are MNIST and XRMB. In both MNIST and XRMB experiments, we set λ decov = λ W = λ γ = 0.05. For training, we used stochastic gradient descent with a learning rate of 0.0001 which was halved every 20 epochs. A momentum of 0.9 is used and a tied dropout probability of 0.5. MNIST half matching The MNIST handwritten digits dataset <ref type="bibr" target="#b19">[20]</ref> contains 60,000 images of handwritten digits for training and 10,000 images for testing. Each image is cut vertically into two halves, resulting in 392 features each. The goal is to maximize the correlation of the top c = 50 canonical variables. The model used is composed of three layers of size 392, 50 and 392 respectively, noted as 392-50-392. The middle layer was taken as the output. X-Ray Microbeam Speech data The XRMB <ref type="bibr" target="#b47">[48]</ref> dataset contains simultaneous acoustic and articulatory recordings.</p><p>The articulatory data is represented as a 112 dimensional vector. The acoustic data are the MFCCs <ref type="bibr" target="#b24">[25]</ref> for the same frames, yielding a 273 dimensional vector at each point in time. For benchmarking, 30,000 random samples are used for training, 10,000 for cross-validation and 10,000 for testing. The correlation is measured across the c = 112 top correlated canonical variables. The same training configuration of the MNIST experiment was used for the XRMB dataset. For XRMB, we tested our model using hidden layer configuration of 560-280-112-680-1365.</p><p>Tab. 1 contains correlation comparisons on the MNIST and XRMB datasets of six CCA variants besides our proposed method. As can be seen, our method ("2WayNet") outperforms all literature methods by a large margin on the XRMB dataset. On the MNIST dataset, in which the literature results are closer to the maximal value of 50, our method is able to regain half of the remaining correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MNIST XRMB Regularized CCA <ref type="bibr" target="#b44">[45]</ref> 28 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image annotation and search</head><p>We next evaluate the proposed model on the sentenceimage matching task. In this task, each dataset contains a set of images and five matching sentences per image. For each dataset, we test our model on two tasks, searching an image given a query sentence and matching a sentence given an image. We measure our performance on three datasets, Flickr8k <ref type="bibr" target="#b13">[14]</ref>, Flickr30k <ref type="bibr" target="#b49">[50]</ref> and COCO <ref type="bibr" target="#b22">[23]</ref>, each containing 8,000, 30,000 and 123,000 images respectively.</p><p>Images are presented by the representation layer of the VGG network <ref type="bibr" target="#b38">[39]</ref> as vectors of size 4096. Sentences are represented using the published code of <ref type="bibr" target="#b18">[19]</ref>. Among the available text encodings, we employ the concatenation of the Fisher Vector encoding (GMM) and the Fisher Vector of the HGLMM distribution introduced in <ref type="bibr" target="#b18">[19]</ref>. Each sentence is thus represented as a 36,000D vector. Going from the image to the much larger sentence representation, we trained networks containing two conventional hidden layers of sizes 2000 and 3000 and an additional locally dense layer of 16000 neurons and m = 2 for Flickr30k and COCO datasets. For Flickr8k, due to the relatively small dataset, we used a dense layer of 4000 neurons. Correlation is used as a similarity measure between images and sentences. To this end we use the middle network representations from each channels, resulting in a representation vector of size 3000.</p><p>The Flickr8k dataset is provided with training, validation, and test splits. For Flickr30K and COCO, no splits are given, and we use the same splits used by <ref type="bibr" target="#b18">[19]</ref>. λ deconv is set to a value of 1/2, which almost eliminated all offdiagonal covariances at the middle layer. The other parameters are set as in the MNIST and XRMB experiments.</p><p>Tab. 2 compare our results to the state-of-the-art methods on the image-sentence matching task. We also report results that we computed for the RCCA method <ref type="bibr" target="#b32">[33]</ref>. The open implementations of the various deep CCA methods do not seem to scale well enough for this benchmark. Our proposed method achieves best performance almost across all scores, especially in the image annotation task, where we improved by a large margin for the three datasets, and especially when considering the top result (r@1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation analysis</head><p>We perform an ablation analysis aimed at isolating the effect of the various architectural novelties suggested. Experiments were conducted on the Flickr8k, Flickr30k, MNIST and XRMB datasets. Each experiment uses the baseline configuration used in previous experiments with only one alternation. Batch Normalization For this experiment, we used different settings for the BN layer. The configuration settings include: (1) without BN, (2) with conventional BN (before ReLU) without regularizing γ, (3) with post-ReLU BN, without regularizing γ, (4) using BN before the ReLU with λ γ = 0.05, and (5) our proposed method: BN applied only after ReLU with λ γ = 0.05. Tab. 3 report the performance of the various configurations in terms of correlation and the mean variance of all features on the validation set.</p><p>As Tab. 3 shows, batch normalization has a profound effect on the network's results. Results taken without batch normalization were trained with lower learning rate, using higher learning rate prevented the training from converging. We can also see that using the 1/γ regularization term significantly increases the variance of the hidden representation, which, in turn, stabilizes the training process and improves correlation. The effect studied in Section 3.2 is clearly visible in the ablation study, positioning the BN layer after the Leaky ReLU prevents an unbalance representations as can be seen by the difference in variances, which increases the correlation of two representation significantly. Tab. 4 contains r@1 results for the same experiments on the Flickr8k dataset. As in Tab. 3 out suggested configuration achieves the base recall rates. Tied Dropout We trained the same base configuration as described above. We tested our proposed method using a conventional dropout instead of a tied dropout and removing dropout altogether. In all experiments, the dropout probability p was set at 0.5.</p><p>As can be seen, the performance drops when using the conventional dropout instead of the proposed tied dropout layer. The benefits of the tied dropout layer are most significant on the large datasets Flickr8k and Flickr30k, where over-fitting is likely. The shrinking effect discussed in Section 3.5 is clearly visible and is manifested as low variance of the output of the model based on conventional dropout, compared to a much higher variance when using the tied dropout. Leaky ReLU We also tested the contribution of other parameters on the model's performance. One of the major benefits was using leaky ReLU non-linearity. Using conventional ReLU resulted in large correlation loss of about 33% (1192 total correlation) for Flickr8k. Loss terms Another aspect we tested is the effect of various loss terms on correlation and recall rates. Removing L h term results in a 31% (1230) decrease of correlation. This settles with Lemma 2 which links the output's correlation and L h loss term. While the L h loss increases the output's correlation, the reconstruction loss terms L x and L y decreases the result's correlation. Removing them both increases correlation by 56% (2752). While the correlation produced between the two views is higher without the two reconstruction losses, the dimensions of each representation are highly correlated resulting in a decrease of 87% in image search and 91% in image annotation performance as measured by recall@1: from the full method's performance of 29.3 and 43.4 for the tasks of image search and image annotation to 4.0 and 3.9 respectively. Regularization The effect for R γ can be viewed in Tab. 3. Removing the R d ecov results in a decrease of all measures. Image search r@1 and r@5 results decrease by 14% and 10% respectively and the image annotation r@1 and r@5 results decrease by 10% and 8% respectively. Moreover, the correlation is reduced by 4%.</p><p>Locally dense layer To test the effect of the proposed locally dense layer, we trained our model on Flickr30k with a regular dense layer of the same size (16000 neurons) and with a regular dense layer of half the size. Image annotation r@1(r@5) results degrade by 7%(3%) and image search by 1%(1%) when using conventional 16000 neurons dense layer. Using dense layer half the size results in a drop of 13%(9%) for image annotation r@1(r@5) rates and 11%(8%) for image search recall rates r@1(r@5). Parameter sensitivity: <ref type="figure" target="#fig_1">Fig. 2(a)</ref> shows the effect of different leakiness coefficient values on the correlation as measured on the validation sets of the MNIST and XRMB data sets. The results were obtained by training the network us-    ing leakiness coefficients ranging between 0 and 0.7. As can be seen, there is a large region of values that provide better performance than the conventional zero-leakiness ReLU. <ref type="figure" target="#fig_1">Fig. 2(b)</ref> shows the effect of the regularization weight λ γ that controls the learned variance of the BN layer. The value used in our experiments seems to be beneficial and lies at a relatively wide high-performance plateau.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we present a method for linking paired samples from two sources. The method significantly outperforms all literature methods in the highly applicable and well studied domain of correlation analysis, including the classical methods, their modern variants, and the recent deep correlation methods. We are unique in that we employ a tied 2-way architecture, reconstructing , and unlike most methods, we employ the Euclidean loss. In order to promote an effective training, we introduce a series of contributions that are aimed at maintaining the variance of the learned representations. Each of these modifications is provided with an analysis that explains its role and together they work hand in hand in order to provide the complete architecture, which is highly accurate.</p><p>Our method is generic and can be employed in any computer vision domain in which two data modalities are used. In addition, our contributions could also help in training univariate regression problems. In the literature, the Euclidean loss is often combined with other losses <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51]</ref>, or replaced by an alternative loss <ref type="bibr" target="#b21">[22]</ref> in order to mitigate the challenges of training regression problems. Our variance injection method can be easily incorporated into any existing network.</p><p>As future work, we would like to continue exploring the use of tied 2-Way networks for matching views from different domains. In almost all of our trained networks, the biases of the batch normalization layers in the solutions tend to have very low values. These biases can probably be eliminated altogether. In addition, in many encoder/decoder schemes, layers are added gradually during training. It is possible to adopt such a scheme to our framework, adding hidden layers in the middle of the network one by one.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The 2-way network model. Each channel transforms one view into the other. A middle representation is extracted for correlation maximization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) The effect of the leakiness parameter on the MNIST and XRMB benchmarks, as measured on the validation set using the sum of correlations divided by the dimension (in percent). The solid red line depicts the MNIST results; the dashed black line depicts the XRMB results. (b) A similar plot showing the effect of coefficient λ γ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between various methods on the XRMB and MNIST datasets. The reported values are the sum of the correlations between the learned representations of the two views. Following the literature, in these benchmarks MNIST employs a 50D shared representation space, and XRMB a 112D one.</figDesc><table><row><cell></cell><cell>.0</cell><cell>16.9</cell></row><row><cell>DCCA [4]</cell><cell>39.7</cell><cell>92.9</cell></row><row><cell>RCCA [33]</cell><cell>44.5</cell><cell>104.5</cell></row><row><cell>DCCAE [47]</cell><cell>25.34</cell><cell>41.47</cell></row><row><cell>CorrNet [7]</cell><cell>48.07</cell><cell>95.01</cell></row><row><cell>NCCA [32]</cell><cell>NA</cell><cell>107.9</cell></row><row><cell>2WayNet</cell><cell>49.15</cell><cell>110.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>NLM [18] 12.5 37.0 18.0 40.9 16.8 42.0 23.0 50.7 NA 49.7 43.4 63.2 36.0 55.6 49.8 67.5 39.7 63.3 55.8 75.2</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Flickr8k</cell><cell></cell><cell cols="2">Flickr30k</cell><cell>COCO</cell></row><row><cell>Model</cell><cell cols="2">Search</cell><cell cols="2">Annotate</cell><cell>Search</cell><cell>Annotate</cell><cell>Search</cell><cell>Annotate</cell></row><row><cell></cell><cell cols="7">r@1 r@5 r@1 r@5 r@1 r@5 r@1 r@5 r@1 r@5 r@1 r@5</cell></row><row><cell>NIC [36]</cell><cell cols="6">19.0 NA 20.0 NA 17.0 NA 17.0 NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell cols="8">SC-NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>m-RNN [28]</cell><cell cols="7">11.5 31.0 14.5 37.2 22.8 50.7 35.4 63.8 29.0 42.2 41.0 73.0</cell></row><row><cell>m-CNN [26]</cell><cell cols="7">20.3 47.6 24.8 53.7 26.2 56.3 33.6 64.1 32.6 68.6 42.8 73.1</cell></row><row><cell>DCCA [49]</cell><cell cols="7">12.7 31.2 17.9 40.3 12.6 31.0 16.7 39.3 NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>BRNN [16]</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell cols="4">NA 15.2 37.7 22.2 48.2 27.4 60.2 38.4 69.9</cell></row><row><cell cols="8">RNN-FV [21] 23.2 53.3 31.6 61.2 27.4 55.9 35.9 62.5 30.2 65.0 40.9 75.0</cell></row><row><cell>VQA-A [24]</cell><cell cols="7">17.2 42.8 24.3 52.2 24.9 52.6 33.9 62.5 37.0 70.9 50.5 80.1</cell></row><row><cell>NLBD [46]</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell cols="4">NA 29.7 60.1 40.3 68.9 39.6 75.2 50.1 79.7</cell></row><row><cell>CCA [19]</cell><cell cols="7">21.3 50.1 31.0 59.3 23.5 52.8 35.0 62.1 25.1 59.8 39.4 67.9</cell></row><row><cell>RCCA [33]</cell><cell cols="7">18.7 31.1 11.7 19.2 22.7 34.2 28.3 48.2 NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>2WayNet</cell><cell>29.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The recall rates for the Flickr8k, Flickr30k and COCO image to sentence matching benchmarks. In image search, we show the percent of correct matches for the top retrieval out of all test images (r@1 for search). In image annotation, given a query image, fetching one of five matching sentences is considered a success. Recall rates for the top five (r@5) denote the cases in which a successful match exists in any of the top five results. The experiments reported for regularized CCA, RCCA, and our 2-way net all use the same sentence and image representation. Sentences are represented as the concatenation of the GMM-FV and the HGLMM-FV representations of<ref type="bibr" target="#b18">[19]</ref>. . Image is represented with the last dense connected of the CNN used in<ref type="bibr" target="#b18">[19]</ref>.</figDesc><table><row><cell>Scenario</cell><cell cols="3">Flickr8k Corr Var x Var y Corr Var x Var y Flickr30k</cell><cell>Corr</cell><cell cols="2">MNIST Var x Var y</cell><cell>Corr</cell><cell cols="2">XRMB Var x Var y</cell></row><row><cell>Suggested method</cell><cell>1758 0.65</cell><cell>0.64 2135 0.41</cell><cell>0.43</cell><cell cols="2">49.15 1.32</cell><cell cols="3">1.27 110.18 1.08</cell><cell>1.06</cell></row><row><cell>No BN</cell><cell>1482 1.90</cell><cell>1.71 1562 1.38</cell><cell>1.40</cell><cell>13.14</cell><cell>0</cell><cell>0</cell><cell>25.58</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">before ReLU, λ γ = 0 1313 0.66</cell><cell>0.44 1385 0.37</cell><cell>0.28</cell><cell>48.40</cell><cell>0.18</cell><cell cols="3">0.18 107.55 0.15</cell><cell>0.15</cell></row><row><cell>after ReLU, λ γ = 0</cell><cell>1598 1.34</cell><cell>1.25 1655 0.73</cell><cell>0.74</cell><cell>48.98</cell><cell>0.38</cell><cell cols="3">0.37 109.42 0.40</cell><cell>0.39</cell></row><row><cell cols="2">before ReLU, λ γ &gt; 0 1423 0.33</cell><cell>0.21 1322 1.80</cell><cell>0.96</cell><cell>48.76</cell><cell>0.73</cell><cell cols="3">0.72 108.79 0.50</cell><cell>0.50</cell></row><row><cell>No Dropout</cell><cell>1091 0.34</cell><cell>0.33 1446 0.57</cell><cell>0.52</cell><cell>49.00</cell><cell>1.33</cell><cell cols="3">1.33 109.69 0.79</cell><cell>0.79</cell></row><row><cell cols="2">Conventional dropout 1557 0.17</cell><cell>0.17 1658 0.12</cell><cell>0.14</cell><cell>48.77</cell><cell>1.90</cell><cell>1.90</cell><cell>93.24</cell><cell>0.24</cell><cell>0.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the Flickr8k, Flickr30k, MNIST and XRMB datasets, testing various batch normalization (BN), variance regularization and dropout options. We measure the variance in both views, X and Y (averaging the variance of all dimensions), and the obtained correlation. The suggested method is to apply BN only after ReLU with λ γ = 0.05 and to employ tied dropout. All BN variants employ tied dropout with probability of 0.5. All dropout variants apply BN similarly to the suggested method.</figDesc><table><row><cell>Scenario</cell><cell cols="2">Search r@1 Annotate r@1</cell></row><row><cell>Suggested method</cell><cell>29.3</cell><cell>43.4</cell></row><row><cell>No BN</cell><cell>21.1</cell><cell>25.6</cell></row><row><cell>before ReLU, λ γ = 0</cell><cell>26.9</cell><cell>39.6</cell></row><row><cell>after ReLU, λ γ = 0</cell><cell>27.9</cell><cell>40.9</cell></row><row><cell>No Dropout</cell><cell>25.64</cell><cell>36.6</cell></row><row><cell>Conventional dropout</cell><cell>29.04</cell><cell>42.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Recall results on Flickr8k for the same experiments as described at Tab. 3.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code can be found at https://github.com/aviveise/ 2WayNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized multiview analysis: A discriminative latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jacobs Abhishek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abhishek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A kernel method for canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Akaho</surname></persName>
		</author>
		<idno>cs/0609071</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kendall&apos;s advanced theory of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliver D Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distribution theory</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Correlational neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="285" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06068</idno>
		<title level="m">Reducing overfitting in deep networks by decorrelating representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Stanford&apos;s cs231n class notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="http://cs231n.github.io/neural-networks-2/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis of video volume tensors for action categorization and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RNN fisher vectors for action recognition and image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Live repetition counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Leveraging visual question answering for image-caption ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01379</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mel frequency cepstral coefficients for music modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Explain images with multimodal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1090</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan Boureau Yann Lecun Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu Jie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multivariate analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermine</forename><surname>Maes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nonlinear feature extraction using generalized canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Melzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks (ICANN)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Nonparametric canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04839</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mineiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Karampatziakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.3409</idno>
		<title level="m">A randomized algorithm for cca</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy Bengio Dumitru Erhan Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toshev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy Sergey Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal human behavior analysis: Learning correlation and interaction across modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimodal Interaction (ICMI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Regularized generalized canonical correlation analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Tenenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Tenenhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">257</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Canonical ridge and econometrics of joint production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Vinod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="166" />
			<date type="published" when="1976-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On deep multi-view representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen Livescu Jeff Bilmes Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">X-ray microbeam speech production database user&apos;s handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Westbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
