<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple Does It: Weakly Supervised Instance and Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simple Does It: Weakly Supervised Instance and Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic labelling and instance segmentation are two tasks that require particularly costly annotations. Starting from weak supervision in the form of bounding box detection annotations, we propose a new approach that does not require modification of the segmentation training procedure. We show that when carefully designing the input labels from given bounding boxes, even a single round of training is enough to improve over previously reported weakly supervised results. Overall, our weak supervision approach reaches ∼ 95% of the quality of the fully supervised model, both for semantic labelling and instance segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional networks (convnets) have become the de facto technique for pattern recognition problems in computer vision. One of their main strengths is the ability to profit from extensive amounts of training data to reach top quality. However, one of their main weaknesses is that they need a large number of training samples for high quality results. This is usually mitigated by using pre-trained models (e.g. with ∼ 10 6 training samples for ImageNet classification <ref type="bibr" target="#b36">[37]</ref>), but still thousands of samples are needed to shift from the pre-training domain to the application domain. Applications such as semantic labelling (associating each image pixel to a given class) or instance segmentation (grouping all pixels belonging to the same object instance) are expensive to annotate, and thus significant cost is involved in creating large enough training sets.</p><p>Compared to object bounding box annotations, pixelwise mask annotations are far more expensive, requiring ∼ 15× more time <ref type="bibr" target="#b24">[25]</ref>. Cheaper and easier to define, box annotations are more pervasive than pixel-wise annotations. In principle, a large number of box annotations (and images representing the background class) should convey enough information to understand which part of the box content is foreground and which is background. In this paper we explore how much one can close the gap between training a  convnet using full supervision for semantic labelling (or instance segmentation) versus using only bounding box annotations.</p><p>Our experiments focus on the 20 Pascal classes <ref type="bibr" target="#b8">[9]</ref> and show that using only bounding box annotations over the same training set we can reach ∼ 95% of the accuracy achievable with full supervision. We show top results for (bounding box) weakly supervised semantic labelling and, to the best of our knowledge, for the first time report results for weakly supervised instance segmentation.</p><p>We view the problem of weak supervision as an issue of input label noise. We explore recursive training as a de-noising strategy, where convnet predictions of the previous training round are used as supervision for the next round. We also show that, when properly used, "classic computer vision" techniques for box-guided instance segmentation are a source of surprisingly effective supervision for convnet training.</p><p>In summary, our main contributions are:</p><p>− We explore recursive training of convnets for weakly supervised semantic labelling, discuss how to reach good quality results, and what are the limitations of the approach (Section 3.1).</p><p>− We show that state of the art quality can be reached when properly employing GrabCut-like algorithms to generate training labels from given bounding boxes, instead of modifying the segmentation convnet training procedure (Section 3.2).</p><p>− We report the best known results when training using bounding boxes only, both using Pascal VOC12 and VOC12+COCO training data, reaching comparable quality with the fully supervised regime (Section 4.2).</p><p>− We are the first to show that similar results can be achieved for the weakly supervised instance segmentation task (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Semantic labelling. Semantic labelling may be tackled via decision forests <ref type="bibr" target="#b37">[38]</ref> or classifiers over hand-crafted superpixel features <ref type="bibr" target="#b10">[11]</ref>. However, convnets have proven particularly effective for semantic labelling. A flurry of variants have been proposed recently <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b45">46]</ref>. In this work we use DeepLab <ref type="bibr" target="#b4">[5]</ref> as our reference implementation. This network achieves state-of-the-art performance on the Pascal VOC12 semantic segmentation benchmark and the source code is available online. Almost all these methods include a post-processing step to enforce a spatial continuity prior in the predicted segments, which provides a non-negligible improvement on the results (2 ∼ 5 points). The most popular technique is DenseCRF <ref type="bibr" target="#b19">[20]</ref>, but other variants are also considered <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Weakly supervised semantic labelling. In order to keep annotation cost low, recent work has explored different forms of supervision for semantic labelling: image labels <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref>, points <ref type="bibr" target="#b2">[3]</ref>, scribbles <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23]</ref>, and bounding boxes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15]</ref> also consider the case where a fraction of images are fully supervised. <ref type="bibr" target="#b43">[44]</ref> proposes a framework to handle all these types of annotations.</p><p>In this work we focus on box level annotations for semantic labelling of objects. The closest related work are thus <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. BoxSup <ref type="bibr" target="#b7">[8]</ref> proposes a recursive training procedure, where the convnet is trained under supervision of segment object proposals and the updated network in turn improves the segments used for training. WSSL <ref type="bibr" target="#b26">[27]</ref> proposes an expectation-maximisation algorithm with a bias to enable the network to estimate the foreground regions. We compare with these works in the result sections. Since all implementations use slightly different networks and training procedures, care should be taken during comparison. Both <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b26">[27]</ref> propose new ways to train convnets under weak supervision. In contrast, in this work we show that one can reach better results without modifying the training procedure (compared to the fully supervised case) by instead carefully generating input labels for training from the bounding box annotations (Section 3).</p><p>Instance segmentation. In contrast to instance agnostic semantic labelling that groups pixels by object class, instance segmentation groups pixels by object instance and ignores classes. Object proposals <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b15">16]</ref> that generate segments (such as <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21]</ref>) can be used for instance segmentation. Similarly, given a bounding box (e.g. selected by a detector), GrabCut <ref type="bibr" target="#b35">[36]</ref> variants can be used to obtain an instance segmentation (e.g. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref>).</p><p>To enable end-to-end training of detection and segmentation systems, it has recently been proposed to train convnets for the task of instance segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>. In this work we explore weakly supervised training of an instance segmentation convnet. We use DeepMask <ref type="bibr" target="#b32">[33]</ref> as a reference implementation for this task. In addition we re-purpose DeepLab-v2 network <ref type="bibr" target="#b5">[6]</ref>, originally designed for semantic segmentation, for the instance segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">From boxes to semantic labels</head><p>The goal of this work is to provide high quality semantic labelling starting from object bounding box annotations. We design our approach aiming to exploit the available information at its best. There are two sources of information: the annotated boxes and priors about the objects. We integrate these in the following cues: C1 Background. Since the bounding boxes are expected to be exhaustive, any pixel not covered by a box is labelled as background.</p><p>C2 Object extend. The box annotations bound the extent of each instance. Assuming a prior on the objects shapes (e.g. oval-shaped objects are more likely than thin bar or full rectangular objects), the box also gives information on the expected object area. We employ this size information during training.</p><p>C3 Objectness. Other than extent and area, there are additional object priors at hand. Two priors typically used are spatial continuity and having a contrasting boundary with the background. In general we can harness priors about object shape by using segment proposal techniques <ref type="bibr" target="#b34">[35]</ref>, which are designed to enumerate and rank plausible object shapes in an area of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Box baselines</head><p>We first describe a naive baseline that serves as starting point for our exploration. Given an annotated bounding box and its class label, we label all pixels inside the box with such given class. If two boxes overlap, we assume the smaller one is in front. Any pixel not covered by boxes is labelled as background. <ref type="figure" target="#fig_1">Figure 2</ref> left side and <ref type="figure" target="#fig_2">Figure 3c</ref> show such example annotations. We use these labels to train a segmentation net-work with the standard training procedure. We employ the DeepLabv1 approach from <ref type="bibr" target="#b4">[5]</ref> (details in Section 4.1).</p><p>Recursive training. We observe that when applying the resulting model over the training set, the network outputs capture the object shape significantly better than just boxes (see <ref type="figure" target="#fig_1">Figure 2</ref>). This inspires us to follow a recursive training procedure, where these new labels are fed in as ground truth for a second training round. We name this recursive training approach Naive.</p><p>The recursive training is enhanced by de-noising the convnet outputs using extra information from the annotated boxes and object priors. Between each round we improve the labels with three post-processing stages:</p><p>1. Any pixel outside the box annotations is reset to background label (cue C1).</p><p>2. If the area of a segment is too small compared to its corresponding bounding box (e.g. IoU&lt; 50%), the box area is reset to its initial label (fed in the first round). This enforces a minimal area (cue C2).</p><p>3. As it is common practice among semantic labelling methods, we filter the output of the network to better respect the image boundaries. (We use DenseCRF <ref type="bibr" target="#b19">[20]</ref> with the DeepLabv1 parameters <ref type="bibr" target="#b4">[5]</ref>). In our weakly supervised scenario, boundary-aware filtering is particularly useful to improve objects delineation (cue C3).</p><p>The recursion and these three post-processing stages are crucial to reach good performance. We name this recursive training approach Box, and show an example result in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Ignore regions. We also consider a second variant Box i that, instead of using filled rectangles as initial labels, we fill in the 20% inner region, and leave the remaining inner area of the bounding box as ignore regions. See <ref type="figure" target="#fig_2">Figure 3d</ref>. Following cues C2 and C3 (shape and spatial continuity priors), the 20% inner box region should have higher chances of overlapping with the corresponding object, reducing the noise in the generated input labels. The intuition is that the convnet training might benefit from trading-off lower recall (more ignore pixels) for higher precision (more pixels are correctly labelled). Starting from this initial input, we use the same recursive training procedure as for Box.</p><p>Despite the simplicity of the approach, as we will see in the experimental section 4, Box / Box i is already competitive with the current state of the art.</p><p>However, using rectangular shapes as training labels is clearly suboptimal. Therefore, in the next section, we propose an approach that obtains better results while avoiding multiple recursive training rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Box-driven segments</head><p>The box baselines are purposely simple. A next step in complexity consists in utilising the box annotations to generate an initial guess of the object segments. We think of this as "old school meets new school": we use the noisy outputs of classic computer vision methods, box-driven figureground segmentation <ref type="bibr" target="#b35">[36]</ref> and object proposal <ref type="bibr" target="#b34">[35]</ref> techniques, to feed the training of a convnet. Although the output object segments are noisy, they are more precise than simple rectangles, and thus should provide improved results. A single training round will be enough to reach good quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">GrabCut baselines</head><p>GrabCut <ref type="bibr" target="#b35">[36]</ref> is the established technique to estimate an object segment from its bounding box. We propose to use a modified version of GrabCut, which we call GrabCut+, where HED boundaries <ref type="bibr" target="#b42">[43]</ref> are used as pairwise term instead of the typical RGB colour difference. (The HED boundary detector is trained on the generic boundaries of BSDS500 <ref type="bibr" target="#b0">[1]</ref>). We considered other GrabCut variants, such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref>; however, the proposed GrabCut+ gives higher quality segments (see supplementary material). Similar to Box i , we also consider a GrabCut+ i variant, which trades off recall for higher precision. For each annotated box we generate multiple (∼ 150) perturbed GrabCut+ outputs. If 70% of the segments mark the pixel as foreground, the pixel is set to the box object class. If less than 20% of the segments mark the pixels as foreground, the pixel is set as background, otherwise it is marked as ignore. The perturbed outputs are generated by jittering the box coordinates (±5%) as well as the size of the outer background region considered by GrabCut (from 10% to 60%). An example result of GrabCut+ i can be seen in <ref type="figure" target="#fig_2">Figure 3g</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Adding objectness</head><p>With our final approach we attempt to better incorporate the object shape priors by using segment proposals <ref type="bibr" target="#b34">[35]</ref>. Segment proposals techniques are designed to generate a soup of likely object segmentations, incorporating as many "objectness" priors as useful (cue C3).</p><p>We use the state of the art proposals from MCG <ref type="bibr" target="#b33">[34]</ref>. As final stage the MCG algorithm includes a ranking based on a decision forest trained over the Pascal VOC 2012 dataset. We do not use this last ranking stage, but instead use all the (unranked) generated segments. Given a box annotation, we pick the highest overlapping proposal as a corresponding segment.</p><p>Building upon the insights from the baselines in Section 3.1 and 3.2, we use the MCG segment proposals to supplement GrabCut+. Inside the annotated boxes, we mark as foreground pixels where both MCG and GrabCut+ agree; the remaining ones are marked as ignore. We denote this approach as MCG ∩ GrabCut+ or M ∩ G+ for short.</p><p>Because MCG and GrabCut+provide complementary information, we can think of M ∩ G+ as an improved version of GrabCut+ i providing a different trade-off between precision and recall on the generated labels (see <ref type="figure" target="#fig_2">Figure 3i</ref>).</p><p>The BoxSup method <ref type="bibr" target="#b7">[8]</ref> also uses MCG object proposals during training; however, there are important differences. They modify the training procedure so as to denoise intermediate outputs by randomly selecting high overlap proposals. In comparison, our approach keeps the training procedure unmodified and simply generates input labels. Our approach also uses ignore regions, while BoxSup does not explore this dimension. Finally, BoxSup uses a longer training than our approach. Section 4 shows results for the semantic labelling task, compares different methods and different supervision regimes. In Section 5 we show that the proposed approach is also suitable for the instance segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Semantic labelling results</head><p>Our approach is equally suitable (and effective) for weakly supervised instance segmentation as well as for semantic labelling. However, only the latter has directly comparable related work. We thus focus our experimental comparison efforts on the semantic labelling task. Results for instance segmentation are presented in Section 6. Section 4.1 discusses the experimental setup, evaluation, and implementation details for semantic labelling. Section 4.2 presents our main results, contrasting the methods from Section 3 with the current state of the art. Section 4.3 further expands these results with a more detailed analysis, and presents results when using more supervision (semisupervised case).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Datasets. We evaluate the proposed methods on the Pascal VOC12 segmentation benchmark <ref type="bibr" target="#b8">[9]</ref>. The dataset consists of 20 foreground object classes and one background class. The segmentation part of the VOC12 dataset contains 1 464 training, 1 449 validation, and 1 456 test images. Following previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>, we extend the training set with the annotations provided by <ref type="bibr" target="#b11">[12]</ref>, resulting in an augmented set of 10 582 training images. In some of our experiments, we use additional training images from the COCO <ref type="bibr" target="#b24">[25]</ref> dataset. We only consider images that contain any of the 20 Pascal classes and (following <ref type="bibr" target="#b47">[48]</ref>) only objects with a bounding box area larger than 200 pixels. After this filtering, 99 310 images remain (from training and validation sets), which are added to our training set. When using COCO data, we first pre-train on COCO and then fine-tune over the Pascal VOC12 training set. All of the COCO and Pascal training images come with semantic labelling annotations (for fully supervised case) and bounding box annotations (for weakly supervised case).</p><p>Evaluation. We use the "comp6" evaluation protocol. The performance is measured in terms of pixel intersectionover-union averaged across 21 classes (mIoU). Most of our results are shown on the validation set, which we use to guide our design choices. Final results are reported on the test set (via the evaluation server) and compared with other state-of-the-art methods.</p><p>Implementation details. For all our experiments we use the DeepLab-LargeFOV network, using the same train and test parameters as <ref type="bibr" target="#b4">[5]</ref>. The model is initialized from a VGG16 network pre-trained on ImageNet <ref type="bibr" target="#b38">[39]</ref>. We use a mini-batch of 30 images for SGD and initial learning rate of 0.001, which is divided by 10 after a 2k/20k iterations (for Pascal/COCO). At test time, we apply DenseCRF <ref type="bibr" target="#b19">[20]</ref>. Our network and post-processing are comparable to the ones used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Note that multiple strategies have been considered to boost test time results, such as multi-resolution or model ensembles <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>. Here we keep the approach simple and fixed. In all our experiments we use a fixed training and test time procedure. Across experiments we only change the input training data that the networks gets to see.   <ref type="table" target="#tab_1">Tables 1 and 2</ref>. Pascal VOC12 validation set results. "Previous best (rectangles/segments)" corresponds to WSSL R /BoxSup MCG in <ref type="table" target="#tab_3">Table 2</ref>.</p><formula xml:id="formula_0">(a) Input image (b) Ground truth (c) Box (d) Box i (e) GrabCut (f) GrabCut+ (g) GrabCut+ i (h) MCG (i) M ∩ G+</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main results</head><p>Box results. <ref type="figure" target="#fig_3">Figure 4</ref> presents the results for the recursive training of the box baselines from Section 3.1. We see that the Naive scheme, a recursive training from rectangles disregarding post-processing stages, leads to poor quality. However, by using the suggested three post-processing stages, the Box baseline obtains a significant gain, getting tantalisingly close to the best reported results on the task <ref type="bibr" target="#b7">[8]</ref>.</p><p>Details of the contribution of each post-processing stage are presented in the supplementary material. Adding ignore re- gions inside the rectangles (Box → Box i ) provides a clear gain and leads by itself to state of the art results. <ref type="figure" target="#fig_3">Figure 4</ref> also shows the result of using longer training for fully supervised case. When using ground truth semantic segmentation annotations, one training round is enough to achieve good performance; longer training brings marginal improvement. As discussed in Section 3.1, reaching good quality for Box/Box i requires multiple training rounds instead, and performance becomes stable from round 5 onwards. Instead, GrabCut+/M ∩ G+ do not benefit from additional training rounds.</p><p>Box-driven segment results. <ref type="table" target="#tab_1">Table 1</ref> evaluates results on the Pascal VOC12 validation set. It indicates the Box/Box i results after 10 rounds, and MCG/GrabCut+/GrabCut+ i /M∩G+ results after one round. "Fast-RCNN" is the result using detections <ref type="bibr" target="#b9">[10]</ref> to generate semantic labels (lower-bound), "GT Boxes" considers the box annotations as labels, and DeepLab ours indicates our fully supervised segmentation network result obtained with a training length equivalent to three training rounds (upperbound for our results). We see in the results that using ignore regions systematically helps (trading-off recall for precision), and that M ∩ G+ provides better results than MCG and GrabCut+ alone. <ref type="table" target="#tab_3">Table 2</ref> indicates the box-driven segment results after 1 training round and shows comparison with other state of the art methods, trained from boxes only using either Pascal VOC12, or VOC12+COCO data. BoxSup R and WSSL R both feed the network with rectangle segments (comparable to Box i ), while WSSL S and BoxSup MCG exploit arbitrary shaped segments (comparable to M ∩ G+). Although our network and post-processing is comparable to the ones in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>, there are differences in the exact training procedure and parameters (details in supplementary material). Overall, our results indicate thatwithout modifying the training procedure -M ∩ G+ is able to improve over previously reported results and reach 95% of the fully-supervised training quality. By training with COCO data <ref type="bibr" target="#b24">[25]</ref> before fine-tuning for Pascal VOC12, we see that with enough additional bounding boxes we can match the full supervision from Pascal VOC 12 (68.9 versus 69.1). This shows that the labelling effort could be significantly reduced by replacing segmentation masks with bounding box annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Additional results</head><p>Semi-supervised case. <ref type="table" target="#tab_3">Table 2</ref> compares results in the semi-supervised modes considered by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>, where some of the images have full supervision, and some have only bounding box supervision. Training with 10% of Pascal VOC12 semantic labelling annotations does not bring much gain to the performance (65.7 versus 65.8), this hints at the high quality of the generated M ∩ G+ input data.</p><p>By using ground-truth annotations on Pascal plus bounding box annotations on COCO, we observe 2.5 points gain (69.1 → 71.6 , see <ref type="table" target="#tab_3">Table 2</ref>). This suggests that the overall performance could be further improved by using extra training data with bounding box annotations.</p><p>Boundaries supervision. Our results from MCG, GrabCut+, and M ∩ G+ all indirectly include information from the BSDS500 dataset <ref type="bibr" target="#b0">[1]</ref> via the HED boundary detector <ref type="bibr" target="#b42">[43]</ref>. These results are fully comparable to BoxSup-MCG <ref type="bibr" target="#b7">[8]</ref>, to which we see a clear improvement. Nonetheless one would like to know how much using dense boundary annotations from BSDS500 contributes to the results. We use the weakly supervised boundary detection technique from <ref type="bibr" target="#b16">[17]</ref> to learn boundaries directly from the Pascal VOC12 box annotations. Training M ∩ G+ using weakly supervised HED boundaries results in 1 point loss compared to using the BSDS500 (64. <ref type="bibr" target="#b7">8</ref>   on Pascal VOC12 validation set). We see then that although the additional supervision does bring some help, it has a minor effect and our results are still rank at the top even when we use only Pascal VOC12 + ImageNet pre-training.</p><p>Different convnet results. For comparison purposes with <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> we used DeepLabv1 with a VGG-16 network in our experiments. To show that our approach also generalizes across different convnets, we also trained DeepLabv2 with a ResNet101 network <ref type="bibr" target="#b5">[6]</ref>. <ref type="table" target="#tab_5">Table 3</ref> presents the results. Similar to the case with VGG-16, our weakly supervised approach M ∩ G+ reaches 93%/95% of the fully supervised case when training with VOC12/VOC12+COCO, and the weakly supervised results with COCO data reach similar quality to full supervision with VOC12 only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">From boxes to instance segmentation</head><p>Complementing the experiments of the previous sections, we also explore a second task: weakly supervised in-   stance segmentation. To the best of our knowledge, these are the first reported experiments on this task. As object detection moves forward, there is a need to provide richer output than a simple bounding box around objects. Recently <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref> explored training convnets to output a foreground versus background segmentation of an instance inside a given bounding box. Such networks are trained using pixel-wise annotations that distinguish between instances. These annotations are more detailed and expensive than semantic labelling, and thus there is interest in weakly supervised training.</p><formula xml:id="formula_1">Image Ground truth Box Box i M ∩ G+ Semi supervised M ∩ G+</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully supervised</head><p>The segments used for training, as discussed in Section 3.2, are generated starting from individual object bounding boxes. Each segment represents a different object instance and thus can be used directly to train an instance segmenta-tion convnet. For each annotated bounding box, we generate a foreground versus background segmentation using the GrabCut+ method (Section 3.2), and train a convnet to regress from the image and bounding box information to the instance segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Instance segmentation results</head><p>Experimental setup. We choose a purposely simple instance segmentation pipeline, based on the "hyper-columns system 2" architecture <ref type="bibr" target="#b13">[14]</ref>. We use Fast-RCNN <ref type="bibr" target="#b9">[10]</ref> detections (post-NMS) with their class score, and for each detection estimate an associated foreground segment. We estimate the foreground using either some baseline method (e.g. GrabCut) or using convnets trained for the task <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>For our experiments we use a re-implementation of the DeepMask <ref type="bibr" target="#b32">[33]</ref> architecture, and additionally we repurpose a DeepLabv2 VGG-16 network <ref type="bibr" target="#b5">[6]</ref> for the instance segmentation task, which we name DeepLab BOX .</p><p>Inspired by <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b3">4]</ref>, we modify DeepLab to accept four input channels: the input image RGB channels, plus a binary map with a bounding box of the object instance to segment. We train the network DeepLab BOX to output the segmentation mask of the object corresponding to the input bounding box. The additional input channel guides the network so as to segment only the instance of interest instead of all objects in the scene. The input box rectangle can also be seen as an initial guess of the desired output. We train using ground truth bounding boxes, and at test time Fast-RCNN  detection boxes are used. We train DeepMask and DeepLab BOX using GrabCut+ results either over Pascal VOC12 or VOC12+COCO data (1 training round, no recursion like in Section 3.1), and test on the VOC12 validation set, the same set of images used in Section 4. The augmented annotation from <ref type="bibr" target="#b11">[12]</ref> provides per-instance segments for VOC12. We do not use CRF post-processing for neither of the networks. Following instance segmentation literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> we report in <ref type="table" target="#tab_7">Table 4</ref> mAP r at IoU threshold 0.5 and 0.75. mAP r is similar to the traditional VOC12 evaluation, but using IoU between segments instead of between boxes. Since we have a fixed set of windows, we can also report the average best overlap (ABO) <ref type="bibr" target="#b34">[35]</ref> metric to give a different perspective on the results.</p><p>Baselines. We consider five training-free baselines: simply filling in the detection rectangles (boxes) with foreground labels, fitting an ellipse inside the box, using the MCG proposal with best bounding box IoU, and using GrabCut and GrabCut+ (see Section 3.2), initialized from the detection box.</p><p>Analysis. The results table 4 follows the same trend as the semantic labelling results in Section 4. GrabCut+ provides the best results among the baselines considered and shows comparable performance to DeepMask, while our proposed DeepLab BOX outperforms both techniques. We see that our weakly supervised approach reaches ∼ 95% Examples of the instance segmentation results from weakly supervised DeepMask (VOC12+COCO) are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. Additional example results are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>The series of experiments presented in this paper provides new insights on how to train pixel-labelling convnets from bounding box annotations only. We showed that when carefully employing the available cues, recursive training using only rectangles as input can be surprisingly effective (Box i ). Even more, when using box-driven segmentation techniques and doing a good balance between accuracy and recall in the noisy training segments, we can reach state of the art performance without modifying the segmentation network training procedure (M∩G+). Our results improve over previously reported ones on the semantic labelling task and reach ∼ 95% of the quality of the same network trained on the ground truth segmentation annotations (over the same data). By employing extra training data with bounding box annotations from COCO we are able to match the full supervision results. We also report the first results for weakly supervised instance segmentation, where we also reach ∼ 95% of the quality of the fully-supervised training.</p><p>Our current approach exploits existing box-driven segmentation techniques, treating each annotated box individually. In future work we would like to explore cosegmentation ideas (treating the set of annotations as a whole), and consider even weaker forms of supervision.</p><p>• Section B analyses the contribution of the postprocessing stages during recursive training ( <ref type="figure" target="#fig_0">Figure  S1</ref>).</p><p>• Section C discusses training differences of our approach in contrast to the related work.</p><p>• We report a comparison of different GrabCut-like methods on Pascal VOC12 boxes in Section D.</p><p>• Section E ( <ref type="figure" target="#fig_1">Figure S2)</ref> shows visualization of the different variants of the proposed segmentation inputs obtained from bounding box annotations for weakly supervised semantic segmentation.</p><p>• Detailed performance of each class for semantic labelling is reported in Section F <ref type="table" target="#tab_3">(Table S2</ref>).</p><p>• Section G provides additional qualitative results for weakly supervised semantic segmentation on Pascal VOC12 ( <ref type="figure" target="#fig_2">Figure S3</ref>).</p><p>• Qualitative results for instance segmentation are shown in Section H ( <ref type="figure" target="#fig_3">Figure S4</ref> and <ref type="figure" target="#fig_4">Figure S5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recursive training with boxes</head><p>In Section 3 of the main paper we recursively train a convnet directly on the full extend of bounding box annotations as foreground labels, disregarding post-processing stages. We name this recursive training approach Naive. Using this supervision and directly applying recursive training leads to significant degradation of the segmentation output quality, see <ref type="figure" target="#fig_0">Figure S1</ref>.</p><p>To improve the labels between the training rounds three post-processing stages are proposed. Here we discuss them in more detail:</p><p>1. Box enforcing: Any pixel outside the box annotations is reset to background label (cue C1, see Section 3 in the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Outliers reset:</head><p>If the area of a segment is too small compared to its corresponding bounding box (e.g. IoU&lt; 50%), the box area is reset to its initial label (fed in the first round). This enforces a minimal area (cue C2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CRF:</head><p>As it is common practice among semantic labelling methods, we filter the output of the network Nai ve r ecur si ve t r ai ni ng Naive <ref type="figure" target="#fig_0">Figure S1</ref>: Recursive training from rectangles only as input.</p><p>Validation set results. All methods use only rectangles as initial input, except "previous best (segments)".</p><p>to better respect the image boundaries. (We use Den-seCRF <ref type="bibr" target="#b19">[20]</ref> with the DeepLabv1 parameters <ref type="bibr" target="#b4">[5]</ref>). In our weakly supervised scenario, boundary-aware filtering is particularly useful to improve objects delineation (cue C3).</p><p>Results. <ref type="figure" target="#fig_0">Figure S1</ref> presents results of the recursive training using boxes as input and shows the contribution of the post-processing stages. We see that the naive recursive training is ineffectual. However as soon as some constraints (box enforcing and outliers reset, cues C1+C2) are enforced, the quality improves dramatically after the first round of recursive training. These results already improve over previous work considering rectangles only input <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> (both using a similar convnet to ours) and achieve 3 points improvement over <ref type="bibr" target="#b26">[27]</ref> (from 52.5 to 55.6 mIoU, see <ref type="figure" target="#fig_0">Figure  S1</ref> "Box enf.+Outliers reset"). Even more, when also adding CRF filtering (+ cue C3) over the training set, we see a steady grow after each round, stabilizing around 61% mIoU. This number is surprisingly close to the best results obtained using more sophisticated techniques <ref type="bibr" target="#b7">[8]</ref>, which achieve around 62% mIoU (see <ref type="table" target="#tab_1">Figure S1 and Table S2</ref>).</p><p>Our results indicate that recursive training of a convnet is robust to input noise as soon as appropriate care is taken to de-noise the output between rounds, enabled by given bounding boxes and object priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training details in comparison with BoxSup and WSSL</head><p>In this work we focus on box level annotations for semantic labelling of objects. The closest related work are thus <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. Since all implementations use slightly different networks and training procedures, care should be taken during comparison. Both <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b26">[27]</ref> propose new ways to train convnets under weak supervision. Both of the approaches build upon the DeepLab network <ref type="bibr" target="#b4">[5]</ref>, however, there are a few differences in the network architecture.</p><p>WSSL <ref type="bibr" target="#b26">[27]</ref> employs 2 different variants of the DeepLab architecture with small and large receptive field of view (FOV) size. For each experiment WSSL evaluates with both architectures and reports the best result obtained (using boxes or segments as input). BoxSup <ref type="bibr" target="#b7">[8]</ref> uses their own implementation of the DeepLab with the small FOV. In our approach all the experiments employ the DeepLab architecture with the large FOV.</p><p>There are also differences in the training procedure. For SGD WSSL uses a mini-batch of 20-30 images and finetunes the network for about 12 hours (number of epochs is not specified) with the standard learning parameters (following <ref type="bibr" target="#b4">[5]</ref>). In the SGD training BoxSup uses a mini-batch size of 20 and the learning rate is divided by 10 after every 15 epochs. The training is terminated after 45 epochs. We use a mini-batch of 30 images for SGD and the learning rate is divided by 10 after every 2k iterations,~6 epochs. Our network is trained for 6k iterations,~18 epochs.</p><p>Similarly to our approach, the BoxSup method <ref type="bibr" target="#b7">[8]</ref> uses MCG object proposals during training. However, there are important differences. They modify the training procedure so as to denoise intermediate outputs by randomly selecting high overlap proposals. In comparison, our approach keeps the training procedure unmodified and simply generates input labels. Our approach also uses ignore regions, while BoxSup does not explore this dimension.</p><p>WSSL <ref type="bibr" target="#b26">[27]</ref> proposes an expectation-maximisation algorithm with a bias to enable the network to estimate the foreground regions. In contrast, in our work we show that one can reach better results without modifying the training procedure (compared to the fully supervised case) by instead carefully generating input labels for training from the bounding box annotations (Section 3.2 in the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. GrabCut variants</head><p>As discussed in Section 3.2 in the main paper we propose to employ box-guided instance segmentation to increase quality of the input data. Our goal is to have weak annotations with maximal quality and minimal loss in recall. In Section 3.1 in the main paper we explored how far could we get with just using boxes as foreground labels. However, to obtain results of higher quality several rounds of recursive training are needed. Starting from less noisier object segments we would like to reach better performance with just one training round.</p><p>For this purpose we explore different GrabCut-like <ref type="bibr" target="#b35">[36]</ref> techniques, the corresponding quantitative results are in <ref type="table" target="#tab_1">Table S1</ref>. For evaluation we use the mean IoU measure. Previous work evaluated using the 50 images from the GrabCut dataset <ref type="bibr" target="#b35">[36]</ref>, or 1k images with one salient object <ref type="bibr" target="#b6">[7]</ref>. The evaluation of Table S1 compares multiple methods over 3.4k object windows, where the objects are not salient, have diverse sizes and occlusions level. This is a more challenging scenario than usually considered for GrabCut-like methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mIoU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GrabCut variants</head><p>DenseCut <ref type="bibr" target="#b6">[7]</ref> 52.5 Bbox-Seg+CRF <ref type="bibr" target="#b26">[27]</ref> 71.1 GrabCut <ref type="bibr" target="#b35">[36]</ref> 72.9 KGrabCut <ref type="bibr" target="#b39">[40]</ref> 73.5 GrabCut+ 75. <ref type="table" target="#tab_1">2   Table S1</ref>: GrabCut variants, evaluated on Pascal VOC12 validation set. See Section D for details.</p><p>GrabCut <ref type="bibr" target="#b35">[36]</ref> is the established technique to estimate an object segment from its bounding box. To further improve its quality we propose to use better pairwise terms. We name this variant GrabCut+. Instead of the typical RGB colour difference the pairwise terms in GrabCut+ are replaced by probability of boundary as generated by HED <ref type="bibr" target="#b42">[43]</ref>. The HED boundary detector is trained on the generic boundaries of BSDS500 <ref type="bibr" target="#b0">[1]</ref>. Moving from GrabCut to GrabCut+ brings a ∼ 2 points improvement, see <ref type="table" target="#tab_1">Table S1</ref>. We also experimented with other variants such as DenseCut <ref type="bibr" target="#b6">[7]</ref> and KGrabCut <ref type="bibr" target="#b39">[40]</ref> but did not obtain significant gains. <ref type="bibr" target="#b26">[27]</ref> proposed to perform foreground/background segmentation by using DenseCRF and the 20% of the centre area of the bounding box as foreground prior. This approach is denoted Bbox-Seg+CRF in <ref type="table" target="#tab_1">Table S1</ref> and underperforms compared to GrabCut and GrabCut+. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Examples of input segmentations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Detailed test set results for semantic labelling</head><p>In <ref type="table" target="#tab_3">Table S2</ref>, we present per class results on the Pascal VOC12 test set for the methods reported in the main paper in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>On average with our weakly supervised results we achieve ∼ 95% quality of full supervision across all classes when training with VOC12 only or VOC12+COCO. H. Qualitative results for instance segmentations <ref type="figure" target="#fig_3">Figure S4</ref> illustrates additional qualitative results for instance segmentations given by the weakly supervised DeepMask and DeepLab BOX models. This figure complements <ref type="figure" target="#fig_5">Figure 6</ref> from the main paper. <ref type="figure" target="#fig_4">Figure S5</ref> shows examples of instance segmentation given by different methods. Our proposed weakly supervised DeepMask model achieves competitive performance with fully supervised results and provides higher quality output in comparison with box-guided segmentation techniques. The DeepLab BOX model also provides similar results, see <ref type="table" target="#tab_7">Table 4</ref> in the main paper.  <ref type="table" target="#tab_3">Table S2</ref>: Per class semantic labelling results for methods trained using Pascal VOC12 and COCO. Test set results. Bold indicates the best performance with the same supervision and training data. M ∩ G+ denotes the weakly or semi supervised model trained with MCG ∩ Grabcut+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative results for semantic labelling</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We propose a technique to train semantic labelling from bounding boxes, and reach 95% of the quality obtained when training from pixel-wise annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example results of using only rectangle segments and recursive training (using convnet predictions as supervision for the next round), see Section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Example of the different segmentations obtained starting from a bounding box annotation. Grey/pink/magenta indicate different object classes, white is background, and ignore regions are beige. M ∩ G+ denotes MCG ∩ GrabCut+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Segmentation quality versus training round for different approaches, see also</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on VOC12. Visually, the results from our weakly supervised method M ∩ G+ are hardly distinguishable from the fully supervised ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Example result from our weakly supervised DeepMask (VOC12+COCO) model. of the quality of fully-supervised case (both on mAP r 0.5 and ABO metrics) using two different convnets, DeepMask and DeepLab BOX , both when training with VOC12 or VOC12+COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>S2 presents examples of the considered weak annotations. This figure extendsFigure 3of the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><label></label><figDesc>S3 presents qualitative results for semantic labelling on Pascal VOC12. The presented semantic la-belling examples show that high quality segmentation can be achieved using only detection bounding box annotations. This figure extends Figure 5 of the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Method</cell><cell>val. mIoU</cell></row><row><cell>-</cell><cell>Fast-RCNN GT Boxes</cell><cell>44.3 62.2</cell></row><row><cell></cell><cell>Box</cell><cell>61.2</cell></row><row><cell></cell><cell>Box i</cell><cell>62.7</cell></row><row><cell>Weakly</cell><cell>MCG</cell><cell>62.6</cell></row><row><cell>supervised</cell><cell>GrabCut+</cell><cell>63.4</cell></row><row><cell></cell><cell>GrabCut+ i</cell><cell>64.3</cell></row><row><cell></cell><cell>M ∩ G+</cell><cell>65.7</cell></row><row><cell cols="2">Fully supervised DeepLab ours [5]</cell><cell>69.1</cell></row></table><note>Weakly supervised semantic labelling results for our baselines. Trained using Pascal VOC12 bounding boxes alone, validation set results. DeepLab ours indicates our fully supervised result.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Semantic labelling results for validation and</cell></row><row><cell>test set; under different training regimes with VOC12</cell></row><row><cell>(V) and COCO data (C). Underline indicates full supervi-</cell></row><row><cell>sion baselines, and bold are our best weakly-and semi-</cell></row><row><cell>supervised results. FS%: performance relative to the best</cell></row><row><cell>fully supervised model (DeepLab ours ). Discussion in Sec-</cell></row><row><cell>tions 4.2 and 4.3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>DeepLabv2-ResNet101 network semantic la-</cell></row><row><cell>belling results on VOC12 validation set, using VOC12 or</cell></row><row><cell>VOC12+COCO training data. FS%: performance relative</cell></row><row><cell>to the full supervision. Discussion in Section 4.3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Instance segmentation results on VOC12 valida- tion set. Underline indicates the full supervision baseline, and bold are our best weak supervision results. Weakly su- pervised DeepMask and DeepLab BOX reach comparable results to full supervision. See Section 6 for details.</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material A. Content</head><p>This supplementary material provides additional quantitative and qualitative results:  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03296</idno>
		<title level="m">The fast bilateral solver</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02106</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densecut: Densely connected crfs for realtime grabcut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and finegrained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to propose objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image segmentation with a bounding box prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Dan Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kraehenbuehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boosting object proposals: From pascal to coco</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Secrets of grabcut and kernel k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Superdifferential cuts for binary energies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.03150</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdelfatah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.03060</idno>
		<title level="m">Loosecut: Interactive image segmentation with loosely bounded boxes</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
