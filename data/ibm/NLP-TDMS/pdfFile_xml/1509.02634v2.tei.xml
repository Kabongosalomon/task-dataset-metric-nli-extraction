<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Image Segmentation via Deep Parsing Network *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Change</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loy</forename><forename type="middle">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Image Segmentation via Deep Parsing Network *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-toend computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy of 77.5%. K k=1 λ k = 1. An intuitive</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Markov Random Field (MRF) or Conditional Random Field (CRF) has achieved great successes in semantic image segmentation, which is one of the most challenging problems in computer vision. Existing works such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref> can be generally categorized into two groups based on their definitions of the unary and pairwise terms of MRF.</p><p>In the first group, researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, * This work has been accepted to appear in ICCV 2015. This is the preprinted version. Content may slightly change prior to the final publication.</p><p>† indicates shared first authorship.</p><p>high-order potentials <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref>, and semantic label contexts <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>. For example, Krähenbühl et al. <ref type="bibr" target="#b15">[16]</ref> attained accurate segmentation boundary by inferring on a fullyconnected graph. Vineet et al. <ref type="bibr" target="#b36">[37]</ref> extended <ref type="bibr" target="#b15">[16]</ref> by defining both high-order and long-range terms between pixels. Global or local semantic contexts between labels were also investigated by <ref type="bibr" target="#b37">[38]</ref>. Although they accomplished promising results, they modeled the unary terms as SVM or Adaboost, whose learning capacity becomes a bottleneck. The learning and inference of complex pairwise terms are often expensive.</p><p>In the second group, people learned a strong unary classifier by leveraging the recent advances of deep learning, such as the Convolutional Neural Network (CNN). With deep models, these works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19]</ref> demonstrated encouraging results using simple definition of the pairwise function or even ignore it. For instance, Long et al. <ref type="bibr" target="#b21">[22]</ref> transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref>. Chen et al. <ref type="bibr" target="#b2">[3]</ref> improved <ref type="bibr" target="#b21">[22]</ref> by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separated components. A recent advance was obtained by <ref type="bibr" target="#b29">[30]</ref>, which jointly trained CNN and MRF by passing the error of MRF inference backward into CNN, but iterative inference of MRF such as the mean field algorithm (MF) <ref type="bibr" target="#b26">[27]</ref> is required for each training image during backpropagation (BP). Zheng et al. <ref type="bibr" target="#b38">[39]</ref> further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar. We found that directly combing CNN and MRF as above is inefficient, because CNN typically has millions of parameters while MRF infers thousands of latent variables; and even worse, incorporating complex pairwise terms into MRF becomes impractical, limiting the performance of the entire system. This work proposes a novel Deep Parsing Network (DPN), which is able to jointly train CNN and complex pairwise terms. DPN has several appealing properties.</p><p>(1) DPN solves MRF with a single feed-forward pass, reducing computational cost and meanwhile maintaining high performance. Specifically, DPN models unary terms by extending the VGG-16 network (VGG <ref type="bibr" target="#b15">16</ref> ) <ref type="bibr" target="#b31">[32]</ref> pretrained on ImageNet, while additional layers are carefully designed to model complex pairwise terms. Learning of these terms is transformed into deterministic end-to-end computation by BP, instead of embedding MF into BP as <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> did. Although MF can be represented by RNN <ref type="bibr" target="#b38">[39]</ref>, it needs to recurrently compute the forward pass so as to achieve good performance and thus is time-consuming, e.g. each forward pass contains hundred thousands of weights. DPN approximates MF by using only one iteration. This is made possible by joint learning strong unary terms and rich pairwise information. (2) Pairwise terms determine the graphical structure. In previous works, if the former is changed, so is the latter as well as its inference procedure. But with DPN, modifying the complexity of pairwise terms, e.g. range of pixels and contexts, is as simple as modifying the receptive fields of convolutions, without varying BP. DPN is able to represent multiple types of pairwise terms, making many previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30]</ref> as its special cases. (3) DPN approximates MF with convolutional and pooling operations, which can be speeded up by lowrank approximation <ref type="bibr" target="#b13">[14]</ref> and easily parallelized <ref type="bibr" target="#b3">[4]</ref> in a Graphical Processing Unit (GPU).</p><p>Our contributions are summarized as below. (1) A novel DPN is proposed to jointly train VGG <ref type="bibr" target="#b15">16</ref> and rich pairwise information, i.e. mixture of label contexts and high-order relations. Compared to existing deep models, DPN can approximate MF with only one iteration, reducing computational cost but still maintaining high performance.</p><p>(2) We disclose that DPN represents multiple types of MRFs, making many previous works such as RNN <ref type="bibr" target="#b38">[39]</ref> and DeepLab <ref type="bibr" target="#b2">[3]</ref>   <ref type="bibr" target="#b15">16</ref> to model unary terms and additional layers are carefully designed for pairwise terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DPN learns MRF by extending VGG</head><p>Overview MRF <ref type="bibr" target="#b9">[10]</ref> is an undirected graph where each node represents a pixel in an image I, and each edge represents relation between pixels. Each node is associated with a binary latent variable, y i u ∈ {0, 1}, indicating whether a pixel i has label u. We have ∀u ∈ L = {1, 2, ..., l}, representing a set of l labels. The energy function of MRF is written as</p><formula xml:id="formula_0">E(y) = ∀i∈V Φ(y u i ) + ∀i,j∈E Ψ(y u i , y v j ),<label>(1)</label></formula><p>where y, V, and E denote a set of latent variables, nodes, and edges, respectively. Φ(y u i ) is the unary term, measuring the cost of assigning label u to the i-th pixel. For instance, if pixel i belongs to the first category other than the second one, we should have Φ(y 1 i ) &lt; Φ(y 2 i ). Moreover, Ψ(y u i , y v j ) is the pairwise term that measures the penalty of assigning labels u, v to pixels i, j respectively.</p><p>Intuitively, the unary terms represent per-pixel classifications, while the pairwise terms represent a set of smoothness constraints. The unary term in Eqn.(1) is typically defined as</p><formula xml:id="formula_1">Φ(y u i ) = − ln p(y u i = 1|I)<label>(2)</label></formula><p>where p(y u i = 1|I) indicates the probability of the presence of label u at pixel i, modeling by VGG <ref type="bibr" target="#b15">16</ref> . To simplify discussions, we abbreviate it as p u i . The smoothness term can be formulated as</p><formula xml:id="formula_2">Ψ(y u i , y v j ) = µ(u, v)d(i, j),<label>(3)</label></formula><p>where the first term learns the penalty of global cooccurrence between any pair of labels, e.g. the output value of µ(u, v) is large if u and v should not coexist, while the second term calculates the distances between pixels, e.g.</p><formula xml:id="formula_3">d(i, j) = ω 1 I i − I j 2 + ω 2 [x i y i ] − [x j y j ] 2 .</formula><p>Here, I i indicates a feature vector such as RGB values extracted from the i-th pixel, x, y denote coordinates of pixels' positions, and ω 1 , ω 2 are the constant weights. Eqn. <ref type="bibr" target="#b2">(3)</ref> implies that if two pixels are close and look similar, they are encouraged to have labels that are compatible. It has been adopted by most of the recent deep models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30]</ref> for semantic image segmentation.</p><p>However, Eqn.</p><p>(3) has two main drawbacks. First, its first term captures the co-occurrence frequency of two labels in the training data, but neglects the spatial context between objects. For example, 'person' may appear beside 'table', but not at its bottom. This spatial context is a mixture of patterns, as different object configurations may appear in different images. Second, it defines only the pairwise relations between pixels, missing their high-order interactions.</p><p>To resolve these issues, we define the smoothness term by leveraging rich information between pixels, which is one of the advantages of DPN over existing deep models. We have</p><formula xml:id="formula_4">Ψ(y u i , y v j ) = K k=1 λ k µ k (i, u, j, v) ∀z∈Nj d(j, z)p v z . (4)</formula><p>The first term in Eqn.(4) learns a mixture of local label contexts, penalizing label assignment in a local region, where K is the number of components in mixture and λ k is an indicator, determining which component is activated.</p><p>We define λ k ∈ {0, 1} and illustration is given in <ref type="figure" target="#fig_0">Fig.1 (b)</ref>, where the dots in red and blue represent a center pixel i and its neighboring pixels j, i.e. j ∈ N i , and (i, u) indicates assigning label u to pixel i.</p><p>Here, µ(i, u, j, v) outputs labeling cost between (i, u) and (j, v) with respect to their relative positions. For instance, if u, v represent 'person' and 'table', the learned penalties of positions j that are at the bottom of center i should be large. The second term basically models a triple penalty, which involves pixels i, j, and j's neighbors, implying that if (i, u) and (j, v) are compatible, then (i, u) should be also compatible with j's nearby pixels (z, v), ∀z ∈ N j , as shown in <ref type="figure" target="#fig_0">Fig.1</ref> (a). Learning parameters (i.e. weights of VGG <ref type="bibr" target="#b15">16</ref> and costs of label contexts) in Eqn.(1) is to minimize the distances between ground-truth label map and y, which needs to be inferred subject to the smoothness constraints.</p><p>Inference Overview Inference of Eqn.(1) can be obtained by the mean field (MF) algorithm <ref type="bibr" target="#b26">[27]</ref>, which estimates the joint distribution of MRF, P (y)= 1 Z exp{−E(y)}, by using a fully-factorized proposal distribution, Q(y) = ∀i∈V ∀u∈L q u i , where each q u i is a variable we need to estimate, indicating the predicted probability of assigning label u to pixel i. To simplify the discussion, we denote Φ(y u i ) and Ψ(y u i , y v j ) as Φ u i and Ψ uv ij , respectively. Q(y) is typically optimized by minimizing a free energy function <ref type="bibr" target="#b14">[15]</ref> of MRF,</p><formula xml:id="formula_5">F (Q) = ∀i∈V ∀u∈L q u i Φ u i + ∀i,j∈E ∀u∈L ∀v∈L q u i q v j Ψ uv ij + ∀i∈V ∀u∈L q u i ln q u i .<label>(5)</label></formula><p>Specifically, the first term in Eqn.(5) characterizes the cost of each pixel's predictions, while the second term characterizes the consistencies of predictions between pixels.</p><p>The last term is the entropy, measuring the confidences of predictions. To estimate q u i , we differentiate Eqn.(5) with respect to it and equate the resulting expression to zero. We then have a closed-form expression,</p><formula xml:id="formula_6">q u i ∝ exp − (Φ u i + ∀j∈Ni ∀v∈L q v j Ψ uv ij ) ,<label>(6)</label></formula><p>such that the predictions for each pixel is independently attained by repeating Eqn. <ref type="bibr" target="#b5">(6)</ref>, which implies whether pixel i have label u is proportional to the estimated probabilities of all its neighboring pixels, weighted by their corresponding smoothness penalties. Substituting Eqn.(4) into (6), we have</p><formula xml:id="formula_7">q u i ∝ exp − Φ u i − K k=1 λ k ∀v∈L ∀j∈Ni (7) µ k (i, u, j, v) ∀z∈Nj d(j, z)q v j q v z ,</formula><p>where each q u i is initialized by the corresponding p u i in Eqn. <ref type="bibr" target="#b1">(2)</ref>, which is the unary prediction of VGG <ref type="bibr" target="#b15">16</ref> . Eqn. <ref type="bibr" target="#b6">(7)</ref> satisfies the smoothness constraints.</p><p>In the following, DPN approximates one iteration of Eqn. <ref type="bibr" target="#b6">(7)</ref> by decomposing it into two steps. Let Q v be a predicted label map of the v-th category. In the first step as shown in <ref type="figure" target="#fig_0">Fig.1 (c)</ref>, we calculate the triple penalty term in <ref type="bibr" target="#b6">(7)</ref> by applying a m × m filter on each position j, where each element of this filter equals d(j, z)q v j , resulting in Q v . Apparently, this step smoothes the prediction of pixel j with respect to the distances between it and its neighborhood. In the second step as illustrated in (d), the labeling contexts can be obtained by convolving Q v with a n × n filter, each element of which equals µ k (i, u, j, v), penalizing the triple relations as shown in (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Parsing Network</head><p>This section describes the implementation of Eq.(7) in a Deep Parsing Network (DPN). DPN extends VGG <ref type="bibr" target="#b15">16</ref> as unary term and additional layers are designed to approximate one iteration of MF inference as the pairwise term. The hyper-parameters of VGG <ref type="bibr" target="#b15">16</ref> and DPN are compared in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>VGG <ref type="bibr" target="#b15">16</ref> As listed in <ref type="table" target="#tab_1">Table 1</ref> (a), the first row represents the name of layer and 'x-y' in the second row represents the size of the receptive field and the stride of convolution, respectively. For instance, '3-1' in the convolutional layer implies that the receptive field of each filter is 3×3 and it is applied on every single pixel of an input feature map, while '2-2' in the max-pooling layer indicates each feature map is pooled over every other pixel within a 2×2 local region. The last three rows show the number of the output feature maps, activation functions, and the size of output </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modeling Unary Terms</head><p>To make full use of VGG <ref type="bibr" target="#b15">16</ref> , which is pre-trained by ImageNet, we adopt all its parameters to initialize the filters of the first ten groups of DPN. To simplify the discussions, we take PASCAL VOC 2012 (VOC12) <ref type="bibr" target="#b6">[7]</ref> as an example. Note that DPN can be easily adapted to any other semantic image segmentation dataset by modifying its hyper-parameters. VOC12 contains 21 categories and each image is rescaled to 512×512 in training. Therefore, DPN needs to predict totally 512×512×21 labels, i.e. one label for each pixel. To this end, we extends VGG 16 in two aspects.</p><p>In particular, let ai and bi denote the i-th group in <ref type="table" target="#tab_1">Table  1</ref> (a) and (b), respectively. First, we increase resolution of VGG <ref type="bibr" target="#b15">16</ref> by removing its max pooling layers at a8 and a10, because most of the information is lost after pooling, e.g. a10 reduces the input size by 32 times, i.e. from 224×224 to 7×7. As a result, the smallest size of feature map in DPN is 64×64, keeping much more information compared with VGG 16 . Note that the filters of b8 are initialized as the filters of a9, but the 3×3 receptive field is padded into 5×5 as shown in <ref type="figure" target="#fig_1">Fig.2 (a)</ref>, where the cells in white are the original values of the a9's filter and the cells in gray are zeros. This is done because a8 is not presented in DPN, such that each filter in a9 should be convolved on every other pixel of a7. To maintain the convolution with one stride, we pad the filters with zeros. Furthermore, the feature maps in b11 are up-sampled to 512×512 by bilinear interpolation. Since DPN is trained with label maps of the entire images, the missing information in the preceding layers of b11 can be recovered by BP. Second, two fully-connected layers at a11 are transformed to two convolutional layers at b9 and b10, respectively. As shown in <ref type="table" target="#tab_1">Table 1</ref> (a), the first 'fc' layer learns 7×7×512×4096 parameters, which can be altered to 4096 filters in b9, each of which is 25×25×512. Since a8 and a10 have been removed, the 7×7 receptive field is padded into 25×25 similar as above and shown in <ref type="figure" target="#fig_1">Fig.2 (b)</ref>. The second 'fc' layer learns a 4096×4096 weight matrix, corresponding to 4096 filters in b10. Each filter is 1×1×4096.</p><p>Overall, b11 generates the unary labeling results, producing twenty-one 512×512 feature maps, each of which represents the probabilistic label map of each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modeling Smoothness Terms</head><p>The last four layers of DPN, i.e. from b12 to b15, are carefully designed to smooth the unary labeling results.</p><p>• b12 As listed in <ref type="table" target="#tab_1">Table 1</ref> (b), 'lconv' in b12 indicates a locally convolutional layer, which is widely used in face recognition <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref> to capture different information from different facial positions. Similarly, distinct spatial positions of b12 have different filters, and each filter is shared across 21 input channels, as shown in <ref type="figure" target="#fig_1">Fig.2 (c)</ref>. It can be formulated as where lin(x) = ax + b representing the linear activation function, ' * ' is the convolutional operator, and k (j,v) is a 50×50×1 filter at position j of channel v. We have k (j,1) = k (j,2) = ... = k (j,21) shared across 21 channels. o 11 (j,v) indicates a local patch in b11, while o 12 (j,v) is the corresponding output of b12. Since b12 has stride one, the result of k j * o 11 (j,v) is a scalar. In summary, b12 has 512×512 different filters and produces 21 output feature maps.</p><formula xml:id="formula_8">o 12 (j,v) = lin(k (j,v) * o 11 (j,v) ),<label>(8)</label></formula><p>Eqn. <ref type="formula" target="#formula_8">(8)</ref> implements the triple penalty of Eqn. <ref type="bibr" target="#b6">(7)</ref>. Recall that each output feature map of b11 indicates a probabilistic label map of a specific object appearing in the image. As a result, Eqn. <ref type="bibr" target="#b7">(8)</ref> suggests that the probability of object v presented at position j is updated by weighted averaging over the probabilities at its nearby positions. Thus, as shown in <ref type="figure" target="#fig_0">Fig.1 (c</ref></p><formula xml:id="formula_9">), o 11 (j,v) corresponds to a patch of Q v centered at j, which has values p v z , ∀z ∈ N 50×50 j . Similarly, k (j,v) is initialized by d(j, z)p v j ,</formula><p>implying each filter captures dissimilarities between positions. These filters remain fixed during BP, other than learned as in conventional CNN 1 .</p><p>• b13 As shown in <ref type="table" target="#tab_1">Table 1</ref> (b) and <ref type="figure" target="#fig_2">Fig.3 (a)</ref>, b13 is a convolutional layer that generates 105 feature maps by using 105 filters of size 9×9×21. For example, the value of (i, u = 1) is attained by applying a 9×9×21 filter at positions {(j, v = 1, ..., 21)}. In other words, b13 learns a filter for each category to penalize the probabilistic label maps of b12, corresponding to the local label contexts in Eqn.(7) by assuming K = 5 and n = 9, as shown in <ref type="figure" target="#fig_0">Fig.1  (d)</ref>.</p><p>• b14 As illustrated in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="figure" target="#fig_2">Fig.3 (b)</ref>, b14 is a block min pooling layer that pools over every 1×1 region with one stride across every 5 input channels, leading to 21 output channels, i.e. 105÷5=21. b14 activates the contextual pattern with the smallest penalty.</p><p>• b15 This layer combines both the unary and smoothness terms by summing the outputs of b11 and b14 in an <ref type="bibr" target="#b0">1</ref> Each filter in b12 actually represents a distance metric between pixels in a specific region. In VOC12, the patterns of all the training images in a specific region are heterogenous, because of various object shapes. Therefore, we initialize each filter with Euclidean distance. Nevertheless, Eqn.(8) is a more general form than the triple penalty in Eqn. <ref type="bibr" target="#b6">(7)</ref>, i.e. filters in <ref type="bibr" target="#b7">(8)</ref> can be automatically learned from data, if the patterns in a specific region are homogenous, such as face or human images, which have more regular shapes than images in VOC12. element-wise manner similar to Eqn. <ref type="bibr" target="#b6">(7)</ref>,</p><formula xml:id="formula_10">o 15 (i,u) = exp ln(o 11 (i,u) ) − o 14 (i,u) 21 u=1 exp ln(o 11 (i,u) ) − o 14 (i,u) ,<label>(9)</label></formula><p>where probability of assigning label u to pixel i is normalized over all the labels.</p><p>Relation to Previous Deep Models Many existing deep models such as <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30]</ref> employed Eqn.(3) as the pairwise terms, which are the special cases of Eqn. <ref type="bibr" target="#b6">(7)</ref>. To see this, let K=1 and j=i, the right hand side of (7) reduces to</p><formula xml:id="formula_11">exp{−Φ u i − v∈L λ 1 µ 1 (i, u, i, v) z∈Ni d(i, z)p v i p v z } = exp{−Φ u i − v∈L µ(u, v) z∈Ni,z =i d(i, z)p v z },<label>(10)</label></formula><p>where µ(u, v) and d(i, z) represent the global label cooccurrence and pairwise pixel similarity of Eqn. <ref type="formula" target="#formula_2">(3)</ref>, respectively. This is because λ 1 is a constant, d(i, i) = 0, and µ(i, u, i, v) = µ(u, v). Eqn. <ref type="formula" target="#formula_0">(10)</ref> is the corresponding MF update equation of (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Algorithms</head><p>Learning The first ten groups of DPN are initialized by VGG <ref type="bibr" target="#b15">16</ref> 2 , while the last four groups can be initialized randomly. DPN is then fine-tuned in an incremental manner with four stages. During fine-tuning, all these stages solve the pixelwise softmax loss <ref type="bibr" target="#b21">[22]</ref>, but updating different sets of parameters.</p><p>First, we add a loss function to b11 and fine-tune the weights from b1 to b11 without the last four groups, in order to learn the unary terms. Second, to learn the triple relations, we stack b12 on top of b11 and update its parameters (i.e. ω 1 , ω 2 in the distance measure), but the weights of the preceding groups (i.e. b1∼b11) are fixed. Third, b13 and b14 are stacked onto b12 and similarly, their weights are updated with all the preceding parameters fixed, so as to learn the local label contexts. Finally, all the parameters are jointly fine-tuned.</p><p>Implementation DPN transforms Eqn. <ref type="bibr" target="#b6">(7)</ref> into convolutions and poolings in the groups from b12 to b15, such that filtering at each pixel can be performed in a parallel manner. Assume we have f input and f output feature maps, N × N pixels, filters with s × s receptive field, and a mini-batch with M samples. b12 takes a total f · N 2 · s 2 · M operations, b13 takes f · f · N 2 · s 2 · M operations, while both b14 and b15 require f · N 2 · M operations. For example, when M =10 as in our experiment, we have 21×512 2 ×50 2 ×10=1.3×10 11 operations in b12, which has the highest complexity in DPN. We parallelize these operations using matrix multiplication on GPU as <ref type="bibr" target="#b3">[4]</ref> did, b12 can be computed within 30ms. The total runtime of the last four layers of DPN is 75ms. Note that convolutions in DPN can be further speeded up by low-rank decompositions <ref type="bibr" target="#b13">[14]</ref> of the filters and model compressions <ref type="bibr" target="#b12">[13]</ref>.</p><p>However, direct calculation of Eqn. <ref type="formula">(7)</ref> is accelerated by fast Gaussian filtering <ref type="bibr" target="#b0">[1]</ref>. For a mini-batch of ten 512×512 images, a recently optimized implementation <ref type="bibr" target="#b15">[16]</ref> takes 12 seconds on CPU to compute one iteration of <ref type="bibr" target="#b6">(7)</ref>. Therefore, DPN makes <ref type="bibr" target="#b6">(7)</ref> easier to be parallelized and speeded up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset We evaluate the proposed approach on the PAS-CAL VOC 2012 (VOC12) <ref type="bibr" target="#b6">[7]</ref> dataset, which contains 20 object categories and one background category. Following previous works such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3]</ref>, we employ 10, 582 images for training, 1, 449 images for validation, and 1, 456 images for testing.</p><p>Evaluation Metrics All existing works employed mean pixelwise intersection-over-union (denoted as mIoU) <ref type="bibr" target="#b21">[22]</ref> to evaluate their performance. To fully examine the effectiveness of DPN, we introduce another three metrics, including tagging accuracy (TA), localization accuracy (LA), and boundary accuracy (BA). (1) TA compares the predicted image-level tags with the ground truth tags, calculating the accuracy of multi-class image classification. (2) LA evaluates the IoU between the predicted object bounding boxes 3 and the ground truth bounding boxes (denoted as bIoU), measuring the precision of object localization. (3) For those objects that have been correctly localized, we compare the predicted object boundary with the ground truth boundary, measuring the precision of semantic boundary similar to <ref type="bibr" target="#b11">[12]</ref>.</p><p>Comparisons DPN is compared with the bestperforming methods on VOC12, including FCN <ref type="bibr" target="#b21">[22]</ref>, Zoom-out <ref type="bibr" target="#b24">[25]</ref>, DeepLab <ref type="bibr" target="#b2">[3]</ref>, WSSL <ref type="bibr" target="#b27">[28]</ref>, BoxSup <ref type="bibr" target="#b4">[5]</ref>, Piecewise <ref type="bibr" target="#b18">[19]</ref>, and RNN <ref type="bibr" target="#b38">[39]</ref>. All these methods are based on CNNs and MRFs, and trained on VOC12 data following <ref type="bibr" target="#b21">[22]</ref>. They can be grouped according to different aspects: (1) joint-train: Piecewise and RNN; (2) w/o joint-train: DeepLab, WSSL, FCN, and BoxSup; (3) pretrain on COCO: RNN, WSSL, and BoxSup. The first and the second groups are the methods with and without joint training CNNs and MRFs, respectively. Methods in the last group also employed MS-COCO <ref type="bibr" target="#b19">[20]</ref> to pre-train deep models. To conduct a comprehensive comparison, the performance of DPN are reported on both settings, i.e., with and without pre-training on COCO.</p><p>In the following, Sec.4.1 investigates the effectiveness of different components of DPN on the VOC12 validation set. <ref type="bibr" target="#b2">3</ref> They are the bounding boxes of the predicted segmentation regions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effectiveness of DPN</head><p>All the models evaluated in this section are trained and tested on VOC12.</p><p>Triple Penalty The receptive field of b12 indicates the range of triple relations for each pixel. We examine different settings of the receptive fields, including '10×10', '50×50', and '100×100', as shown in <ref type="table" target="#tab_3">Table 2</ref> (a), where '50×50' achieves the best mIoU, which is sightly better than '100×100'. For a 512×512 image, this result implies that 50×50 neighborhood is sufficient to capture relations between pixels, while smaller or larger regions tend to under-fit or over-fit the training data. Moreover, all models of triple relations outperform the 'baseline' method that models dense pairwise relations, i.e. VGG 16 +denseCRF <ref type="bibr" target="#b15">[16]</ref>.</p><p>Label Contexts Receptive field of b13 indicates the range of local label context. To evaluate its effectiveness, we fix the receptive field of b12 as 50×50. As summarized in <ref type="table" target="#tab_3">Table 2</ref> (b), '9×9 mixtures' improves preceding settings by 1.7, 0.5, and 0.2 percent respectively. We observe large gaps exist between '1×1' and '5×5'. Note that the 1×1 receptive field of b13 corresponds to learning a global label co-occurrence without considering local spatial contexts. More importantly, mIoU of all the categories can be improved when increasing the size of receptive field and learning a mixture. Specifically, for each category, the improvements of the last three settings in <ref type="table" target="#tab_3">Table 2</ref> (b) over the first one are 1.2±0.2, 1.5±0.2, and 1.7±0.3, respectively.</p><p>We also visualize the learned label compatibilities and contexts in <ref type="figure" target="#fig_4">Fig.4 (a)</ref>   <ref type="table">bird  boat  bottle  bus  car  cat  chair  cow  table  dog  horse   sheep   mbike  person  plant   bkg  areo  bike  bird  boat  bottle  bus  car  cat  chair  cow  table  dog  horse  mbike  person  plant  sheep  sofa</ref>   is non-symmetry. For example, when 'horse' is presented, 'person' is more likely to present than the other objects. Also, 'chair' is compatible with 'table' and 'bkg' is compatible with all the objects. (b) visualizes some contextual patterns, where 'A:B' indicates that when 'A' is presented, where 'B' is more likely to present. For example, 'bkg' is around 'train', 'motor bike' is below 'person', and 'person' is sitting on 'chair'. Incremental Learning As discussed in Sec.3.3, DPN is trained in an incremental manner. The right hand side of Table 3 (a) demonstrates that each stage leads to performance gain compared to its previous stage. For instance, 'triple penalty' improves 'unary term' by 2.3 percent, while 'label contexts' improves 'triple penalty' by 1.8 percent. More importantly, joint fine-tuning all the components (i.e. unary terms and pairwise terms) in DPN achieves another gain of 1.3 percent. A step-by-step visualization is provided in <ref type="figure" target="#fig_5">Fig.5</ref>.</p><p>We also compare 'incremental learning' with 'joint learning', which fine-tunes all the components of DPN at the same time. The training curves of them are plotted in <ref type="figure">Fig.6 (a)</ref>, showing that the former leads to higher and more stable accuracies with respect to different iterations, while the latter may get stuck at local minima. This difference is easy to understand, because incremental learning only introduces new parameters until all existing parameters  have been fine-tuned.</p><p>One-iteration MF DPN approximates one iteration of MF. <ref type="figure">Fig.6 (b)</ref> illustrates that DPN reaches a good accuracy with one MF iteration. A CRF <ref type="bibr" target="#b15">[16]</ref> with dense pairwise edges needs more than 5 iterations to converge. It also has a large gap compared to DPN. Note that the existing deep models such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30]</ref> required 5∼10 iterations to converge as well.</p><p>Different Components Modeling Different Information We further evaluate DPN using three metrics. The results are given in <ref type="figure" target="#fig_6">Fig.7</ref>. For example, (a) illustrates that the tagging accuracy can be improved in the third stage, as it captures label co-occurrence with a mixture of contextual patterns. However, TA is decreased a little after the final stage. Since joint tuning maximizes segmentation accuracies by optimizing all components together, extremely small objects, which rarely occur in VOC training set, are discarded. As shown in (b), accuracies of object localization are significantly improved in the second and the final stages. This is intuitive because the unary prediction can be refined by long-range and high-order pixel relations, and joint training further improves results. (c) discloses that the second stage also captures object boundary, since it measures dissimilarities between pixels.</p><p>Per-class Analysis  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Overall Performance</head><p>As shown in <ref type="table" target="#tab_6">Table 3</ref> (b), we compare DPN with the best-performing methods 5 on VOC12 test set based on two settings, i.e. with and without pre-training on COCO. The approaches pre-trained on COCO are marked with ' †'. We evaluate DPN on several scales of the images and then average the results following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>DPN outperforms all the existing methods that were trained on VOC12, but DPN needs only one MF iteration to solve MRF, other than 10 iterations of RNN, DeepLab, and Piecewise. By averaging the results of two DPNs, we achieve 74.1% accuracy on VOC12 without outside training data. As discussed in Sec.3.3, MF iteration is the most complex step even when it is implemented as convolutions. Therefore, DPN at least reduces 10× runtime compared to <ref type="bibr" target="#b4">5</ref> The results of these methods were presented in either the published papers or arXiv pre-prints. previous works.</p><p>Following <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref>, we pre-train DPN with COCO, where 20 object categories that are also presented in VOC12 are selected for training. A single DPN † has achieved 77.5% mIoU on VOC12 test set. As shown in <ref type="table" target="#tab_6">Table 3</ref> (b), we observe that DPN † achieves best performances on more than half of the object classes. Please refer to the appendices for visual quality comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed Deep Parsing Network (DPN) to address semantic image segmentation, which has several appealing properties. First, DPN unifies the inference and learning of unary term and pairwise terms in a single convolutional network. No iterative inference are required during backpropagation. Second, high-order relations and mixtures of label contexts are incorporated to its pairwise terms modeling, making existing works serve as special cases. Third, DPN is built upon conventional operations of CNN, thus easy to be parallelized and speeded up.</p><p>DPN achieves state-of-the-art performance on VOC12, and multiple valuable facts about semantic image segmention are revealed through extensive experiments. Future directions include investigating the generalizability of DPN to more challenging scenarios, e.g. large number of object classes and substantial appearance/scale variations.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Illustration of the pairwise terms in DPN. (b) explains the label contexts. (c) and (d) show that mean field update of DPN corresponds to convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) and (b) show the padding of the filters. (c) illustrates local convolution of b12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) and (b) illustrates the convolutions of b13 and the poolings in b14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and (b), respectively. (a) is obtained by summing each filter in b13 over 9×9 region, indicating how likely a column object would present when a row object is presented. Blue represents high possibility. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of (a) learned label compatibility (b) learned contextual information. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Step-by-step visualization of DPN. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Stage-wise analysis of (a) mean tagging accuracy (b) mean localization accuracy (c) mean boundary accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Visual quality comparison of different semantic image segmentation methods: (a) input image (b) ground truth (c) FCN [22] (d) DeepLab [3] and (e) DPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Visual quality of DPN label maps: (a) input image (b) ground truth (white labels indicating ambiguous regions) and (c) DPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>as its special cases. (3) Extensive experiments investigate which component of DPN is crucial to achieve high performance. A single DPN model achieves a new state-of-the-art accuracy of 77.5% on the PASCAL VOC 2012 [7] test set. (4) We analyze the time complexity of DPN on GPU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>)Table 1 :</head><label>1</label><figDesc>VGG16: 224×224×3 input image; 1×1000 output labels DPN: 512×512×3 input image; 512×512×21 output label maps The comparisons between the network architectures of VGG<ref type="bibr" target="#b15">16</ref> and DPN, as shown in (a) and (b) respectively. Each table contains five rows, representing the 'name of layer', 'receptive field of filter'−'stride', 'number of output feature maps', 'activation function' and 'size of output feature maps', respectively. Furthermore, 'conv', 'lconv','max', 'bmin', 'fc', and 'sum' represent the convolution, local convolution, max pooling, block min pooling, fully connection, and summation, respectively. Moreover, 'relu', 'idn', 'soft', 'sigm', and 'lin' represent the activation functions, including rectified linear unit<ref type="bibr" target="#b17">[18]</ref>, identity, softmax, sigmoid, and linear, respectively. feature maps, respectively. As summarized inTable 1(a), VGG<ref type="bibr" target="#b15">16</ref> contains thirteen convolutional layers, five maxpooling layers, and three fully-connected layers. These layers can be partitioned into twelve groups, each of which covers one or more homogenous layers. For example, the first group comprises two convolutional layers with 3×3 receptive field and 64 output feature maps, each of which is 224×224.</figDesc><table><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer</cell><cell>2×conv</cell><cell>max</cell><cell>2×conv</cell><cell>max</cell><cell>3×conv</cell><cell>max</cell><cell>3×conv</cell><cell>max</cell><cell>3×conv</cell><cell>max</cell><cell>2×fc</cell><cell>fc</cell><cell></cell><cell></cell><cell></cell></row><row><cell>filter-stride</cell><cell>3-1</cell><cell>2-2</cell><cell>3-1</cell><cell>2-2</cell><cell>3-1</cell><cell>2-2</cell><cell>3-1</cell><cell>2-2</cell><cell>3-1</cell><cell>2-2</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>#channel</cell><cell>64</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>256</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>1</cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>activation</cell><cell>relu</cell><cell>idn</cell><cell>relu</cell><cell>idn</cell><cell>relu</cell><cell>idn</cell><cell>relu</cell><cell>idn</cell><cell>relu</cell><cell>idn</cell><cell>relu</cell><cell>soft</cell><cell></cell><cell></cell><cell></cell></row><row><cell>size</cell><cell>224</cell><cell>112</cell><cell>112</cell><cell>56</cell><cell>56</cell><cell>28</cell><cell>28</cell><cell>14</cell><cell>14</cell><cell>7</cell><cell>4096</cell><cell>1000</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">(b) 1 2 3 4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell></row><row><cell>layer</cell><cell>2×conv</cell><cell>max</cell><cell>2×conv</cell><cell>max</cell><cell>3×conv</cell><cell>max</cell><cell>3×conv</cell><cell>3×conv</cell><cell>conv</cell><cell>conv</cell><cell>conv</cell><cell>lconv</cell><cell>conv</cell><cell>bmin</cell><cell>sum</cell></row><row><cell>filter-stride</cell><cell>3-1</cell><cell>2-2</cell><cell>3-1</cell><cell>2-2</cell><cell>3-1</cell><cell>2-2</cell><cell>3-1</cell><cell>5-1</cell><cell>25-1</cell><cell>1-1</cell><cell>1-1</cell><cell>50-1</cell><cell>9-1</cell><cell>1-1</cell><cell>1-1</cell></row><row><cell>#channel</cell><cell>64</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>256</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>4096</cell><cell>4096</cell><cell>21</cell><cell>21</cell><cell>105</cell><cell>21</cell><cell>21</cell></row><row><cell>activation</cell><cell>relu</cell><cell>idn</cell><cell>relu</cell><cell>idn</cell><cell>relu</cell><cell>idn</cell><cell>relu</cell><cell>relu</cell><cell>relu</cell><cell>relu</cell><cell>sigm</cell><cell>lin</cell><cell>lin</cell><cell>idn</cell><cell>soft</cell></row><row><cell>size</cell><cell>512</cell><cell>256</cell><cell>256</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row></table><note>(a</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of hyper-parameters.</figDesc><table><row><cell>Sec.4.2 compares DPN with the state-of-the-art methods on</cell></row><row><cell>the VOC12 test set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 (</head><label>2</label><figDesc>c) shows that the pairwise terms of DPN are more effective than DSN and DeepLab 4 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>(a) reports the per-class accuracies of four evaluation metrics, where the first four rows represent the mIoU of four stages, while the last three rows represent TA, LA, and BA, respectively. We have several valuable observations, which motivate future researches. (1) Joint training benefits most of the categories, except animals such as 'bird', 'cat', and 'cow'. Some instances of these categories are extremely small so that areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv Avg. Term (mIoU) 77.5 34.1 76.2 58.3 63.3 78.1 72.5 76.5 26.6 59.9 40.8 70.0 62.9 69.3 76.3 39.2 70.4 37.6 72.5 57.3 62.4 + Triple Penalty 82.3 35.9 80.6 60.1 64.8 79.5 74.1 80.9 27.9 63.5 40.4 73.8 66.7 70.8 79.0 42.0 74.1 39.1 73.2 58.5 64.7 + Label Contexts 83.2 35.6 82.6 61.6 65.5 80.5 74.3 82.6 29.9 67.9 47.5 75.2 70.3 71.4 79.6 42.7 77.8 40.6 75.3 59.1 66.5 + Joint Tuning 84.8 37.5 80.7 66.3 67.5 84.2 76.4 81.5 33.8 65.8 50.4 76.8 67.1 74.9 81.1 48.3 75.9 41.8 76.6 60.4 67.8 TA (tagging Acc.) 98.8 97.9 98.4 97.7 96.1 98.6 95.2 96.8 90.1 97.5 95.7 96.7 96.3 98.1 93.3 96.1 98.7 92.2 97.4 96.3 96.4 LA (bIoU) 81.7 76.3 75.5 70.3 54.4 86.4 70.6 85.6 51.8 79.6 57.1 83.3 79.2 80.0 74.1 53.1 79.1 68.4 76.3 58.8 72.1 BA (boundary Acc.) 95.9 83.9 96.9 92.6 93.8 94.0 95.7 95.6 89.5 93.3 91.4 95.2 94.2 92.7 94.5 90.4 94.8 90.5 93.7 96.6 93.3 (a) Per-class results on VOC12 val. 61.6 87.7 66.8 74.7 91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8 85.6 85.4 63.6 87.3 61.3 79.4 66.4 77.5 (b) Per-class results on VOC12 test. The approaches pre-trained on COCO [20] are marked with † .</figDesc><table><row><cell cols="2">Unary areo bike bird boat bottle bus car</cell><cell>cat</cell><cell>chair cow table dog horse mbike person plant sheep sofa train tv</cell><cell>mIoU</cell></row><row><cell>FCN [22]</cell><cell cols="4">76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2</cell></row><row><cell>Zoom-out [25]</cell><cell cols="4">85.6 37.3 83.2 62.5 66.0 85.1 80.7 84.9 27.2 73.2 57.5 78.1 79.2 81.1 77.1 53.6 74.0 49.2 71.7 63.3 69.6</cell></row><row><cell>Piecewise [19]</cell><cell cols="4">87.5 37.7 75.8 57.4 72.3 88.4 82.6 80.0 33.4 71.5 55.0 79.3 78.4 81.3 82.7 56.1 79.8 48.6 77.1 66.3 70.7</cell></row><row><cell>DeepLab [3]</cell><cell cols="4">84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6</cell></row><row><cell>RNN [39]</cell><cell cols="4">87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0</cell></row><row><cell>WSSL  † [28]</cell><cell cols="4">89.2 46.7 88.5 63.5 68.4 87.0 81.2 86.3 32.6 80.7 62.4 81.0 81.3 84.3 82.1 56.2 84.6 58.3 76.2 67.2 73.9</cell></row><row><cell>RNN  † [39]</cell><cell cols="4">90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7</cell></row><row><cell>BoxSup  † [5]</cell><cell cols="4">89.8 38.0 89.2 68.9 68.0 89.6 83.0 87.7 34.4 83.6 67.1 81.5 83.7 85.2 83.5 58.6 84.9 55.8 81.2 70.7 75.2</cell></row><row><cell>DPN</cell><cell cols="4">87.7 59.4 78.4 64.9 70.3 89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0 83.5 82.3 60.5 83.2 53.4 77.9 65.0 74.1</cell></row><row><cell>DPN  †</cell><cell>89.0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Per-class results on VOC12.joint training discards them for smoother results.<ref type="bibr" target="#b1">(2)</ref> Training DPN with pixelwise label maps implicitly models image-level tags, since it achieves a high averaged TA of 96.4%. (3) Object localization always helps. However, for the object with complex boundary such as 'bike', its mIoU is low even it can be localized, e.g. 'bike' has high LA but low BA and mIoU. (4) Failures of different categories have different factors. With these three metrics, they can be easily identified. For example, the failures of 'chair', 'table', and 'plant' are caused by the difficulties to accurately capture their bounding boxes and boundaries. Although 'bottle' and 'tv' are also difficult to localize, they achieve moderate mIoU because of their regular shapes. In other words, mIoU of 'bottle' and 'tv' can be significantly improved if they can be accurately localized.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the released VGG 16 model, which is public available at http://www.robots.ox.ac.uk/˜vgg/research/very_ deep/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The other deep models such as RNN and Piecewise did not report the exact imrprovements after combining unary and pairwise terms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://dl.caffe.berkeleyvision.org/ fcn-8s-pascal.caffemodel</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A. Fast Implementation of Locally Convolution b12 in DPN is a locally convolutional layer. As mentioned in Eqn.(3), the local filters in b12 are computed by the distances between RGB values of the pixels. XY coordinates are omitted here because they could be precomputed. To accelerate the computation of locally convolution, lookup table-based filtering approach is employed. We first construct a lookup table storing distances between any two pixel intensities (ranging from 0 to 255), which results in a 256 × 256 matrix. Then when we perform locally convolution, the kernels' coefficients can be obtained efficiently by just looking up the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Quality Comparisons</head><p>In the following, we inspect visual quality of obtained label maps. <ref type="figure">Fig.8</ref> demonstrates the comparisons of DPN with FCN <ref type="bibr" target="#b21">[22]</ref> and DeepLab <ref type="bibr" target="#b2">[3]</ref>. We use the publicly released model 6 to re-generate label maps of FCN while the results of DeepLab are extracted from their published paper. DPN generally makes more accurate predictions in both image-level and instance-level.</p><p>We also include more examples of DPN label maps in <ref type="figure">Fig.9</ref>. We observe that learning local label contexts helps differentiate confusing objects and learning triple penalty facilitates the capturing of intrinsic object boundaries.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast high-dimensional filtering using the permutohedral lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">cudnn: Efficient primitives for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01640v2</idno>
		<imprint>
			<date type="published" when="2015-05-18" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient belief propagation for early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Class segmentation and object localization with superpixel neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="670" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected crfs with gaussian edge potentials. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="513" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013v2</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Nonparametric scene parsing via label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2368" to="2382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2480" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pedestrian parsing via deep decompositional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2648" to="2655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">From naive mean field theory to the tap equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734v2</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351v1</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning crfs using graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="582" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Posefield: An efficient mean-field based method for joint estimation of human pose, segmentation, and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sheasby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="180" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Filter-based meanfield inference for random fields with higher-order terms and product label-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context driven scene parsing with attention to rare classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3294" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03240v2</idno>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
