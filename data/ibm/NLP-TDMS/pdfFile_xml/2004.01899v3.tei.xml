<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefei</forename><surname>Ning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Weixin Group</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<email>yu-wang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Neural architecture search (NAS), Predictor-based NAS</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a novel Graph-based neural ArchiTecture Encoding Scheme, a.k.a. GATES, to improve the predictor-based neural architecture search. Specifically, different from existing graphbased schemes, GATES models the operations as the transformation of the propagating information, which mimics the actual data processing of neural architecture. GATES is a more reasonable modeling of the neural architectures, and can encode architectures from both the "operation on node" and "operation on edge" cell search spaces consistently. Experimental results on various search spaces confirm GATES's effectiveness in improving the performance predictor. Furthermore, equipped with the improved performance predictor, the sample efficiency of the predictor-based neural architecture search (NAS) flow is boosted. Codes are available at https://github.com/walkerning/aw_nas.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, Neural Architecture Search (NAS) has received extensive attention due to its capability to discover neural network architectures in an automated manner. Substantial studies have shown that the automatically discovered architectures by NAS are able to achieve highly competitive performance.</p><p>Generally speaking, there are two key components in a NAS framework, the architecture searching module and the architecture evaluation module. Specifically, the architecture evaluation module provides the signals of the architecture performance, e.g., accuracy, latency, etc., which are then used by the architecture searching module to explore architectures in the search space. In the seminal work of <ref type="bibr" target="#b29">[30]</ref>, the architecture evaluation is conducted by training every candidate architecture until convergence, and thousands of architectures need to be evaluated during the architecture search process. As a result, the computational burden of the whole NAS process is extremely large. There are two directions to address this issue, which focus on improving the searching and evaluation module, respectively. 1) Evaluation: accelerating the evaluation of each individual architecture, and in the meanwhile, keep the evaluation meaningful in the sense of ranking correlation; 2) Searching: increasing the sample efficiency so that fewer architectures are needed to be evaluated for discovering a good architecture.</p><p>To improve the sample efficiency of the architecture searching module, a promising idea is to learn an approximated performance predictor, and then utilize the predictor to sample architectures that are more worth evaluating. We refer to these NAS methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref> as the predictor-based NAS methods, and their general flow will be introduced in Sec. 3.1. The generalization ability of the predictor is crucial to the sample efficiency of predictor-based NAS flows. Our work follows the line of research of predictor-based NAS, and focus on improving the performance predictor of neural architectures.</p><p>A performance predictor predicts the performance of architectures based on the encoding of them. Existing neural architecture encoding schemes include the sequence-based scheme and the graph-based scheme. The sequencebased schemes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> rely on specific serialization of the architecture. They model the topological information only implicitly, which deteriorates the representational power and interpretability of the predictor. Existing graph-based schemes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> usually apply graph convolutional networks (GCN) <ref type="bibr" target="#b7">[8]</ref> to encode the neural architectures. For the "operation on node" (OON) search spaces, in which the operations (e.g., Conv3x3) are on the nodes of the directed acyclic graph (DAG), GCN can be directly applied to encode architectures. Nevertheless, since a neural architecture is a "data processing" graph, where the operations behave as the data processing functions (e.g., Conv3x3, MaxPool), existing methods' modeling of operations as the node attributes in OON search spaces is not suitable. Instead of modeling the operations as node attributes, a more natural solution is to treat them as the transforms of the node attributes (i.e., mimic the processing of the information). On the other hand, for the "operation on edge" (OOE) search spaces, <ref type="bibr" target="#b3">4</ref> the handling of edge information in the existing graph-based scheme <ref type="bibr" target="#b4">[5]</ref> is even more unsatisfying regarding its poor generalizability and flawed handling of architecture isomorphism.</p><p>In this work, we propose a general encoding scheme: Graph-based neural Ar-chiTecture Encoding Scheme (GATES), which is suitable for the representation learning of data processing graphs such as neural architectures. Specifically, to encode a neural architecture, GATES models the information flow of the actual data processing of the architecture. First, GATES models the input information as the attributes of the input nodes. And the input information will be propagated along the architecture DAG. The data processing of the operations (e.g., Conv3x3, MaxPool) are modeled by GATES as different transforms of the information. Finally, the output information is used as the embedding of the cell architecture. Since the encoding process of GATES mimics the actual computation flow of the architectures, GATES intrinsically maps isomorphic architectures to the same representation. Moreover, GATES can encode architectures from both the OON and OOE cell search spaces in a consistent way. Due to the superior representational ability of GATES, the generalization ability of the architecture performance predictor using GATES is significantly better than other encoders. Experimental results confirm that GATES is effective in improving the architecture performance predictors. Furthermore, by utilizing the improved performance predictor, the sample efficiency of the NAS process is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture Evaluation Module</head><p>One commonly used technique to accelerate architecture evaluation is parameter sharing <ref type="bibr" target="#b16">[17]</ref>, where a super-net is constructed such that all architectures in the search space share a superset of weights and the training costs of architectures are amortized to an "one-shot" super-net training. Parameter sharing dramatically reduces the computational burden and is widely used by recent methods. However, recent studies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref> find that the ranking of architecture candidates with parameter sharing does not reflect their true rankings well, which dramatically affects the effectiveness of the NAS algorithm. Moreover, the parameter sharing technique is not generally applicable, since it is difficult to construct the supernet for some search spaces, for example, in NAS-Bench-101 <ref type="bibr" target="#b27">[28]</ref>, one operation can have different output dimensions in different candidate architectures. Due to these limitations, this work does not use the parameter sharing technique, and focus on improving the sample efficiency of the architecture searching module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture Searching Module</head><p>To improve the sample efficiency of the architecture search module, a variety of search strategies have been used, e.g., RL-based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4]</ref>, Evolutionary methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>, gradient-based method <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref>, Monte Carlo Tree Search (MCTS) method <ref type="bibr" target="#b15">[16]</ref>, etc.</p><p>A promising direction to improve the sample efficiency of NAS is to utilize a performance predictor to sample new architectures, a.k.a. predictor-based NAS. An early study <ref type="bibr" target="#b9">[10]</ref> trains a surrogate model (predictor) to identify promising architectures with increasing complexity. NASBot <ref type="bibr" target="#b5">[6]</ref> design a distance metric in the architecture space and exploits gaussian process to get the posterior of the architecture performances. Then, it samples new architectures based on the acquisition function calculated using the posterior. NAO <ref type="bibr" target="#b14">[15]</ref> trains an LSTMbased autoencoder together with a performance predictor based on the latent representation. After updating the latent representation following the predictor's gradients, NAO decodes the latent representation to sample new architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural Architecture Encoders</head><p>Existing neural architecture encoding schemes include the sequence-based and the graph-based schemes. In the sequence based scheme, the neural architecture is flattened into a string encoding the architecture decisions, then encoded using either an LSTM <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> or a Multi-Layer Perceptron (MLP) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. In these methods, the topological information could only be modeled implicitly, which deteriorates the encoder's representational ability. Also, the search efficiency would deteriorate since these encoders could not guarantee to map isomorphic architectures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23]</ref> to the same representation, and data augmentation and regularization tricks are utilized to alleviate this issue <ref type="bibr" target="#b14">[15]</ref>.</p><p>Recently, the graph-based encoding scheme that utilizes the topological information explicitly has been used to get better performance. In these graph-based schemes, graph convolutional networks (GCN) <ref type="bibr" target="#b7">[8]</ref> are usually used to embed the graphs to fixed-length vector representations. For the "operation on node" search spaces, in which the operations (e.g., Conv3x3) are on the nodes of the DAG, GCN can be directly applied <ref type="bibr" target="#b21">[22]</ref> to encode architectures, i.e., using adjacency matrix and operation embedding of each node as the input. However, for the "operation on edge" search spaces, in which the operations are on the edges, GCN cannot be applied directly. A recent study <ref type="bibr" target="#b4">[5]</ref> proposes an ad-hoc solution for the ENAS search space. They represent each node by the concatenation of the operation embeddings on the input edges. This solution is contrived and cannot generalized to search spaces where nodes could have different input degrees. Moreover, since the concatenation is not commutative, this encoding scheme could not handle isomorphic architectures correctly. In brief, existing graphbased encoding schemes are specific to different search spaces, and a generic approach for encoding the neural architectures is desirable in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predictor-Based Neural Architecture Search</head><p>The principle of predictor-based NAS is to increase the sample efficiency of the NAS process, by utilizing an approximated performance predictor to sample architectures that are more worth evaluating. Generally speaking, the flow of predictor-based NAS could be summarized as in Alg. 1 and <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In line 6 of Alg. 1, the architecture candidates are sampled based on the approximated evaluation of the predictor. Utilizing a more accurate predictor, we could choose better architectures for further evaluation. The better the generalization ability of the predictor is, the fewer architectures are needed to be exactly evaluated to get a highly accurate predictor. Therefore, the generalization ability of the predictor is crucial for the efficiency and effectiveness of the NAS method.</p><p>The model design (i.e., how to encode the neural architectures) of the predictor is crucial to its generalization ability. We'll introduce our main effort to improve the predictor from the "model design" aspect in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GATES: A Generic Neural Architecture Encoder</head><p>A performance predictor P is a model that takes a neural architecture a as input, and outputs a predicted scoreŝ. Usually, the performance predictor is constructed by an encoder followed by an MLP, as shown in Eq. 1. The encoder Enc Evaluate</p><formula xml:id="formula_0">architectures in S (k) , getS (k) = {(a (k) j , y (k) j )} j=1,··· ,N (k) (y is the per- formance) 8:</formula><p>Optimizing P using the ground-truth architecture evaluation dataS = ∪ k i=1S (i) 9: end while 10: Output aj * ∈ ∪ k i=1 S (i) with best corresponding yj * ; Or, a * = argmax a∈A P(a) maps a neural architecture into a continuous embedding space, and its design is vital to the generalization ability of the performance predictor. Existing encoders include the sequence-based ones (e.g., MLP, LSTM) and the graph-based ones (e.g., GCN). We design a new graph-based neural architecture encoder GATES that is more suitable for modeling neural architectures.</p><formula xml:id="formula_1">s = P(a) = MLP(Enc(a))<label>(1)</label></formula><p>To encode a cell architecture into an embedding vector, GATES follows the ideology of modeling the information flow in the architecture, and uses the output information as the embedding of the architecture. The notations are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Specifically, we models the input information as the embedding of the input nodes E ∈ R ni×hi , where n i is the number of input nodes, and h i is the embedding size of the information. The information (embedding of the input nodes) is then "processed" by the operations and "propagates" along the DAG.  The encoding process of GATES goes as follows: Upon each unary operation o (e.g., Conv3x3, MaxPool, etc.), the input information x in of this operation is processed by a linear transform W x and then elementwise multiplied with a soft</p><formula xml:id="formula_2">attention mask m = σ(EMB(o)W o ) ∈ R 1×hi . x out = m x in W x<label>(2)</label></formula><p>where denotes the elementwise multiplication. And the mask m is calculated from the operation embedding</p><formula xml:id="formula_3">EMB(o) = onehot(o) T EMB ∈ R 1×ho .</formula><p>Multiple pieces of information are aggregated at each node using summation. Finally, after obtaining the virtual information at all the nodes, the information at the output node is used as the embedding of the entire cell architecture. For search spaces with multiple cells (e.g., normal and reduce cells in ENAS), GATES encodes each cell independently, and concatenate the embeddings of cells as the embedding of the architecture. <ref type="figure" target="#fig_8">Fig. 2</ref> illustrates two examples of the encoding process in the OON and OOE search spaces. As can be seen, the encoding process of GATES mimics the actual feature map computation. For example, in the example of the OON search space, the actual feature map computation at node 2 is F 2 = Conv3x3(F 0 + F 1 ), Feature map computation:</p><formula xml:id="formula_4">• ! : Input feature map • " = Conv1x1( ! ) • # = MaxPool ( ! ) + AvgPool ( " ) • $ = Conv1x1( " ) + Conv3x3( # ) • % = Aggregate( " , # , $ )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operation on Edge</head><formula xml:id="formula_5">1 2 3 4 0 = σ EMB CONV1x1 ! ⨀ ! " Conv1x1 Conv3x3 σ(EMB CONV3x3 ! ) ⨀( # + $ ) " MaxPool σ(EMB(MaxPool) !) ⨀( $+ %) "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operation on Node</head><formula xml:id="formula_6">N ! N ! N " N " N # N " N $ Output Sum( " , $ ) σ(EMB CONV1x1 !) ⨀ # " σ(EMB CONV1x1 ! ) ⨀ $ " + σ(EMB CONV3x3 ! ) ⨀ % "</formula><p>Feature map computation:</p><formula xml:id="formula_7">• ! : Input feature map • " = Conv1x1( ! ) • # = Conv3x3( ! + " ) • $ = MaxPool( " + # ) • % = Aggregate( " , $ ) 1 2 3 4 0 Conv 1x1 MaxPool Conv 1x1 AvgPool = Conv 3x3 Output Sum( " , # , $ ) σ(EMB MaxPool !) ⨀ # "+ σ(EMB AvgPool !) ⨀ $ "</formula><p>GATES encoding process (4 steps):</p><formula xml:id="formula_8">• ! : Input information E • " = σ(EMB CONV1x1 &amp; ) ⨀ ! ' • # = σ(EMB CONV3x3 &amp; ) ⨀( ! + " ) ' • $ = σ(EMB(MaxPool) &amp; ) ⨀( " + # ) ' • % = Sum( " , $ )</formula><p>GATES encoding process (4 steps):</p><formula xml:id="formula_9">• ! : Input information E • " = σ(EMB CONV1x1 &amp; ) ⨀ ! ' • # = σ(EMB MaxPool &amp; ) ⨀ ! ' + σ EMB AvgPool &amp; ⨀ " ' • $ = σ(EMB CONV1x1 &amp; ) ⨀ " ' + σ EMB CONV3x3 &amp; ⨀ # ' • % = Aggregate( " , # , $ ) Fig. 2.</formula><p>Feature map (Fi) computation and GATES encoding process (Ni). Left: The "operation on node" cell search space, where operations (e.g., Conv3x3) are on the nodes of the DAG (e.g., NAS-Bench-101 <ref type="bibr" target="#b27">[28]</ref>, randomly wired search space <ref type="bibr" target="#b25">[26]</ref>). Right: The "operation on edge" cell search space, where operations are on the edges of the DAG. (e.g., NAS-Bench-201 <ref type="bibr" target="#b2">[3]</ref>, ENAS <ref type="bibr">[17])</ref> where F i is the feature map at node i. To model the information processing of this feature map computation, GATES calculates the information (node embedding) at node 2 by</p><formula xml:id="formula_10">N 2 = σ(EMB(Conv3x3)W o ) (N 0 + N 1 )W x , where σ(·)</formula><p>is the sigmoid function, and W o ∈ R ho×hi is a transformation matrix that transforms the h o -dim operation embedding into a h i -dim feature. That is to say, the summation of feature maps F 0 + F 1 corresponds to the summation of the virtual information N 0 + N 1 , and the data processing function o(·) (Conv3x3) corresponds to a transform f (·) that processes the information</p><formula xml:id="formula_11">x = N 0 + N 1 by f o (x) = σ(EMB(o)W o ) xW x .</formula><p>Intuitively, to model a cell architecture, GATES models the operations in the architecture as the "soft gates" that control the flow of the virtual information, and the output information is used as the embedding of the cell architecture.</p><p>The key difference between GATES and GCN is: In GATES, the operations (e.g., Conv3x3) are modeled as the processing of the node attributes (i.e., virtual information), whereas GCN models them as the node attributes themselves.</p><p>The representational power of GATES for neural architectures comes from two aspects: 1) The more reasonable modeling of the operations in data-processing DAGs.</p><p>2) The intrinsic proper handling of DAG isomorphism. The discussion and experiments on how GATES handles the isomorphism are in the "Discussion on Isomorphism" section in the appendix.</p><p>In practice, to calculate the information propagation following the topological order of different graphs in a batched manner, we use a stack of GATES layers.</p><p>In the forward process of each layer, one step of information propagation is taken place at every node. That is to say, if a graph is input to a GATES encoder with N layers, the information is propagated and aggregated for N steps along the graph. The batched formulas and specific implementations of a GATES layer for OON and OOE search spaces are elaborated in the "Implementation of GATES" section in the appendix.</p><p>The Optimization of GATES The most common practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> to train the architecture performance predictors is to minimize the Mean Squared Error (MSE) between the predictor outputs and the true performances.</p><formula xml:id="formula_12">L({a j , y j } j=1,··· ,N ) = N j=1 (P(a j ) − y j ) 2<label>(3)</label></formula><p>where a j denotes one architecture, and y j denotes the true performance of a j . In NAS applications, what is really required to guide the search of architectures is the relative ranking order of architectures rather than the absolute performance values. In this paper, we adopt Kendall's Tau ranking correlation <ref type="bibr" target="#b19">[20]</ref> as the measure as the direct criterion for evaluating architecture predictors. And since ranking losses are better surrogate losses <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref> for the ranking correlation than the regression loss, in addition to the MSE loss, we use a hinge pair-wise ranking loss with margin m=0.1 to train the predictors. 5</p><formula xml:id="formula_13">L({a j , y j } j=1,··· ,N ) = N j=1 i,yi&gt;yj max[0, m − (P(a i ) − P(a j ))]<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Architecture Search Utilizing the Predictor</head><p>We follow the flow in Alg. 1 to conduct the architecture search. There are multiple ways of utilizing the predictor P to sample architectures (line 6 in Alg. 1), i.e., the choice of the inner search method. In this work, we use two inner search methods for sampling architecture for further evaluation: 6</p><p>-Random sample n architectures from the search space, then choose the best k among them according to the evaluation of the predictor. -Search with Evolutionary Algorithm (EA) for n steps, and then choose the best k with the highest predicted scores among the seen architectures.</p><p>Compared with the evaluation (line 7 in Alg. 1) in the outer search process, the evaluation of each architecture in the inner search process is very efficient with only a forward pass of the predictor. The sample ratio r = n k indicates the 5 A more comprehensive comparison of the MSE regression loss and multiple ranking losses is shown in the appendix. <ref type="bibr" target="#b5">6</ref> Note that this inner search component could be easily substituted with other search strategies.</p><p>equivalent number of the architectures need to be evaluated by the predictor to make one sample decision. And it is not the case that bigger r leads to better sample efficiency of the overall NAS process. If n is too large (the limiting case is to exhaustive test the whole search space with n = |A|), the sampling process would overfit onto exploiting the current performance predictor and fails to explore. Therefore, there is a trade-off between exploration and exploitation controlled by n, which we verify in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The experiments in Sec. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predictor Evaluation on NAS-Bench-101</head><p>Setup NAS-Bench-101 <ref type="bibr" target="#b27">[28]</ref> provides the performances of the 423k unique architectures in a search space. The NAS-Bench-101 search space is an OON search space, in which sequence based encoding schemes <ref type="bibr" target="#b23">[24]</ref>, and graph based encoding schemes <ref type="bibr" target="#b21">[22]</ref> are proposed for encoding architectures. We use the Kendall's Tau ranking correlation <ref type="bibr" target="#b19">[20]</ref> as the measure for evaluating the architecture performance predictors. The first 90% (381262) architectures are used as the training data, and the other 42362 architectures are used for testing. <ref type="bibr" target="#b6">7</ref> We conduct a more comprehensive comparison of the MSE loss and multiple ranking losses on NAS-Bench-101, and the results are shown in the appendix. We find that compared to the MSE loss, ranking losses bring consistent improvements, and hinge pair wise loss is a good choice. Therefore, in our experiments, unless otherwise stated, the hinge pairwise loss with margin 0.1 is used to train all the predictors.  enables one to learn a good performance predictor for unseen architectures after evaluating only a small set of architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In the Kendall's Tau measure, all discordant pairs are treated equally. However, in NAS applications, the relative rankings among the poorly performing architectures are not of concern. Therefore, we compare different predictors in the form of other measures that have a more direct correspondence with the NAS flow: 1) N@K: The best true ranking among the top-K architectures selected according to the predicted scores. 2) Precision@K: The proportion of true top-K architectures among the top-K predicted architectures. <ref type="table" target="#tab_10">Table. 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Predictor Evaluation on NAS-Bench-201</head><p>Setup NAS-Bench-201 <ref type="bibr" target="#b2">[3]</ref> is another NAS benchmark that provides the performances of 15625 architectures in an OOE search space. In our experiments, we use the first 50% (7813) as the training data, and the remaining 7812 architectures as the testing data. Since GCN encoders could not be directly applied to  Results <ref type="table" target="#tab_2">Table 2</ref> shows the evaluation results of GATES. GATES could achieve significantly higher ranking correlations than the baseline encoders, especially when there are only a few training samples. For example, with 78 training samples, "GATES + Pairwise loss" could achieve a Kendall's Tau of 0.7401, while the best baseline result is 0.5550 ("LSTM + Pairwise loss"). The N@K and Precision@K measures on NAS-Bench-201 are shown in Table 5 and <ref type="figure" target="#fig_3">Fig. 3(b)</ref>, respectively. We can see that GATES can achieve an N@5 of 1 on the 7812 testing architectures, with either ranking loss or regression loss. And, not surprisingly, GATES outperforms the baselines consistently on the Precision@K measure too. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Neural Architecture Search on NAS-Bench-101</head><p>Equipped with a better performance predictor, the sample efficiency of the predictor-based NAS process can be significantly improved. To verify that, we conduct the architecture search on NAS-Bench-101 using various searching strategies. As the baseline of our method, we run a random search, regularized evolution <ref type="bibr" target="#b17">[18]</ref>, and predictor-based NAS methods equipped with the baseline encoders (i.e., LSTM, MLP, GCN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Sample Efficiency</head><p>The results of running predictor-based NAS methods with different encoders are shown in <ref type="figure" target="#fig_11">Fig. 4</ref>. We conduct experiments with two inner search methods: random search, and evolutionary algorithm. In each stage, 100 random samples are used to train the predictor (50 for evolutionary algorithm), and the predictor is trained for 50 epochs with hinge ranking loss. When using random search, n = 2500 architectures are randomly sampled, and the top k = 5 architectures with high predicted scores are chosen to be further evaluated by the ground truth evaluator. When using the evolutionary algorithm for the inner search, n is set to 100, and k is set to 1. And  the population and tournament size is 20 and 5, respectively. We can see that the sample efficiency using GATES surpasses the baselines with different inner search methods. This verifies the analysis that utilizing a better neural architecture encoder in the predictor-based NAS flow leads to better sample efficiency. The comparison of the sample efficiency of two baseline searching strategies and the predictor-based method with GATES is shown in <ref type="figure" target="#fig_5">Fig. 5(a)</ref>. The median counts of evaluated architectures of RS, Regularized EA and GATES-powered NAS over 100 runs are 220400, 23700 and 400 (50 as the granularity), respectively. GATES-powered NAS is 551.0× and 59.25× more sample efficient than the random search and evolution algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Search</head><p>Ablation Study of the Sample Ratio r The ablation study of the sample ratio r (Sec. 3.3) is shown in <ref type="figure" target="#fig_5">Fig. 5(b)</ref>. We run GATES-powered predictorbased search with evolutionary algorithm, and shows the architectures needed to evaluate before finding the architecture with the best validation accuracy. We can see that the sample ratio r should be neither too big nor too small, since a too small n leads to bad exploitation and a too large n leads to bad exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Neural Architecture Search in the ENAS Search Space</head><p>In this section, we apply our method on the ENAS search space. This search space is an OOE search space that is much larger than the benchmark search spaces. We first randomly sample 600 architectures and train them for 80 epochs. Then we train a GATES predictor using the performance of the 600 architectures and use it to sample 200 architectures, by randomly sampling 10k architectures and taking the top 200 with the highest predicted scores (sample ratio r = 50). After training these 200 architectures for 80 epochs, we pick the architecture with the best validation accuracy. Finally, after the channel and layer augmentation, the architecture is trained from scratch for 600 epochs.</p><p>The comparison of the test errors of different architectures is shown in Table 6, and the discovered architecture is shown in the appendix. As can be seen, our discovered architecture can achieve a test error rate of 2.58%, which is better than those architectures discovered with parameter sharing evaluation. Compared to the other methods, much fewer samples are truly evaluated to discover an architecture with better or comparable performance. When transferred to ImageNet, the discovered architecture achieves a competitive top-1 error of 24.1% with 5.6M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose GATES, a graph-based neural architecture encoder with better representation ability for neural architectures. Due to its reasonable modeling of the neural architectures and intrinsic ability to handle DAG isomorphism, GATES significantly improves the architecture performance predictor for different cell-based search spaces. Utilizing GATES in the predictor-based NAS flow leads to consistent improvements in sample efficiency. Extensive experiments demonstrate the effectiveness and rationality of GATES. Employing GATES to encode architectures in larger or hierarchical topological search spaces is an interesting future direction.</p><p>for Information Science and Technology (BNRist). The authors thank Novauto for the support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices: A Generic Graph-based Neural</head><p>Architecture Encoding Scheme for Predictor-based NAS 1 Implementation of GATES</p><p>In practice, to calculate the information propagation following the topological order of different graphs in a batched manner, we use a stack of GATES layers.</p><p>In the forward process of each GATES layer, one step of information propagation is taken place at every node. The detailed formulas and implementations of one GATES layer for "operation on node" and "operation on edge" search spaces are shown as follows, and the notations are summarized in <ref type="table">Table.</ref> 1.</p><p>Operation On Node (OON) Search Space For the OON case, we take the NAS-Bench-101 search space as an example. In the cell architecture, there is n i = 1 input node, and at most V = 7 nodes. For batch computation, we pad zero columns and rows into the adjacent matrix to ensure that all adjacent matrices are of size 7 × 7, and also add none operations into the corresponding positions in the operation list. The calculation of the k-th GATES layer could be written as</p><formula xml:id="formula_14">X (0) = CONCAT(Ẽ, 0 b×V −ni×h (0) i , dim=1) X (k) = σ(EMB(o)W (k) o ) (AX (k−1) W (k) x ) (1) whereẼ = repeat(E, [b, 1, 1]) ∈ R b×ni×h (0) i , and E, EMB, W (k) o , W (k) x</formula><p>are trainable parameters.</p><p>In practice, we found that for the OON search space, adding a self-loop of the information propagation would lead to slightly better performance.</p><formula xml:id="formula_15">X (k) = σ(EMB(o)W (k) o ) (ÃX (k−1) W (k) x ) A = A + I (2)</formula><p>Operation On Edge (OOE) Search Space For the OOE search spaces, the calculation of a GATES layer could be written as</p><formula xml:id="formula_16">X (0) = CONCAT(Ẽ, 0 b×V −ni×h (0) i , dim=1) S = EXPAND(X (k−1) W (k)</formula><p>x , 1)  For the search spaces where there is at most one edge between each pair of nodes (e.g., NAS-Bench-201), the above calculation could be simplified to</p><formula xml:id="formula_17">X (k) = SUM( n d d=1 EXPAND(A, 3) σ(EMB(o d )W (k) o ) S, dim=2) (3)</formula><formula xml:id="formula_18">X (0) = CONCAT(Ẽ, 0 b×V −ni×h (0) i , dim=1) S = EXPAND(X (k−1) W (k) x , 1) X (k) = SUM(EXPAND(A, 3) σ(EMB(o)W (k) o ) S, dim=2)<label>(4)</label></formula><p>2 Discussion on Isomorphism GATES maps ismorphic architectures to the same representation The encoding process of GATES mimics the actual computation flow: GATES uses multiplicative transforms to mimic the forward process of operations (e.g., Conv3x3), and uses commutative aggregation to mimic actual commutative aggregation of the feature maps. Naturally, GATES would encode two architectures that give out the same feature map results into the same representation. That is to say, the embedding space of GATES is more meaningful. However, GATES might fail to map non-isomorphic architectures to different representations. And we leave it to future work to ameliorate this problem to further increase the discriminative power of GATES.</p><p>In the search spaces which we have experimented with (i.e., NAS-Bench-101, NAS-Bench-201, and ENAS), the combination of feature maps at internal nodes is done via addition operation, which is commutative. Therefore, for encoding the architecture, GATES also uses commutative addition to combine the "virtual information". Note that if the feature map aggregation at some internal node is not commutative (e.g., concatenation), we should use a non-commutative aggregation of the virtual information too.</p><p>Another thing to note is that, in the NAS-Bench-101 and ENAS search spaces, the tensors going to the final output node in the cell are concatenated instead of being added together. Since the concatenation operation is not commutative, different concatenation orders result in different architectures. Nevertheless, in these two search spaces, these models are equivalent through the rearrangement of channels in the following operations. Therefore, we use addition to aggregate the information at the output node, too. We emphasize that this is a search space specific discussion.</p><p>We conduct a simple experiment to verify GATES's ability to map isomorphic architectures to the same representation on NAS-Bench-101.   <ref type="figure" target="#fig_0">Fig. 1</ref>. An ad-hoc graph-based solution <ref type="bibr" target="#b4">[5]</ref> for encoding the architecture in the ENAS search space (an OOE search space) fails to map isomorphic architectures to the same representation. In the upper case, the two architectures are the same graph, but the embeddings of Node 1 differ. This case could be solved by imposing an order of the operations when the two incoming edges come from the same previous node. In the lower case, these two architecture are isomorphic, since the feature map aggregation at Node 3 is a commutative element-wise addition. However, this encoding scheme cannot guarantee to map these two architectures to the same representation, since the original node embeddings already differ at Node 3. The failure to handle the isomorphism is due to the non-commutative characteristics of the concatenation operation.</p><p>isomorphic counterparts. We test different predictors trained with 0.1% training samples on these 116k architectures and show the results in <ref type="table">Table.</ref> 2. Since the sequence-based encoding schemes cannot map isomorphic architectures to the same representation, the ranking correlation decreases if no de-duplication procedure is carried out. The last column shows the sum of the variances of the predicted scores in every isomorphic architecture group. We can see that GATES and GCN can map isomorphic architectures to the same representation (a variance of 0 with negligible numeric errors), since only isomorphism-invariant aggregation operations are used in the encoding process.</p><p>Two counter examples of the ad-hoc solution <ref type="bibr" target="#b4">[5]</ref> Since GCN cannot be directly applied to encoding architectures from the OOE search spaces, a recent study <ref type="bibr" target="#b4">[5]</ref> proposes an ad-hoc solution for the ENAS search space. They represent each node by the concatenation of the operation embeddings on the input edges. This solution cannot generalize to search spaces where nodes could have different input degrees. Whats more, since the concatenation operation is not commuta- tive, this encoding scheme could not map isomorphic architectures to the same representation correctly. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates two minimal counterexamples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Setup and Additional Results</head><p>Setup and Results on NAS-Bench-101 The setup of all the experiments on NAS-Bench-101 goes as follows. An ADAM optimizer <ref type="bibr" target="#b6">[7]</ref> with learning rate 1e-3 is used to optimize the performance predictors for 200 epochs. And the average of the ranking correlations in the last 5 epochs is reported. The batch size is set to 512. And a hinge pairwise ranking loss with margin 0.1 is used. For the construction of the MLP and LSTM encoder, we follow the serialization method and the model settings in <ref type="bibr" target="#b23">[24]</ref>. The MLP is constructed by 4 fully-connected layers with 512, 2048, 2048, and 512 nodes, and the output of dimension 512 is used as the cell's embedding. The embedding and hidden sizes of the LSTM are both set to 100, and the final hidden state is used as the cell's embedding. For the GCN and GATES encoders, we construct the encoder by stacking five 128-dim GCN or GATES layers. All the embedding sizes are set to 48, including the operation embedding in GCN, and the operation and information embedding in GATES. For GCN, the average of all the nodes' features is used as the cell's embedding. In GCN with global node <ref type="bibr" target="#b21">[22]</ref>, the features of the global node are used as the cell's embedding. <ref type="figure" target="#fig_8">Fig. 2</ref> shows the prediction results on the 42362 testing architectures with different encoders trained on 0.1% training data. As can be seen, compared with the GCN and MLP encoders, the predictions of GATES are much more accurate in the sense of ranking correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup and Results on NAS-Bench-201</head><p>The setup of all the experiments on NAS-Bench-201 goes as follows. An ADAM optimizer with learning rate 1e-3 and batch size 512 is used to train the predictors for 200 epochs, and the average of testing Kendall's Taus in the last 5 epochs is reported. For the sequence-based baselines (MLP and LSTM), we use the 6 elements of the lower triangular portion, excluding the diagonal ones. We use 4 fullyconnected layers with 512, 2048, 2048, 512 nodes for the MLP encoder. The embedding size and hidden size of the 1-layer LSTM is set to 100, and the final hidden stage is used as the embedding of the cell architecture. As for GATES, we use a 5-layer GATES encoder without self-loop.</p><p>Since GCN encoders could not be directly applied to the OOE search spaces, we implement a line graph solution for applying GCNto encode OOE architectures following these three steps: 1) convert the graph to a line graph; 2) apply an 5-layer GCN; 3) concatenate the node embeddings as the graph representation. The results of this "Line Graph GCN" solution are listed in Tab. 5, and are not satisfying enough. We suppose that it is due to that the power of GNN cannot be fully utilized as converting to line graph results in identical adjacent matrices for all NAS-Bench-201 architectures.</p><p>Ablation Study: GATES Layer Number We show the ablation study of the layer number in the GATES encoders in <ref type="figure" target="#fig_3">Fig. 3</ref>. We can see that the regression loss fails to instruct the learning of deep GCN and GATES encoders. Even with the ranking loss, the GCN's performance degrades as the layer number increases, while the GATES encoder is more robust.</p><p>Another interesting fact is that a GATES layer number larger than or equal 3 is a good choice on NAS-Bench-201, and as we know, the most common longest path length is 3 too. As for NAS-Bench-101, a GATES layer number larger than or equal 4 is a good choice. The longest possible path length on NAS-Bench-101 is 6, but only in a small portion of architectures. The ablation results match with the "virtual information flow" intuition of the GATES design and give evidence of the rationality of using GATES for neural architecture encoding.  Neural Architecture Search in the ENAS Search Space The setup of the predictor training goes as follows. The predictor is constructed by four 64-dim GATES layers. Both the operation and information embedding sizes are set to 32. During the training of the predictor, the total epoch is set to 80, and the batch size is set to 128, and a pairwise hinge loss with margin 0.1 and an ADAM optimizer with learning rate 1e-3 are used. For the true performance evaluation of the 800 architectures (600 randomly sampled, 200 sampled utilizing the predictor), we train them for 80 epochs using an SGD optimizer with weight decay 3e-4. The learning rate is decayed from 0.05 to 0.001 following a cosine schedule. The base channel number is 16, and the number of layers is 8.</p><p>The discovered architecture is shown in <ref type="figure" target="#fig_11">Fig. 4</ref>. To evaluate the final performance of the discovered cell architecture, we first apply the channel and layer augmentation. Specifically, 20 cells are stacked to construct the network, and the base channel number is increased from 16 to 36. The augmented model is trained for 600 epochs on CIFAR-10 with batch size 128, and the learning rate is decayed from 0.05 to 0.001 following a cosine schedule. The cutout data augmentation with length 16 is used. The weight decay is set to 3e-4, and the dropout rate before the fully-connected classifier is set to 0.1. For other regularization techniques, we follow existing studies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref> to use auxiliary towers with weight 0.4 and the scheduled drop-path of probability 0.2. For transferring the discovered architecture to ImageNet, we increase the base channel number to 48 and stack 14 cells to construct the model. The augmented model is trained for 300 epochs with batch size 256, and the learning rate is decayed from 0.1 to 0 following a cosine schedule. The weight decay is set to 3e-5 and auxiliary towers with weight 0.4 is used, no dropout is used. The comparison with a few previous methods is illustrated in Tab. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ranking Losses for Predictor Optimization</head><p>The ranking correlation of the performance predictor on unseen architectures is the key to the success of predictor-based NAS. Since ranking losses are better surrogates of the ranking measures than the regression loss <ref type="bibr" target="#b1">[2]</ref>, training the performance predictor with ranking losses could lead to better ranking correlation.</p><p>We utilize different pairwise and listwise ranking losses for training the predictor <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. The pairwise ranking loss could be written as</p><formula xml:id="formula_19">L p (S) = N i=1 j∈{j|yi&lt;yj } φ(P (a j ), P (a i ))<label>(5)</label></formula><p>We experiment with two different choices of φ. 1) The binary cross entropy function φ(s j , s i ) = log(1 + e (sj −si) ); 2) The hinge loss function φ(s j , s i ) = max(0, m − (s j − s i )), where m is a positive margin.</p><p>We also experiment with a pairwise comparator: We construct an MLP that takes the concatenation of two architecture embeddings as input and outputs a score: s = MLP([E(a j ), E(a i )], and a positive s indicates that a j is better than a i . Note that the total-orderness of the architectures is not guaranteed using </p><p>We design the listwise ranking loss following ListMLE <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_21">L l (S) = U ⊂S |U | i=1</formula><p>{−P (a (i),U ) + log |U | j=i exp(P (a (j),U ))} <ref type="bibr" target="#b6">(7)</ref> where U are subsets ofS, |U | denotes the size of U , a (i),U denotes the architecture whose true performance y (i),U is the i-th best in the subset U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation of Ranking Losses</head><p>Setup In the experiments of evaluating the ranking losses, the training settings and the construction of the GATES model are the same as in the evaluation of GATES. One exception is that, for the listwise ranking loss (ListMLE), we train the predictor for 80 epochs (list length is 4), since the training converges much faster with the listwise ranking loss. Still, the average of the ranking correlations in the last 5 epochs is reported.  For the baseline evaluation of MSE regression loss with MLP and Line Graph GCN encoders, we use a learning rate of 1e-4, since we find out that these encoders cannot be learned with a learning rate of 1e-3.</p><p>The evaluation of the comparator-based ranking loss is a little different than other ranking losses. For other ranking losses, we can calculate the ranking correlation between the predicted scores P(a) and the true accuracies. However, a comparator trained using the comparator-based ranking loss must take a pair of architectures as the input and output a comparison results. Therefore, for evaluating the performance of the comparator, we run the randomized quicksort procedure with the comparator to get the predicted rankings of the testing architectures. Since the comparator might not be a proper total order operator, different choices of the random pivots in randomized quick-sort could lead to different sorted sequences. Therefore, we run randomized quick-sort with 3 different random seeds, and report the average Kendall's Tau. In practice, we find that the Kendall's Taus calculated using different random seeds are very close. For example, three tests with random seed 1, 12 and 123 of the predictor trained on the whole training set give the Kendall's Taus of 0.90106, 0.90107 and 0.90113, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on NAS-Bench-101</head><p>We train GATES-powered predictors with four types of ranking losses: 1) Pairwise loss with binary cross-entropy φ. 2) Pairwise loss with a hinge loss function φ. 3) Pairwise comparator loss. 4) Listwise (ListMLE). <ref type="table" target="#tab_4">Table 4</ref> shows the comparison of using different losses to train the predictors on NAS-Bench-101. Compared with the regression loss, ranking losses bring consistent improvements. The performances of different ranking losses are close, and the pairwise hinge loss is a good choice. We also find that training with regression loss requires a smaller learning rate and longer time to converge, and does not work well with deep GCN or GATES models. <ref type="table" target="#tab_5">Table 5</ref> shows the comparison of using regression and ranking losses to train the predictors on NAS-Bench-201. We can see that training using ranking losses leads to better-correlated predictors consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on NAS-Bench-201</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>The flow of predictor-based neural architecture search 1: A: Architecture search space 2: P : A → R: Performance predictor that outputs the predicted performance given the architecture 3: N (k) : Number of architectures to sample in the k-th iteration 4: k = 1 5: while k ≤ MAX ITER do 6: Sample a subset of architectures S (k) = {a (k) j } j=1,··· ,N (k) from A, utilizing P 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The overview of the proposed algorithm. Upper: The general flow of the predictor-based NAS. Lower: Illustration of the encoding processes of GATES of an OON cell architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and Figure. 3(a) show these two measures of the predictors with different encoders on the testing set of NAS-Bench-101. As can be seen, GATES achieves consistently better performances than other encoders across different Ks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Precision@K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Ablation study of the sample ratio r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Left: Number of architectures evaluated to acquire the best validation accuracy on NAS-Bench-101 over 100 runs. We use the mean validation accuracy as the search reward. GATES-powered predictor-based NAS is 511.0× and 59.25× more sample efficient than random search and regularized evolution. Right: Number of architectures evaluated to acquire the best validation accuracy over 10 runs with different r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>information in the k-th layer E ∈ R n i ×h (0) i the embedding of the information at the input nodes EMB ∈ R No×ho the operation embeddingsW (k) o ∈ R ho×h (k) ithe transformation matrix on the operation embedding (the k-th layer)W (k) x ∈ R h (k−1) i ×h (k) i the transformation matrix on previous layer's output information (the k-th layer) b batch size A ∈ R b×V ×V adjacency matrix X (k) ∈ R b×V ×h (k)i the output virtual information of the k-th layer EMB(o) ∈ R b×V ×ho (NAS-Bench-101) the embeddings of the operations on nodes EMB(o) ∈ R b×V ×V ×ho (NAS-Bench-201) the embeddings of the operations on edges n d (ENAS) maximum input degree of nodes EMB(o d ) ∈ R b×V ×V ×ho (ENAS) the embeddings of operations on the d-th input edge for nodes whereẼ = repeat(E, [b, 1, 1]) ∈ R b×ni×h (0) i , and EXPAND(A, dim) denotes the operation to insert a new dimension as dimension dim.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 2 .</head><label>2</label><figDesc>NAS-Bench-101: The true rankings (y-axis) and predicted rankings (x-axis) of 2000 architectures among the 42362 testing architectures. 0.1% training data are used to train these encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 3 .</head><label>3</label><figDesc>The effect of the number of GCN or GATES layers. (a) NAS-Bench-101. The proportion of training samples is 0.1% (381 training, 42362 testing). (b) NAS-Bench-201. The proportion of training samples is 10% (781 training, 7812 testing)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 .</head><label>4</label><figDesc>Discovered cell architectures on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Table 4 .</head><label>4</label><figDesc>The Kendalls Tau of using different loss functions on NAS-Bench-101. The first 90% (381262) architectures in the dataset are used as the training data, and the other 42362 architectures are used as the testing data. All experiments except "Regression (MSE) + GCN" are carried out with GATES encoder. ) + GCN † 0.4536 0.5058 0.5587 0.5699 0.5846 0.5871 0.5901 0.5941 Regression (MSE) + GATES † 0.4935 0.5425 0.5739 0.6323 0.7439 0.7849 0.8247 0.8352 Pairwise (BCE) 0.7460 0.7696 0.8352 0.8550 0.8828 0.8913 0.9006 0.9042 Pairwise (Comparator) 0.7250 0.7622 0.8367 0.8540 0.8793 0.8891 0.8987 0.9011 Pairwise (Hinge) 0.7634 0.7789 0.8434 0.8594 0.8841 0.8922 0.9001 0.9030 Listwise (ListMLE) 0.7359 0.7604 0.8312 0.8558 0.8852 0.8897 0.9003 0.9009 †: For the baseline evaluation of regression loss, we use a GCN encoder with 1 layer, and a GATES encoder with 3 layers rather than 5 layers, since training deep GCN or GATES encoder with MSE regression loss is unstable, and often fails to learn anything meaningful. With MSE loss, 1 layer of GCN and 3 layers of GATES achieve the best results among layer number configurations using 0.1% training data. this comparator. So, we add a simple anti-symmetry regularization term in the training of the comparator. The loss for training the comparator is: L p (S) = N i=1 j∈{j|yi&lt;yj } max(0, m − MLP([E(a j ), E(a i )]) + max(0, m + MLP([E(a i ), E(a j )]))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>5 .</head><label>5</label><figDesc>The Kendalls Tau of using different encoders and loss functions on NAS-Bench-201. The first 50% (7813) architectures in the dataset are used as the training data, and the other 7812 architectures are used as the testing (MSE) † 0.0646 0.1520 0.2316 0.5156 0.6089 LSTM + Regression (MSE) 0.4405 0.5435 0.6002 0.8169 0.8614 Line Graph GCN + Regression (Hinge) -0.0481 0.3376 0.4988 0.6609 0.7006 GATES + Regression (MSE) 0.6823 0.7528 0.8042 0.8950 0.9115 MLP + Pairwise (Hinge) 0.0974 0.3959 0.5388 0.8229 0.8703 LSTM + Pairwise (Hinge) 0.5550 0.6407 0.7268 0.8791 0.9002 Line Graph GCN + Pairwise (Hinge) 0.5063 0.6822 0.7567 0.8676 0.9002 GATES + Pairwise (Hinge) 0.7401 0.8628 0.8802 0.9192 0.9259 †:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .the information Searcher Inner Searcher Predictor Search Space Ground-truth Performance Architecture Evaluator Retrain the Predictor Cell Architecture</head><label>1</label><figDesc>Notations of GATES. E, EMB, Wo and Wx are all trainable parameters R n i ×h i the embedding of the information at the input nodes EMB ∈ R No×ho the operation embeddings Wo ∈ R ho×h i the transformation matrix on the operation embedding Wx ∈ R h i ×h i the transformation matrix on</figDesc><table><row><cell>ni</cell><cell>number of input nodes: 1, 1, 2 for NAS-Bench-101, NAS-Bench-201 and ENAS, respectively</cell></row><row><cell>No</cell><cell>number of operation primitives</cell></row><row><cell>ho</cell><cell>embedding size of operation</cell></row><row><cell>hi</cell><cell>embedding size of information</cell></row><row><cell>E ∈</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.1 and Sec. 4.2 verify the effectiveness of the GATES encoder on both the OON and OOE search spaces. Then, in Sec. 4.3, we demonstrate that by utilizing GATES, the sample efficiency of the NAS process surpasses other searching strategies, including the predictor-based methods with other baseline encoders. Finally, in Sec. 4.4, we apply the proposed algorithm to the ENAS search space.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>shows the comparison of the GATES encoder and various baseline encoders trained using different proportions of the training data. As can be seen, GATES could achieve higher Kendall's Taus on the testing architectures than the baseline encoders consistently with different training proportions. The advantages are especially significant when there are few training architectures. For example, when only 190 (0.05%) architectures are seen by the performance predictor, utilizing the same training settings, GATES achieves a test Kendall's Tau of 0.7634, whereas the Kendall's Tau results achieved by MLP, LSTM, and the best GCN variant are 0.3971, 0.5509 and 0.5343, respectively. This demonstrates the surpassing generalization ability of the GATES encoder, which</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The Kendalls Tau of using different encoders on the NAS-Bench-101 dataset. The first 90% (381262) architectures in the dataset are used as the training data, and the other 42362 architectures are used as the testing data</figDesc><table><row><cell>Encoder</cell><cell></cell><cell cols="4">Proportions of 381262 training samples</cell></row><row><cell></cell><cell cols="2">0.05% 0.1% 0.5%</cell><cell>1%</cell><cell>5%</cell><cell>10%</cell><cell>50% 100%</cell></row><row><cell>MLP [24]</cell><cell cols="5">0.3971 0.5272 0.6463 0.7312 0.8592 0.8718 0.8893 0.8955</cell></row><row><cell>LSTM [24]</cell><cell cols="5">0.5509 0.5993 0.7112 0.7747 0.8440 0.8576 0.8859 0.8931</cell></row><row><cell cols="6">GCN (w.o. global node) 0.3992 0.4628 0.6963 0.8243 0.8626 0.8721 0.8910 0.8952</cell></row><row><cell cols="6">GCN (global node) [22] 0.5343 0.5790 0.7915 0.8277 0.8641 0.8747 0.8918 0.8950</cell></row><row><cell>GATES</cell><cell cols="5">0.7634 0.7789 0.8434 0.8594 0.8841 0.8922 0.9001 0.9030</cell></row><row><cell>Encoder</cell><cell cols="2">Ranking Loss</cell><cell></cell><cell cols="2">Regression Loss</cell></row><row><cell></cell><cell>N@5</cell><cell>N@10</cell><cell></cell><cell>N@5</cell><cell>N@10</cell></row><row><cell>MLP [24]</cell><cell cols="5">57 (0.13%) 58 (0.13%) 1397 (3.30%) 552 (1.30%)</cell></row><row><cell cols="6">LSTM [24] 1715 (4.05%) 1715 (4.05%) 1080 (2.54%) 312 (0.73%)</cell></row><row><cell cols="6">GCN [22] 2025 (4.77%) 1362 (3.21%) 405 (0.95%) 405 (0.95%)</cell></row><row><cell>GATES</cell><cell cols="5">22 (0.05%) 22 (0.05%) 27 (0.05%) 27 (0.05%)</cell></row></table><note>Table 3. N@K on NAS-Bench-101. All predictors are trained with 0.1% of the training data (i.e., 381 architectures)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Encoder</cell><cell cols="5">Proportions of 7813 training samples</cell></row><row><cell></cell><cell>1%</cell><cell>5%</cell><cell>10%</cell><cell>50%</cell><cell>100%</cell></row><row><cell cols="6">MLP [24] 0.0974 0.3959 0.5388 0.8229 0.8703</cell></row><row><cell cols="6">LSTM [24] 0.5550 0.6407 0.7268 0.8791 0.9002</cell></row><row><cell cols="6">GATES 0.7401 0.8628 0.8802 0.9192 0.9259</cell></row></table><note>The Kendalls Tau of using different encoders on the NAS-Bench-201 dataset. The first 50% (7813) architectures in the dataset are used as the training data, and the other 7812 architectures are used as the testing datathe OOE search spaces, we compare GATES with the sequence-based encoders: MLP and LSTM. 8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>N@K on NAS-Bench-201. All the predictors are trained using 10% of the training data (i.e., 781 architectures)</figDesc><table><row><cell></cell><cell cols="2">Encoder</cell><cell cols="2">Ranking Loss</cell><cell></cell><cell cols="4">Regression Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>N@5</cell><cell>N@10</cell><cell></cell><cell cols="2">N@5</cell><cell cols="3">N@10</cell></row><row><cell></cell><cell cols="10">MLP [24] 7 (0.09%) 7 (0.09%) 1538 (19.7%) 224 (3.87%)</cell></row><row><cell></cell><cell cols="10">LSTM [24] 8 (1.02%) 2 (0.01%) 250 (6.65%) 234 (2.99%)</cell></row><row><cell></cell><cell cols="10">GATES 1 (0.00%) 1 (0.00%) 1 (0.00%) 1 (0.00%)</cell></row><row><cell>0.948 0.948 0.949 0.949</cell><cell></cell><cell></cell><cell></cell><cell>0.948 0.948 0.950 0.950</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.943 0.944 0.945 0.946 0.947 0.943 0.944 0.946 0.947 Accuracy Accuracy 0.945</cell><cell>200 200</cell><cell>400 400 #Archs 600 600 #Archs</cell><cell cols="2">800 800 GATES 1000 GCN LSTM MLP 1000 Accuracy Accuracy 0.940 0.942 0.944 0.946 0.944 0.946 GATES GCN LSTM MLP 0.940 0.942</cell><cell>100</cell><cell>100</cell><cell>200</cell><cell>300 #Archs 200 #Archs 300</cell><cell>400</cell><cell>500 GATES LSTM MLP GCN 400 GATES 500 GCN LSTM MLP</cell></row><row><cell cols="5">(a) RS inner search method (r = 500)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(b) EA inner search method (r = 100) Fig. 4. Comparison of predictor-based NAS with different encoders: The best validation accuracy during the search process over 10/15 runs for the RS and EA inner serach method, respectively. r is the sample ratio (see Sec. 3.3)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison of NAS-discovered architectures on CIFAR-10 As discussed in Sec. 2, the challenge faced by one-shot NAS lies in the evaluation correlation rather than sample efficiency, thus we do not report the sample efficiency of the one-shot (parameter sharing) NAS methods.</figDesc><table><row><cell>Method</cell><cell cols="3">Test Error (%) #Params (M) #Archs Evaluated</cell></row><row><cell>NASNet-A + cutout [30]</cell><cell>2.65</cell><cell>3.3</cell><cell>20000</cell></row><row><cell>AmoebaNet-B + cutout [18]</cell><cell>2.55</cell><cell>2.8</cell><cell>27000</cell></row><row><cell>NAONet [15]</cell><cell>2.98</cell><cell>28.6</cell><cell>1000</cell></row><row><cell>PNAS [10]</cell><cell>3.41</cell><cell>3.2</cell><cell>1160</cell></row><row><cell>NAONet-WS  † [15]</cell><cell>3.53</cell><cell>2.5</cell><cell>-</cell></row><row><cell>DARTS+cutout  † [12]</cell><cell>2.76</cell><cell>3.3</cell><cell>-</cell></row><row><cell>ENAS + cutout  † [17]</cell><cell>2.89</cell><cell>4.6</cell><cell>-</cell></row><row><cell>Ours + cutout</cell><cell>2.58</cell><cell>4.1</cell><cell>800</cell></row><row><cell>†:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 .</head><label>1</label><figDesc>Notations used in the batched computation of the GATES encoder</figDesc><table><row><cell>V</cell><cell>maximum number of nodes: 7, 4, 6 for NAS-Bench-101 [28], NAS-Bench-201 [3] and ENAS [17], respectively</cell></row><row><cell>ni</cell><cell>number of input nodes: 1, 1, 2 for NAS-Bench-101, NAS-Bench-201 and ENAS, respectively</cell></row><row><cell>No</cell><cell>number of operation primitives</cell></row><row><cell>ho</cell><cell>embedding size of operation</cell></row><row><cell>h</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .</head><label>2</label><figDesc>The Kendall's Tau τ on 1) NAS-Bench-101 test set 2) the 7-vertex subset of the test set 3) all the isomorphic counterparts of the 7-vertex subset (without deduplication). The last column shows the sum of the variances of the predicted scores in every isomorphic architectures group, and there are negligible numerical errors in the variance results of GATES and GCN. All the predictors are trained using the hinge pairwise ranking loss on 0.1% of the training data.</figDesc><table><row><cell></cell><cell cols="4">test set 7-vertex test set 7-vertex test set w.o. de-dup.</cell></row><row><cell>Encoders</cell><cell>(42362)</cell><cell>(36064)</cell><cell></cell><cell>(116102)</cell></row><row><cell></cell><cell>τ</cell><cell>τ</cell><cell>τ</cell><cell>Total Var.</cell></row><row><cell cols="2">MLP [24] 0.5272</cell><cell>0.5143</cell><cell>0.4729</cell><cell>43.58</cell></row><row><cell cols="2">LSTM [24] 0.5993</cell><cell>0.5877</cell><cell>0.5656</cell><cell>18.80</cell></row><row><cell cols="2">GCN [22] 0.5790</cell><cell>0.5876</cell><cell>0.6169</cell><cell>1.16E-11</cell></row><row><cell cols="2">GATES 0.7789</cell><cell>0.7724</cell><cell>0.7758</cell><cell>9.24E-12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 .</head><label>3</label><figDesc>Comparison of NAS-discovered architectures on ImageNet</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 Test Error (%) #Params (M)</cell></row><row><cell>NASNet-A [30]</cell><cell>26.0</cell><cell>5.3</cell></row><row><cell>AmoebaNet-B [18]</cell><cell>27.2</cell><cell>5.3</cell></row><row><cell>PNAS [10]</cell><cell>25.8</cell><cell>5.1</cell></row><row><cell>DARTS [12]</cell><cell>26.9</cell><cell>4.9</cell></row><row><cell>GHN [29]</cell><cell>27.0</cell><cell>6.1</cell></row><row><cell>Ours</cell><cell>24.1</cell><cell>5.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Figure 2illustrates the OON and OOE search spaces.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">See "Setup and Additional Results" section in the appendix for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We also implement an ad-hoc solution of applying GCN on OOE architectures referred to as the Line Graph GCN solution, in which the graph is first converted to a line graph. See "Setup and Additional Results" section in the appendix for more details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ranking measures and loss functions in learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yan Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Bengio, Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I., Culotta, A.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJxyZkBKDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Breaking the curse of space explosion: Towards effcient nas with curriculum search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nat: Neural architecture transformer for accurate and compact architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="735" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2016" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards fast adaptation of neural architectures with meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00436</idno>
		<title level="m">Hierarchical representations for efficient architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding and improving one-shot neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10815</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8007-neural-architecture-optimization.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7816" to="7827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<title level="m">Deeparchitect: Automatically designing and training deep architectures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<title level="m">Efficient neural architecture search via parameter sharing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<title level="m">Evaluating the search phase of neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimates of the regression coefficient based on kendall&apos;s tau</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">324</biblScope>
			<biblScope unit="page" from="1379" to="1389" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ranking with large margin principle: Two approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-objective neural architecture search via predictive network performance optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09336</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural network structures and isomorphisms: Random walk characteristics of the search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stagge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks. Proceedings of the First IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
	<note>Cat. No. 00</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Alphax: exploring neural architectures with deep neural networks and monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jinnai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07440</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Listwise approach to learning to rank: theory and algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1192" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1284" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Renas:relativistic evaluation of neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09635</idno>
		<title level="m">Nasbench-101: Towards reproducible neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05749</idno>
		<title level="m">Graph hypernetworks for neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.01578" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
