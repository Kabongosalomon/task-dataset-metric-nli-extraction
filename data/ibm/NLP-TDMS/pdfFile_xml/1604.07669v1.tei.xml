<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Action Recognition with Enhanced Motion Vector CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Embedded System and Service Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanli</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Embedded System and Service Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-time Action Recognition with Enhanced Motion Vector CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The deep two-stream architecture <ref type="bibr" target="#b22">[23]</ref> exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition aims to enable computer automatically recognize human action in real world video. Recent years have witnessed extensive research efforts and significant research progresses in this area. The existence of large action datasets and worldwide competitions, like UCF101 <ref type="bibr" target="#b24">[25]</ref>, HMDB51 <ref type="bibr" target="#b14">[15]</ref>, and THUMOS14 <ref type="bibr" target="#b10">[11]</ref> promote researches in this area. Early approaches in this area utilize a Bag-of-Visual-Words paradigm and its variants <ref type="bibr" target="#b19">[20]</ref>, which mainly consists of feature extraction, feature encoding, and classification steps. The performance of these approaches highly depends on the hand crafted features. Previous studies show that iDT descriptors and Fisher vector representation yield superior performance on various * Corresponding author. <ref type="figure">Figure 1</ref>. Comparison of motion vector and optical flow in x and y components. We can see that motion vector contains lots of noisy movement information and it is much coarser than optical flow. datasets <ref type="bibr" target="#b27">[28]</ref>. More recently researchers exploit deep learning for action recognition. One successful example along this line is the two-stream framework <ref type="bibr" target="#b22">[23]</ref> which utilizes both RGB CNN and optical flow CNN for classification and achieves the state-of-the-art performance on several large action datasets. However, two-stream CNNs cannot process videos at real-time. The calculation of optical flow is time consuming which hinders the processing speed of twostream CNNs.</p><p>This paper aims to develop a real-time action recognition method with high performance based on the successful two-stream framework. This is challenging, since optical flow itself is computationally expensive and cannot be estimated in real-time with most current algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. It takes 60 ms <ref type="bibr" target="#b0">[1]</ref> to calculate optical flows per frame in K40 GPU, which is far from the requirement of real-time processing. To circumvent this difficulty, instead of using optical flow, this paper leverages motion vector as the input of CNN, which can be decoded directly from standard video compressed files with very low computational cost.</p><p>Motion vectors represent movement patterns of image blocks which resemble optical flows in terms of describing local motions. Early research <ref type="bibr" target="#b11">[12]</ref> indicates that motion vectors include useful information for action recognition. However, the purpose of motion vector is not to unveil the temporal relationship of two macro blocks as accurate as possible, but to exploit temporal redundancy between ad-jacent frames to reduce the bit rate in video compression. Thus motion vector only contains coarse movement patterns which are usually not precise. Moreover, motion vector lacks fine motion information of pixels. Directly replacing optical flows with motion vectors will severely degrade recognition performance of CNN as observed in our experiments.</p><p>Here our key insight to improve the recognition performance of motion vector CNN is that optical flow and motion vector share some similar characteristics which allows us to transfer the fine features/knowledge learned in optical flow CNN (OF-CNN) to that of motion vectors (MV-CNN). Both optical flow and motion vector can be extracted at each frame, and both of them contain motion information of local regions. Motion vector contains coarse and inaccurate motion information while optical flow carries fine and accurate ones. Due to the fine quality of optical flow fields, OF-CNN can learn elaborate filters and achieve better performance than MV-CNN. These facts inspire us to leverage the knowledge learned with OF-CNN to enhance MV-CNN. More specifically, we take OF-CNN learned in optical flow domain as a teacher net, and teach student MV-CNN in spirit of the knowledge distillation techniques proposed by <ref type="bibr" target="#b6">[7]</ref>. We call this new CNN as optical flow enhanced motion vector CNN. Note we only require optical flow in the training phase, while the testing can be conducted with motion vector solely. Our experiments demonstrate that this novel strategy significantly boosts the performance of motion vectors, and achieves comparable performance with optical flow based action recognition methods <ref type="bibr" target="#b22">[23]</ref>.</p><p>The main contributions of this paper are summarized as below. Firstly, we propose a real-time CNN based action recognition method which achieves comparable performance with the state-of-the-art two-stream approach <ref type="bibr" target="#b22">[23]</ref>. Secondly, we firstly introduce motion vector as the input of CNN to avoid the heavy computational cost of optical flow. Finally, we propose techniques to transfer the knowledge of optical flow CNN to motion vector CNN, which significantly improves the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition has been widely studied in recent years. Early approaches extracted local spatio-time descriptors from input video and encoded these descriptors with Bag of Visual Words or its variants for classification. Laptev <ref type="bibr" target="#b16">[17]</ref> proposed spatio-time interest points by extending Harris corner into spatio-time dimension. Wang et al. <ref type="bibr" target="#b27">[28]</ref> further exploited trajectorires to model temporal relationship of continuous frames. Furthermore, Kviatkovsky et al. <ref type="bibr" target="#b15">[16]</ref> proposed a covariance descriptor to realize online action recognition. Popular local descriptors for video representation include HOG <ref type="bibr" target="#b1">[2]</ref>, HOF <ref type="bibr" target="#b16">[17]</ref>, MBH <ref type="bibr" target="#b2">[3]</ref> and TBC <ref type="bibr" target="#b28">[29]</ref>. And feature encoding techniques include hard quantization <ref type="bibr" target="#b23">[24]</ref>, VLAD <ref type="bibr" target="#b8">[9]</ref>, and Fisher Vector <ref type="bibr" target="#b21">[22]</ref>. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>  <ref type="table">exploited mid-level representations by proposing MoFAP and  Motionlets.</ref> Recent renaissance of deep neural network remarkably accelerates the progresses in image classification. Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b17">[18]</ref> can learn powerful features from large scale image datasets, which greatly alleviates the difficulty of designing hand-crafted features. Extensive experiments have demonstrated that CNN can achieve superior performance on various image and video classification tasks, e.g. ImageNet object classification <ref type="bibr" target="#b13">[14]</ref>, face recognition <ref type="bibr" target="#b25">[26]</ref>, and event classification <ref type="bibr" target="#b4">[5]</ref>. These successes inspire researchers to extend CNN for video classification tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Karpathy et al. <ref type="bibr" target="#b12">[13]</ref> proposed several convolutional neural network (CNN) architectures based on stacked RGB images for video classification. They designed several fusion strategy for RGB flows to utilize temporal information in stacked RGB frames. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref> modeled temporal information by designing 3D filter to directly learn feature from videos. Unlike 2D filters used in image classification, 3D filters can learn temporal relationship from continuous RGB frames. Simonyan and Zisserman <ref type="bibr" target="#b22">[23]</ref> proposed the two-stream architecture which exploits two CNNs to model RGB and optical flow respectively. This method achieved excellent performance in practice. We will use two-stream network as baseline of this study. Based on two-stream CNNs, Wang et al. <ref type="bibr" target="#b32">[33]</ref> proposed Trajectory-pooled Deep-Convolutional Descriptors to obtains the merits of CNNs and trajectory based method. Wang et al. <ref type="bibr" target="#b34">[35]</ref> further extended two-stream CNNs to very deep two-stream CNNs and achieve superior results on several datasets. Recent works also showed that the temporal structure in video contains discriminative information. Ng et al. <ref type="bibr" target="#b18">[19]</ref> utilized the recurrent LSTM architecture to capture temporal structure of actions. Wu et al. <ref type="bibr" target="#b35">[36]</ref> showed that integrating LSTM and two-stream methods can further improve the recognition performance. The main focus of this paper is to accelerate action recognition with deep learning while preserving the high performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motion Vector for Deep Action Recognition</head><p>Although two-stream CNNs <ref type="bibr" target="#b22">[23]</ref> achieve state-of-thearts performance in action recognition, it is computational expensive and cannot be deployed for real-time process. Two-stream CNNs consist of spatial net and temporal net, which take RGB image and optical flow as input respectively. In the testing phase, feed forward computation of CNNs can be conducted in short time (around 30ms) with GPU implementation. The most computationally expensive step in the original two-stream framework comes from the calculation of optical flows. With efficient implementation <ref type="bibr" target="#b11">[12]</ref>, it takes around 360ms to calculate optical flow (Farneback's Flow <ref type="bibr" target="#b3">[4]</ref>) for one frame in CPU. Even with GPU acceleration <ref type="bibr" target="#b22">[23]</ref>, the calculation still takes 60 ms (Brox's Flow <ref type="bibr" target="#b0">[1]</ref>) which cannot meet the requirement of real-time process. Thus optical flow is the main bottleneck that prohibits the classical two-stream to be real-time.</p><p>Similar to optical flow, motion vector is a twodimensional vector used for describing the moving offsets of local blocks with respect to a reference frame. Motion vector is widely used in various video compression standards, thus can be obtained directly in video decoding process without extra calculation. These facts make motion vector an attractive feature for efficient video content analysis. Actually, the previous work <ref type="bibr" target="#b11">[12]</ref> has used motion vectors together with VLAD encoding for action recognition and achieve good performance. Different from this work <ref type="bibr" target="#b11">[12]</ref>, however, we explore motion vector in the deep CNN framework. Here, the major challenge comes from the fact that motion vector has low resolution and is imprecise for describing fine motions. This fact can largely harm the recognition performance of CNN if we directly train networks with motion vectors.</p><p>To relieve this problem, we propose several training methods to enhance motion vector CNN (MV-CNN) for better recognition performance. Our key insight is that the knowledge and features learned with optical flow CNN (OF-CNN) can be helpful for MV-CNN. Thus, we may leverage OF-CNN as a teacher net to guide the training of MV-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motion Vector</head><p>We begin with a short introduction to motion vectors and then analyze the difficulty of training motion vector CNNs (MV-CNNs). Motion vectors are designed for describing macro blocks movement from one frame to the next, and are widely used in various video compression standards such MPEG series, HEVC. Temporal redundancy of two neighboring frames yields important cues for compressing video data. Motion vectors exploit temporal relationship in neighboring frames by recording how macro blocks move in the next and are one of the essential ingredients in modern video coding algorithms. <ref type="figure">Figure 1</ref> illustrates an example of motion vectors. As motion vectors are already calculated and encoded in compressed videos, we can obtain them at very low computational cost.</p><p>However, it is challenging to train MV-CNNs with high performance. This is because that motion vector is designed for video compression where precious motion information are not obligatory. Compared with optical flow, motion vectors exhibit coarse structure and may contain noisy and inaccurate movements. As shown in <ref type="figure">Figure 1</ref>, motion vector contains macro blocks with different sizes in motion estimation, ranging from 8×8 pixels to 16×16 pixels. Unlike dense optical flows, which are pixel-level and provide fine movement information of single pixel, motion vectors only yield block-level motion information. Thus motion vectors exhibit much coarser structures than optical flows. Fine details contained in optical flows can not be delivered in motion vectors, such as the structure of bow in Archery videos and the baseball hold by the man's hand. These information loss can badly impact the final performance of MV-CNN.</p><p>Secondly, noisy information contained in motion vector poses a barrier for MV-CNN to achieve high performance.</p><p>Unlike modern optical flow algorithm <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref>, motion vectors simply use three or four comparison steps to find the most matching block. There are much more noisy patterns contained in motion vectors than optical flow fields as shown in <ref type="figure">Figure 1</ref>. The inaccurate information and existence of noisy point are due to the fact that video compression algorithms need to balance between the speed of encoding and the compression rate. Therefore, motion vectors can only provide noisy block movement information, which hamper performance of temporal CNN.</p><p>Thirdly, not every frame contains motion vectors. Partial reason for this is that frames are clustered as group of pictures (GOP). One typical GOP contains three types of frames: I-frame, P-frame and B-frame. I-frame is an intra-coded frame encoded based on its own, which means I-frame contains no movement information. P-frame and B-frame are acronyms of predicted frame and bi-predictive frame respectively. They contain movement information. Clearly, in action recognition with CNNs, empty I-frame can hinder CNN training process and degrade the performance. In this paper, we deal with this problem by simply replacing I-frame with the motion vectors of previous frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Real-time Action Recognition Frameworks</head><p>Our proposed real-time action recognition framework contains two components ( <ref type="figure" target="#fig_0">Figure 2</ref>). The first component is video decoder, which extracts RGB images and motion vectors from input compressed videos. The second component follows the two-stream architecture <ref type="bibr" target="#b22">[23]</ref>, which can be decomposed into spatial CNN (RGB-CNN) and temporal CNN (MV-CNN). The main difference comes from the fact that we use motion vector as input for temporal CNN while two-stream uses optical flows. As both RGB images and motion vectors can be obtained directly from the video decoding process, our method avoids the computationally expensive step to estimate optical flow, which is most timecostly in the original two-stream framework.</p><p>In the training phase, we extract RGB images and frames of motion vectors from video. These images and motion vectors inherit the labels of original videos. To augment training samples, we crop image and MV frames in spatial domain. Then we train RGB-CNN and MV-CNN with these cropped samples, respectively. In the testing phase, raw images and motion vector frames from testing video are fed forward into RGB-CNN and MV-CNN. The action recognition decision will be made by weighted average of prediction scores from two CNNs. The weight for spatial and temporal CNN are set as 1 and 2.</p><p>We use the architecture of ClarifaiNet <ref type="bibr" target="#b36">[37]</ref> for both RGB-CNN and MV-CNN, since ClarifaiNet keeps a balance between efficiency and performance. For spatial net, dropout layers are added after FC6 and FC7 layer. We follow <ref type="bibr" target="#b22">[23]</ref> by setting dropout ratio for spatial CNN to 0.9 to avoid over-fitting. Spatial net is pre-trained on ImageNet ILSVRC-2012 datasets. Batch size for spatial and temporal nets are set as 256. The learning rate is initialized as 10 −3 and decreases into 10 −4 after 14k steps. The training procedure stops at 20k iterations.</p><p>Our temporal CNN is slightly different with ClarifaiNet by replacing all ReLU layers with PReLU <ref type="bibr" target="#b5">[6]</ref> layers, which exhibits better performance and quick convergence. Dropout ratio for temporal net is set to 0.9 and 0.8 after FC6 and FC7 layer respectively. Temporal CNN is trained from scratch by stacking 10-frame motion vectors as input. Its learning rate starts from 10 −2 , and drops to 10 −3 at 30k steps. We then decrease the learning rate to 10 −4 after 70k iterations. The whole learning process stops at 90k steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Enhanced Motion Vector CNNs</head><p>As analyzed above, motion vectors lack fine details and contain noisy and inaccurate motion patterns, which makes training motion vector CNN (MV-CNN) more challenging. We observe that simply replacing optical flow with motion vector can lead to significant recognition performance degradation of around 7%. In this section we try to keep the high speed merit of motion vectors while achieve the high performance as optical flow. Here our key insight to circumvent this difficulty is that motion vector and optical flow are inherent correlated to each other. This fact enables us to leverage the rich knowledge and fine features learned in optical flow CNN (OF-CNN) to enhance MV-CNN. This can be seen as a knowledge transfer problem from optical flow domain to motion vector domain. More specifically, we take OF-CNN as a teacher net and use it to teach the MV-CNN net. It should be noticed that OF-CNN teaches MV-CNN only in the training phase. Thus we do not need to calculate optical flows in testing phase, and the system speed performance will not be affected by the proposed algorithm.</p><p>In particular, we propose three strategies to transfer knowledge from OF-CNN to MV-CNN. To begin with, several notations are introduced at first. Parameters for teacher CNN in optical flow domain is denoted by T p = {T 1 p , T 2 p , ..., T n p }, where n represents the total number of layers. As for student CNN (MV-CNN), its parameter is defined as S p = {S 1 p , S 2 p , ..., S n p }. In this paper, we assume MV-CNN has the same network structure as OF-CNN, while the techniques can be easily generalized to those with different structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Teacher Initialization</head><p>Extensive works show that the initialization of network parameters can largely impact the final performance. Both optical flow and motion vector describe the motion information of local regions, and are inherently correlated. This fact inspires us to initialize the parameters of MV-CNN as those of its teacher's net OF-CNN, S t p = T t p , t = 1, ..., n. Then we fine-tune the parameters of MV-CNN with the motion vector samples until convergence. This process can also be seen as pre-training MV-CNN with the fine optical flow samples, which transfers the knowledge learned by teacher net to student net directly.</p><p>For implementation details, the learning rate is initiated from 10 −3 , and then drops to 10 −4 and 10 −5 at 30k and 70k steps respectively. The training stops at 90k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Supervision Transfer</head><p>Initialization of MV-CNN with OF-CNN's parameters is simple, but the initial knowledge transferred to MV-CNN may be lost during the fine-tuning process with motion vector samples. To relieve this problem, we introduce the supervision transferring approach which takes account of additional supervision from the teacher net in the training process of MV-CNN. OF-CNN can extract effective representation from input frame. Here we take the representation obtained in the FC layer of OF-CNN as a new supervision for training MV-CNN. This technique is in spirit similar to Hinton et al.'s recent work on knowledge distillation <ref type="bibr" target="#b6">[7]</ref>. The aim of knowledge distillation is to compress a cumbersome network (teacher) into a small network (student) which achieves similar performance as the cumbersome one. The cumbersome network and small network have the same input. Different from this work, in our problems, the teacher (OF-CNN) and the student (MV-CNN) take different types of input, i.e. optical flow vs. motion vector, but have the same network structure. Moreover, our objective is not to compress the student to a small net, but to enhance the performance of the student with low quality input.</p><p>Formally, for a given frame I with optical flow feature o and motion vector v , we calculate the output of the last FC layer of teacher CNN as: T n (o) = softmax(T n−1 (o)), where 'softmax' function is used to transform the feature T n−1 to a probability score of multiple classes. Similarly, the output of student's last layer is defined as: S n (v) = softmax(S n−1 (v)).</p><p>To transfer knowledge from teacher to student, we hope S n (v) can approximate T n (o) as closely as possible. We introduce a teacher supervision loss function to minimize the difference between S n (v) and T n (o). The crossentropy loss is used to measure the difference. Following <ref type="bibr" target="#b6">[7]</ref>, we introduce a temperature T emp to soften the nextto-last layer output. The softmax output of teacher net is softened as P T = softmax(T n−1 /T emp), Student net's softmax output for second target is similarly defined as: P S = softmax(S n−1 /T emp), Then the teacher supervision loss (TSL) with cross-entropy function is defined by :</p><formula xml:id="formula_1">L T SL = − k i=1 P T (i) log P S (i),<label>(2)</label></formula><p>where k is the dimension of the student's output (same as that of the teacher).</p><p>In addition to the teacher supervision loss, we also minimize the cross entropy between student's output and the ground truth Q, which is given by,</p><formula xml:id="formula_2">L GT = − i 1[Q = i] log S n (i),<label>(3)</label></formula><p>where S n and Q represent the hard output distribution vectors and the ground truth label respectively.</p><p>Our final loss function combines the teacher supervision loss (Eq.2) and the ground truth loss (Eq.3):</p><formula xml:id="formula_3">L = L T SL + w × L GT (4)</formula><p>where w is a weight to balance these two terms. Thus, the student MV-CNN can receive supervision signal from both the teacher OF-CNN and the ground truth. It should be noted that in supervision transfer, the weights of teacher model are frozen. In this way, the knowledge is transferred from the teacher net to the student one. For implementation details, we set the initial learning rate as 10 −3 . We decay the learning rate to 10 −4 at 50k and 10 −5 at 70k steps. The whole training procedure stops at 90k iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Combination</head><p>In the third strategy, we combine the initialization with teacher's parameters and supervision transfer to further enhance the performance of student's net. We first train the teacher OF-CNN with optical flow fields. Then we initialize the student MV-CNN with OF-CNN's parameters. After this, we train the student net with supervision signal from both the teacher and the ground truth (Eq. 4). In this way, the student net not only inherits teacher's parameters from initialization, but also mimics teacher net's prediction during fine tuning procedure. This allows us to combine the merits of two previous strategies and further boost the generalization ability of student MV-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>In this section we first describe the evaluation datasets. Then we report and analyze the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Protocol</head><p>The evaluation is conducted on two challenging datasets: UCF101 <ref type="bibr" target="#b24">[25]</ref> and THUMOS14 <ref type="bibr" target="#b10">[11]</ref>. UCF101 contains 13,320 videos. The datasets has three splits for training and testing. We follow the standard setup and report average accuracy over three splits on this datasets.</p><p>THUMOS14 is a dataset for action recognition challenge 2014. It contains 13,320 videos for training, 1,010 videos for validation, and 1,574 videos for testing. Unlike UCF101, THUMOS14 uses untrimmed videos for validation and testing. Lots of irrelevant frames make the training and testing of CNNs more difficult. We use training and validation datasets to train our CNNs. Official evaluation tool is utilized to evaluate our system performance. According to the standard setup of this dataset, we report the mean Average Precision (mAP) on testing dataset. As videos in THUMOS14 are untrimmed and have large number of frames, we conduct CNN testing at every 20 frames.</p><p>For experiments on both datasets, the speed evaluation is measured as frames per seconds (fps) on a single-core CPU (E5-2640 v3) and a K40 GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>In order to learn robust features from CNNs, we use three data augmentation strategies. Firstly, we randomly crop a 224 × 224 patch from image set. Random cropping can provide more training data for CNNs to learn better features. Secondly, we horizontally flip cropped patches by random. Furthermore, following [35] 1 , we use a scale jittering strategy to help CNN to learn robust features. We crop a patch from dataset on three scales 1, 0.875, and 0.75, which yield patches of size 224 × 224, 196 × 196 and 168 × 168 respectively. The patches are then resized to 224 × 224 after this multi-scale strategy. In testing phase, we crop one 224×224 patch from the center of testing image. No data augmentation strategy is used in evaluation phase.</p><p>Our teacher CNN is trained on TV-L1 optical flow <ref type="bibr" target="#b20">[21]</ref> that achieves 81.6% on UCF101 Split1, which is comparable with the performance in the original paper 81.2% <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Parameter Sensitivity</head><p>We first analyze the parameter sensitivity. There are two important parameters in our real-time action recognition system: temperature T emp for Supervision Transfer and weight w for soft target. Following <ref type="bibr" target="#b6">[7]</ref> we set soft target weight w = T emp 2 to balance the gradients of soft target. We test three different temperature settings on UCF101 split1 by setting T emp to 1, 2 and 3. The corresponding soft target weight w is set as 1, 4 and 9. Accuracy grows up from 79.2% to 79.3% from temperature 1 to 2 and goes down to 78.9% at temperature 3. We can see that the recognition results of different temperatures are very close, which implies that our method is robust to temperature. According to this study, we set T emp = 2 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation of MV-CNNs and EMV-CNNs</head><p>In this subsection, we compare and analyze different training strategies for motion vector CNN (MV-CNN) on UCF101 Split1 and THUMOS14. The result are shown in <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref>. As <ref type="bibr" target="#b22">[23]</ref> did not provide results on THUMOS14, we re-implement two-stream CNNs on this dataset.</p><p>First, from these results, we can conclude that directly using motion vector to replace optical flow degrades system's performance. Compared with OF-CNN, MV-CNN trained from scratch degrades performance by 7% and 25%, which achieves 74.4% and 29.8% on UCF101 Split1 and THUMOS14 respectively. It indicates that coarse structure and imprecise movement information in motion vector harm the performance of CNNs. The performance on THUMOS14 is particularly large, as validation and testing videos in THUMOS14 are untrimmed and labels for a large part of frames are not correct. Furthermore, lots of shots shift in videos aggravate the difficulties of training MV-CNN on THUMOS14 dataset.</p><p>Second, comparing Enhanced Motion Vector CNN (EMV-CNN) and MV-CNN, we observe a performance improvement of 4.9% and 12% on UCF101 Split1 and THU-MOS14 respectively. This indicates that our newly designed method for training EMV-CNN is effective and can improve the generalization ability of MV-CNNs.</p><p>Finally, combined with spatial CNN, EMV-CNN still outperforms MV-based CNN, which indicates that the knowledge of EMV-CNN can be more complementary to spatial CNN. In addition, combining EMV-based with spatial CNN exhibits only a minor performance loss compared with OF-based CNN on UCF101. Specifically, we study the effect of our proposed transferring techniques as follows.</p><p>Teacher initialization technique can provide an improvement of 3.8%. From this result, we can see, similar to the study in image classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>, teacher initialization on OF-CNN can benefit the MV-CNN by providing a good initial point to train. Supervision transfer strategy can provide 3.1% performance improvement on UCF101 Split1. It shows that knowledge provided by OF-CNN during supervision transfer process in training phase can be helpful to MV-CNN. Combing supervision transfer and teacher ini- Accuracy FPS MV+FV (CPU) (re-implement) <ref type="bibr" target="#b11">[12]</ref> 78.5% 132.8 C3D (1 net) (GPU) <ref type="bibr" target="#b26">[27]</ref> 82.3% 313.9 C3D (3 net) (GPU) <ref type="bibr" target="#b26">[27]</ref> 85.2% -iDT+FV (CPU) <ref type="bibr" target="#b27">[28]</ref> 85.9% 2.1 Two-stream CNNs (GPU) <ref type="bibr" target="#b22">[23]</ref> 88.0% 14.3 EMV+RGB-CNN 86.4% 390.7 <ref type="table">Table 7</ref>. Comparison of speed and performance with state-of-theart on UCF101 tialization strategy can further boost the performance. As indicated in <ref type="bibr" target="#b6">[7]</ref>, supervision transfer can be used to regularize MV-CNN from over-fitting. Supervision strategy utilizes the soft codes produced by OF-CNNs during training process. These codes are further softened by dividing T emp. Unlike hard ground truth labels, these soft targets encode more rich information and guide the training of EMV-CNN, contributing to improve the generalization ability of EMV-CNNs.</p><formula xml:id="formula_4">UCF101(Split1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Speed Evaluation</head><p>In this subsection, we analyze the speed of different components in our action recognition approach. In our implementation, We use CPU to extract motion vector while use GPU to conduct the feed forward process of CNN. It should be noticed that our system process a volume of 10 frames of MV and 1 frame of RGB together. Our speed is measured based on frames instead of volume.</p><p>We first compare the speed performance of 1 crop with 5 crops+mirror on UCF101 and THUMOS14 in <ref type="table">Table 5</ref> and <ref type="table">Table 6</ref>. We can observe that 1 crop only achieves a slight performance degradation than 5 crops+mirror, while 1 crop is 4 times faster than the latter.</p><p>Next, we evaluate speed performance of each component on UCF101 datasets and THUMOS14 datasets. The spatial resolution of video in UCF101 dataset is 320 × 240, while for video in THUMOS14, its resolution is 320 × 180. As shown in <ref type="table">Table 3</ref>, our action recognition system achieves 390.7 fps and 403.2 fps on UCF101 and THUMOS14 respectively, which is one order faster than real-time requirement (25 fps).</p><p>Finally, we compare the efficiency of extracting motion vectors and optical flow in <ref type="table">Table 4</ref>. Motion vector extraction is almost 30 times faster than real-time requirement. Although the spatial resolutions for videos in UCF101 and THUMOS14 are different, the time consumption for extraction of motion vectors are similar. The estimation of motion vector in CPU is 44 times faster than calculating Brox's flow <ref type="bibr" target="#b0">[1]</ref> with GPU. The calculation of optical flows poses the main bottleneck in boost the speed of classical two-stream framework, which prohibits it to be conducted in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison with the State of the Art</head><p>In this section, we compare our method with several state-of-the-art methods. Since the I/O is related to hardware and operating system, we report computational time cost without I/O. Unlike those using Support Vector Machine (SVM) to perform classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, our action recognition approach has an end-to-end structure that combines feature extraction and classification together.</p><p>We first compare speed and accuracy performance on UCF101 (3 Splits) dataset. Results are given in <ref type="table">Table 7</ref>. Our method is 3 times and 180 times faster than motion vector + fisher vector (MV FV) and iDT+FV respectively. It should be noted that this may not be a fair comparison since iDT and FV are implemented in CPU. At the same time, our method achieves higher accuracy than these methods. For MV FV, we use the public code offered by <ref type="bibr" target="#b11">[12]</ref>. Although iDT used Farneback optical flows <ref type="bibr" target="#b3">[4]</ref> which provides more precise movement information than motion vector, our methods still obtain higher performance than iDT.</p><p>We also make comparison with deep learning methods, namely two-stream CNNs and C3D. Our method achieves 390.7 fps on UCF101 and is the fastest among all methods compared ( <ref type="table">Table 7)</ref>. And our recognition accuracy is 4.1% and 1.2% higher than C3D (1 net) and C3D (3 net). As for two-stream CNNs, our method achieves comparable results but ours is 27 times faster than them.</p><p>Finally, we compare our speed and mean average precision with <ref type="bibr" target="#b7">[8]</ref> on THUMOS14 dataset <ref type="table">(Table 8</ref>). Our Accuracy FPS Objects (GPU) <ref type="bibr" target="#b7">[8]</ref> 44.7% -iDT+CNN (CPU+GPU) <ref type="bibr" target="#b31">[32]</ref> 62.0% &lt; 2.38 Motion (iDT+FV) (CPU) <ref type="bibr" target="#b7">[8]</ref> 63.1% 2.38 Objects+Motion (CPU+GPU) <ref type="bibr" target="#b7">[8]</ref> 71.6% &lt; 2.38 EMV+RGB-CNN 61.5% 403. <ref type="table">2   Table 8</ref>. Comparison of speed and performance with state-of-theart on THUMOS14 method achieves better performance than Objects for 16.8% and obtains comparable performance with Motion (iDT) and iDT+CNN, but exhibits worse performance than Ob-jects+Motion. However, as Objects+Motion is built on the iDT features, our method is 200 times faster than it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Visualization of Filters</head><p>In order to further explore the effectiveness of EMV-CNN, we visualize filters of the first layer (Conv1) for MV-CNN, EMV-CNN and OF-CNN in <ref type="figure" target="#fig_2">Figure 4</ref>. It can be clearly noticed that filters of MV-CNN contain coarse and noisy information, which may be ascribed to the fact that motion vector lacks fine grained information as shown in <ref type="figure">Figure 1</ref>. Compared with filters of MV-CNN, EMV-CNN can obtain detailed information, which shows that our proposed learning scheme successfully transfer knowledge from OF-CNN to EMV-CNN. Filters of EMV-CNN are cleaner than MV-CNN and have fine structures, contributing to improve recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper we have proposed a motion vector CNN to accelerate the speed of deep learning methods for action recognition. Motion vectors can be extracted directly in video decoding process without extra computation. However, motion vectors lacks fine and accurate motion information which degrades recognition performance. To relieve this problem, we developed three knowledge transfer techniques to adapt the models of optical flow CNN to motion vector CNN, which significantly boost the recognition performance of the latter. Our method achieves 391 fps and 403 fps with high performance on UCF101 and THUMOS14 respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Structure for real-time action recognition system. In spatial and temporal CNN, F stands for kernel size and S means stride step. O represents for output number and P is pad size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Structure of three strategies. Blue dash lines represent copying the initial weights from teacher net to student net. Green lines are the backward propagation path. Blue full lines mean feed forward paths of teacher flow. Orange lines are feed forward paths of student net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Samples of filters for Conv1 layer. Left to right: MV-CNN, EMV-CNN and OF-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .Table 4 .</head><label>234</label><figDesc>Performance of EMV-CNNs and MV-CNNs on THU-MOS 14 dataset. We also report the results of two-stream CNNs. Speed of each components in Real-time Action Recognition System. MV stands for motion vector extraction, while CNN means convolutional neural network processing. Comparison of speed for optical flow fields and motion vectors. MV means motion vector.</figDesc><table><row><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell>MAP</cell></row><row><cell></cell><cell>RGB CNN</cell><cell></cell><cell></cell><cell>57.7%</cell></row><row><cell></cell><cell>OF-CNN</cell><cell></cell><cell></cell><cell>55.3%</cell></row><row><cell></cell><cell cols="3">RGB CNN+OF-CNN</cell><cell>66.1%</cell></row><row><cell></cell><cell>MV-CNN</cell><cell></cell><cell></cell><cell>29.8%</cell></row><row><cell></cell><cell>EMV-CNN</cell><cell></cell><cell></cell><cell>41.6%</cell></row><row><cell></cell><cell cols="3">RGB CNN+MV-CNN</cell><cell>58.7%</cell></row><row><cell></cell><cell cols="4">RGB CNN+EMV-CNN 61.5%</cell></row><row><cell></cell><cell></cell><cell>MV</cell><cell cols="2">CNN Total</cell></row><row><cell></cell><cell>Dataset</cell><cell cols="2">(fps)</cell><cell>(fps)</cell><cell>(fps)</cell></row><row><cell></cell><cell>UCF101</cell><cell cols="3">735.3 833.3 390.7</cell></row><row><cell></cell><cell cols="4">THUMOS14 781.3 833.3 403.2</cell></row><row><cell></cell><cell cols="2">Spatial</cell><cell cols="2">Brox's Flow[1]</cell><cell>MV</cell></row><row><cell>Dataset</cell><cell cols="2">Resolution</cell><cell cols="2">(GPU) (fps)</cell><cell>(CPU) (fps)</cell></row><row><cell>UCF101</cell><cell cols="2">320 × 240</cell><cell></cell><cell>16.7</cell><cell>735.3</cell></row><row><cell cols="3">THUMOS14 320 × 180</cell><cell></cell><cell>17.5</cell><cell>781.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/yjxiong/caffe</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;14</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient feature extraction, encoding, and classification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2593" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;12</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;11</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Online action recognition using covariance of shape and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kviatkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice. CoRR, abs/1405</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4506</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TV-L1 optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPOL Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="222" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;13</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">C3D: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human action recognition with trajectory based covariance descriptor in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1175" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mining motion atoms and phrases for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2680" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Motionlets: Mid-level 3D parts for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2674" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THUMOS Action Recognition challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">MoFAP: A multi-level representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards Good Practices for Very Deep Two-Stream ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
