<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HIN: Hierarchical Inference Network for Document-Level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
							<email>tanghengzhu@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
							<email>caoyanan@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
							<email>zhangzhenyu1996@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
							<email>caojiangxia@iie.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
							<email>fangfang0703@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Wang</surname></persName>
							<email>wangshi@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
							<email>yinpengfei@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HIN: Hierarchical Inference Network for Document-Level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Relation extraction · Hierarchical inference network · Multi granularity</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level RE requires reading, inferring and aggregating over multiple sentences. From our point of view, it is necessary for document-level RE to take advantage of multi-granularity inference information: entity level, sentence level and document level. Thus, how to obtain and aggregate the inference information with different granularity is challenging for document-level RE, which has not been considered by previous work. In this paper, we propose a Hierarchical Inference Network (HIN) to make full use of the abundant information from entity level, sentence level and document level. Translation constraint and bilinear transformation are applied to target entity pair in multiple subspaces to get entity-level inference information. Next, we model the inference between entity-level information and sentence representation to achieve sentence-level inference information. Finally, a hierarchical aggregation approach is adopted to obtain the document-level inference information. In this way, our model can effectively aggregate inference information from these three different granularities. Experimental results show that our method achieves state-of-the-art performance on the large-scale Do-cRED dataset. We also demonstrate that using BERT representations can further substantially boost the performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims to detect the semantic relation between entities in plain text, which plays an important role in knowledge base population and natural language understanding. Most previous work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, deep learning models have been widely applied to sentence-level RE and achieved remarkable success <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Input: <ref type="bibr" target="#b0">[1]</ref> "Nisei" is the ninth episode of the third season of the American science fiction television series The X-Files. <ref type="bibr" target="#b1">[2]</ref> It premiered on the Fox network on November 24, 1995. <ref type="bibr" target="#b2">[3]</ref> It was directed by David Nutter, and written by Chris Carter, Frank Spotnitz and Howard Gordon. <ref type="bibr" target="#b3">[4]</ref> "Nisei" featured guest appearances by Steven Williams, Raymond J. Barry and Stephen McHattie ... <ref type="bibr" target="#b7">[8]</ref> The show centers on FBI special agents Fox Mulder (David Duchovny) and Dana Scully (Gillian Anderson) who work on cases linked to the paranormal, called X-Files … Subject: Chris Carter Object: Fox Mulder Relation: creator Supporting Sentences: 1, 3, 8 Despite the great success of previous work, sentence-level RE suffers from a serious restriction in practice: a large amount of relational facts are expressed in multiple sentences. Taking <ref type="figure" target="#fig_0">Figure 1</ref> as an example, in order to identify the relational fact (Chris Carter, creator, Fox Mulder ), one should first identify the fact "Nisei" is an episode of the American science fiction television series from sentence 1, then identify the facts that Fox Mulder is a character in "Nisei" and Chris Carter is one of the writers of "Nisei" from sentence 8 and 3 respectively. To extract these relational facts, it is necessary to infer and aggregate over multiple sentences. Obviously, most traditional sentence-level RE models often fail to generalize extraction to this situation. To move RE forward from sentence level to document level, many efforts have been made <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, but most previous methods used only entity-level information and this is not adequate. Thus, there are still some deep-seated problems unsolved in document-level RE.</p><p>To predict the relation between two entities, we argue that the documentlevel RE model requires taking advantage of multi-granularity inference information: entity level, sentence level and document level. Lets go back to the former example, entity-level inference information is derived from the semantic of all mentions of Chris Carter and Fox Mulder in the document, sentence-level inference information represents the information related to relational facts in each sentence, document-level inference information aggregates all the necessary information in supporting sentences (sentence 1, 3 and 8) and discards information in noise sentences. Technically, it is clear that document-level RE faces two main challenges: <ref type="bibr" target="#b0">(1)</ref> How to obtain the inference information with different granularity; (2) How to aggregate these different granularity inference information and make the final prediction.</p><p>In this paper, we propose a new neural architecture, Hierarchical Inference Network (HIN), to tackle above challenges. Specifically, inspired by translation constraint <ref type="bibr" target="#b0">[1]</ref>, which models a relational fact r(e h , e t ) with e h + r ≈ e t , we apply this translation constraint to target entity pair. Besides, a bi-affine layer is also used to obtain bilinear representation for the target entity pair. To jointly attend to information from different representation subspaces, we implement the above two transformations in multiple subspaces in parallel, and acquire entitylevel inference information. To obtain the sentence-level inference information, we first apply vanilla attention mechanism to calculate the vector representation for each sentence, which enables our model to pay more attention to the informative words. Then we adopt the semantic matching method which is widely used in natural language inference (NLI) domain to compare the entity-level inference information with each sentence vector. Furthermore, in order to calculate the document-level inference information, we apply a hierarchical BiLSTM and again use attention mechanism to distinguish crucial sentence-level inference information for overall document-level inference representation. Finally, we aggregate inference information of different granularity, the entity-level and document-level inference representations are combined into a fixed-length vector, which is further fed into a classification layer for prediction.</p><p>To summarize, we make the following contributions: 1. We propose a Hierarchical Inference Network (HIN) for document-level RE, which is capable of aggregating inference information from entity level to sentence level and then to document level.</p><p>2. We conduct thorough evaluation on DocRED dataset. Results show that our model achieves the state-of-the-art performance. We further demonstrate that using BERT representations further substantially boosts the performance.</p><p>3. We analyze the effectiveness of our model on different number of supporting sentences and experimental results show that our model performs much better than previous work when the number of supporting sentences is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>For document-level RE, the input is a document with annotated entities, as well as multiple occurrences of each entity, i.e., entity mentions, the goal is to identify all the related entity pairs in the document. Following <ref type="bibr" target="#b14">[15]</ref>, we transform RE into a classification problem. We use upper case letters to represent entities (E 1 , · · · , E m ) and lower case letters to represent mentions (e 1 , · · · , e m ). The RE model is given a relation candidate (E a , E b , D) and expected to output the relations between E a and E b , where E a and E b are entities in the document D.</p><p>3 Proposed Approach <ref type="figure" target="#fig_1">Figure 2</ref> gives an illustration of our model. We describe the details of different components in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Layer</head><p>-Word Embeddings In order to capture the meaningful semantic information of words, we map each word into a low-dimensional word embedding vector. The dimension of word embeddings is d w .</p><formula xml:id="formula_0"># % &amp; ' )&amp; )# )'</formula><p>Sentence-level Inference -Entity Type Embeddings We utilize the entity type information to enrich the representation of the input. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) into a vector. The dimension of entity type embeddings is d t . -Coreference Embeddings Usually each entity may be mentioned many times in a document. Following previous work, we assign entity mentions corresponding to the same entity with the same entity id, which is determined by the order in which entities appear in the document. Then entity ids are embedded into vectors. The dimension of coreference embeddings is d c .</p><formula xml:id="formula_1">* 1 2 sigmoid 3 )3 …… …… ℎ&amp;&amp; ℎ&amp;# ℎ&amp;5 6 &amp;&amp; &amp;# &amp;56 … ℎ#&amp; ℎ## ℎ#5 8 #&amp; ## #58 … ℎ'&amp; ℎ'# ℎ'5 9 '&amp; '# '59 … ℎ3&amp; ℎ3# ℎ35 : 3&amp; 3# 35: … #&amp; ## #58 … '&amp; '# '59 … 3&amp; 3# 35: … &lt; a a a Document-level Inference … … … … …… Entity-level Inference &amp;&amp; &amp;# &amp;56 … &lt; &lt; &lt; = ? @</formula><p>We concatenate all three embeddings together for each word w i , and a document is transformed into a matrix X = [w 1 , w 2 , . . . , w n ], where each word vector w i ∈ R dw+dt+dc and n is the length of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity-Level Inference Module</head><p>In this section, we compute the entity-level inference information for target entity pair. To represent each word in its context, we encode the document</p><formula xml:id="formula_2">X = {w i } n i=1 into a hidden state vector sequence {h i } n i=1 with bi-directional LSTM: hi = BiLSTME (wi) , i ∈ [1, n].<label>(1)</label></formula><p>where h i ∈ R d is a contextualized representation of w i , summarizing the context information centered around w i .</p><p>Considering that an entity may be mentioned many times in a document and a mention may also contain more than one word, we represent each entity and mention with the average of the embeddings of different elements. Correspondingly, the mention representation is formed as the average of the words that the mention contains, the entity representation is computed as the average of the mention representations associated with the entity:</p><formula xml:id="formula_3">e l = avgw i ∈e l (hi), Ea = avge l ∈Ea (e l )<label>(2)</label></formula><p>We claim that it is beneficial to allow the model to jointly attend to information from different representation subspaces, thus, we use different learnable projection matrices to project entities into K subspaces:</p><formula xml:id="formula_4">E k a = W (1) k (ReLU (W (0) k Ea)) (3) where E k a ∈ R k corresponds to the representation of E a in the k-th latent space, W (0) k ∈ R d×d and W (1) k</formula><p>∈ R d×k are the learnable projection matrices corresponding to the k-th subspace. For each of these projected versions, we perform the entity-level inference in parallel. These are concatenated and once again projected, resulting in the final entity-level inference information.</p><p>Inspired by TransE <ref type="bibr" target="#b0">[1]</ref> which modelled a triple r(e h , e t ) with e h + r ≈ e t , we argue that (E b − E a ) could represent the relation between E a and E b in the document to some extent. In addition, a bilinear representation can be obtained by a bi-affine layer to enhance the expression ability of model. We define the following formula as entity-level inference representation in the k-th latent space:</p><formula xml:id="formula_5">I k e = Concat E k a R k E k b ; E k b − E k a ; E k a ; E k b (4)</formula><p>where R k ∈ R k×k×k is a learned bi-affine tensor, Concat denotes concatenation. Moreover, we believe that the relative distances between two target entities can help us better judge the relations. Empirically, we use the relative distances between the first mentions of the two entities as the relative distances between two target entities. Finally, all entity-level inference representations in different latent space and the relative distance embeddings are fed into a feed-forward neural network (FFNN) to form the final entity-level inference information:</p><formula xml:id="formula_6">Ie = Ge I 1 e ; ...; I K e ; M (d ba ) − M (d ab )<label>(5)</label></formula><p>here G e is a FFNN with ReLU activation function, M is an embedding matrix, d ab and d ba are the relative distances between E a and E b in the document. I e ∈ R d describes relation features between E a and E b at entity level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Document-Level Inference Module</head><p>In this section, we propose a hierarchical inference mechanism, inference information is aggregated from entity level to sentence level and then to document level. In this way, our model can aggregate all useful information of the document.</p><p>Sentence-Level Inference Assume that a document contains L sentences, and w jt represent the t-th word in the j-th sentence. Given the j-th sentence S j , to represent words in its context, the sentence is fed into a BiLSTM encoder:</p><formula xml:id="formula_7">hjt = BiLSTMS (wjt) , t ∈ [1, Tj].<label>(6)</label></formula><p>Since different words in a sentence are differentially informative, inspired by <ref type="bibr" target="#b13">[14]</ref>, we introduce the vanilla attention mechanism to enable our model to selectively assign higher weights for the informative words and lower weights for the other words. Then we aggregate the representations of those informative words to form a sentence vector. Specifically,</p><formula xml:id="formula_8">α jt = u w tanh (W w h jt + b w )<label>(7)</label></formula><formula xml:id="formula_9">a jt = exp (α jt ) t exp (α jt ) (8) S j = t a jt h jt<label>(9)</label></formula><p>where u w , b w ∈ R d and W w ∈ R d×d are learnable parameters. Word hidden state h jt ∈ R d is first fed through a one-layer MLP, then we obtain weights of words by measuring "which words are more related to the target entities". Finally, we compute the sentence vector S j as a weighted sum of the word hidden states. For obtaining the sentence-level inference information, we adopt a semantic matching method which is used in previous NLI model <ref type="bibr" target="#b1">[2]</ref>. Through comparing sentence vector S j with entity-level inference representation I e , we can derive sentence-level inference representation I sj for the j-th sentence:</p><formula xml:id="formula_10">Isj = Gs ([Sj; Ie; Sj − Ie; Sj • Ie]) .<label>(10)</label></formula><p>where G s is FFNN with ReLU function, a matching trick with elementwise subtraction and multiplication is used for building better matching representations <ref type="bibr" target="#b9">[10]</ref>. I sj represents the inference information derived from the j-th sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-Level Inference</head><p>In order to distinguish crucial sentence-level inference information for overall document-level inference representation, vanilla attention mechanism is again used. We build a BiLSTM followed by the attention network on top of the sentence-level inference vectors (I s ) to aggregate all essential evidence information scattered in different sentences:</p><formula xml:id="formula_11">c sj = BiLSTM D (I sj ) , j ∈ [1, L]<label>(11)</label></formula><formula xml:id="formula_12">α j = u s tanh (W s c sj + b s )<label>(12)</label></formula><formula xml:id="formula_13">a j = exp (α j ) j exp (α j )<label>(13)</label></formula><formula xml:id="formula_14">I d = t a j c sj<label>(14)</label></formula><p>here u s , b s ∈ R d and W s ∈ R d×d are learnable parameters, I d ∈ R d is the document-level inference representation which represents all the inference information that we can obtain from the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction Layer</head><p>To better integrate inference information of different granularity, we concatenate entity-level inference representation I e and document-level inference representation I d together to form the final inference representation. Since there are often multiple relations holding between an entity pair, we use a FFNN with the sigmoid function to calculate the probability of each relation:</p><formula xml:id="formula_15">P (r|Ea, E b ) = sigmoid Wr Ie I d + br .<label>(15)</label></formula><p>where W r , b r are the weight matrix and bias for the linear transformation. A binary label vector y is set to indicate the set of true relations holding between the entity pair, where 1 means an relation is in the set, and 0 otherwise. In our experiments, we use the binary cross entropy (BCE) as training loss:</p><formula xml:id="formula_16">Loss = − l r=1 yr log (pr) + (1 − yr) log (1 − pr) .<label>(16)</label></formula><p>where y r ∈ {0, 1} is the true value on label r and l is the number of relations. Given a document, we rank the predicted results by their confidence and traverse this list from top to bottom by F1 score on dev set, the probability value corresponding to the maximum F1 is picked as threshold δ. This threshold is used to control the number of extracted relational facts on test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To evaluate the effectiveness of our model, we use the DocRED dataset <ref type="bibr" target="#b14">[15]</ref>, which is the largest human-annotated document-level RE dataset constructed from Wikidata and Wikipedia. DocRED contains over 5,053 documents, 40,276 sentences, 132,375 entities and 96 frequent relation types. Entity types in Do-cRED are annotated. It is also introduced by the author of DocRED that about 40.7% of relational facts can only be extracted from multiple sentences and 61.1% relational instances require a variety of reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Models &amp; Evaluation Metrics</head><p>We compare our model against the following document-level RE baselines:</p><p>CNN/LSTM/BiLSTM-RE: They first encode a document into a hidden state vector sequence with CNN/LSTM/BiLSTM as encoder, and then predict relations for each entity pair by feeding them into a bilinear function <ref type="bibr" target="#b14">[15]</ref>. Context-Aware: It uses an LSTM-based encoder to jointly learn representations for all relations in the context, and then combines other context relations with target relation to make the final prediction <ref type="bibr" target="#b11">[12]</ref>.</p><p>BERT-RE: It uses BERT to encode the document, entities are represented by their average word embedding. A BiLinear layer is applied to predict the relation between entity pairs <ref type="bibr" target="#b12">[13]</ref>.</p><p>BERT-Two-Step: Based on BERT-RE, it models the document-level RE through a two-step process. The first step is to predict whether or not two entities have a relation, the second step is to predict the specific relation <ref type="bibr" target="#b12">[13]</ref>.</p><p>HIN: This is the main model of this paper. Multi-granularity inference information is used to better model complex interactions between entities.</p><p>The widely used metric F1 is used in our experiments. Moreover, since some relational facts present in both training and dev/test sets, we also report the F1 excluding those relational facts and denote it as Ign F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We try two embedding methods in our experiments: 100-dimensional GloVe <ref type="bibr" target="#b10">[11]</ref> embeddings and BERT representations <ref type="bibr" target="#b2">[3]</ref>. For the BERT representations, the base uncased English model with dimension 768 is used, we map word representations into 100 dimensional vectors by a linear projection layer. Once the word representations are initialized, they are fixed during training. The embedding dimensions of coreference, distance and entity type are all set to be 20. For LSTM encoder, the dimension of the hidden units is 128. The number of latent space is 2. Furthermore, we regularize our network using dropout and the dropout ratio is 0.2. We optimized our model using Adam <ref type="bibr" target="#b4">[5]</ref>, with learning rate of 10 −4 , β 1 = 0.9, β 2 = 0.999. The batch size is set to be 12 and the value of threshold δ is determined by the performance on the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results and Analyses</head><p>Overall Performance Experimental results are shown in <ref type="table" target="#tab_0">Table 1</ref>. From the results, we can observe that: (1) Compared with BiLSTM-RE, the state-of-the- art model without BERT, our HIN-GloVe achieves significant improvements of 2.24% in F1, we claim that it is mainly due to the reasoning mechanism and hierarchical aggregation structure in HIN, which will be further discussed in ablation study.</p><p>(2) Even though BERT based models provides strong prediction power, HIN-BERT consistently improves over them, which further proves the effectiveness of our hierarchical inference network. <ref type="formula">(3)</ref> Although Context-Aware model combines context relations with the target relation, it can't use the evidence information in document as effectively as HIN. Hence our model also outperforms it by 2.60% in F1. (4) BERT representations further boost the performance of our model, the HIN-BERT approach outperforms all these previous methods, which indicates the importance of prior knowledge.</p><p>Ablation Study To study the contribution of each component in HIN-BERT, we run an ablation study on DocRED dev set (see <ref type="table" target="#tab_1">Table 2</ref>). From these ablations, we find that: (1) When we remove the translation mechanism and bilinear transformation, F1 score drops by 1.21% and 2.02% respectively, which indicates that these two transformations can enhance the expression ability of HIN at the entity level. (2) Removing the multi-space projection hurts the result by 1.72%, which proves that it is beneficial to allow the model to jointly attend to information from different representation subspaces. (3) F1 drops by 1.25% when we remove the sentence-level inference mechanism, i.e., replacing the sentence-level inference vector with sentence vector. (4) F1 drops by 2.81% when we discard the hierarchical aggregation approach. Instead, we run BiLSTM followed by meanpooling layer over the whole document to get the document vector. <ref type="bibr" target="#b4">(5)</ref> We also observe that F1 drops by 4.21% when we discard the above all factors together. In summary, all components play an important role in our model.</p><p>Analysis by the number of supporting sentences As we discussed before, it is challenging for document-level RE to reason from multiple sentences. To further prove the effectiveness of HIN, we analyze the recall on relational facts with different number of supporting sentences here. <ref type="bibr" target="#b3">4</ref> As shown in <ref type="figure" target="#fig_2">Figure 3</ref> find that our model always performs better than other baselines, especially when the number of supporting sentences increases gradually. More specifically, HIN-GloVe even outperforms BERT-RE when the number of supporting sentences exceeds 4, which fully proves the superiority of HIN. Note that when the number of supporting sentences exceeds 7, HIN-GloVe and other baselines behave the same. We think this is because there are very few samples with more than 7 supporting sentences in dev set. We believe when the number of relational facts with more supporting sentences increase our model will achieve better results.</p><p>Case Study We compare our model with BERT-RE on some cases from dev set, as shown in <ref type="table" target="#tab_2">Table 3</ref>. (1) Example 1 represents the situation that logical reasoning is required. Specifically, in order to identify the relational fact, we have to first identify the fact that Galaxy S series is a line of Samsung from sentence 0 and 2, then identify the fact Samsung Galaxy S9 is the latest smartphones in the Galaxy S series from sentence 4. We explain that our model uses a hierarchical aggregation approach to collect inference information from multiple sentences, so that it can better deal with this complex inter-sentence relationship. (2) Example 2 represents the case of coreference reasoning. In this situation, we claim that the attention and reasoning mechanisms in sentence-level inference module can help us to identify that "He" refers to Robert Kingsbury Huntington in sentence 3. In the end, our model can identify the right relation while BERT-RE mistakenly assumes that Los Angeles is the place where Robert Kingsbury Huntington died.</p><p>(3) Example 3 is a case that needs to combine context information with commonsense knowledge. Through some external common-sense knowledge, we might know that South America is a continent and So Paulo is a city, which is the useful information to help judge their relation. We think the problem can be solved by adding some external knowledge and we leave it as our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In recent years, more and more neural models have been applied to RE. Zeng el al. <ref type="bibr" target="#b16">[17]</ref> employed a one-dimensional CNN with additional lexical features to encode relations. Miwa et al. <ref type="bibr" target="#b8">[9]</ref> used LSTM with tree structures for RE. Zhou el al. <ref type="bibr" target="#b17">[18]</ref> showed that combining CNN/RNN with attention mechanism can further improve performance. And the emergence of various optimization algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> makes these neural models more effective. Most existing RE work focuses on modeling within a single sentence. However, usually documents provide more information than sentences. Moving research from sentence level to document level is necessary. Recently, there has been increasing interest in document-level RE. Yao et al. <ref type="bibr" target="#b14">[15]</ref> proposed a large-scale human-annotated document-level RE dataset, DocRED, and first compute the representations for all entities then predict relations for each entity pair by feeding them into a bilinear function. Wang et al. <ref type="bibr" target="#b12">[13]</ref> used BERT to encode the document, it also used bilinear layer to predict the relation between entity pairs, but it modelled the document-level RE through a two-step process. Most previous methods used only entity-level information and this is not adequate. In this paper, we propose to effectively aggregate the inference information of different granularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a Hierarchical Inference Network (HIN) for documentlevel RE. It uses a hierarchical inference method to aggregate the inference information of different granularity: entity level, sentence level and document level. We show that our method achieves state-of-the-art performance on the largest human-annotated DocRED dataset. Experimental analysis shows that both the inference mechanism and hierarchical aggregation approach in our model play an important role. In the future, we plan to incorporate external knowledge to further improve the proposed model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An example from DocRED. Each document in DocRED is annotated with named entity mentions, coreference information, relations, and supporting sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of the Hierarchical Inference Network(HIN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Recall of models on relational facts with different number of supporting sentences. Numbers in parentheses represent the number of relational facts with different number of supporting sentences in dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of different models on DocRED (%).</figDesc><table><row><cell>Model</cell><cell cols="2">Dev Ign F1</cell><cell>F1</cell><cell cols="2">Test Ign F1</cell><cell>F1</cell></row><row><cell>CNN-RE [15]</cell><cell>41.58</cell><cell></cell><cell>43.45</cell><cell>40.33</cell><cell>42.26</cell></row><row><cell>LSTM-RE [15]</cell><cell>48.44</cell><cell></cell><cell>50.68</cell><cell>47.71</cell><cell>50.07</cell></row><row><cell>BiLSTM-RE [15]</cell><cell>48.87</cell><cell></cell><cell>50.94</cell><cell>48.78</cell><cell>51.06</cell></row><row><cell>Context-Aware [12]</cell><cell>48.94</cell><cell></cell><cell>51.09</cell><cell>48.40</cell><cell>50.70</cell></row><row><cell>HIN-GloVe</cell><cell>51.06</cell><cell cols="2">52.95</cell><cell>51.15</cell><cell>53.30</cell></row><row><cell>BERT-RE [13]</cell><cell>-</cell><cell></cell><cell>54.16</cell><cell>-</cell><cell>53.20</cell></row><row><cell>BERT-Two-Step [13]</cell><cell>-</cell><cell></cell><cell>54.42</cell><cell>-</cell><cell>53.92</cell></row><row><cell>HIN-BERT</cell><cell>54.29</cell><cell cols="2">56.31</cell><cell>53.70</cell><cell>55.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of ablation study (%).</figDesc><table><row><cell>Setting</cell><cell>Ign F1</cell><cell>Dev</cell><cell>F1</cell></row><row><cell>HIN-BERT</cell><cell>54.29</cell><cell></cell><cell>56.31</cell></row><row><cell>-Translation mechanism</cell><cell>53.09</cell><cell></cell><cell>55.10</cell></row><row><cell>-Bilinear transformation</cell><cell>52.15</cell><cell></cell><cell>54.29</cell></row><row><cell>-Multispace</cell><cell>52.44</cell><cell></cell><cell>54.59</cell></row><row><cell>-Sentence inference</cell><cell>52.82</cell><cell></cell><cell>55.06</cell></row><row><cell>-Hierarchical aggregation</cell><cell>51.36</cell><cell></cell><cell>53.50</cell></row><row><cell>-Above all</cell><cell>49.95</cell><cell></cell><cell>52.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The results predicted by BERT-RE and HIN-BERT. The reasoning type of each example is different and the first row for each example is the input document. The head , tail , relation and supporting sentences are colored accordingly. The Galaxy S series is a line of Samsung Electronics, a division of Samsung<ref type="bibr" target="#b1">[2]</ref> Galaxy S line has ... being Samsung 's flagship smartphones.<ref type="bibr" target="#b3">[4]</ref> the latest smartphones in Galaxy S series are the Samsung Galaxy S9 ...Relation Lable: manufacturer BERT-RE: None HIN-BERT: manufacturer Robert Kingsbury Huntington, was a naval aircrewman and member of Torpedo Squadron 8. [2] ... Huntington was shot down during the Battle of Midway ... [3] He was born in Los Angeles , California ... Relation Lable: birth place BERT-RE: death place HIN-BERT: birth place IBM Research Brazil is one of twelve research laboratories comprising IBM Research , its first in South America. [1] It was established in June 2010 , with locations in So Paulo and Rio de Janeiro ... Relation Lable: continent BERT-RE: country HIN-BERT: country</figDesc><table><row><cell>Logical reasoning [0] Coreference reasoning [0] Common-[0]</cell></row><row><cell>sense</cell></row><row><cell>reasoning</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Since there is no official code for BERT-Two-Step, its results are not counted.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<title level="m">Enhanced lstm for natural language inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical relation extraction with coarse-to-fine grained attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Lingo: linearized grassmannian optimization for nuclear norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Riemannian submanifold tracking on low-rank algebraic variety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning robust low-rank approximation for crowdsourcing on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Context-aware representations for knowledge base relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<title level="m">Fine-tune bert for docred with two-step process</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Docred: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond word attention: using segment attention in neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>COLING</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attention-based bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
