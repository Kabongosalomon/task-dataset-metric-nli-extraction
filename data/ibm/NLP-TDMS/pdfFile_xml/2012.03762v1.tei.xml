<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
							<email>xuyan1@link.</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong (Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LiDAR point cloud analysis is a core task for 3D computer vision, especially for autonomous driving. However, due to the severe sparsity and noise interference in the single sweep Li-DAR point cloud, the accurate semantic segmentation is nontrivial to achieve. In this paper, we propose a novel sparse Li-DAR point cloud semantic segmentation framework assisted by learned contextual shape priors. In practice, an initial semantic segmentation (SS) of a single sweep point cloud can be achieved by any appealing network and then flows into the semantic scene completion (SSC) module as the input. By merging multiple frames in the LiDAR sequence as supervision, the optimized SSC module has learned the contextual shape priors from sequential LiDAR data, completing the sparse single sweep point cloud to the dense one. Thus, it inherently improves SS optimization through fully end-toend training. Besides, a Point-Voxel Interaction (PVI) module is proposed to further enhance the knowledge fusion between SS and SSC tasks, i.e., promoting the interaction of incomplete local geometry of point cloud and complete voxelwise global structure. Furthermore, the auxiliary SSC and PVI modules can be discarded during inference without extra burden for SS. Extensive experiments confirm that our JS3C-Net achieves superior performance on both SemanticKITTI and SemanticPOSS benchmarks, i.e., 4% and 3% improvement correspondingly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>LiDAR point clouds, compared with data from other sensors, such as cameras and radars in autonomous driving perception, have advantages of both accurate distance measurements and fine semantic descriptions. Semantic segmentation of LiDAR point clouds is usually conducted by assigning a semantic class label to each point. It is traditionally viewed as a typical task in the computer vision community. In autonomous driving, accurate and effective point cloud semantic segmentation undoubtedly plays a critical role.</p><p>Previous studies <ref type="bibr" target="#b35">(Thomas et al. 2019;</ref><ref type="bibr" target="#b43">Wu, Qi, and Fuxin 2019)</ref> about point cloud semantic segmentation mainly focused on the complete or dense point cloud scenarios, which are post-processed by merging multiple collected Li-DAR or RGB-D sequences (e.g., ScanNet <ref type="bibr" target="#b5">(Dai et al. 2017)</ref>,  <ref type="figure">Figure 1</ref>: Learning shape priors from multiple frames. For the sparse per-sweep point cloud shown in (a), it is nontrivial for current methods to recognize the truck from partial components. However, if we introduce the auxiliary information from adjacent frames (b) and (c), it is much easier to segment the complete truck in (d).</p><p>S3DIS <ref type="bibr" target="#b1">(Armeni et al. 2016</ref>) and Semantic3D <ref type="bibr" target="#b12">(Hackel et al. 2017)</ref>). However, raw per-sweep LiDAR point clouds, as the original input of autonomous driving, are much sparser. Their sparsity usually increases with the reflection distance, which often leads to extremely shapes missing and uneven point sampling for various categories. Therefore, despite the promising performance on complete data (e.g., 80% mIOU on Semantic3D), the semantic segmentation of sparse single sweep LiDAR point cloud still remains a big challenge, which extremely limits its accuracy in real applications.</p><p>In this paper, we try to break through the barrier of semantic segmentation on sparse single sweep LiDAR point clouds. One plausible way to solve this problem is to fully utilize the sequential nature of LiDAR data. Taking the scenario shown in <ref type="figure">Fig. 1(a)</ref> as an example, for a per-sweep point cloud with extremely sparse points of the truck, it seems impossible for previous methods to conduct accurate segmentation. Nevertheless, such segmentation would be possible, if we introduce the richer shape information from the other two frames, i.e., <ref type="figure">Fig. 1(b)</ref> and <ref type="figure">Fig. 1(c)</ref>, to reconstruct a shape-complete truck as shown in <ref type="figure">Fig. 1(d)</ref>. For this purpose, some previous works utilized historical adjacent frames to supplement the local details missing from the point clouds. For instance, SpSequenceNet <ref type="bibr" target="#b31">(Shi et al. 2020</ref>) and MeteorNet <ref type="bibr" target="#b22">(Liu, Yan, and Bohg 2019)</ref> use the point cloud of the current frame to query the nearest neighbors from the previous frames, following which a feature aggregation is conducted to fuse the adjacent-frame information. PointRNN <ref type="bibr" target="#b7">(Fan and Yang 2019)</ref> applies Recurrent Neural Networks (RNNs) to select available features from previous scenes. However, all of the above methods become unavailable in most real scenarios since the following reasons: (1) These methods exclusively use historical frames of the current scene in LiDAR sequence. Thus, they cannot introduce priors for newly incoming objects in this scene, i.e., they cannot utilize future frames. (2) Their proposed feature aggregation methods (i.e., through kNN or RNN) inevitably increase the computational burden, which makes it less effective and unsuitable for self-driving task.</p><p>To solve the above issues, we propose an enhanced Joint single sweep LiDAR point cloud Semantic Segmentation by exploiting learned shape prior form Scene Completion network, i.e., JS3C-Net. Specifically, by merging dozens of consecutive frames in a LiDAR sequence, a large complete point cloud is achieved as ground truth for the Semantic Scene Completion (SSC) task without extra annotation. The optimized SSC by using these annotations could capture the compelling shape priors, making the incomplete input complete to the acceptable shape with semantic labels <ref type="bibr" target="#b32">(Song et al. 2017)</ref>. Therefore, the completed shape priors can inherently benefit the semantic segmentation (SS) due to the fully end-to-end training strategy. Furthermore, a well-designed Point-Voxel Interaction (PVI) module is further proposed for implicit mutual knowledge fusion between the SS and SSC tasks. Concretely, the point-wise segmentation and voxelwise completion are leveraged to maintain the coarse global structure and fine-grained local geometry through PVI module. More importantly, we design our SSC and PVI modules to be disposable. To achieve this, JS3C-Net combines the SS and SSC in a cascaded manner, which means that it would not influence the information flow for SS while discarding the SSC and PVI modules in inference stage. Thus, it can prevent bringing the extra computing burden from generating complete high-resolution dense volumes.</p><p>Our main contributions are: 1) To the best of our knowledge, the proposed JS3C-Net is the first to achieve the enhanced sparse single sweep LiDAR semantic segmentation via auxiliary scene completion. 2) For better trade-off between performance and effectiveness, our auxiliary components are designed in cascaded and disposable manners, and a novel point-voxel interaction (PVI) module is proposed for better feature interaction and fusion between the two tasks. 3) Our method shows superior results in both SS and SSC on two benchmarks, i.e., SemanticKITTI  and SemanticPOSS <ref type="bibr" target="#b25">(Pan et al. 2020)</ref>, by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Point Cloud Semantic Segmentation. Unlike 2D images with regular grids, point clouds are often sparse and disordered. Thus, point clouds processing is a challenging task. There are three main strategies to approach this problem: projection-based, voxel-based and point-based. <ref type="formula" target="#formula_0">(1)</ref> Projection-based methods map point clouds onto 2D pixels, so that traditional CNN can play a normal role. Previous works projected all points scanned by the rotating LiDAR onto 2D images by plane projection <ref type="bibr" target="#b19">(Lawin et al. 2017;</ref><ref type="bibr" target="#b3">Boulch, Le Saux, and Audebert 2017;</ref><ref type="bibr" target="#b34">Tatarchenko et al. 2018)</ref> or spherical projection <ref type="bibr" target="#b41">(Wu et al. 2018</ref>.</p><p>(2) Considering the sparsity of point clouds and memory consumption, it is not very effective to directly voxelize point clouds and then use 3D convolution for feature learning. Various subsequent improved methods have been proposed, e.g., efficient spatial sparse convolution <ref type="bibr" target="#b4">(Choy, Gwak, and Savarese 2019;</ref><ref type="bibr" target="#b10">Graham, Engelcke, and van der Maaten 2018)</ref> and octree based convolutional neural networks <ref type="bibr" target="#b37">(Wang et al. 2017;</ref><ref type="bibr" target="#b29">Riegler, Osman Ulusoy, and Geiger 2017)</ref>. Also <ref type="bibr" target="#b33">(Tang et al. 2020</ref>) use NAS to obtain a more efficient feature representation. (3) Point-based methods directly process raw point clouds <ref type="bibr">(Qi et al. 2017a,b)</ref>. Usually, most methods use sampling strategies to select sub-points from the original point clouds, and then use local grouping with feature aggregation function for local feature learning of each sub-point. Among these methods, graph-based learning <ref type="bibr" target="#b36">(Wang et al. 2019a;</ref><ref type="bibr" target="#b18">Landrieu and Simonovsky 2018;</ref><ref type="bibr" target="#b17">Landrieu and Boussaha 2019;</ref><ref type="bibr" target="#b39">Wang et al. 2019c</ref>) and convolution-like operations <ref type="bibr" target="#b35">(Thomas et al. 2019;</ref><ref type="bibr" target="#b43">Wu, Qi, and Fuxin 2019;</ref><ref type="bibr" target="#b15">Hu et al. 2020</ref>) are widely used. However, previous methods often suffer from local information missing in LiDAR scenes due to insufficient priors and bias in data collection.</p><p>Semantic Scene Completion. Semantic scene completion (SSC) aims to produce a complete 3D voxel representation from an incomplete input. Concretely, Song et al. <ref type="bibr" target="#b32">(Song et al. 2017</ref>) firstly use single-view depth as input to construct an end-to-end model SSCNet, which can predict the results of scene completion and semantic labeling simultaneously. Spatially sparse group convolution is used in Zhang et al. <ref type="bibr" target="#b46">(Zhang et al. 2018)</ref> for fast 3D dense prediction. Meanwhile, coarse-to-fine strategies (e.g., LSTM based model) are used in <ref type="bibr" target="#b6">(Dai et al. 2018;</ref><ref type="bibr" target="#b14">Han et al. 2017)</ref> to recover missing parts of 3D shapes. More recently, some works introduce color information <ref type="bibr" target="#b8">(Garbade et al. 2019a</ref>) and use more powerful two-stream feature extractor <ref type="bibr" target="#b20">(Li et al. 2019)</ref> or feature fusion  to enhance the performance. However, SSC is rarely studied in large-scale LiDAR scenarios, and the serious geometric details missing and real-time requirements make it difficult.</p><p>Multi-task Learning on Segmentation. Multi-task learning aims to improve the learning efficiency and prediction accuracy for each task through knowledge transfer, which is widely used in 2D images segmentation <ref type="bibr" target="#b16">(Kendall, Gal, and Cipolla 2018)</ref>. <ref type="bibr" target="#b38">Wang et al. (Wang et al. 2019b</ref>), <ref type="bibr" target="#b26">Pham et al. (Pham et al. 2019)</ref> and <ref type="bibr" target="#b40">Wei et al. (Wei et al. 2020)</ref> innovatively combine semantic and instance segmentation with specific-designed fusion modules to improve the performance. OccuSeg ) proposes a 3D voxel projection-based segmentation network with voxel occupancy size regression and owns advantages of robustness in prediction. However, the shape priors brought by completion tasks are often neglected in previous works, while a proper  <ref type="figure">Figure 2</ref>: Overall pipeline of JS3C-Net. Given a sparse incomplete single sweep point cloud, it firstly uses a sparse convolution U-Net to conduct point feature encoding F enc . Based on the initial encoding, MLP 1 is used to generate shape embedding (SE) F SE , which flows into MLP 3 together with initial encoding transferred through MLP 2 to generate F out for point cloud semantic segmentation. Afterwards, the incomplete fine-grained point features from SE and complete voxel features from semantic scene completion (SSC) module flows into the Point-Voxel Interaction (PVI) module to achieve the refined features, which finally outputs the completion voxels with supervision. Note that the SSC and PVI modules can be discarded during inference.</p><p>use could improve the performance of segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Overview</head><p>The pipeline of JS3C-Net 1 is illustrated in <ref type="figure">Fig. 2</ref>. In practice, we firstly use the general appealing point cloud segmentation network to obtain initial point semantic segmentation and a shape embedding (SE) for each incomplete single-frame point cloud. Then SSC module takes results of segmentation network as input and generates the completed voxel of the whole scene with dense convolution neural network. Meanwhile, a point-voxel interaction (PVI) module is proposed to conduct shape-aware knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation</head><p>In general, a point cloud has two components: the points P ∈ R N ×3 and their features F ∈ R N ×D , where the points record spatial coordinates of N points and D-dimensional features can include any point-wise information, e.g., RGB information. Here we only use point coordinates as inputs. For semantic segmentation stage, we simply choose Submanifold SparseConv (Graham, Engelcke, and van der Maaten 2018) as our backbone. Unlike traditional voxelbased methods <ref type="bibr" target="#b30">(Ronneberger, Fischer, and Brox 2015;</ref><ref type="bibr" target="#b4">Choy, Gwak, and Savarese 2019)</ref> directly transforming all points into the 3D voxel grids by averaging all input features, it only stores non-empty voxels by the Hash table, and conduct convolution operations only on these non-empty voxels with more efficient way. Afterwards, the voxel-based output from sparse convolution based U-Net (SparseConv U-Net) is transformed back to the point-wise features F enc ∈ R N ×F by nearest-neighbor interpolation. To further introduce shape priors (see latter section) to point-wise features, we use multi-layer perceptions (MLP 1 ) to transfer their features to shape embedding (SE) F SE ∈ R N ×D e , which works as the input of the subsequent point-voxel interaction module. Furthermore, an element-wise addition operation after MLP 2 is used to fuse shape embedding with features from SparseConv U-Net. Finally, F out ∈ R N ×C are 1 https://github.com/yanx27/JS3C-Net. generated by MLP 3 and prepare for further semantic scene completion stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cascaded Semantic Scene Completion</head><p>The Semantic Scene Completion (SSC) module aims to introduce contextual shape priors from the entire LiDAR sequence. For stage of SSC, it takes the semantic probability F out from the SparseConv as input, and then predict the completion results.</p><p>The architecture of our SSC module is depicted in <ref type="figure">Fig. 3  (a)</ref>. Taking an incomplete point cloud with per points categorical probability as input, the network firstly conducts voxelization to obtain high-resolution 3D volume, and uses one convolution layer following by a pooling layer to reduce the resolution and the complexity of computation. Then, several basic blocks using convolutions with skip-connection are exploited to learn a local geometry representation. Afterwards, the features from different scales are concatenated to aggregate information from multiple scales. For achieving original resolution of SSC output, we leverage dense upsampling <ref type="bibr" target="#b21">(Liu et al. 2018)</ref> shown in <ref type="figure">Fig. 3</ref>(a) to avoid the interpolation inaccuracy, instead of dilation convolution based upsampling <ref type="bibr" target="#b32">(Song et al. 2017</ref>). Finally, we obtain a voxel output with C + 1 channels (C semantic categories label and one non-object label). This coarse completion will be fed into the PVI module for further mutual enhancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape-aware Point-Voxel Interaction</head><p>To fully utilize implicit knowledge transfer for mutual improvements of two tasks, we innovatively propose a shapeaware Point-Voxel Interaction (PVI) module for knowledge fusion between incomplete point clouds and complete voxels from former two steps. Although the SSC module can generate voxel-wise output with complete shape, such output is relatively coarse due to the voxelization procedure, leading to local geometric details missing. The entire geometric representation in raw point cloud data can, nevertheless, provide semantic guidance during completion process despite missing parts.</p><p>The inner structure of PVI module is shown in <ref type="figure">Fig. 3 (b)</ref>, which aims to conduct a coarse-to-fine process for the SSC  <ref type="figure">Figure 3</ref>: Part (a) shows the inner structure of SSC module, which uses semantic probability from segmentation network as inputs, generating complete volume by several convolution blocks and dense upsample. Part (b) illustrates a 2D case of PVI module, which uses the center points of the coarse global structure of number '5' to query k nearest neighbors from the raw point cloud, and then applies graph-based aggregation to achieve the completed '5' through fine-grained local geometry.</p><p>prediction. To be more precise, per point shape embedding F SE ∈ R N ×D e and coarse completion from SSC module V flow into PVI module as input. Afterwards, PVI firstly selects geometric centers of all non-empty voxels from V as a new point cloud P v ∈ R N ×(C+1) , then it uses k-nearest neighbor by Euclidean distance to query the closest points from original point cloud P. To this end, a graph convolutional network is further employed to enhance the relation learning between P v and P in both spatial and semantic spaces. In particular, the nodes of the graph are defined by the point positions with associated points features. For each point p v i ∈ P v and its j-th neighboring point p j ∈ P, we adopt the convolutional operator from DGCNN <ref type="bibr" target="#b39">(Wang et al. 2019c</ref>) to define edge-features e ij between two points as:</p><formula xml:id="formula_0">e ij = φ([p v i , f v i ], [p v i , f v i ] − [p j , f j ]),<label>(1)</label></formula><p>where f v i and f j are features of point p v i and p j respectively, and [·, ·] means concatenation operation. The shareweighted non-linear fuction φ is the multi-layer perceptron (MLP) in this paper (any differentiable architecture alternative). Finally, by stack l graph convolutional network (GCN) layers, we obtain final fine-grained completion.</p><p>The feature interaction process enables features of sparse point cloud to acquire the ability to predict semantics of complete voxels. Therefore, the information on complete details can positively affect the segmentation part through back propagation. Furthermore, PVI module enhances the probability of predicting whether the corresponding voxel of p v i represents an object in the fine-grained architecture, which fully utilizes spatial and semantic relationships between each p j and p v i . Finally, this enhanced feature will be added to the original coarse completion output through a residual connection for further refinement (see the refined number '5' in <ref type="figure">Fig. 3 (b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty-weighted Multi-task Loss</head><p>In order to further balance these two tasks and avoid complicated manual attempts during the end-to-end training, we use the uncertainty weighting method proposed in <ref type="bibr" target="#b16">(Kendall, Gal, and Cipolla 2018)</ref>. It introduces acquirable parameters to automatically adjust the optimal proportion between different tasks. Specifically, the joint loss can be written as:</p><formula xml:id="formula_1">L(W, σ 1 , σ 2 ) = 1 2σ 2 1 L seg (W seg ) + 1 2σ 2 2 L complet (W ) +logσ 1 + logσ 2 ,<label>(2)</label></formula><p>where losses L seg for segmentation and L complet for completion are both weighted cross-entropy losses exploited to update the network parameters W . Note that gradients from segmentation outputs are only conducted on parameters of segmentation network W seg ∈ W . In addition, we use trainable parameters σ 1 and σ 2 to weight the proportion of these two tasks for optimal trade-off. Their uncertainty can be deduced as two log term to control their values. Finally, during the training process, these two tasks will promote each other through back-propagation of joint learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disposable Properties of Auxiliary Components</head><p>Our proposed JS3C-Net is a general joint learning framework to improve the point cloud segmentation by introducing complete shape extracted by LiDAR sequence itself. The network used in semantic segmentation stage is flexible and can be replaced by other appealing networks. Furthermore, our JS3C-Net is effective enough for real-time applications, since the auxiliary components (i.e., SSC module and PVI module) can be discarded during inference to prevent introducing any computing burden for segmentation. That is to say, the completion part are only exploited in the training process as the dotted line shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset</head><p>To further verify the effectiveness of our method, we evaluate JS3C-Net on two benchmarks SemanticKITTI (Behley  <ref type="bibr" target="#b25">(Pan et al. 2020)</ref>. Se-manticKITTI is currently the largest LiDAR sequential dataset with point-level annotations, which consists of 43552 densely annotated LiDAR scans belonging to 21 sequences. These scans are annotated with a total of 19 valid classes and each scan spans up to 160×160×20 meters with more than ∼ 10 5 points. We follow the official split for training, validation and online testing. SemanticPOSS is a newly proposed dataset with 11 similar annotated categories with SemanticKITTI. However, it is more challenging because each scene contains more than twenty times sparse small objects (i.e., people and bicycle), while the total frames number are only 1/20 of SemanticKITTI. On both two datasets, we firstly merge consecutive 70 frames for every single frame to generate the complete volume of the entire scans. Then we select a volume of 51.2m in front of the LiDAR, 25.6m to each side of the LiDAR, and 6.4m in height with the resolution of 0.2m, which results in a volume of 256 × 256 × 32 voxels for prediction. Each voxel is assigned a single label based on a majority vote over all labeled points inside a voxel. Voxels containing no point are labeled as empty voxels. Note that voxels absent in all frames will not be considered in the loss calculation and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Learning Protocol</head><p>During the end-to-end training process of JS3C-Net, we use the Adam optimizer as our optimizer. The batch size is set to 6 for the total 50 epochs. The initial learning rate is set as 0.001 and decreases by 30% after every 5 epochs. The weighted terms σ 1 and σ 2 in Eqn. 2 are randomly initialized and are trained with ×10 learning rates. For semantic segmentation, 0.05m grids are used to conduct voxelization in SparseConv model. We randomly rotate the input point cloud along the y-axis during the training process and randomly scale it in the range of 0.9 to 1.1. During the inference, we apply the general voting strategy <ref type="bibr" target="#b35">(Thomas et al. 2019;</ref><ref type="bibr" target="#b15">Hu et al. 2020)</ref> to average multiple prediction results of randomly augmented point clouds. Similar data augmen- tation and voting strategies are also exploited for semantic scene completion. However, due to the particularity of the input volume, we use random flip along the x-axis and zaxis, and randomly rotate along y-axis by 90 degrees. All experiments are conducted on an Nvidia Tesla V100 GPU. More network details will be described in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation</head><p>In Tab. 1, we compare our JS3C-Net with recent methods on SemanticKITTI benchmark. The upper, medium and bottom parts of the table contain projection-based, point-based and voxel-based methods, respectively. The class averaged interactions over union (mIoU) is used in evaluation. As shown in Tab. 1, our JS3C-Net surpasses all existing methods by a large margin. Merely using the Spar-seConv (Graham and van der Maaten 2017), training from scratch already improves upon prior arts. Yet, using our joint-learning strategy achieves markedly better segmentation results in mIoU. Specifically, JS3C-Net achieves significant improvements on small objects (e.g., motorcycle, bicycle and etc), where these objects always lose geometric details during the LiDAR collection. Thanks to the contextual shape priors from SSC, our JS3C-Net can segment   <ref type="figure" target="#fig_1">Fig. 4</ref> presents some visualization results of JS3C-Net on the validation split, which demonstrates great improvements on small objects, in particular. Tab. 2 illustrates the semantic segmentation results on Se-manticPOSS dataset, where we compare result state-of-theart methods. These results show that our proposed JS3C-Net can achieve larger improvement compared with our baseline on more challenging data with remarkable small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Scene Completion</head><p>With the semantic guidance from segmentation network, our JS3C-Net can also make significant breakthrough in semantic scene completion (SSC) task. Tab. 3 illustrates the results of SSC on SemanticKITTI benchmark, where we compare our JS3C-Net with recent state-of-the-art methods. All methods are implemented with same settings and the detailed implementation and concrete results will be further elaborated in supplementary.</p><p>Since semantic scene completion requires to simultaneously predict the occupation status and the semantic label of a voxel, we follow the evaluation protocol of <ref type="bibr" target="#b32">(Song et al. 2017)</ref> to compute the precision, recall and IoU for  <ref type="figure">Fig. 5</ref> shows visualization results of semantic scene completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design Analysis</head><p>Ablation Study. The ablation results are summarized in Tab. 4. Since the limit of submission times, all ablated networks are tested on the validation set of SemanticKITTI dataset . The baseline (model A) is set to learn without joint learning, i.e., train two tasks separately. The baseline only gets IoU of 63.1% on semantic segmentation (SS) and 51.1% on scene completion (SC). This convincingly confirms the effectiveness of joint learning (JL) in model B (we manually set weights of task losses as 1:0.8), which is significantly improved to 66.1% for SS and 55.0% for SC by a large margin. Correspondingly, in model C, uncertainty multi-task loss (UMTL) is used to achieve the optimal trade-off between two tasks automatically. As a result, improvements of 0.4% and 1.1% on SS and SC are further obtained through UMTL (model C). Then, with PVI module for knowledge fusion, our JS3C-Net achieves the best results on both tasks. Mutual Promotion. To further study the reciprocal effects between two tasks, we conducted comparative experiments between the single task (SS or SSC) and the multiple tasks (JS3C-Net). As shown in <ref type="figure">Fig. 6</ref>, when the JS3C-Net is introduced (i.e., SS and SSC learn jointly), the performances of the two tasks are both largely enhanced, where we show the top 10 mIoU gains in <ref type="figure">Fig. 6</ref>. It shows that the IoUs of all 19 classes have been improved, especially in the case of small objects, such as trucks, bicycles and persons. The plausible explanation is that, for small objects, their raw point clouds are very sparse and usually lose local details. When SS and SSC learn jointly, SS can take advantage of the contextual shape prior of small objects from SSC, which benefits SS to classify each point more precisely. Therefore, these two tasks can benefit one from the other mutually. Complexity Analysis. In this section, we evaluate the overall complexity of JS3C-Net. As shown in Tab. 5, our proposed JS3C-Net is much more light-weighted and faster than previous point-based methods (1/5 model size and 1/3 inference time of KPConv). More importantly, since the disposable properties of our SSC module and PVI module, JS3C-Net has the same speed with segmentation backbone, which makes it more suitable for the real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose an single sweep LiDAR point cloud semantic segmentation framework via contextual shape priors from semantic scene completion network, named JS3C-Net. By exploiting some sophisticated pipelines, interactive modules, and reasonable loss function, our JS3C-Net model achieves state-of-the-art results on both semantic segmentation and scene completion tasks, outperforming previous methods by a large margin. We believe that our work can be applied to a wider range of other scenarios in the future, such as indoor point cloud sequence. Meanwhile, our method provides an alternative solution to the comprehension of large-scale LiDAR scenes with severe local details missing. It can improve the performance through contextual shape priors learning and interactive knowledge transferring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we first show our experimental details. Besides, we provide additional discussion to further demonstrate the superiority of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concrete Experimental Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architectures</head><p>Our model, JS3C-Net, consists of three parts: a semantic segmentation network, a semantic scene completion (SSC) decoder, and a point-voxel interaction (PVI) module.</p><p>For semantic segmentation stage, we use Submanifold SparseConv <ref type="bibr" target="#b10">(Graham, Engelcke, and van der Maaten 2018)</ref> as our backbone due to its superior performance. The encoder consists of 7 blocks of 3D sparse convolutions, each of which has two 3D sparse convolutions inside. Going deeper, we gradually increase the number of channels <ref type="bibr">(i.e., 16, 32, 48, 64, 80, 96, 112)</ref>. We also apply a 3D sparse pooling operation after each block to reduce the spatial resolution of the feature maps. For the decoder, we use the same structure but in the reverse order and replace the 3D sparse pooling layers with unpooling operations. Furthermore, it concatenates the features from the encoder phase on the decoder features at each scale through skip-connection. Three MLPs are used to generate shape embedding for PVI module and C-category prediction.</p><p>The architecture of SSC decoder is already shown in the manuscript. In fact, we use five basic blocks and the channel dimension of each block is identically 32. As for the pointvoxel interaction (PVI) module, graph-based edge learning is implemented by a three-layer MLP with 32 channels. In experiment, we set number of GCN layer l = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation</head><p>In this section, we illustrate in details the concrete results of semantic segmentation and implementation of compared methods. For SemanticKITTI benchmark, we directly compare our results with results on official benchmark. For Se-manticPOSS dataset (see Tab.3, we use results of Point-Net++ and SequeezeSegV2 in their official paper, and results of RandLA-Net and KPConv are implemented by official codes of correspoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Scene Completion</head><p>In this section, we illustrate in details the concrete results of semantic scene completion and implementation of compared methods, which are already provided by . Tab. 5 depicts the results of semantic scene completion on benchmark ). Among the methods to be compared, SSCNet <ref type="bibr" target="#b32">(Song et al. 2017)</ref>, EsscNet <ref type="bibr" target="#b46">(Zhang et al. 2018)</ref>, TS3D <ref type="bibr" target="#b9">(Garbade et al. 2019b</ref>) directly use sparse point cloud as input, TS3D 2 and TS3D 3 are multi-stage methods, which use segmentation results of DarkNet53Seg  as semantic priors. SSCNet. SSCNet <ref type="bibr" target="#b32">(Song et al. 2017</ref>) is a weak baseline that directly uses processed volumes as input, then several 3D convolution layers with the same structure introduced in their paper are conducted.  TS3D. Two Stream (TS3D) approach <ref type="bibr" target="#b9">(Garbade et al. 2019b)</ref> makes use of the additional information (processed from pre-trained DeepLab v2) from the RGB image corresponding to the input laser scan, and then it performs a 4 fold downsampling in a forward process but it renders them incapable of dealing with details of the scene. TS3D 2 . It uses the semantic segmentation results of Dark-Net53Seg  to enhance the semantic scene completion. However, without a suitable joint learning strategy, the pre-processed feature cannot improve the result of semantic scene completion effectively and significantly. TS3D 3 . It replaces the backbone of the TS3D 2 with SAT-Net <ref type="bibr" target="#b21">(Liu et al. 2018</ref>) without downsampling, and divide each whole volume into six equal parts for more fine-grained prediction.</p><p>EsscNet. We also compare our method with EsscNet <ref type="bibr" target="#b46">(Zhang et al. 2018)</ref>, which uses spatial group convolution (SGC) for memory saving. It uses the entire scene as input. However, due to the lack of priors, it still cannot perform very well. Also, the large amount of parameters shown in the manuscript makes it unable to meet the requirement for realtime applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Discussion</head><p>Why Use Semantic Scene Completion?</p><p>In this section, we further discuss why our model should use semantic scene completion as the object of joint learning. Note that semantic segmentation (SS) and semantic scene completion (SSC) both include semantic labeling process, which seems to be redundant in these two tasks. Therefore, we conduct an experiment that uses scene completion (SC) instead of semantic scene completion (SSC) for the joint learning with semantic segmentation (SS). As shown in Tab. 1, when using scene completion and semantic segmentation for joint learning (model B), although it can somehow improve the result of scene completion, the segmentation results become worse than directly conducting segmentation network (model A). This proves that directly adding scene completion will introduce more bias into segmentation network.</p><p>The explanation for such results can be summarized as follows: (1) Due to the cascaded architecture of our model, the results of SS and SSC actually have spatial alignment, which allows our SSC decoder to focus more on completing the shapes of each category rather than the entire scene. This setting allows the process of back propagation to bring shape prior to each category of semantic segmentation with less noise. (2) PVI module uses both spatial coordinates and features information to construct a learnable weight of graph edges, where features from SS and SSC are mainly used to compare the differences in feature space. Therefore, it can further enhance the distinction of features in segmentation network through using 20 categories to predict the SSC with strong semantic priors instead of simply predicting whether a certain voxel has an object or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do Moving Objects Affect the Results?</head><p>As shown in the red circle of <ref type="figure">Fig. 1 (left)</ref>, when using multiple frames to generate an entire dense scene volume, moving objects will be inevitably reconstructed into a long bar, which may introduce bias to the semantic segmentation. Therefore, to further test whether these moving objects will affect segmentation results, we design an experiment as follows. Firstly, we normally reconstruct moving objects, as shown in <ref type="figure">Fig. 1 (left)</ref>. Then, as shown in <ref type="figure">Fig. 1 (right)</ref>, we use the annotations of moving objects in both two datasets, and only use 5 frames to reconstruct each moving object (instead of 70 frames).</p><p>The quantitative results are shown in Tab. 2. It can be seen that, when we remove moving objects in semantic scene completion, there is only 0.2% improvement in semantic segmentation. This is because moving objects only account for a small proportion in the datasets. Finally, since there are also moving objects in SSC benchmark of , we keep moving objects in our training data in order to make a fair comparison with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can We Only Consider Historical Frames?</head><p>To further illustrate the advantages of using semantic scene completion, we also compare the results of only consider historical frames. Here we consider the method proposed in SpSequenceNet <ref type="bibr" target="#b31">(Shi et al. 2020)</ref> to use attention mechanism and feature aggregation from historical frames.</p><p>Tab. 4 shows the results of the correlative ablation study. Since results of SpSequenceNet on SemanticKITTI is much lower than ours, we re-implement their framework with our baseline. Experiment results show that only using attention or feature aggregation with previous frames cannot better learn the complete shape priors from LiDAR sequences. Our results of only using historical frames to generate ground truth is still higher than their results. Note that only using historical frames will ignore the shape details of newly incoming objects in each frame, thus hampers the results. When we consider future frames of LiDAR sequence, our results will be further improved. Furthermore, their method inevitably introduces more computational burden to segmentation, while our auxiliary components are fully discardable (see latency in table).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Visualization</head><p>In <ref type="figure">Fig. 2</ref>, we show more cases on SemanticPOSS dataset.  <ref type="table">Method  precision  recall  IoU  road  sidewalk  parking  other-ground   building   car  truck  bicycle  motorcycle  other-vehicle  vegetation   trunk  terrain  person  bicyclist  motorcyclist   fence  pole  traffic</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of JS3C-Net on the validation set of SemanticKITTI. Red circles show that our method performs better in many details than recent state-of-the-art KPConv<ref type="bibr" target="#b35">(Thomas et al. 2019)</ref>. Results for SemanticPOSS dataset are illustrated in supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Qualitative results of SSC task on the validation set of SemanticKITTI (Behley et al. 2019). Top-10 mIoU gains between JS3C-Net and splittrained single task (SS or SSC) on the validation set of Se-manticKITTI (Behley et al. 2019), where (a) and (b) illustrate SS and SSC respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc><ref type="bibr" target="#b9">Garbade et al. 2019b;</ref><ref type="bibr" target="#b2">Behley et al. 2019;</ref><ref type="bibr" target="#b21">Liu et al. 2018</ref>) 80.5 57.7 50.6 62.2 31.6 23.3 6.5 34.1 30.7 The selected example of ground truths for SSC, where (left) and (right) illustrate reconstructed results with and without moving objects. The visualization results on SemanticPOSS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Semantic segmentation on the SemanticKITTI benchmark. Underline marks results that ∼ 10% higher than baseline.88.6 67.6 45.8 17.7 73.7 81.8 13.4 18.5 17.9 14.0 71.8 35.8 60.2 20.1 25.1 3.9 41.1 20.2 26.3   </figDesc><table><row><cell>Method</cell><cell>Size</cell><cell>mIoU</cell><cell>road</cell><cell>sidewalk</cell><cell>parking</cell><cell>other-ground</cell><cell>building</cell><cell>car</cell><cell>truck</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>other-vehicle</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>fence</cell><cell>pole</cell><cell>traffic sign</cell></row><row><cell cols="22">SqueezeSegV2 (Wu et al. 2019) 39.7 DarkNet53Seg (Behley et al. 2019) 49.9 91.8 74.6 64.8 27.9 84.1 86.4 25.5 24.5 32.7 22.6 78.3 50.1 64.0 36.2 33.6 4.7 55.0 38.9 52.2 64*2048 RangeNet53++ (Milioto et al. 2019) 52.2 91.8 75.2 65.0 27.8 87.4 91.4 25.7 25.7 34.4 23.0 80.5 55.1 64.6 38.3 38.8 4.8 58.6 47.9 55.9 pixels 3D-MiniNet (Alonso et al. 2020) 55.8 91.6 74.5 64.2 25.4 89.4 90.5 28.5 42.3 42.1 29.4 82.8 60.8 66.7 47.8 44.1 14.5 60.8 48.0 56.6</cell></row><row><cell>SqueezeSegV3 (Xu et al. 2020)</cell><cell></cell><cell cols="20">55.9 91.7 74.8 63.4 26.4 89.0 92.5 29.6 38.7 36.5 33.0 82.0 58.7 65.4 45.6 46.2 20.1 59.4 49.6 58.9</cell></row><row><cell>PointNet++ (Qi et al. 2017b)</cell><cell></cell><cell cols="20">20.1 72.0 41.8 18.7 5.6 62.3 53.7 0.9 1.9 0.2 0.2 46.5 13.8 30.0 0.9 1.0 0.0 16.9 6.0 8.9</cell></row><row><cell>TangentConv (Tatarchenko et al. 2018)</cell><cell></cell><cell cols="20">40.9 83.9 63.9 33.4 15.4 83.4 90.8 15.2 2.7 16.5 12.1 79.5 49.3 58.1 23.0 28.4 8.1 49.0 35.8 28.5</cell></row><row><cell>PointASNL (Yan et al. 2020)</cell><cell>50K pts</cell><cell cols="20">46.8 87.4 74.3 24.3 1.8 83.1 87.9 39.0 0.0 25.1 29.2 84.1 52.2 70.6 34.2 57.6 0.0 43.9 57.8 36.9</cell></row><row><cell>RandLA-Net (Hu et al. 2020)</cell><cell></cell><cell cols="20">55.9 90.5 74.0 61.8 24.5 89.7 94.2 43.9 29.8 32.2 39.1 83.8 63.6 68.6 48.4 47.4 9.4 60.4 51.0 50.7</cell></row><row><cell>KPConv (Thomas et al. 2019)</cell><cell></cell><cell cols="20">58.8 90.3 72.7 61.3 31.5 90.5 95.0 33.4 30.2 42.5 44.3 84.8 69.2 69.1 61.5 61.6 11.8 64.2 56.4 47.4</cell></row><row><cell>PolarNet (Zhang et al. 2020)</cell><cell></cell><cell cols="20">54.3 90.8 74.4 61.7 21.7 90.0 93.8 22.9 40.3 30.1 28.5 84.0 65.5 67.8 43.2 40.2 5.6 61.3 51.8 57.5</cell></row><row><cell>SparseConv (Baseline)</cell><cell>50K pts</cell><cell cols="20">61.8 89.9 72.1 56.5 29.6 90.5 94.5 43.5 51.0 42.4 31.3 83.9 67.4 68.3 60.4 61.3 41.1 65.6 57.9 67.7</cell></row><row><cell>JS3C-Net (Ours)</cell><cell></cell><cell cols="20">66.0 88.9 72.1 61.9 31.9 92.5 95.8 54.3 59.3 52.9 46.0 84.5 69.8 67.9 69.5 65.4 39.9 70.8 60.7 68.7</cell></row></table><note>et al. 2019) and SemanticPOSS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Semantic segmentation results on the Semantic-POSS benchmark. The upper, medium and bottom parts contain previous projection-based, point-based and voxel-based methods, respectively.</figDesc><table><row><cell></cell><cell cols="3">Selected 3 classes</cell><cell>11 classes</cell></row><row><cell>Method</cell><cell cols="3">People Rider Bike</cell><cell>avg IoU</cell></row><row><cell>SequeezeSegV2</cell><cell>18.4</cell><cell>11.2</cell><cell>32.4</cell><cell>29.8</cell></row><row><cell>PointNet++</cell><cell>20.8</cell><cell>0.1</cell><cell>0.1</cell><cell>20.1</cell></row><row><cell>RandLA-Net</cell><cell>69.2</cell><cell>26.7</cell><cell>43.9</cell><cell>53.5</cell></row><row><cell>KPConv</cell><cell>77.3</cell><cell>29.4</cell><cell>53.2</cell><cell>55.2</cell></row><row><cell>SparseConv</cell><cell>76.3</cell><cell>30.5</cell><cell>53.5</cell><cell>57.2</cell></row><row><cell>JS3C-Net (Ours)</cell><cell>80.0</cell><cell>39.1</cell><cell>59.8</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Semantic scene completion results on the Se-manticKITTI benchmark. Only the recent published approaches are compared.</figDesc><table><row><cell>Method</cell><cell cols="4">precision recall IoU mIoU</cell></row><row><cell>SSCNet</cell><cell>31.7</cell><cell>83.4</cell><cell>29.8</cell><cell>9.5</cell></row><row><cell>TS3D</cell><cell>31.6</cell><cell>84.2</cell><cell>29.8</cell><cell>9.5</cell></row><row><cell>TS3D 2</cell><cell>25.9</cell><cell>88.3</cell><cell>25.0</cell><cell>10.2</cell></row><row><cell>EsscNet</cell><cell>62.6</cell><cell>55.6</cell><cell>41.8</cell><cell>17.5</cell></row><row><cell>TS3D 3</cell><cell>80.5</cell><cell>57.7</cell><cell>50.6</cell><cell>17.7</cell></row><row><cell>JS3C-Net (Ours)</cell><cell>71.5</cell><cell>73.5</cell><cell>56.6</cell><cell>23.8</cell></row><row><cell>them well. Meanwhile,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on SemanticKITTI validation set for semantic segmentation (SS), semantic scene completion (SSC) and scene completion (SC).</figDesc><table><row><cell>Model JL UMTL PVI</cell><cell>SS</cell><cell>SSC</cell><cell>SC</cell></row><row><cell>A</cell><cell cols="3">63.1 19.4 51.1</cell></row><row><cell>B</cell><cell cols="3">66.1 22.6 55.0</cell></row><row><cell>C</cell><cell cols="3">66.4 23.0 56.1</cell></row><row><cell>D</cell><cell cols="3">67.5 24.0 57.0</cell></row><row><cell cols="4">the task of scene completion (SC) while ignoring the se-</cell></row><row><cell cols="4">mantic label. Meanwhile, the mIoU over the 19 classes is</cell></row><row><cell cols="4">also exploited for the evaluation of semantic scene com-</cell></row><row><cell cols="4">pletion (SSC). As shown in Tab. 3, our JS3C-Net achieves</cell></row><row><cell cols="4">state-of-the-art results on both SC and SSC tasks. Benefit</cell></row><row><cell cols="4">from semantic guidance, joint learning strategy and well-</cell></row><row><cell cols="4">designed interaction module, our SSC module can generate</cell></row><row><cell cols="4">more faithful geometric details. The results of our methods</cell></row><row><cell cols="4">are 6% higher than previous state-of-the-art TS3D 3 (Gar-</cell></row><row><cell cols="4">bade et al. 2019b; Behley et al. 2019; Liu et al. 2018) for</cell></row><row><cell cols="4">scene completion, which uses segmentation results from</cell></row><row><cell cols="4">DarkNet53Seg (Behley et al. 2019) as inputs as well.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Complexity Analysis.</figDesc><table><row><cell>Model size and latency for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>The results of different joint learning strategies on SemanticKITTI validation set. SS, SC and SSC mean semantic segmentation, scene completion and semantic scene completion, respectively. JL means learn two tasks jointly.</figDesc><table><row><cell>Model SS SC SSC JL</cell><cell>SS</cell><cell>SSC</cell><cell>SC</cell></row><row><cell>A</cell><cell cols="3">63.1 19.4 51.1</cell></row><row><cell>B</cell><cell>62.5</cell><cell>-</cell><cell>55.7</cell></row><row><cell>C</cell><cell cols="3">67.5 24.0 57.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Effect of moving objects on SemanticKITTI and</figDesc><table><row><cell cols="3">SemanticPOSS validation set for semantic segmentation,</cell></row><row><cell cols="2">where w and w/o mean 'with' and 'without'.</cell><cell></cell></row><row><cell></cell><cell cols="2">SemanticKITTI SemanticPOSS</cell></row><row><cell>w moving objects</cell><cell>67.5</cell><cell>60.2</cell></row><row><cell>w/o moving objects</cell><cell>67.7</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation results on the SemanticPOSS dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">people rider</cell><cell>car</cell><cell cols="9">traffic sign trunk plants pole fence building bike road mIoU</cell></row><row><cell>PointNet++ (Qi et al. 2017a,b)</cell><cell>20.8</cell><cell>0.1</cell><cell>8.9</cell><cell>21.8</cell><cell>4.0</cell><cell>51.2</cell><cell>3.2</cell><cell>6.0</cell><cell>42.7</cell><cell>0.1</cell><cell>62.2</cell><cell>20.1</cell></row><row><cell>SequeezeSegV2 (Wu et al. 2019)</cell><cell>18.4</cell><cell cols="2">11.2 34.9</cell><cell>11.0</cell><cell>15.8</cell><cell>56.3</cell><cell>4.5</cell><cell>25.5</cell><cell>47.0</cell><cell cols="2">32.4 71.3</cell><cell>29.8</cell></row><row><cell>RandLA-Net (Hu et al. 2020)</cell><cell>69.2</cell><cell cols="2">26.7 77.2</cell><cell>24.3</cell><cell>27.3</cell><cell>73.4</cell><cell>30.9</cell><cell>51.4</cell><cell>82.5</cell><cell cols="2">43.9 81.2</cell><cell>53.5</cell></row><row><cell>KPConv (Thomas et al. 2019)</cell><cell>77.3</cell><cell cols="2">29.4 78.2</cell><cell>23.0</cell><cell>28.1</cell><cell>71.6</cell><cell>32.4</cell><cell>51.5</cell><cell>81.8</cell><cell cols="2">53.2 80.6</cell><cell>55.2</cell></row><row><cell>SparseConv (Baseline)</cell><cell>76.3</cell><cell cols="2">30.5 80.8</cell><cell>28.3</cell><cell>29.1</cell><cell>74.9</cell><cell>39.8</cell><cell>51.8</cell><cell>83.7</cell><cell cols="2">53.5 80.5</cell><cell>57.2</cell></row><row><cell>JS3C-Net(Ours)</cell><cell>80.0</cell><cell cols="2">39.1 83.2</cell><cell>28.9</cell><cell>31.6</cell><cell>76.1</cell><cell>44.7</cell><cell>54.0</cell><cell>83.9</cell><cell cols="2">59.8 81.7</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>The results on SemanticKITTI validation set.</figDesc><table><row><cell>Ablation</cell><cell cols="2">Latency (ms) mIoU</cell></row><row><cell>SparseConv (baseline)</cell><cell>471</cell><cell>63.1</cell></row><row><cell>SparseConv + SpSequenceNet</cell><cell>972</cell><cell>64.3</cell></row><row><cell>JS3C-Net (historical)</cell><cell>471</cell><cell>66.6</cell></row><row><cell>JS3C-Net (historical+future)</cell><cell>471</cell><cell>67.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Semantic scene completion results on the SemanticKITTI benchmark. Only the recent published approaches are compared.</figDesc><table><row><cell>Scene Completion</cell><cell>Semantic Scene Completion</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported in part by the Key Area R&amp;D Program of Guangdong Province with grant No.2018B030338001, the National Key R&amp;D Program of China with grant No.2018YFB1800800, NSFC-Youth 61902335, Guangdong Regional Joint Fund-Key Projects 2019B1515120039, Shenzhen Outstanding Talents Training Fund, Shenzhen Institute of Artificial Intelligence and Robotics for Society, Guangdong Research Project No.2017ZT07X152 and CCF-Tencent Open Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10893</idno>
		<title level="m">3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unstructured Point Cloud Semantic Labeling Using Deep Segmentation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<idno>3DOR 2: 7</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4578</biblScope>
			<biblScope unit="page">4587</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">PointRNN: Point recurrent neural network for moving point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two Stream 3D Semantic Scene Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03847</idno>
		<title level="m">Semantic3d. net: A new large-scale point cloud classification benchmark</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">OccuSeg: Occupancy-aware 3D Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-Resolution Shape Completion Using Deep Neural Networks for Global Structure and Local Geometry Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 85 C93</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RandLA-Net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boussaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7440" to="7449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep projective 3D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RGBD based dimensional decomposition residual network for 3D semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7693" to="7702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MeteorNet: Deep learning on dynamic 3D point cloud sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9246" to="9255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07269</idno>
		<title level="m">3D Gated Recurrent Fusion for Semantic Scene Completion</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09147</idno>
		<title level="m">SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic Instances</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">JSIS3D: joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SpSequenceNet: Semantic Segmentation Network on 4D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4574" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and Deformable Convolution for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-Path Region Mining For Weakly Supervised 3D Semantic Segmentation on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for realtime road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01803</idno>
		<title level="m">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks with Adaptive Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient Semantic Scene Completion Network with Spatial Group Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">733</biblScope>
			<biblScope unit="page">749</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9601" to="9610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
