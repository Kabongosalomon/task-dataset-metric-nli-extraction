<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Video Segmentation by Gated Recurrent Flow Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nilsson</surname></persName>
							<email>david.nilsson@math.lth.se</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Lund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
							<email>cristian.sminchisescu@math.lth.se</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">Lund University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Video Segmentation by Gated Recurrent Flow Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology to video segmentation that is capable of leveraging information present in unlabeled data in order to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that are able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recognition and the gated temporal propagation modules can be trained jointly, end-to-end. The temporal, gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our extensive experiments in the challenging CityScapes and Camvid datasets, and based on multiple deep architectures, indicate that the resulting model can leverage unlabeled temporal frames, next to a labeled one, in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little extra computation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning to see by moving is central to humans, starting very early during their perceptual and cognitive development <ref type="bibr" target="#b11">[12]</ref>. Human observers are able to continuously improve the ability to understand their dynamic visual environment and accurately identify object categories, scene structures, actions or interactions. Supervision is necessary during early cognitive development but is practically impossible (and apparently unnecessary) at frame level. In contrast, current computer vision systems are not yet good at processing large amounts of complex video data, or at leveraging partial supervision within temporally coherent visual streams, in order to build increasingly more accurate semantic models of the environment. Successful semantic video segmentation models would be widely useful for indexing digital content, robotics, navigation or manufacturing.</p><p>The problem of semantic image segmentation has received increasing attention lately, with some of the most successful methods being based on deep, fully trainable convolutional neural network (CNN) architectures. Data for training and refining single frame, static models is now quite diverse <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref>. In contrast, fully trainable approaches to semantic video segmentation would face the difficulty of obtaining detailed annotations for the individual video frames, although datasets are emerging for the (unsupervised) video segmentation problem <ref type="bibr" target="#b10">[11]</ref>. Therefore, for now some of the existing, pioneering approaches to semantic video segmentation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24]</ref> rely on single frame models with corresponding variables connected in time using random fields with higher-order potentials, and mostly prespecified parameters. Fully trainable approaches to video are rare. The computational complexity of video processing further complicated matters.</p><p>One possible practical approach to designing semantic video segmentation models in the long run can be to only label frames, sparsely, in video, as it was done for static datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref>. Then one should be able to leverage temporal dependencies in order to propagate information, then aggregate in order to decrease uncertainty during both learning and inference. This would require a model that can integrate spatio-temporal warping across video frames. Approaches based on CNNs seem right -one option could be to construct a fully trainable convolutional video network. Designing it naively could require e.g. matching edges in every possible motion direction as features. This would require a number of filters that is the number of filters for a single image CNN multiplied by possible motion directions. This holds for two frames. Extending it to even more frames would further amplify the issue. The problem here is how to choose a network architecture and how to connect different pixels temporally. Instead of building a spatio-temporal CNN directly, we will rely on existing single-frame CNNs but augment them with spatial transformer structures that implement warping along optical flow 1 arXiv:1612.08871v2 [cs.CV] 2 Oct 2017 <ref type="figure">Figure 1</ref>. Overview of our Spatio-Temporal Transformer Gated Recurrent Unit (STGRU), combining a Spatial Transformer Network ( §3.1) for optical flow warping with a Gated Recurrent Unit ( §3.2) to adaptively propagate and fuse semantic segmentation information over time.</p><formula xml:id="formula_0">ℎ −1 ℎ STGRU GRU ( ) ST ( ) CNN ( ) −1,</formula><p>fields. These will be combined with adaptive recurrent units in order to learn to optimally fuse estimates from single (unlabeled) frames with temporal information from nearby ones, properly grated based on their uncertainty. The proposed model is differentiable and end-to-end trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our semantic video segmentation work relates to the different fields of semantic segmentation in images, as well as, more remotely, to (unsupervised) video segmentation and temporal modeling for action recognition. We will here only briefly review the vast literature with some attention towards formulations based on deep architectures which represent the foundation of our approach.</p><p>The breakthrough paper <ref type="bibr" target="#b21">[22]</ref> introduced an architecture for image classification where a deep convolutional network was trained on the Imagenet dataset. This has later been improved by <ref type="bibr" target="#b35">[36]</ref> who reduced the filter sizes and increased the depth substantially. Many semantic segmentation methods start with the network in <ref type="bibr" target="#b35">[36]</ref> and refine it for semantic segmentation. In <ref type="bibr" target="#b14">[15]</ref> residual connections are used making it possible to increase depth substantially. <ref type="bibr" target="#b30">[31]</ref> obtained semantic segmentations by turning a network for classification <ref type="bibr" target="#b35">[36]</ref> into a dense predictor by computing segmentations at different scales and then combining all predictions. The network was made fully convolutional. Another successful approach is to apply a dense conditional random field (CRF) <ref type="bibr" target="#b20">[21]</ref> as a post-processing step on top of individual pixel or frame predictions. <ref type="bibr" target="#b4">[5]</ref> use a fully convolutional network to predict a segmentation and then apply the dense CRF as a post processing step. <ref type="bibr" target="#b43">[44]</ref> realized that inference in dense CRFs can be formulated as a fix point iteration implementable as a recurrent neural network. The deep architecture our work is based on is <ref type="bibr" target="#b40">[41]</ref> where max pooling layers are replaced with dilated convolutions. The network was extended by introducing a context module where convolutions with increasingly large dilation sizes are used.</p><p>Video segmentation has received significant attention starting from early methodologies based on temporal extensions to normalized cuts <ref type="bibr" target="#b33">[34]</ref>, random field models and tracking <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25]</ref>, motion segmentation <ref type="bibr" target="#b31">[32]</ref> or efficient hierarchical graph-based formulations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40]</ref>. More recently proposal methods where multiple figure-ground estimates or multipart superpixel segmentations are generated at each time-step, then linked through time using optical flow <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref>, have become popular. <ref type="bibr" target="#b23">[24]</ref> used the dense CRF of <ref type="bibr" target="#b20">[21]</ref> for semantic video segmentation by using pairwise potentials based on aligning the frames using optical flow.</p><p>Temporal modeling using deep architectures is also a timely topic in action recognition. One deep learning method <ref type="bibr" target="#b34">[35]</ref> uses two streams to predict actions. The authors use images in one stream and optical flow in the other stream, merged at a later stage in order to predict the final action label. This has also been implemented as a recurrent neural network in <ref type="bibr" target="#b41">[42]</ref> where the output of the two streams is sent into a recurrent neural network, where, at later stages the features are pooled to produce a final labelling. <ref type="bibr" target="#b18">[19]</ref> use frames at different timesteps and merge them as input to a neural network. <ref type="bibr" target="#b36">[37]</ref> attempt to make the problem of learning spatio-temporal features easier by using separable filters. Specifically, they first apply spatial filters and then temporal filters in order to avoid learning different temporal combinations for every spatial filter. <ref type="bibr" target="#b1">[2]</ref> learn spatiotemporal filters by stacking GRUs at different locations in a deep neural network and iterating through video frames. In a similar spirit <ref type="bibr" target="#b26">[27]</ref> use an LSTM and an attention mechanism to predict actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The visual illustration of how our semantic video segmentation model aggregates information in adjacent video frames is presented in <ref type="figure">fig. 1</ref>. We start with a semantic segmentation at the previous time step, h t−1 and warp it along the optical flow to align it with the segmentation at time t, by computing w t = φ t−1,t (h t−1 ) where φ is a mapping along the optical flow. This is fed as the hidden state to a gated recurrent unit (GRU) where the other input is the estimate x t computed by a single frame CNN for semantic segmentation. The information contained in w t and x t has significant redundancy, as one expects from nearby video frames, but in regions where it is hard to find the correct segmentation, or where significant motion occurs between frames, they might contain complementary information. The final segmentation h t combines the two segmentations by means of learnt GRU parameters and should include segments where either of the two are very confident.</p><p>Our overall video architecture can operate over multiple timesteps both forward and backward with respect to the timestep t, say, where semantic estimates are obtained.</p><p>In training, the model has the desirable property that it can rely only on sparsely labeled video frames, but can take advantage of the temporal coherency in the unlabeled video neighborhoods centered at the ground-truth. Specifically, given an estimate of our static (per-image) semantic segmentation model at timestep t, as well as estimates prior and posterior to it, we can warp these using the confidence gated optical flow forward and backward in time (using the Spatio-Temporal Transformer Gated Recurrent Unit, STGRU, illustrated in <ref type="figure">fig. 1</ref>) towards t where ground truth information is available, then fuse estimates in order to obtain a prediction. The resulting model is conveniently differentiable. The loss signal will then be used to backpropagate information for training both the parameters of the gated recurrent units (θ) and the parameters of the (perframe) semantic segmentation network (δ). The illustration of this mechanism is shown in <ref type="figure" target="#fig_0">fig. 2</ref>. In testing the network can operate either statically, per frame, or take advantage of video frames prior and (if available) posterior to the current processing timestep.</p><p>Given these intuitions we will now describe the main components of our model: the spatio-temporal transformer warping, the gated recurrent units, and the forward and backward implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatio-Temporal Transformer Warping</head><p>We will use optical flow as input to warp the semantic segmentation estimates across successive frames. We will modify the spatial transformer network <ref type="bibr" target="#b17">[18]</ref> for operation in the spatio-temporal video domain. Elements on a two-dimensional grid x ij will map to y ij according to</p><formula xml:id="formula_1">y ij = m,n x mn k(i + f y ij − m, j + f x ij − n). (1) where (f x ij , f y ij )</formula><p>is the optical flow at position (i.j). We will use a bilinear interpolation kernel k(x, y) = max(0, 1 − |x|) max(0, 1 − |y|). The mapping is differentiable and we can backpropagate gradients from y to both x and f . The sum contains only 4 non-zero terms when using the bilinear kernel, so it can be computed efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gated Recurrent Units for Semantic Video Segmentation</head><p>To connect the probability maps for semantic segmentation at different frames, h t−1 and h t , we will use a modified convolutional version of the Gated Recurrent Unit <ref type="bibr" target="#b6">[7]</ref>. In particular we will design a gating function based on the flow, so we only trust the semantic segmentation values warped from h t−1 at location where the flow is certain. We also use gating to predict the new segmentation probabilities taking into account if either h t−1 or x t have a high confidence for a certain class in some region of the image. Like an LSTM <ref type="bibr" target="#b15">[16]</ref>, the GRU also have gating functions to reset hidden states and to control the influence of the input x t for the new hidden state h t . The equations for a generic GRU with input x t and hidden state h t are</p><formula xml:id="formula_2">r t = σ(W xr x t + W hr h t−1 ) (2) h t = tanh(W xh x t + W hh (r t h t−1 )) (3) z t = σ(W xz x t + W hz h t−1 ) (4) h t = (1 − z t ) h t−1 + z t h t<label>(5)</label></formula><p>where r t is a reset gate,h t is the candidate hidden state, z t is a gate controlling the weighing between the old hidden state h t−1 and the candidate hidden stateh t , and W are weight matrices for fully connected layers. We use to denote the element-wise product.</p><p>To adapt a generic GRU for semantic video segmentation, we first change all fully connected layers to convolutions. The hidden state h t and the input variable x t are no longer vectors but tensors of size H × W × C where H is the image height, W is the image width and C is the number of channels, corresponding to the different semantic classes. The input x t is normalized using softmax and x t (i, j, c) models the probability that the label is c for pixel (i, j). We let φ t−1,t (x) denote the warping of a feature map x from time t − 1 to t, using optical flow given as additional input. The proposed adaptation of the GRU for semantic video segmentation is</p><formula xml:id="formula_3">−2 CNN ( ) −1 CNN ( ) CNN ( ) CNN ( ) +1 CNN ( ) +2 CNN ( ) STGRU ( ) Ground Truth Prediction Loss Error STGRU ( ) STGRU ( ) STGRU ( ) STGRU ( ) STGRU ( ) . . . . . .</formula><formula xml:id="formula_4">w t = φ t−1,t (h t−1 ) (6) r t = 1 − tanh(|W ir * (I t − φ t−1,t (I t−1 )) + b r |) (7) h t = W xh * x t + W hh * (r t w t ) (8) z t = σ(W xz * x t + W hz * w t + b z ) (9) h t = softmax(λ(1 − z t ) w t + z t h t ).<label>(10)</label></formula><p>Instead of relying on a generic parametrization for the reset gate, we use a confidence measure for the flow by comparing the image I t with the warped image of I t−1 . We also discard tanh when computingh t and instead use softmax in order to normalize h t . We multiply with λ in order to compensate for a possibly different scaling ofh t relative to the warped h t−1 due to the convolutions with W hh and W xh . Note that h t−1 only enters when we compute the warping w t so we only use the warped h t−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>For the static (per-frame) component of our model, we rely on a deep neural network pre-trained on the CityScapes dataset and fed as input to the gated recurrent units. We conducted experiments using both the Dilation architecture <ref type="bibr" target="#b40">[41]</ref> and LRR <ref type="bibr" target="#b12">[13]</ref>. The convolutions in the STGRU were all of size 7 × 7. We use the standard log-likelihood loss for semantic segmentation</p><formula xml:id="formula_5">L(θ) = − i,j log p(y ij = c ij |I, θ)<label>(11)</label></formula><p>where p(y ij = c ij |I, θ) is the softmax normalized output of the STGRU. The recurrent network was optimized using Adam <ref type="bibr" target="#b19">[20]</ref> with β 1 = 0.95, β 2 = 0.99 and learning rate 2 · 10 −5 . Due to GPU memory constraints, the static computations had to be performed one frame at a time with only the final output saved in memory. When training the system end-to-end the intermediate activations for each frame had to be recomputed. We used standard gradient descent with momentum for refining the static network. The learning rate was 2 · 10 −11 and momentum was 0.95. Note that the loss was not normalized, hence the small learning rate.</p><p>For flow we used Fullflow <ref type="bibr" target="#b5">[6]</ref> and we precomputed the flow between the frames closest to the labelled ones in the training set both forward and backward. For validation and testing we used DIS <ref type="bibr" target="#b22">[23]</ref> which gave a higher accuracy. We used the version of DIS with the highest accuracy but the slowest runtime (0.5 Hz). We also conducted experiments where we used optical flow from a neural network <ref type="bibr" target="#b9">[10]</ref> and refined the flow network jointly with the static network and the STGRU units. The images we trained on had output size 512x512, whereas in testing their size was increased to full resolution because intermediate computations do not have to be stored for backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform an extensive evaluation on the challenging CityScapes and CamVid datasets, where video experiments remain nevertheless difficult to perform due to the large volume of computation. We evaluate under two different perspectives, reflecting the relevant, key aspects of our method. First we evaluate semantic video segmentation. We will compare our method with other methods for semantic segmentation and show that by using temporal information we can improve segmentation accuracy over a network where the predictions are per frame and unlabeled video data is not used. In the second evaluation we run our method in order to produce semantic segmentation for all frames in a longer video. We will then compare its temporal consistency against the baseline method where the predictions are performed per frame. We will show that our method gives a temporally much more consistent segmentation compared to the baseline.  <ref type="table">Table 1</ref>. Results on the validation set of CityScapes when using a different number of frames for inference. The model was trained using 5 frames. Notice that using more than one frame improves performance which however saturates beyond 4 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic Video Segmentation</head><p>In CityScapes <ref type="bibr" target="#b7">[8]</ref>, we are given sparsely annotated frames, and each labeled frame is the 20th frame in a 30 frame video snippet. The GRFP model we use for most experiments is a forward model trained using 5 frames (T=4 in <ref type="figure" target="#fig_0">fig. 2</ref>). We apply the loss to the final frame. Notice however that due to computational considerations, while the STGRU unit parameters θ were trained based on propagating information from 5 frames, the unary network parameters δ were refined based on back-propagating gradient from the 2 STGRU units closest to the loss. Unless stated otherwise, we use the Dilation10 <ref type="bibr" target="#b40">[41]</ref> network as our unary network.</p><p>In <ref type="table">Table 1</ref> we show the mean IoU (mIoU) over classes versus the number of frames used for inference. One can see that under the current representation, in inference, not much gain is achieved by the forward model beyond propagating information from 4 frames. The results are pre-  <ref type="bibr" target="#b30">[31]</ref> 65.3 85.7 <ref type="table">Table 3</ref>. Results on the CityScapes test set for different published methods. We experimented with our methodology using both Di-lation10 and LRR-4x as our baselines, and we are able to improve the mIoU accuracy with 0.7 and 1.0 percentage points respectively. All other methods only process a single frame and do not use video. Notice that our GRFP methodology proposed for video is applicable to most of the other semantic segmentation methods that predict each frame independently, and they can all benefit from potential performance improvements at no additional labeling cost.</p><p>sented in more detail in <ref type="table">Table 2</ref> where we show the estimates produced by the pre-trained Dilation10 network and the per-frame Dilation network with parameters refined by our model, GRFP(1), as well as the results of our GRFP model operating over 5 frames GRFP <ref type="bibr" target="#b4">(5)</ref>. We also note that <ref type="bibr" target="#b23">[24]</ref> achieve a score of 0.703 on the validation set but their method requires substantially more computation in infer-  <ref type="table">Table 4</ref>. Results on the CityScapes validation set for various forward-backward models and for models using flownet and training end-toend. See <ref type="figure" target="#fig_0">fig. 2</ref> for how the parameters are defined. The first three models are described in more detail in <ref type="table">Table 2</ref>. (a) A forward-backward model evaluated with T = 2 using the same parameters for the backward STGRU as the best forward model GRFP(5) both in the forward direction and backward direction. We used θ and δ as for GRFP <ref type="formula" target="#formula_2">(5)</ref>    <ref type="figure" target="#fig_2">fig. 3</ref>. Notice the increased uncertainty in <ref type="figure" target="#fig_2">fig. 3</ref> as learnt by the STGRU model. ence (similar optical flow computation and unary prediction costs from the Dilation network, but differences between fully connected CRF belief propagation in <ref type="bibr" target="#b23">[24]</ref> versus convolutional predictions as one STGRU pass in our case). Notice also that while the average of our GRFP(1) is identical to the one of the pre-trained Dilation10, the individual class accuracies are different. It is apparent that most of our gains come from contributions due to temporal propagation and consistency reasoning in our STGRU models.</p><p>We also tried our model using LRR <ref type="bibr" target="#b12">[13]</ref> as backend to our model. As when we used the Dilation network, we got improved performance by using our video methodol-ogy compared to the static per-frame baseline. We show the detailed results on the validation set in <ref type="table" target="#tab_1">Table 5</ref> and test set scores are shown in <ref type="table">Table 3</ref>. Note that the accuracy is higher for all classes. For this experiment we used Flownet2 <ref type="bibr" target="#b16">[17]</ref>.</p><p>In <ref type="table">Table 3</ref> we show semantic segmentation results on the CityScapes test set. We present the performance of a number of state of the art semantic segmentation models trained using Cityscapes but which do not use video. We used our GRFP methodology trained using both Dila-tion10 and LRR-4x as baseline models and in both cases we show improved labelling accuracy. Notice also that our methodology can be used with any semantic segmentation method that processes each frame independently. Since we showed improvements using both baselines, we can predict that other single-frame methods can benefit from our proposed video methodology as well. <ref type="figure">Figure 5</ref> shows several illustrative situations where our proposed GRFP methodology outperforms the single frame baseline. In particular, our method is capable to more accurately segment the car, the right wall, and the left pole. In all cases it is apparent that inference for the current frame becomes easier when information is integrated over previous frames, as in GRFP. <ref type="figure">Figure 5</ref>. From top to bottom: the image, video segmentation by GRFP, static segmentation by Dilation10, and the ground truth. Notice the more accurate semantic segmentation of the car in the left example, the right wall in the middle example, and the left pole in the right example. For the first two examples the static method fails where the some object has a uniform surface over some spatial extent. The pole in the right image may be hard to estimate based on the current frame alone, but the inference problem becomes easier if earlier frames are considered, a property our GRFP model has.</p><p>Combining forward and backward models. In <ref type="table">Table 4</ref> we show accuracy on the CityScapes validation set for various settings where we used the forward and backward models and averaged the predictions. This joint model was described in <ref type="figure" target="#fig_0">fig. 2</ref>. We got the best result if we averaged predictions using 5 frames going forward and adding the prediction going backward, that is, using I t+4 , I t+3 , . . . , I t . We also tried to combine the forward and backward predictions by stacking the two predictions and then having 3 layers where we learned to combine the predictions. We trained the new layers jointly with the forward and backward parameters θ and α. This did not result in any improvement in accuracy.</p><p>Joint training including optical flow. To make our model entirely end-to-end trainable we include optical flow estimation as a deep neural network component <ref type="bibr" target="#b9">[10]</ref>. We conducted experiments where we used the flow from this network and refined everything, the STGRU parameters, the Dilation10 parameters and the flownet parameters jointly. Although the quality of the optical flow in <ref type="bibr" target="#b9">[10]</ref> is signifi-cantly lower than the ones of other flow methods we used, training the model jointly still produced competitive results, see (e) and (f) in <ref type="table">Table 4</ref>. We note that the error signal passed to the flownet comes from a loss based on semantic segmentation. This is a very weak form of supervision to refine optical flow. Since the complete model is dependent on good optical flow and the STGRU units' performance decreases when using lower quality flow, the error signal to the flow network might be very noisy when starting the training, preventing efficient learning for the flownet. Note also that the STGRU units lose supervision in form of high quality optical flow.</p><p>CamVid To show that our method is not limited to CityScapes we also provide additional experiments on the CamVid dataset <ref type="bibr" target="#b3">[4]</ref>. This dataset consists of 4 video sequences that are annotated at 1 Hz. In total there are 367 training images, 100 validation images and 233 test images. We use the Dilation network <ref type="bibr" target="#b40">[41]</ref> as our baseline and we use our GRFP methodology to improve the performance by using video and not processing each frame independently. The results can be seen in <ref type="table">Table 6</ref>. We can see that we are able to improve the segmentation accuracy by using additional video frames as input, and our accuracy results are on par with the state-of-the-art method <ref type="bibr" target="#b23">[24]</ref>. We were not able to perfectly replicate the numbers reported in <ref type="bibr" target="#b40">[41]</ref>, but the mean IoU is the same and the class differences are marginal.</p><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Temporal Consistency</head><p>As in <ref type="bibr" target="#b23">[24]</ref>, we evaluate the temporal consistency of our semantic video segmentation method by computing trajectories in the video using <ref type="bibr" target="#b37">[38]</ref> and calculating for how many of the trajectories the labelling is the same in all frames. The results are shown in <ref type="table" target="#tab_3">Table 7</ref>. We use the demo videos provided in the CityScapes dataset, that are 600, 1100 and 1200 frames long, respectively. Due to computational considerations, we only used the middle 512x512 crop of the larger CityScapes images. The results can be seen in <ref type="table" target="#tab_3">Table 7</ref> where improvements are achieved for all videos at an average of about 4 percentage points. Qualitatively, there is a lot less flickering and noise when we are using the proposed GRFP semantic video segmentation methodology compared to models that rely on single-frame estimates. Semantic segmentations for consecutive frames in a video for both GRFP and the single-frame Dilation10 network are shown in <ref type="figure">fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Timing</head><p>We report the timing of the different components of our method using a Titan X GPU. For the flow computations we report the time using either Flownet1 <ref type="bibr" target="#b9">[10]</ref> or Flownet2 <ref type="bibr" target="#b16">[17]</ref>. The numbers in <ref type="table">Table 8</ref> show the timing per frame of the three main components of our framework: the static prediction, the flow computation and the STGRU computations. We report the time to process (testing, not training) one frame in a video with resolution 512x512. With our methodology we can obtain an improved temporal consistency and labelling accuracy with an additional runtime of 335 ms per frame with Flownet2 and 75 ms with Flownet1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented a deep, end-to-end trainable methodology for semantic video segmentation (including the capability of jointly refining the recognition, optical flow and temporal propagation modules), that is capable of taking advantage of the information present in unlabeled frames in order to improve estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that learns to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. Our extensive experiments on the challenging CityScapes and CamVid datasets, and for multiple deep semantic models, indicate that our resulting model can successfully propagate information from labeled video frames towards nearby unlabeled ones in order to improve both the semantic video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little supplementary computation.  <ref type="table">Table 6</ref>. Results on the test set of CamVid. Note that out method GRFP gets a higher score than Dilation8 which it is based on. <ref type="figure">Figure 6</ref>. Qualitative examples from the CamVid test set. From top to bottom: the image, static segmentation by Dilation8, video segmentation by GRFP, and the ground truth. In the two examples to the left, notice that the poles are better segmented by our video method. In the two right images, the sidewalk is better segmented using our video methodology.  <ref type="table">Table 8</ref>. Timing of the different components of out GRFP methodology using a Titan X GPU. We are able to show improved segmentation accuracy and temporal consistency by incurring an additional runtime of 75 ms per frame if we use Flownet1 or 335 ms per frame if we use Flownet2. <ref type="figure">Figure 7</ref>. Examples of semantic segmentations in consecutive video frames from CityScapes. From top to bottom we have the frames, the static segmentation by Dilation10 and the video segmentation by GRFP. In the first example we have a more consistent estimation of vegetation over time and in the second example we see that the GRFP suppresses some spurious objects in the middle of the image that appear in a few of the frames for the Dilation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of our temporal architecture entitled Gated Recurrent Flow Propagation (GRFP) based on Spatio-Temporal Transformer Gated Recurrent Units (STGRU), illustrated in fig.1. The model can integrate both forward only and forward-backward calculations, under separate recurrent units with different parameters θ and α. Each of the forward and backward recurrent units have parameters that are tied across timesteps. The parameters of the semantic segmentation architecture are shared. The predictions from the forward model aggregated over frames t − T, . . . , t − 1, t (in the above illustration T = 2) and (when available and desirable) backward model aggregated over frames t + T, . . . t + 1, t are fused at the central location t in order to make a prediction that is then compared against the ground-truth available only at frame t by means of a semantic segmentation loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Frames</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the flow gating as estimated by our Spatio-Temporal Transformer Gated Recurrent Unit. We show three pairs of consecutive frames, the flow and the its confidence as estimated by our STGRU model. Red regions indicate a confident flow estimate whereas blue regions are uncertain. Compare these to the errors of the optical flow method we use<ref type="bibr" target="#b22">[23]</ref>, shown infig.4. It is noticeable that our estimates integrate errors in the flow but exhibit additional uncertainty due to semantic labeling constraints in the STGRU model ( §3.2).Method Dilation10 GRFP(5) GRFP(1)fwbw(a) fwbw(b) fwbw(c) fwbw(d) Flownet(e) Flownet(f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and set α = θ. (b) as (a) but with T = 4. (c) We used θ and δ from GRFP(5) but refined α. We used T = 4. (d) We refined all parameters θ, α and δ and used T = 4. (e) We used a setting identical to GRFP(5) but used flownet [10] to compute optical flow. (f) We used the setting in GRFP(5) and the network in [10] for optical flow and trained the Dilation network, the flow network and the recurrent network jointly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Per-pixel errors in the optical flow estimation for three pairs of images shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>show several qualitative examples in fig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 5 .</head><label>5</label><figDesc>Average IoU (mIoU) on the CityScapes validation set for the LRR baseline and our model GRFP based on LRR. By using our video methodology we can see labelling improvements for all classes.</figDesc><table><row><cell>Class</cell><cell cols="2">LRR LRR+GRFP</cell></row><row><cell>Road</cell><cell>0.977</cell><cell>0.978</cell></row><row><cell>Sidewalk</cell><cell>0.830</cell><cell>0.831</cell></row><row><cell>Building</cell><cell>0.912</cell><cell>0.914</cell></row><row><cell>Wall</cell><cell>0.496</cell><cell>0.514</cell></row><row><cell>Fence</cell><cell>0.568</cell><cell>0.581</cell></row><row><cell>Pole</cell><cell>0.607</cell><cell>0.611</cell></row><row><cell cols="2">Traffic light 0.656</cell><cell>0.675</cell></row><row><cell>Traffic sign</cell><cell>0.757</cell><cell>0.764</cell></row><row><cell>Vegetation</cell><cell>0.918</cell><cell>0.919</cell></row><row><cell>Terrain</cell><cell>0.622</cell><cell>0.626</cell></row><row><cell>Sky</cell><cell>0.940</cell><cell>0.942</cell></row><row><cell>Person</cell><cell>0.786</cell><cell>0.792</cell></row><row><cell>Rider</cell><cell>0.559</cell><cell>0.572</cell></row><row><cell>Car</cell><cell>0.934</cell><cell>0.937</cell></row><row><cell>Truck</cell><cell>0.606</cell><cell>0.624</cell></row><row><cell>Bus</cell><cell>0.762</cell><cell>0.786</cell></row><row><cell>Train</cell><cell>0.635</cell><cell>0.649</cell></row><row><cell cols="2">Motorcycle 0.487</cell><cell>0.532</cell></row><row><cell>Bicycle</cell><cell>0.730</cell><cell>0.741</cell></row><row><cell>Average</cell><cell>0.725</cell><cell>0.736</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>52.0 87.0 58.5 13.4 86.2 25.3 17.9 16.0 60.5 24.8 46.4 DeepLab-LFOV [5] 81.5 74.6 89.0 82.2 42.3 92.2 48.4 27.2 14.3 75.4 50.1 61.6 Dilation8 [41] 83.0 76.3 90.3 84.1 47.9 92.7 56.1 36.8 20.2 76.4 54.1 65.3 GRFP 83.1 76.6 90.3 84.6 48.7 93.3 56.8 37.1 24.3 77.9 54.7 66.1 FSO [24] 84.0 77.2 91.3 85.6 49.9 92.5 59.1 37.6 16.9 76.0 57.2 66.1</figDesc><table><row><cell></cell><cell>Tree</cell><cell>Sky</cell><cell>Car</cell><cell>Sign</cell><cell>Road</cell><cell>Pedestrian</cell><cell>Fence</cell><cell>Pole</cell><cell>Sidewalk</cell><cell>Bicycle</cell><cell>mean IoU</cell></row><row><cell>SegNet [1]</cell><cell>68.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Temporal consistency for the different demo videos in the CityScapes dataset. We note that our GRFP semantic video segmentation method achieves a more consistent segmentation than the single frame baseline.</figDesc><table><row><cell>Video</cell><cell>Dilation10</cell><cell>GRFP</cell><cell>FSO [24]</cell><cell></cell><cell>Dilation10</cell><cell>LRR</cell></row><row><cell>stuttgart 00</cell><cell>79.18 %</cell><cell cols="2">84.29 % 91.31 %</cell><cell>Segmentation module</cell><cell>350 ms</cell><cell>200 ms</cell></row><row><cell>stuttgart 01</cell><cell>86.13 %</cell><cell cols="2">88.87 % 93.32 %</cell><cell>Flownet2/Flownet1</cell><cell cols="2">300/40 ms 300/40 ms</cell></row><row><cell>stuttgart 02</cell><cell>76.77 %</cell><cell cols="2">81.96 % 89.01 %</cell><cell>STGRU</cell><cell>35 ms</cell><cell>35 ms</cell></row><row><cell>Average</cell><cell>80.69 %</cell><cell cols="2">85.04 % 91.21 %</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was funded in part by the European Research Council, ERC, Consolidator Grant SEED.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video object segmentation by salient segment chain composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, IPGM Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Perceptual and Cognitive Development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Au</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C F</forename><surname>Golnaz Ghiasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast optical flow using dense inverse search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Track to the future: Spatio-temporal video segmentation with long-range motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01794</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Motion coherent tracking using multi-label MRF optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Streaming hierarchical video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
