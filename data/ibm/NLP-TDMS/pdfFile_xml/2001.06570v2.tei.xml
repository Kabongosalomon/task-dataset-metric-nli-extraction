<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harmonic Convolutional Networks based on Discrete Cosine Transform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Ulicny</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science &amp; Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">A</forename><surname>Krylov</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rozenn</forename><surname>Dahyot</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science &amp; Statistics</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">ADAPT Centre</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harmonic Convolutional Networks based on Discrete Cosine Transform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Harmonic Network</term>
					<term>Convolutional Neural Network</term>
					<term>Discrete Cosine Transform</term>
					<term>Image Classification</term>
					<term>Object Detection</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain. We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CNNs have been designed to take advantage of implicit characteristics of natural images, specifically correlation in local neighborhood and feature equivariance. Standard CNNs rely on learned convolutional filters hence finetuned to the data available. However, it can be advantageous to revert to preset filter banks: for instance, with limited training data <ref type="bibr" target="#b0">[1]</ref>, using a collection of preset filters can help in avoiding overfitting and in reducing the computational complexity of the system. Scattering networks are examples of such networks with preset (wavelet based) filters which have achieved state-of-the-art results in handwritten digit recognition and texture classification <ref type="bibr" target="#b1">[2]</ref>.</p><p>We propose instead to replace the standard convolutional operations in CNNs by harmonic blocks that learn the weighted sums of responses to the Discrete Cosine Transform (DCT) filters, see <ref type="figure" target="#fig_0">Fig. 1</ref>. DCT has been successfully used for JPEG encoding to trans- form image blocks into spectral representations to capture the most information with a small number of coefficients. Motivated by frequency separation and energy compaction properties of DCT, the proposed harmonic networks rely on combining responses of window-based DCT with a small receptive field. Our method learns how to optimally combine spectral coefficients at every layer to produce a fixed size representation defined as a weighted sum of responses to DCT filters. The use of DCT filters allows one to easily address the task of model compression. Other works that propose convolutional filters decomposition to particular basis functions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> have predominantly focused on network compression. In our study we demonstrate that prior information coming from well chosen filter basis can not only be used to compress but also speeds up training convergence and improves performance.</p><p>Based on our earlier works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref>, this paper contributions are as follows. First we demonstrate that the theoretical computational overheads of the optimised formulation of a harmonic block are minimal (experimentally, within 3-7%) whereas the memory footprint requirements are comparable to those of the benchmark architecture based on standard convolutional blocks (and are lower if harmonic blocks undergo compression). Second, we substantially expand experimental validation to demonstrate a consistent increase in performance due to the use of harmonic blocks. Specifically, on the small NORB dataset we achieve state-of-the-art results and demonstrate how DCT-based harmonic blocks allow one to efficiently generalise to unseen lighting conditions. We further report quantitative as well as qualitative results of application of harmonic blocks to a representative variety of vision tasks: object detection and instance/semantic segmentation. We observe a consistent average improvement of 1% AP on these tasks, which demonstrates the practical appeal of using harmonic networks. Section 2 presents the relevant background research to our harmonic network formulation (Sec. 3). It is extensively validated against state-of-the-art alternatives for image classification (Sec. 4), for object detection, instance and semantic segmentation (Sec. 5). All our architectures in the reported results are denoted as Harm; the PyTorch implementations for our harmonic networks are publicly available at https://github.com/matej-ulicny/harmonic-networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work 2.1 DCT &amp; CNNs</head><p>Networks trained on DCT coefficients are frequently used in forensics for detection of tampered parts in images. These parts are assumed to have different distribution of DCT coefficients from the rest of the image. A common practice is to classify histograms of preselected JPEG-extracted DCT coefficients by 1-D or 2-D convolutional network <ref type="bibr" target="#b5">[6]</ref>. A number of studies have also investigated the use of spectral image representations for object and scene recognition. DCT features from an entire image were used to train Radial Basis Function Network for face recognition <ref type="bibr" target="#b6">[7]</ref>. A DCT-based scene descriptor was used together with a CNN classifier <ref type="bibr" target="#b7">[8]</ref>. A significant convergence speedup and case-specific accuracy improvement have been achieved by applying DCT transform to early stage learned feature maps in shallow CNNs <ref type="bibr" target="#b8">[9]</ref> whereas the later stage convolutional filters were operating on a sparse spectral feature representation. In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> it was demonstrated how DCT coefficients can be efficiently used to train CNNs for classification, where the DCT coefficients can be computed or taken directly from JPEG image format. Wang et al. <ref type="bibr" target="#b11">[12]</ref> compresses CNN filters by separating cluster centers and residuals in their DCT representation. Weights in this form were quantized and transformed via Huffman coding (used for JPEG compression) for limiting storage. Lastly, DCT image representation has been used for calculation of more informative loss function in generative learning <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Wavelets &amp; CNNs</head><p>Wavelet networks As an alternative to DCT, scattering networks <ref type="bibr" target="#b1">[2]</ref> are built on complex-valued wavelets. The scattering network has its filters designed to extract translation and rotation invariant representations. It was shown to effectively reduce the input representation while preserving discriminative information for training CNN on image classification and object detection task <ref type="bibr" target="#b13">[14]</ref> achieving performance comparable to deeper models. Williams et al. <ref type="bibr" target="#b14">[15]</ref> have advocated image preprocessing with wavelet transform, but used different CNNs for each frequency subband. Wavelet filters were also used as a preprocessing method prior to NN-based classifier <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral based CNNs</head><p>Other works have used wavelets in CNN computational graphs. Low-frequency components of the Dual-Tree Complex Wavelet transform were used in a noise suppressing pooling operator <ref type="bibr" target="#b16">[17]</ref>. Ripperl et al. have designed a spectral pooling <ref type="bibr" target="#b17">[18]</ref> based on Fast Fourier Transform and truncation of high-frequency coefficients. They also proposed to parameterise filters in the Fourier domain to decrease their redundancy and speed up the convergence when training the network. The pooled features were recovered with Inverse Discrete Fourier Transform, thus the CNN still operates in the spatial domain.</p><p>A Wavelet Convolutional Network proposed by Lu et al. <ref type="bibr" target="#b18">[19]</ref> learns from both spatial and spectral information that is decomposed from the first layer features. The higher-order coefficients are concatenated along with the feature maps of the same dimensionality. However, contrary to our harmonic networks, Wavelet CNNs decompose only the input features and not the features learned at intermediate stages. Robustness to object rotations was addressed by modulating learned filters by oriented Gabor filters <ref type="bibr" target="#b19">[20]</ref>. Worrall et al. incorporated complex circular harmonics into CNNs to learn rotation equivariant representations <ref type="bibr" target="#b20">[21]</ref>. Similarly to our harmonic block, the structured receptive field block <ref type="bibr" target="#b21">[22]</ref> learns new filters by combining fixed filters, a set of Gaussian derivatives with considerably large spatial extent. Additionally, an orthogonal set of Gaussian derivative bases of small spatial extend have been used by Kobayashi to express convolutional filters <ref type="bibr" target="#b22">[23]</ref>. DCFNet <ref type="bibr" target="#b2">[3]</ref> expresses filters by truncated expansion of Fourier-Bessel basis, maintaining accuracy of the original model while reducing the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Harmonic Networks</head><p>A convolutional layer extracts correlation of input patterns with locally applied learned filters. The idea of convolutions applied to images stems from the observation that pixels in local neighborhoods of natural images tend to be strongly correlated. In many image analysis applications, transformation methods are used to decorrelate signals forming an image <ref type="bibr" target="#b23">[24]</ref>. In contrast with spatial convolution with learned kernels, this study proposes feature learning by weighted combinations of responses to predefined filters. The latter extracts harmonics from lower-level features in a region. The use of well selected predefined filters allows one to reduce the impact of overfitting and decrease computational complexity. We focus here on the use of DCT as the underlying transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discrete Cosine Transform</head><p>DCT is an orthogonal transformation method that decomposes an image to its spatial frequency spectrum. A 2D signal is expressed as a sum of sinusoids with different frequencies. The contribution of each sinusoid towards the whole signal is determined by its coefficient calculated during the transformation. DCT is also a separable transform and due to its energy compaction properties on natural images <ref type="bibr" target="#b23">[24]</ref> it is commonly used for image and video compression in widely used JPEG and MPEG formats. Note that Karhunen-Loève transform (KLT) is considered to be optimal in signal decorrelation, however it transforms signal via unique basis functions that are not separable and need to be estimated from the data. On locally correlated signals such as natural images DCT was shown to closely approximate KLT <ref type="bibr" target="#b23">[24]</ref>.</p><p>We use the most common DCT formulation, noted DCT-II, computed on a 2dimensional grid of an image X of size A × B representing the image patch with 1 pixel discretisation step:</p><formula xml:id="formula_0">Y u,v = A−1 x=0 B−1 y=0 α u A α v B X x,y cos π A x + 1 2 u cos π B y + 1 2 v .<label>(1)</label></formula><p>Y u,v is the DCT coefficient of the input X using a sinusoid with horizontal and vertical frequencies noted u and v respectively. Basis functions are typically normalized with factors α u = 1 (resp. α v = 1) when u = 0 (resp. when v = 0) and α u = 2 (resp. α v = 2) otherwise to ensure their orthonormality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Harmonic blocks</head><p>We propose the harmonic block to replace a conventional convolutional operation hence relying on processing the data in two stages (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Firstly, the input features undergo harmonic decomposition using window-based DCT. In the second stage, the transformed signals are combined by learned weights. The fundamental difference from standard convolutional network is that the optimization algorithm is not searching for filters that extract spatial correlation, rather learns the relative importance of preset feature extractors (DCT filters) at multiple layers. Harmonic blocks are integrated as a structural element in the existing or new CNN architectures. We thus design harmonic networks that consist of one or more harmonic blocks and, optionally, standard learned convolutions and fully-connected layers, as well as any other structural elements of a neural net. Spectral decomposition of input features into block-DCT representation is implemented as a convolution with DCT basis functions. A 2D kernel with size K × K is constructed for each basis function, comprising a filter bank of depth K 2 , which is separately applied to each of the input features. Convolution with the filter bank isolates coefficients of DCT basis functions to their exclusive feature maps, creating a new feature map per each channel and each frequency considered. The number of operations required to calculate this representation can be minimized by decomposing 2D DCT filter into two rank-1 filters and applying them as separable convolution to rows and columns sequentially.</p><p>Each feature map h l at depth l is computed as a weighted linear combination of DCT coefficients across all input channels N :</p><formula xml:id="formula_1">h l = N −1 n=0 K−1 u=0 K−1 v=0 w l n,u,v ψ u,v * * h l−1 n<label>(2)</label></formula><p>where ψ u,v is a u, v frequency selective DCT filter of size K × K, * * the 2-dimensional convolution operator and w l n,u,v is learned weight for u, v frequency of the n-th feature.</p><p>The linear combination of spectral coefficients is implemented via a convolution with 1 × 1 filter that scales and sums the features, see <ref type="figure" target="#fig_0">Fig. 1</ref>. In our implementation we use a fixed collection of DCT bases. Specifically, if we are to replace a K × K convolution layer, the DCT filter bank ψ u,v ∈ R K×K ; u, v ∈ N; 0 ≤ u, v &lt; K has filters defined for every filter coordinate x, y as given in Eq. 1. Since the DCT is a linear transformation, backward pass through the transform layer is performed similarly to a backward pass through a convolution layer. Harmonic blocks are designed to learn the same number of parameters as their convolutional counterparts. Such blocks can be considered a special case of depth-separable convolution with predefined spatial filters. DCT is distinguished by its energy compaction capabilities which typically results in higher filter responses in lower frequencies. The behaviour of relative loss of high frequency information can be efficiently handled by normalizing spectrum of the input channels. This can be achieved via batch normalization that adjusts per frequency mean and variance prior to the weighted combination. The spectrum normalization transforms Eq. (2) into:</p><formula xml:id="formula_2">h l = N −1 n=0 K−1 u=0 K−1 v=0 w l n,u,v ψ u,v * * h l−1 n − µ l n,u,v σ l n,u,v ,<label>(3)</label></formula><p>with parameters µ l n,u,v and σ l n,u,v estimated per input batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Harmonic Network Compression</head><p>The JPEG compression encoding relies on stronger quantisation of higher frequency DCT coefficients. This is motivated by the human visual system which often prioritises low frequency information over high frequencies.</p><p>We propose to employ similar idea in the harmonic network architecture. Specifically, we limit the visual spectrum of harmonic blocks to only several most informative low frequencies, which results in a reduction of number of parameters and operations required at each block. The coefficients are (partially) ordered by their relative importance for the visual system in triangular patterns starting at the most important zero frequency at the top-left corner, see <ref type="figure">Fig. 2</ref>. We limit the spectrum of considered frequencies by hyperparameter λ representing the number of levels of coefficients included perpendicularly to the main diagonal direction starting from zero frequency: DC only for λ = 1, three coefficients used for λ = 2, and six coefficients used for λ = 3. <ref type="figure">Fig. 2</ref> illustrates filters used at various levels assuming a 3 × 3 receptive field. Thus, reformulating convolutional layers as harmonic allows one to take advantage of this natural approach to model compression, and in doing also introduce additional regularization into the model. The empirical impact of harmonic model compression is investigated experimentally in more details Sections 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overlapping cosine transform</head><p>DCT computed on overlapping windows is also known as Lapped Transform or Modified DCT (MDCT), related to our harmonic block using strides. The overlapped DCT has a long history in signal compression and reduces artefacts at window edges <ref type="bibr" target="#b24">[25]</ref>. Dedicated strategies for efficient computations have been proposed <ref type="bibr" target="#b24">[25]</ref>, including algorithms and hardware optimisations. DCT is equivalent to the discrete Fourier transform of real valued functions with even symmetry within twice larger window. DCT lacks imaginary component given by the sine transform of real valued odd functions. However, harmonic block allows convolution with DCT basis with an arbitrary stride, creating redundancy in the representation. Ignoring the boundary limitations, sine filter basis can be devised by shifting the cosine filters. Given the equivariant properties of convolution, instead of shifting the filters the same result is achieved by applying original filters to the shifted input. Considering DCT-II formulation for 1D signal X with N values, the DCT coefficient at frequency k is:</p><formula xml:id="formula_3">F k = N −1 n=0 X n cos π N n + 1 2 k (4)</formula><p>a corresponding sine transform is</p><formula xml:id="formula_4">G k = N −1 n=0 X n sin π N n + 1 2 k (5)</formula><p>which is equivalent to</p><formula xml:id="formula_5">G k = N −1 n=0 X n cos π 2 + 2πz − π N n + 1 2 k .<label>(6)</label></formula><p>The shift given by π/2 + 2πz for any z ∈ Z can be directly converted to shift in pixels applied to data X. After simplification, sine transform can be expressed as</p><formula xml:id="formula_6">G k = N −1 n=0 X n cos π N n − N (1 + 4z) 2k + 1 2 k (7)</formula><p>which is equivalent to the cosine transform of the image shifted by δ = N (1 + 4z) /2k defined in <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_7">F k [δ] = N −1 n=0 X n+ N (1+4z) 2k cos π N n + 1 2 k .<label>(8)</label></formula><p>This value represents the stride to shift the cosine filters to capture correlation with sine function. In other words, by applying DCT with a certain stride it is possible to obtain the feature representation as rich as that obtained with the full Fourier transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Computational Requirements</head><p>Harmonic blocks are designed to learn the same number of parameters as their convolutional counterparts. Requirements for the DCT transform scale linearly with the number of input channels and result in a modest increase to the theoretical number of operations.</p><p>Standard convolutional layer used in many popular architectures that has N input and M output channels with a kernel size K × K learns N M K 2 parameters and performs N M K 2 AB operations if the filter is applied A and B times in particular directions. Harmonic block with K 2 transformation filters of size K × K upsamples representation to N K 2 features and then learns one weight for each upsampled-output feature pair hence N K 2 M weights. Transformation of an A × B feature set costs N K 2 K 2 AB on top of weighted combination N K 2 M AB that matches the number of multiply-add operations of K × K convolution. The total number of operations is thus</p><formula xml:id="formula_8">N K 2 AB M + K 2 .</formula><p>The theoretical number of multiply-add operations over the standard convolutional layer increases by a factor of K 2 /M . If we assume truncated spectrum (use of λ ≤ K) given by P = λ(λ + 1)/2 filters, proportion of operations becomes P/K 2 + P/M . While keeping the number of parameters intact, a harmonic block requires additional memory during training and inference to store transformed feature representation. In our experiments with WRN models (Sec.4.2), the harmonic network trained with full DCT spectrum requires almost 3 times more memory than the baseline. This memory requirement can be reduced by using the DCT spectrum compression.</p><p>Despite the comparable theoretical computational requirements, the run time of harmonic networks is larger compared to the baseline models, at least twice slower (on GPU) in certain configurations. This effect is due to generally less efficient implementation of separable convolution and the design of harmonic block that replaces a single convolutional layer by a block of 2 sequential convolutions (with individual harmonic filters and 1x1 convolution). Most blocks do not need BN between the convolutions and thus represent a combined linear transformation. The associativity property of convolutions allows one to reformulate the standard harmonic block defined above so that the DCT transform and linear combination can be effectively merged into a single linear operation:</p><formula xml:id="formula_9">h l = N −1 n=0 K−1 u=0 K−1 v=0 w n,u,v ψ u,v * * h l−1 n = N −1 n=0 K−1 u=0 K−1 v=0 w n,u,v ψ u,v * * h l−1 n (9) Algorithm 1: Memory efficient harmonic block Input: h l−1 Define updates g ∈ R M ×N ×K×K ; for m ∈ {0..M − 1} do for n ∈ {0..N − 1} do g l m,n ← K−1 u=0 K−1 v=0 w m,n,u,v ψ u,v ; end end h l ← g l * * h l−1 ; Output: h l</formula><p>In other words, equivalent features can be obtained by factorizing filters as linear combinations of DCT basis functions. We thus propose a faster Algorithm 1 that is a more memory efficient alternative to the standard two-stage harmonic block formulation and uses dense convolution.</p><p>The Algorithm 1 overhead in terms of multiply-add operations with respect to the standard convolutional layer is only K 2 /AB, where the input image size for the block is A × B. The experimental performance of the algorithm is evaluated in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Image Classification</head><p>The performance of the harmonic networks is assessed for image classification on small (NORB, Sec. 4.1), medium (CIFAR-10 and CIFAR-100, Sec 4.2) and large (ImageNet-1K, Sec. 4.3) scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Small NORB dataset</head><p>The small NORB dataset <ref type="bibr" target="#b25">[26]</ref> is a synthetic set of 96 × 96 binocular images of 50 toys sorted into 5 classes (four-legged animals, human figures, airplanes, trucks, and cars), captured under different lighting and pose conditions (i.e. 18 angles, 9 elevations and 6 lighting conditions induced by combining different light sources). Training and test sets used in our experiments are retained original <ref type="bibr" target="#b25">[26]</ref>. We show first that harmonic networks outperform standard and state-of-the-art CNNs in both accuracy and compactness (c.f. Section 4.1.1) and also illustrate how Harmonic networks can be naturally resilient to unseen illumination changes without resorting to using data augmentation (Sec. 4.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparisons CNN vs. Harmonic Nets</head><p>Baseline architectures. Our baseline CNN2 consists of 2 convolution and 2 fullyconnected layers. Features are subsampled by convolution with stride and overlapping max-pooling. All hidden layer responses are batch normalized and rectified by ReLU. We also use a slightly deeper network CNN3 with an additional convolutional layer preceding the first pooling. Details of the architectures are summarised in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Optimisation. The baseline CNNs are trained with stochastic gradient descent for 200 epochs with momentum 0.9 and weight decay 0.0005. The initial learning rate 0.01 is decreased by factor 10 every 50 epochs. The network is trained with batches of 64 stereo image pairs, each pair is zero-padded 5 pixels and a random crop of 96×96 pixels is fed to the network. Harmonic Networks architectures. Several versions of harmonic networks are considered (Tab. 1), by substituting the first, first two or all three of CNN2 and CNN3 convolution layers by harmonic blocks. Furthermore, the first fully-connected layer can be transformed to a harmonic block taking global DCT transform of the activations. The first harmonic block uses 4×4 DCT filters, the further blocks mimic their convolutional counterparts.</p><p>Performance evaluation. The baseline CNN architecture shows poor generalization performance in early stages of training, see <ref type="figure" target="#fig_2">Fig. 3</ref>. Baseline CNN2 achieved mean error 3.48%±0.50 from 20 trials, while CNN2 utilizing harmonic blocks without explicit normalization of harmonic responses exhibits similar behavior resulting in lower mean error of 2.40%±0.39. Normalizing DCT responses at the first block prevents harmonic network from focusing too much on pixel intensity, allows using 10× higher learning rate, significantly speeds up convergence, improves performance and stability. All variants of the harmonic network perform comparably. Particularly we observe the overlapping average pooling to work well in combination with harmonic blocks. The best result was obtained by the Harm-CNN4 model with 4 harmonic blocks (the latter replaces the fully-connected layer), misclassifying only 1.10%±0.16 of test samples.</p><p>Comparison with state-of-the-art.  Harmonic network compression. We further proceed to designing a very compact version of Harm-CNN4 (cf. <ref type="table" target="#tab_1">Table 2</ref>). To do so, the fully connected layer is reduced to only 32 neurons and the dropout is omitted. The modified network reaches 1.17%±0.20 error and has 131k parameters. When applying compression with λ = K (i.e. 3 for 3 × 3 filters) the network reaches 1.34%±0.21 and requires less than 88k parameters. By applying λ = K −1 the total count is less than 45k parameters and error of 1.64%±0.22 is achieved on the test set, in contrast with small capsule network <ref type="bibr" target="#b26">[27]</ref> with 68k parameters scoring a higher error of 2.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Harmonic Networks for illumination changes</head><p>Spectral representation of input data has a few interesting properties. Average feature intensity is captured into DC coefficient, while other (AC) coefficients capture the signal structure. DCT representation has been successfully used <ref type="bibr" target="#b6">[7]</ref> to build illumination invariant representation. This gives us strong motivation to test illumination invariance properties of harmonic networks and to compare them with standard CNNs. Objects in the small NORB dataset are normalized and are presented with their shadows over a uniform background. The six lighting conditions are obtained by various combination of up to 4 fixed light sources at different positions and distances from the objects. Usual approaches to reduce sensitivity to lighting conditions include image standardization with ZCA whitening or illumination augmentation. Brightness and contrast manipulations encourage the network to focus on features that are independent of the illumination. Contrary to these methods in our approach we achieve the same effect by    <ref type="table" target="#tab_3">Table 3</ref>. Harmonic networks consistently achieve lower error under various unseen lighting conditions in comparison to baseline CNNs, with and without random brightness and contrast (only for dark images) augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CIFAR-10/100 datasets</head><p>The second set of experiments is performed on popular benchmark datasets of small natural images CIFAR-10 and CIFAR-100.  <ref type="bibr" target="#b19">[20]</ref>.</p><p>Baseline. For experiments on CIFAR datasets we adopt WRNs <ref type="bibr" target="#b27">[28]</ref> with 28 layers and width multiplier 10 (WRN-28-10) as the main baseline. Model design and training procedure are kept as in the original paper. Harmonic WRNs are constructed by replacing convolutional layers by harmonic blocks with the same receptive field, preserving batch normalization and ReLU activations in their original positions after every block. Results. We first investigate whether the WRN results can be improved if only trained on spectral information, replacing only the first convolutional layer (denoted as Harm1-WRN). The network learns more useful features if the RGB spectrum is explicitly normalized by integrating the BN block as demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, surpassing the classification error of the baseline network on both CIFAR-10 and CIFAR-100 datasets, see <ref type="table" target="#tab_4">Table 4</ref>. We then construct a fully harmonic WRN (Harm-WRN) by replacing all convolutional layers with harmonic blocks. This harmonic network also outperforms the baseline WRN, see <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Analysis of fully harmonic WRN weights learned with 3x3 spectrum revealed that the deeper layers tend to favour low-frequency information over high frequencies when learning representations. Relative importance of weights corresponding to different frequencies shown in <ref type="figure" target="#fig_4">Fig. 4</ref> motivates truncation of high-frequency coefficients for compression purposes. While preserving the input image spectrum intact, we train the harmonic networks on limited spectrum of hidden features for λ=2 and λ=3 using 3 and 6 DCT bases respectively. To assess the loss of accuracy associated with parameter reduction we train baselines with reduced widths having comparable numbers of parameters: WRN-28-8 and WRN-28-6, see <ref type="figure" target="#fig_5">Fig. 5</ref>. Fully harmonic WRN-28-10 with λ=3 has comparable error to the network using the full spectrum and outperforms the larger baseline WRN-28-10, showing almost no loss in discriminatory information. On the other hand Harm-WRN-28-10 with λ=2 is better on CIFAR-100 and slightly worse on CIFAR-10 compared to the similarly sized WRN-28-6. The performance degradation indicates that some of the truncated coefficients carry important discriminatory information.</p><p>We further compare performance with the Gabor CNN 3-28 <ref type="bibr" target="#b19">[20]</ref> that relies on modulating learned filters with Gabor orientation filters. To operate on a similar model we remove dropouts and reduce complexity by applying progressive λ: no compression for filters on 32x32 features, λ=3 for 16x16, and λ=2 for the rest. With a smaller number of parameters the Harm-WRN-28-10 performs similarly on CIFAR-10 and outperforms Gabor CNN on CIFAR-100. Harmonic block implementations. Here we compare the standard harmonic block implementation with its memory efficient version introduced in Algorithm 1, see <ref type="table" target="#tab_8">Table 8</ref>. The comparison on CIFAR-10 dataset demonstrates that Algorithm 1 provides similar overall performance but reduces both the runtime and memory requirements nearly three times. We will therefore use solely this implementation of the harmonic block except for the root (first) layer due to the use of BN on that first layer. Ablation study. The effect of filter parametrisation by DCT bases is investigated by replacing particular layers of WRN-16-4 (w/o dropout) with harmonic blocks, see <ref type="table" target="#tab_5">Table 5</ref>. We consider replacing the root convolution layer (with or without BN), or layers in residual blocks. Replacing each layer has provided the greatest improvement, while BN in the first block decreases the variance by half. These observations correspond to the results obtained on NORB dataset. We will always be employing BN as part of the root harmonic block. Compressing existing models. Section 3.3 described how convolutional filters in certain layer can be approximated with fewer parameters. So far we have only considered uniform coefficient truncation by truncating the same frequencies in all the layers, or a simple progressive compression. This scheme omits higher number of frequencies in deeper layers, but the same subset of coefficients is used in all harmonic blocks applied to feature maps of a particular size. A better compression-accuracy trade-off can be achieved by using more elaborate coefficient selection at each layer. In this experiment we start with the WRN-28-10 baseline trained without dropout which has been converted  to harmonic WRN-28-10 net (omitting BN in the first harmonic block) by re-expressing each 3×3 filter as a combination of DCT basis functions. The first harmonic block is kept intact (no compression in DCT representation), while all other blocks are compressed. We compare three different coefficient selection strategies:</p><p>• Uniform selection: at every layer the same λ is used;</p><p>• Progressive selection: the level of compression is selected based on the depth of the layer λ progr = max(α, min(2K − 1, T /Depth )) for α = 1 or 2, constant T , and K is the size of filter (λ = 2K − 1 corresponds to no compression);</p><p>• Adaptive selection: the compression level is selected adaptively for each layer; the filter is excluded if its L1 compared to the other frequencies in the same layer is too low. Specifically, if |w i,j | 1 / K−1 u,v=0 |w u,v | 1 &lt; T then the coefficient is truncated.  <ref type="figure">Figure 6</ref>: Accuracy of compressed WRN-28-10 on CIFAR-100 dataset using different coefficient truncation strategies.</p><p>The results reported in <ref type="figure">Fig. 6</ref> confirm the behavior observed in <ref type="figure" target="#fig_4">Fig. 4</ref>, i.e. the high frequencies appear to be more relevant in the early layers of the network compared to deeper layers. The uniform compression discards the same amount of information in all the layers, and is surpassed by the other compression strategies. By using progressive or adaptive coefficient selection a model can be compressed by over 20% without a loss in accuracy. The best progressive method loses less than 1% of accuracy when compressed by 45% without a need for finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ImageNet dataset</head><p>We present here results obtained on ImageNet-1K classification task. ResNet <ref type="bibr" target="#b28">[29]</ref> with 50 layers is adopted as the baseline. To reduce memory consumption maxpooling is not used, instead the first convolution layer employs stride 4 to produce equally-sized features; we refer to this modification as ResNet-50 (no maxpool). The following harmonic modifications refer to this baseline without maxpooling after the first layer. We investigate the performance of three harmonic modifications of the baseline: (i) replacing solely the initial 7x7 convolution layer with harmonic block (with BN) with 7x7 DCT filters, (ii) replacing all convolution layers with receptive field larger than 1x1 with equally-sized harmonic blocks, (iii) compressed version of the fully-harmonic network. The models are trained as described in <ref type="bibr" target="#b4">[5]</ref>, and here we report accuracy after 100 epochs. <ref type="table" target="#tab_6">Table 6</ref> reports error rates on ImageNet validation set using central 224×224 crops from images resized such that the shorter side is 256. All three harmonic networks have similar performance and improve over the baseline by 0.6 − 1% in top1 and 0.4 − 0.6% in top5 accuracy. We observe similar progress of the three modifications during training, see  <ref type="bibr" target="#b29">[30]</ref> 25.6M 23.87 7.14 ScatResNet-50 <ref type="bibr" target="#b13">[14]</ref> 27.8M 25.5 8.0 JPEG-ResNet-50 <ref type="bibr" target="#b10">[11]</ref> 28.4M 23.94 6.98 ResNet-101 (maxpool) <ref type="bibr" target="#b29">[30]</ref> 44.5M 22.63 6.45  <ref type="figure" target="#fig_6">Fig. 7</ref>. ResNet-50 architecture has 17 layers with spatial filters which correspond to 11M parameters. We reduce this number by using progressive λ compression: λ=3 on 14x14 features and λ=2 on the smallest feature maps. This reduces the number of weights roughly by half, in total by about 23% of the network size. The compressed network loses almost no accuracy and still clearly outperforms the baseline. Even with compression the proposed Harm-ResNet-50 confidently outperforms the standard ResNet-50 (maxpool), as well as the ScatResNet-50 <ref type="bibr" target="#b13">[14]</ref> and JPEG-ResNet-50 <ref type="bibr" target="#b10">[11]</ref>. Furthermore, we also observe a substantial improvement of 1.15 in top-1 error % associated with the introduction of harmonic blocks into a deeper ResNet-101. We validate the use of harmonic blocks on an architecture without residual connections as well, specifically the VGG16 <ref type="bibr" target="#b30">[31]</ref> architecture with BN layers. Harm-VGG16-BN, obtained by replacing all convolutional layers by harmonic blocks yields an improvement of ∼0.8% in top-1 classification error. This demonstrates that the improvement is not limited to residual-based architectures.</p><p>Finally, we evaluate conversion of weights of a pretrained non-harmonic network to those of its harmonic version. Each learned filter in the pretrained baseline (ResNet-50 without maxpooling after 90 epochs of training) is transformed into DCT domain, skiping BN inside the first harmonic block. The direct conversion resulted in the exact same numerical performance due to the basis properties of DCT. We then finetune the converted model for another 5 epochs with the learning rate of 0.001, which results in the top1 (top5) performance improvement of 0.21% (0.19%) over the pretrained baseline, see <ref type="table" target="#tab_7">Table 7</ref>. We also investigate the conversion to a harmonic network with progressive λ compression. After casting the pretrained filters into the available number of DCT filters (from full basis at the early layers to 3 out of 9 filters at the latest layers), the top1 performance degrades by 6.3% due to loss of information. However, if we allow finetuning for as few as 5 epochs the top1 (top5) accuracy falls 0.24% (0.09%) short of the baseline, while reducing the number of parameters by 23%. This analysis shows how the harmonic networks can be used to improve the accuracy and/or compress the existing pretrained CNN models. Comparison with the cutting-edge techniques. Here we verify the use of DCTbased harmonic blocks in the more elaborate state-of-the-art models. To this end we modify ResNeXt architecture <ref type="bibr" target="#b31">[32]</ref>, which is similar to ResNets and uses wider bottleneck and grouped convolution to decrease the amount of FLOPS and the number of parameters. The model is further boosted using several state-of-the-art adjustments: (i) identity mappings that downsample features are extended by average pooling to prevent information loss; (ii) squeeze and excitation blocks (SE) <ref type="bibr" target="#b32">[33]</ref> are used after every residual connection. The network is further regularized by stochastic depth and dropout on the last layer. Training is performed via stochastic gradient decent with learning rate 0.1 and batch size 256, with the former decayed according to one cosine annealing cycle. In addition to mirroring and random crops of size 224, images are augmented with rotations and random erasing.</p><p>Our ResNeXt modification with 101 layers and 32 groups per 4 convolutional filters in a residual block is trained for 120 epochs. The use of DCT bases provides a subtle improvement over the standard bases. Furthermore, we upscale the network to use 64 groups of filters, replace max-pooling in the first layer by increased stride and train this network for 170 epochs. From <ref type="table" target="#tab_9">Table 9</ref> we conclude that our model outperforms all other "handcrafted" architectures that do not use extra training images and performs comparably to the networks of similar complexity found via neural architecture search. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Object Detection and Segmentation</head><p>Representations learned from features expressed via harmonic basis are versatile and can serve well for transfer learning. We demonstrate here that popular vision architectures relying on harmonic backbones provide a notable improvement in accuracy compared to the use of standard convolution-based backbone models. To this end we assess the performance of harmonic networks in object detection, instance and semantic segmentation tasks. For object detection, the popular single stage RetinaNet <ref type="bibr" target="#b35">[36]</ref> and multistage Faster <ref type="bibr" target="#b36">[37]</ref> and Mask R-CNN <ref type="bibr" target="#b37">[38]</ref> frameworks are built upon our harmonic ResNet backbones. The semantic segmentation pipeline extends these backbones to DeepLabV3 <ref type="bibr" target="#b38">[39]</ref> models. A set of experiments is conducted on the datasets Pascal VOC <ref type="bibr" target="#b39">[40]</ref> (Sec. 5.1) and MS COCO <ref type="bibr" target="#b40">[41]</ref> (Sec. 5.2).   <ref type="table" target="#tab_0">Table 10</ref> for different depths of ResNet backbones and configurations of the dataset. From <ref type="table" target="#tab_0">Table 10</ref> we conclude that the models built on our harmonic backbones surpass their conventional convolution-based counterparts in all configurations as well as on both training sets. We observe a consistent improvement due to the Harmonic architecture: by 1% AP for ResNet-50 and 0.8% AP in case of ResNet-101 using the Faster R-CNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Object Detection on MS COCO</head><p>Common Objects in COntext (COCO) dataset poses a greater challenge due to a higher variety of target classes and generally smaller object sizes. The networks are trained following the standard procedure, images resized so that their shortest side is 800 pixels. The learning rate is initialized by linear scaling method lr = 0.02 × (bs/16) using default hyperparameters set up by Chen et al. <ref type="bibr" target="#b41">[42]</ref>. All models are trained with standard 12 (24) epochs schedules with learning rate decreased by 10 after epochs 8 (16) and 11 <ref type="bibr" target="#b21">(22)</ref>. <ref type="table" target="#tab_0">Table 11</ref> shows that the use of our harmonic backbones consistently improves both single-stage RetinaNet and multi-stage Faster and Mask R-CNN detectors by 0.7-1.3 AP with identical training procedures employed.  <ref type="bibr" target="#b41">[42]</ref>.</p><p>The state-of-the-art detectors rely on a cascade of detection heads with progressively increasing IoU thresholds, which refines the bounding boxes and thus improves localization accuracy <ref type="bibr" target="#b43">[44]</ref>. In <ref type="table" target="#tab_0">Table 12</ref>, we report comparisons achieved with the Cascade R-CNN architecture, trained using the 20-epoch schedule suggested in <ref type="bibr" target="#b43">[44]</ref>. The use of our harmonic ResNet-101 provides a 1.0 AP improvement for object detection similar to Faster &amp; Mask R-CNNs, and it also improves instance segmentation AP by 0.7 (see <ref type="table" target="#tab_0">Table 12</ref>). Moreover, a similar improvement of 1.1 AP is observed for hybrid task cascade R-CNN <ref type="bibr" target="#b44">[45]</ref> that alters the mask refinement procedure and exploits semantic segmentation information to incorporate additional contextual information.  <ref type="bibr" target="#b41">[42]</ref>.</p><p>These experiments on object detection and localization demonstrate that the harmonic versions of the backbones provide a meaningful improvement of about 1.0 AP in terms of both bounding boxes and masks to the state-of-the-art detection architectures. Our harmonic networks retain this improvement from the purely classification task through the transformation to the Feature Pyramid Networks (FPNs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semantic Segmentation on Pascal VOC</head><p>We now assess our proposed harmonic networks on the task of semantic segmentation using the Pascal VOC 2012 benchmark. Training images are augmented into a set of 10,582 samples as in <ref type="bibr" target="#b38">[39]</ref>. Performance is measured in terms of intersection over union (IoU) on a large validation set consisting of 1449 images. The segmentation is performed using the DeepLabV3 architecture <ref type="bibr" target="#b38">[39]</ref>. We extend PyTorch implementation of this model 2 and retrain baseline models with ResNet-50 and ResNet-101 backbones for 30,000 iterations with batch size 16, learning rate 0.1 and output stride parameter equal to <ref type="bibr">16.</ref> We replace the backbone model of the segmentation network with harmonic ResNets using two settings for the backbone: (i) converting from the original backbone models or (ii) taking a harmonic models pre-trained on ImageNet (90 epochs), see <ref type="table" target="#tab_6">Table 6</ref>. The results are summarized in <ref type="table" target="#tab_0">Table 13</ref>. The DeepLabV3 models with harmonic backbones pretrained on ImageNet improve IoU scores by approximately 1.1%. This experiment also validates the application of harmonic blocks with dilated convolution, which is dissimilar to the classical dense formulation in that the spatial correlation patterns may be weaker due to dilatation. DeepLabV3 with harmonic backbone has the strongest average improvement on classes "chair" (+9.5%), "sheep" (+3.9%), "boat" (+3.2%), "cow" (+2.9%) and "tvmonitor" (+2.9%), and performs worse on "pottedplant" (-3.4%) and "aeroplane" (-0.8%). A selection of image samples from these classes is presented in <ref type="figure">Fig. 8</ref> showing how harmonic networks impact image segmentation quality. We observe some non-trivial improvements of the segmentation masks due to the use of harmonic blocks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a novel approach to explicitly incorporate spectral information extracted via DCT into CNN models. We have empirically evaluated the use of our harmonic blocks with the well-established state-of-the-art CNN architectures, and shown that our approach improves results for a range of applications including image classification (0.7-1.2% accuracy on ImageNet), object detection (0.7-1.1 AP on Pascal VOC and MS COCO) and semantic segmentation (1.1% IoU on Pascal VOC). We further establish that the memory footprint of harmonic nets is similar and the computational complexity increases only slightly when compared to the standard convolutional baseline architectures. We ascertain that harmonic networks can be efficiently set-up by converting the pretrained CNN baselines. The use of DCT allows one to order the harmonic block parameters by their significance from the most relevant low frequency to less important high frequencies. This enables efficient model compression by parameter truncation with only minor degradation in the model performance. Current efforts aim at investigating robustness of harmonic networks and at compressing weights according to correlations across filters in depth direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation DeepLabV3</head><p>Harm-DeepLabV3 <ref type="figure">Figure 8</ref>: Examples of semantic segmentation on Pascal VOC 2012 validation images.</p><p>The first 4 rows show where harmonic network is more successful than the baseline, while the last row displays case where it fails. DeepLabV3 with ResNet-101 backbone is used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: Design of the harmonic block. Boxes show operation type, size of filter (if applicable) and the number of output channels given the block filter size K, number of input channels N and output channels M . Batch normalization (BN) block is optional. Right: Visualization of the harmonic block applied to an input layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 : 3 ×</head><label>23</label><figDesc>3 DCT filter bank employed in the harmonic networks and its compression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Mean classification error on small NORB test set. Weak generalization of CNN (green) and harmonic network (blue) is observed during the early stages of training. Filled areas (best seen in color) show 50% empirical confidence intervals from 20 runs. Batch normalization of DCT spectrum (first block) significantly speeds up convergence of harmonic network (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>removing the filter corresponding to the DC component from the first harmonic block. Such network is invariant towards global additive changes in pixel intensity by definition. Set-up. The dataset is split into 3 parts based on lighting conditions during image capturing: the bright images (conditions 3,5) dark images (cond.<ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4)</ref> and images under standard lighting conditions (cond. 0,1). The models are trained (w/wo data augmentation) only on data from one split and tested on images from the other two splits that contain unseen lighting conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Distribution of weights (averaged in each layer) assigned to DCT filters in the first harmonic block (left-most) and the remaining blocks in the Harm-WRN-28-10 model trained on CIFAR-10. Vertical lines separate the residual blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Decrease of classification error as a function of model size on CIFAR-10 (left) and CIFAR-100 (right). Parameters of harmonic networks are controlled by the compression parameter λ, the WRN baselines by the width multiplier w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Training of harmonic networks on ImageNet classification task. Left: comparison with the baseline showing validation error (solid line) and training error (dashed). Right: last 40 epochs of training for all the ResNet-50 based models including scores reported for the benchmark models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Models used in NORB experiments. Convolution and harmonic operation are denoted as {conv, harm} M,K×K/S with M output features, kernel size K and stride S; similarly for pooling K×K/S and fully connected layers fc M.</figDesc><table><row><cell>Resol.</cell><cell>CNN2</cell><cell>CNN3</cell><cell cols="2">Harm-CNN2 Harm-CNN3</cell><cell>Harm-CNN4</cell></row><row><cell cols="6">96x96 conv 32, 5x5/2 conv 32, 5x5/2 harm 32, 4x4/4 harm 32, 4x4/4 harm 32, 4x4/4</cell></row><row><cell>48x48</cell><cell>pool 3x3/2</cell><cell>conv 64, 3x3/2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">24x24 conv 64, 3x3/2</cell><cell>pool 2x2/2</cell><cell cols="3">harm 64, 3x3/2 harm 64, 3x3/2 harm 64, 3x3/2</cell></row><row><cell>12x12</cell><cell cols="2">pool 3x3/2 conv 128, 3x3/2</cell><cell>pool 3x3/2</cell><cell>pool 3x3/2</cell><cell>pool 3x3/2</cell></row><row><cell>6x6</cell><cell>fc 1024</cell><cell>pool 2x2/2</cell><cell>fc 1024</cell><cell cols="2">harm 128, 3x3/2 harm 128, 3x3/2</cell></row><row><cell>3x3</cell><cell>-</cell><cell>fc 1024</cell><cell>-</cell><cell>fc 1024</cell><cell>harm 1024, 3x3/3</cell></row><row><cell>1x1</cell><cell>dropout 0.5</cell><cell>dropout 0.5</cell><cell>dropout 0.5</cell><cell>dropout 0.5</cell><cell>dropout 0.5</cell></row><row><cell>1x1</cell><cell>fc 5</cell><cell>fc 5</cell><cell>fc 5</cell><cell>fc 5</cell><cell>fc 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>shows that these results surpass the best previously reported error rate for this dataset to the best of our knowledge. The capsule network<ref type="bibr" target="#b26">[27]</ref> claims 1.4% error rate, however estimated under a different evaluation protocol.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art on small NORB dataset, showing the proposed method outperforms other reported results.</figDesc><table><row><cell>Method</cell><cell cols="2">Parameters ↓ Error % ↓</cell></row><row><cell>CNN3</cell><cell>1.28M</cell><cell>3.43 ± 0.31</cell></row><row><cell>CapsNet [27] multi-crop</cell><cell>310K</cell><cell>1.4*</cell></row><row><cell>Harm-CNN2</cell><cell>2.39M</cell><cell>1.56 ± 0.18</cell></row><row><cell>Harm-CNN3</cell><cell>1.28M</cell><cell>1.15 ± 0.22</cell></row><row><cell>Harm-CNN4</cell><cell>1.28M</cell><cell>1.10 ± 0.16</cell></row><row><cell cols="3">*score reported by the authors of the corresponding paper.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Means of classification error over 10 runs on NORB test images captured in unseen illumination conditions. Harmonic networks improve classification error of CNN by 5-16%.</figDesc><table><row><cell>Augmentation</cell><cell cols="2">None</cell><cell cols="2">Brightness &amp; contrast</cell></row><row><cell cols="4">Lighting Condition CNN ↓ Harmonic↓ CNN ↓</cell><cell>Harmonic↓</cell></row><row><cell>Bright</cell><cell>26.3±2.6</cell><cell>10.2±0.4</cell><cell>17.6±0.7</cell><cell>9.4±0.7</cell></row><row><cell>Standard</cell><cell>30.2±1.8</cell><cell>18.0±1.9</cell><cell>22.5±1.1</cell><cell>15.1±1.1</cell></row><row><cell>Dark</cell><cell>31.2±1.8</cell><cell>18.9±1.2</cell><cell>20.1±1.3</cell><cell>14.5±1.4</cell></row></table><note>Performance evaluation. Classification errors of the best CNN and harmonic net- work architectures on the test images (unseen illumination conditions) are reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Settings and median error rates (%) out of 5 runs achieved by WRNs and their harmonic modifications on CIFAR datasets. Number of parameters reported for CIFAR-10.</figDesc><table><row><cell>Method</cell><cell cols="3">Dropout Param. ↓ CIFAR-10 ↓ CIFAR-100 ↓</cell></row><row><cell>WRN-28-10 [28]</cell><cell>36.5M</cell><cell>3.91</cell><cell>18.75</cell></row><row><cell>Gabor CNN 3-28 [20]</cell><cell>17.6M</cell><cell>3.88*</cell><cell>20.13*</cell></row><row><cell>Harm1-WRN-28-10 (no BN)</cell><cell>36.5M</cell><cell>4.10</cell><cell>19.17</cell></row><row><cell>Harm1-WRN-28-10</cell><cell>36.5M</cell><cell>3.90</cell><cell>18.80</cell></row><row><cell>Harm1-WRN-28-10</cell><cell>36.5M</cell><cell>3.64</cell><cell>18.57</cell></row><row><cell>Harm-WRN-28-10</cell><cell>36.5M</cell><cell>3.86</cell><cell>18.57</cell></row><row><cell>Harm-WRN-28-10, progr. λ</cell><cell>15.7M</cell><cell>3.93</cell><cell>19.04</cell></row><row><cell></cell><cell>*scores reported by</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Modifications of the WRN-16-4 baseline on CIFAR-100: mean classification errors and standard deviations from 5 runs when replacing particular layers by harmonic blocks.</figDesc><table><row><cell>Root block Harmonic root BN Residual blocks Error % ↓</cell></row><row><cell>24.07 ± 0.24</cell></row><row><cell>23.79 ± 0.24</cell></row><row><cell>23.67 ± 0.12</cell></row><row><cell>23.22 ± 0.28</cell></row><row><cell>23.25 ± 0.25</cell></row><row><cell>23.21 ± 0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Classification errors on ImageNet validation set using central crops.</figDesc><table><row><cell>Model</cell><cell cols="3">Parameters ↓ Top-1 % ↓ Top-5 % ↓</cell></row><row><cell>VGG16-BN</cell><cell>138.4M</cell><cell>25.86</cell><cell>8.05</cell></row><row><cell>Harm-VGG16-BN</cell><cell>138.4M</cell><cell>25.55</cell><cell>8.01</cell></row><row><cell>ResNet-50 (no maxpool)</cell><cell>25.6M</cell><cell>23.83</cell><cell>7.01</cell></row><row><cell>Harm1-ResNet-50</cell><cell>25.6M</cell><cell>23.01</cell><cell>6.47</cell></row><row><cell>Harm-ResNet-50</cell><cell>25.6M</cell><cell>22.98</cell><cell>6.64</cell></row><row><cell>Harm-ResNet-50, progr. λ</cell><cell>19.7M</cell><cell>23.21</cell><cell>6.67</cell></row><row><cell>Harm-ResNet-101</cell><cell>44.5M</cell><cell>21.45</cell><cell>5.78</cell></row><row><cell>Benchmarks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 (maxpool)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance of the converted harmonic networks (error on ImageNet).</figDesc><table><row><cell cols="3">Training Epochs Model</cell><cell cols="2">Top-1%↓ Top-5%↓</cell></row><row><cell>full</cell><cell>90</cell><cell>ResNet-50 (no maxpool)</cell><cell>24.36</cell><cell>7.33</cell></row><row><cell>finetuned</cell><cell cols="2">90+5 ResNet-50 (no maxpool)</cell><cell>24.34</cell><cell>7.30</cell></row><row><cell>finetuned</cell><cell cols="2">90+5 ResNet-50 ⇒ Harm-ResNet-50</cell><cell>24.06</cell><cell>7.12</cell></row><row><cell>finetuned</cell><cell cols="2">90+5 ResNet-50 ⇒ Harm-ResNet-50, progr. λ</cell><cell>24.62</cell><cell>7.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>GPU training memory requirements and speed of harmonic block implementations on CIFAR-10 and ImageNet. All ImageNet models use harmonic blocks based on Alg. 1. Values are measured on Nvidia RTX6000 using batch size 128.</figDesc><table><row><cell>Model</cell><cell cols="2">GPU memory↓ train. infer. Images/s ↑</cell><cell>Error %↓</cell></row><row><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>WRN-28-10 [28]</cell><cell>4.6GB</cell><cell>606.4 1876.9</cell><cell>3.89</cell></row><row><cell>Harm-WRN-28-10 (non-optimized)</cell><cell>14.1GB</cell><cell>211.0 600.4</cell><cell>3.71</cell></row><row><cell>Harm-WRN-28-10 (Alg. 1)</cell><cell>4.8GB</cell><cell>573.3 1736.5</cell><cell>3.78</cell></row><row><cell>ImageNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 (no maxpool)</cell><cell>11.2GB</cell><cell>306.2 820.5</cell><cell>23.83</cell></row><row><cell>ResNet-50 (maxpool)</cell><cell>12.1GB</cell><cell>292.9 790.1</cell><cell>23.87</cell></row><row><cell>Harm-ResNet-50</cell><cell>11.4GB</cell><cell>296.3 766.5</cell><cell>22.98</cell></row><row><cell>ResNet-101 (maxpool)</cell><cell>17.4GB</cell><cell>174.1 526.7</cell><cell>22.63</cell></row><row><cell>Harm-ResNet-101</cell><cell>16.9GB</cell><cell>174.4 507.9</cell><cell>21.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>SE-ResNeXt networks: harmonic vs. baseline errors and comparison with the state of the art on ImageNet.</figDesc><table><row><cell>Model</cell><cell>Param</cell><cell cols="6">224×224 FLOPS Top-1 Top-5 FLOPS Top-1 Top-5 320×320 / 331×331</cell></row><row><cell>ResNeXt-101 (RNX):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RNX (64x4d) [32]</cell><cell>83.6M</cell><cell>15.5B</cell><cell>20.4</cell><cell>5.3</cell><cell>31.5B</cell><cell>19.1</cell><cell>4.4</cell></row><row><cell>SE-RNX(32x4d)</cell><cell>49.0M</cell><cell>8.0B</cell><cell cols="2">19.73 4.90</cell><cell>16.3B</cell><cell cols="2">18.49 4.05</cell></row><row><cell>Harm-SE-RNX(32x4d)</cell><cell>49.0M</cell><cell>8.1B</cell><cell cols="2">19.39 4.73</cell><cell>16.5B</cell><cell cols="2">18.48 4.06</cell></row><row><cell>Harm-SE-RNX(64x4d)</cell><cell>88.2M</cell><cell cols="3">15.4B 18.37 4.34</cell><cell cols="3">31.4B 17.15 3.56</cell></row><row><cell>Benchmarks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PolyNet [33]</cell><cell>92M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>34.7B</cell><cell cols="2">18.71 4.25</cell></row><row><cell>DualPathNet-131 [33]</cell><cell>79.5M</cell><cell>16.0B</cell><cell cols="2">19.93 5.12</cell><cell>32.0B</cell><cell cols="2">18.55 4.16</cell></row><row><cell>SENet-154 [33]</cell><cell cols="4">145.8M 20.7B 18.68 4.47</cell><cell>42.3B</cell><cell cols="2">17.28 3.79</cell></row><row><cell>NASNet-A [34]</cell><cell>88.9M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.8B</cell><cell>17.3</cell><cell>3.8</cell></row><row><cell>AmoebaNet-A [34]</cell><cell>86.7M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.1B</cell><cell>17.2</cell><cell>3.9</cell></row><row><cell>PNASNet-5 [34]</cell><cell>86.1M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.0B</cell><cell>17.1</cell><cell>3.8</cell></row><row><cell>EfficientNet-B7* [35]</cell><cell>66M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>37B</cell><cell>15.6</cell><cell>2.9</cell></row><row><cell></cell><cell cols="4">*model trained on 600×600 crops.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Mean average precision of Faster R-CNN models after 5 runs on Pascal VOC07 test set. ResNet-101-based models are trained once. Object Detection on Pascal VOC We extend PyTorch implementation provided by Chen et al. [42] and train Faster R-CNN model based on our harmonic ResNets with 50 and 101 layers. Region proposal network (RPN) is applied on the feature pyramid [43] constructed from the network layers. RPN layers as well as regression and classification heads are randomly initialized and use standard (non-harmonic) convolution/fully connected layers. Images are resized to set their shortest sides at 600 pixels. The Faster R-CNN is trained with the learning rate lr = 0.01 × (bs/16) dependant on a particular batch size bs. Models are trained on the union of VOC 2007 training and validation sets with about 5000 images for 17 epochs, decreasing the learning rate by a multiplicative factor of 0.1 after epoch 15. We train the networks with original and harmonic backbones using the same setting. Additionally, these models are also trained on the combination of training sets of VOC 2007 and VOC 2012, consisting of about 16 500 images, for 12 epochs with learning rate dropped at epoch 9. All models are tested on VOC 2007 test set and the official evaluation metric, the mean average precision (AP), is averaged over 5 runs. Final results are reported in</figDesc><table><row><cell>Backbone</cell><cell cols="2">↑ Box AP VOC07 ↑ Box AP VOC07+12</cell></row><row><cell>ResNet-50</cell><cell>73.8 ± 0.3</cell><cell>79.7 ± 0.3</cell></row><row><cell>Harm-ResNet-50</cell><cell>75.0 ± 0.4</cell><cell>80.7 ± 0.2</cell></row><row><cell>ResNet-101</cell><cell>76.1</cell><cell>82.1</cell></row><row><cell>Harm-ResNet-101</cell><cell>77.4</cell><cell>82.9</cell></row><row><cell>5.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Mean average precision for different backbones and detector types on MS COCO 2017 validation set. All backbones are transformed to FPNs.</figDesc><table><row><cell>Backbone</cell><cell>Type</cell><cell cols="4">Box AP ↑ 12 epochs 24 epochs 12 epochs 24 epochs Mask AP ↑</cell></row><row><cell>ResNet-50</cell><cell>Faster</cell><cell>36.4</cell><cell>37.7*</cell><cell>-</cell><cell>-</cell></row><row><cell>Harm-ResNet-50</cell><cell>Faster</cell><cell>37.2</cell><cell>38.4</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-50</cell><cell>Retina</cell><cell>35.6*</cell><cell>36.4*</cell><cell>-</cell><cell>-</cell></row><row><cell>Harm-ResNet-50</cell><cell>Retina</cell><cell>36.3</cell><cell>36.8</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-101</cell><cell>Faster</cell><cell>38.5</cell><cell>39.3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Harm-ResNet-101 Faster</cell><cell>39.7</cell><cell>40.3</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-101</cell><cell>Retina</cell><cell>37.7*</cell><cell>38.1*</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Harm-ResNet-101 Retina</cell><cell>39.0</cell><cell>39.2</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-50</cell><cell>Mask</cell><cell>37.3*</cell><cell>38.5*</cell><cell>34.2*</cell><cell>35.1*</cell></row><row><cell>Harm-ResNet-50</cell><cell>Mask</cell><cell>38.1</cell><cell>38.9</cell><cell>34.7</cell><cell>35.5</cell></row><row><cell>ResNet-101</cell><cell>Mask</cell><cell>39.4*</cell><cell>40.3*</cell><cell>35.9*</cell><cell>36.5*</cell></row><row><cell cols="2">Harm-ResNet-101 Mask</cell><cell>40.7</cell><cell>41.5</cell><cell>36.8</cell><cell>37.3</cell></row><row><cell></cell><cell></cell><cell cols="2">*scores reported by</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Mean average precision on Cascade R-CNN architecture on MS COCO 2017 validation set. All backbones are transformed to FPNs. Cascade R-CNN Backbone Type Box AP ↑ Mask AP ↑</figDesc><table><row><cell>ResNet-101</cell><cell>Faster</cell><cell>42.5*</cell><cell>-</cell></row><row><cell>Harm-ResNet-101</cell><cell>Faster</cell><cell>43.5</cell><cell>-</cell></row><row><cell>ResNet-101</cell><cell>Mask</cell><cell>43.3*</cell><cell>37.6*</cell></row><row><cell>Harm-ResNet-101</cell><cell>Mask</cell><cell>44.3</cell><cell>38.3</cell></row><row><cell>ResNet-101</cell><cell>Hybrid</cell><cell>44.9*</cell><cell>39.4*</cell></row><row><cell>Harm-ResNet-101</cell><cell>Hybrid</cell><cell>46.0</cell><cell>40.2</cell></row><row><cell></cell><cell>*scores reported by</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Intersection over Union (IoU) of DeepLabV3 architecture semantic segmentation on Pascal VOC 2012 validation set. IoU shown is the median of 5 trials ± empirical std. dev. ResNet-50 76.31 ± 0.07 76.65 ± 0.07 77.40 ± 0.08 -ResNet-101 78.31 ± 0.07 77.92 ± 0.11 79.49 ± 0.29 77.21<ref type="bibr" target="#b38">[39]</ref> </figDesc><table><row><cell>Backbone</cell><cell>Baseline</cell><cell>Harm converted</cell><cell>Harm pre-trained</cell><cell>Benchmark</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">These models were trained on larger image crops than our harmonic network, whereby such training typically improves the accuracy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/VainF/DeepLabV3Plus-Pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the ADAPT Centre for Digital Content Technology funded under the SFI Research Centres Programme (Grant 13/RC/2106) and co-funded under the European Regional Development Fund. We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Harmonic networks with limited training samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.00135" />
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DCFNet: Deep neural network with decomposed convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 35th International Conference on Machine Learning</title>
		<meeting>of the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4198" to="4207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Basisconv: A method for compressed representation and learning in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahalanobis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04509</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Harmonic networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of British Machine Vision Conference (BMVC)</title>
		<meeting>of British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on image tampering and its detection in real-world photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Thing</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2018.12.022</idno>
		<ptr target="https://doi.org/10.1016/j.jvcir.2018.12.022" />
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="380" to="399" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-speed face recognition based on discrete cosine transform and rbf neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="679" to="691" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Representing scenes for real-time context classification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ravì</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tomaselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guarnera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Battiato</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2014.05.014</idno>
		<idno>1086 -1100. doi</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2014.05.014" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep feature extraction in the DCT domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3536" to="3541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On using CNN with DCT based image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Irish Machine Vision and Image Processing Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gueguen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3933" to="3944" />
		</imprint>
	</monogr>
	<note>Faster neural networks straight from JPEG</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">CNNpack: Packing convolutional neural networks in the frequency domain</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="253" to="261" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial framework for depth filling via wasserstein metric, cosine transform and domain transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Payen De La Garanderie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.02.010</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2019.02.010" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="232" to="244" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compressing the input for CNNs with the first-order scattering transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advanced image classification using wavelets and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="233" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep wavelet network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jemai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hassairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ejbali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaied</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="922" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.11.015</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2016.11.015" />
		<title level="m">Sar image segmentation based on convolutional-wavelet neural network and markov random field, Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="255" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<title level="m">Spectral representations for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2449" to="2457" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A dual-tree complex wavelet transform based convolutional neural network for human thyroid medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Healthcare Informatics (ICHI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gabor convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4357" to="4366" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2610" to="2619" />
		</imprint>
	</monogr>
	<note>Structured receptive fields in CNNs</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analyzing filters toward efficient convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5619" to="5628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Introduction to orthogonal transforms: with applications in data processing and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lapped transforms for efficient transform/subband coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Malvar</surname></persName>
		</author>
		<idno type="DOI">10.1109/29.56057</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proc. of the 2004 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Matrix capsules with EM routing</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Proc. of British Machine Vision Conference (BMVC)</title>
		<editor>E. R. H. Richard C. Wilson, W. A. P. Smith</editor>
		<meeting>of British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
	<note>Wide residual networks</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<ptr target="https://pytorch.org/docs/stable/torchvision/models.html" />
		<title level="m">Torchvision models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv preprint:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2011" to="2023" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efficientnet</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision</title>
		<meeting>of the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision</title>
		<meeting>of the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartwig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00511</idno>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4969" to="4978" />
		</imprint>
	</monogr>
	<note>Hybrid task cascade for instance segmentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
