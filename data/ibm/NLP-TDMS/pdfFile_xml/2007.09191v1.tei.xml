<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention2AngioGAN: Synthesizing Fluorescein Angiography from Retinal Fundus Images using Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharif</forename><forename type="middle">Amit</forename><surname>Kamran</surname></persName>
							<email>skamran@nevada.unr.edu*</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khondker</forename><surname>Fariha Hossain</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Deakin University Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Tavakkoli</surname></persName>
							<email>tavakkol@unr.edu‡</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Lee Zuckerbrod</surname></persName>
							<email>szuckerbrod@houstoneye.com§</email>
							<affiliation key="aff2">
								<orgName type="institution">Houston Eye Associates Houston</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Reno</settlement>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention2AngioGAN: Synthesizing Fluorescein Angiography from Retinal Fundus Images using Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Generative Adversarial Networks</term>
					<term>Image-to- image Translation</term>
					<term>Fluorescein Angiography</term>
					<term>Retinal Fun- doscopy</term>
					<term>Residual Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fluorescein Angiography (FA) is a technique that employs the designated camera for Fundus photography incorporating excitation and barrier filters. FA also requires fluorescein dye that is injected intravenously, which might cause adverse effects ranging from nausea, vomiting to even fatal anaphylaxis. Currently, no other fast and non-invasive technique exists that can generate FA without coupling with Fundus photography. To eradicate the need for an invasive FA extraction procedure, we introduce an Attention-based Generative network that can synthesize Fluorescein Angiography from Fundus images. The proposed gan incorporates multiple attention based skip connections in generators and comprises novel residual blocks for both generators and discriminators. It utilizes reconstruction, featurematching, and perceptual loss along with adversarial training to produces realistic Angiograms that is hard for experts to distinguish from real ones. Our experiments confirm that the proposed architecture surpasses recent state-of-the-art generative networks for fundus-to-angio translation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Retinal Funduscopy along with Fluorescein Angiography (FA) has been a popular diagnosing tool for retinal vascular and adverse pigment epithelial-choroidal conditions <ref type="bibr" target="#b0">[1]</ref>. In Fluorescein Angiography, a fluorescent dye is injected in the optic vein. It becomes noticeable after 10 minutes of insertion, depending on the age and the cardiovascular structure of the retinal layers <ref type="bibr" target="#b1">[2]</ref>. While commonly considered healthy, nonfatal complications can arise, such as allergic reactions, nausea, vomiting, etc. Moreover, fatal cases have been documented, with symptoms such as anaphylaxis, heart attack, anaphylactic shock due to the leakage of the dye in the intravenous space <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>Many automated systems have been proposed for the diagnosis of intrinsic conditions and diseases from fundus photos. They generally comprise of different image processing techniques and machine learning algorithms <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>. Currently, there is no computational inexpensive alternative for generating reliable and reproducible fluorescein angiography images. Retinal fundoscopy is the only alternative for differential diagnosis that is easily available and financially viable. Optical coherence Tomography combined with basic image processing <ref type="bibr" target="#b8">[9]</ref> can be utilized for the diagnosis of retinal disease but is too expensive and not widely accessible in developing economies. As it stands, an efficient and faster procedure is crucial for avoiding any potential hazards associated with invasive fluorescein angiography.</p><p>In this paper, we introduce Attention-AngioGAN, a robust conditional Generative Adversarial Network (GAN) to produce fluorescein angiograms from retinal fundus images. For qualitative evaluation, we compare our automated technique with recent state-of-the-art GAN architectures such as pix2pixHD <ref type="bibr" target="#b9">[10]</ref>, U-GAT-IT <ref type="bibr" target="#b10">[11]</ref>, and Stargan-V2 <ref type="bibr" target="#b11">[12]</ref>. Moreover, we used Frechet inception Distance (FID) <ref type="bibr" target="#b12">[13]</ref> and Kernel Inception Distance (KID) <ref type="bibr" target="#b13">[14]</ref> scores to quantify image quality and calculate similarity with real angiograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LITERATURE REVIEW</head><p>Recently, there has been a surge of Generative adversarial networks(GAN) based applications ranging from image translation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, editing <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and image style transfer <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>. GANs can potentially extract and learn fine and coarse information from images by combining multiple architectures having multi-scale resolution <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Such examples are wide-spread in both Conditional <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and Unconditional GAN settings <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. By incorporating multiple highresolution architectures, they can learn distinct domain-specific features with high precision and robustness.</p><p>Emphasizing image to image translation, numerous prior work has been proposed, where they focused on architectural changes to acquire a higher quality result. To illustrate, pix2pixHD <ref type="bibr" target="#b9">[10]</ref> utilized PatchGAN as a multi-scale discriminator to achieve better visual representation containing local and global information. On the other hand, U-GAT-IT <ref type="bibr" target="#b10">[11]</ref>, an unsupervised learning architecture, extracts local features and texture by incorporating AdaLIN (Adaptive Layer-Instance Normalization). While most architectures ensure high quality of images, Stargan-V2 <ref type="bibr" target="#b11">[12]</ref>, a style-transfer network, focuses on the domain-specific features. By doing so, they allow diverse and scalable image-to-image translation in multiple domains within a single model. Most of the image-to-image translation models are either focused on domain level transformation or combining style and textures of two images. For instance, high-resolution images generated by U-GAT-IT <ref type="bibr" target="#b10">[11]</ref> and Stargan-V2 <ref type="bibr" target="#b11">[12]</ref> use attention modules to extract local features information and don't utilize perceptual loss. Whereas StyleGAN <ref type="bibr" target="#b25">[26]</ref>, pix2pixHD <ref type="bibr" target="#b9">[10]</ref> emphasizes more on incorporate perceptual loss with different styles of target images. By incorporating both these ideas we propose an architecture, where we combine perceptual loss and multi-scale discriminator to retain global information like the shape of optic-disc, color, contrast, etc. On the other hand, we utilize feature matching loss and introduce new multiattention modules to retain local features like retinal venules, arteries, protein buildup, and microaneurysm. The visual representation and quantitative result prove that our proposed technique surpasses state-of-the-architectures and tricks, expert ophthalmologists, to think they are authentic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHODOLOGY</head><p>This paper proposes an attention-based generative adversarial network (GAN) comprising of separate new residual blocks for generators and discriminators. Moreover, the training includes perceptual, feature matching, and reconstruction loss to synthesize more vivid looking angiograms from retinal fundus images. First, we discuss about coarse and fine generators in section III-A. Next, we elaborate our building blocks in Section III-B, III-C, III-D. We then delve into the multi-scale discriminators and their interconnection with the generators to define the whole end-to-end pipeline for the generative network in sections III-E. Ultimately, in section III-F, we discuss the loss minimizing and maximizing function and loss weight distributions for different losses interrelated to each of the separate architecture that forms the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Coarse and Fine Generators</head><p>Coupling coarse-to-fine generator for image translation tasks results in very pristine and high quality images , as witnessed in recent architectures, such as pix2pixHD <ref type="bibr" target="#b9">[10]</ref>, SPADE <ref type="bibr" target="#b25">[26]</ref>, and Starganv2 <ref type="bibr" target="#b11">[12]</ref>. We incorporate this into technique in our architecture by using two generators (G f ine and G coarse ), as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The generator G f ine synthesizes smooth FA from fundus images by learning local information such as retinal venules, blood vessels, hemorrhages, exudates, and protein buildup. On the contrary, the generator G coarse tries to extract and preserve global information, such as the structures of the macula, optic disc, color intensity, contrast, and illumination, while producing less detailed angiograms. The generators consist of multiple encoders, decoder, attention, residual blocks, and a feature fusion block between the fine and coarse generator. G f ine has an input dimension of 512 × 512 and produces outputs with the same resolution. Likewise, G coarse takes an image with half the resolution (256 × 256) and synthesizes an image with the same size. Additionally, the G coarse outputs a feature vector of the size 256 × 256 × 64 that is combined with one of the intermediate layers of G f ine using the fusion operation. The representation of these generators is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. In the following sections, we elaborate on each of these blocks in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoder and Decoder Blocks</head><p>Both generators and discriminators incorporate the encoder blocks for downsampling the feature maps. On the contrary, only the generators use decoder blocks for upsampling to get the desired feature maps and output. The encoder block consists of a convolution layer followed by a batch-norm layer <ref type="bibr" target="#b26">[27]</ref> and Leaky-ReLU activation function, as illustrated in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. In contrast, the decoder block comprises of transposed convolution layer and successive batch-norm <ref type="bibr" target="#b26">[27]</ref> and Leaky-ReLU activation 2(b). Interestingly, G coarse is downsampled twice (×2) using the encoder. After successive residual blocks, the decoder blocks are using to upsample twice again. For G f ine , the encoder is utilized once, and after the repetition of residual blocks, a single decoder is used to get the same spatial dimension of the output. We use a kernel size, k = 3 and stride, s = 2 for both of our convolution, and transposed convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Distinct Residual Blocks for Generator and Discriminator</head><p>Lately, residual blocks have become the standard for generative models accomplishing image-to-image translation, image inpainting, and semantic segmentation tasks <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The fundamental design consists of a residual unit with two con-secutive convolution layers and a skip connection that adds feature tensor of the input with the output. Regular convolution layers are computationally inefficient as opposed to separable convolution <ref type="bibr" target="#b27">[28]</ref>. Separable convolution consists of a depthwise convolution followed by a point-wise convolution. By doing so, it extracts and retains the depth and spatial information through the network. Recent studies show that combining separable convolutional layers with dilation allows for more robust feature extraction <ref type="bibr" target="#b8">[9]</ref>. We incorporate this technique to design two distinct novel residual blocks for our generators and discriminators, as shown in <ref type="figure" target="#fig_1">Fig. 2(d</ref> <ref type="figure" target="#fig_1">Fig. 2</ref>(e). The residual block for our generator consists of a separable convolution layer followed by two branches of separable convolution. The difference is that one branch consists of a separable convolution with a dilation rate of, d = 1 and the other with dilation rate, d = 2. We use a kernel size, k = 3 and stride, s = 1 for all of our separable convolution layers. Each separable convolution is preceded by a Reflection padding layer and succeded by a Batch-Normalization and Leaky-ReLU activation layer. The skip connection and output of the two branches are all added together to produce the final output. In contrast, the residual block for the discriminator consists of a Separable convoluton layer, followed by Batch-Normalization and Leaky-ReLU activation function. The separable convolution has a kernel size of k = 3 and stride, s = 1.</p><formula xml:id="formula_0">) &amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Attention block</head><p>Next, we elaborate our proposed attention block, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>(c). The block consists of two successive residual units, Convolution, BatchNorm, and Leaky-ReLU layers. Both convolution layer has kernel size of k = 3 and stride, s = 1.</p><p>Other than that, there are two skip connections one coming from the input and being added to the output of the first residual unit. The other one is coming from the input and summed with the output of the last residual unit. We use attention block for combining feature information from the bottom layers of the network with the top layers of the network, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. G coarse comprises of two attention block, coming out of the with two encoders and being added with the two decoders successively. In contrast, the G f ine has only one attention block between the encoder and decoder. The reason behind utilizing attention block is to retrieve and retain spatial information that can be combined with the learned features of the later layers of the architectures as observed in similar GAN architectures <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi-scale Markovian Discriminators</head><p>GAN discriminators need to adjust to coarse and fine generated outputs for distinguishing between real and synthesized images. To solve this underlying issue, we need a dense network with a huge amount of computable parameters. Alternatively, convolution with a wider receptive field can be utilized for extracting spatial information. This can easily lead to overfitting while training the model. To address this issue, we exploit the idea of using two Markovian discriminators, first introduced in a technique called PatchGAN <ref type="bibr" target="#b30">[31]</ref>. The method consists of discriminators with variable sized input resolution and can help with the overall adversarial training of the architecture as observed in <ref type="bibr" target="#b9">[10]</ref> We use four discriminators that incorporates almost the same network structure but operate at two different resolutions. We organize the four discriminators into two sets, <ref type="figure" target="#fig_0">Fig. 1</ref>. We resize each of the coarse and fine angiograms and fundus with size 512 × 512 and 256 × 256 by a factor of 2 using the Lanczos filter <ref type="bibr" target="#b31">[32]</ref>. D2 f and D2 c have a unique average pooling layer right after the input which resizes the resolution to 256 × 256 and 128 × 128. Other than that, all four discriminators have identical layers consisting of three repetitive encoder and residual block pairs (in <ref type="figure" target="#fig_1">Fig. 2(a)</ref> and <ref type="figure" target="#fig_1">Fig. 2(e)</ref>. Lastly, convolution layer is used for getting spatial dimension of 64 × 64 and 32 × 32 for D1 f , D2 f and 32 × 32 and 16 × 16 for D1 c , D2 c as outputs.</p><formula xml:id="formula_1">D f ine = [D1 f , D2 f ] and D coarse = [D1 c , D2 c ] as illustrated in</formula><p>The coarse discriminators one that learns feature at a lower resolution tries to convince the coarse generator to retain more global features such as the macula, spherical optic disc, appearance, and illumination. On the other hand, the fine discriminators dictate the fine generator to produce more detailed local features such as retinal vessels, arteries, exudates, etc. By doing this we fuse features from both generators while training them autonomously with their joined multi-scale discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Weighted Object Function and Adversarial Loss</head><p>With the given discriminators and generators, the objective function for our whole network can be formulated as Eq. 1. It's a multi-objective problem of maximizing the loss of the discriminators while diminishing the loss of the generators.</p><formula xml:id="formula_2">min G f ,Gc max D f ,Dc L adv (G f , G c , D f , D c )<label>(1)</label></formula><p>For adversarial training, we use Hinge-Loss <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b32">[33]</ref> as illustrated in Eq. 2 and Eq. 3. Effectively, all the fundus images and their corresponding angiogram pairs, are normalized to [−1, 1]. This in turn helps with widening the gap between the pixel intensities of the real and synthesized angio images. In Eq. 4 we add them and use λ adv as weight multiplier with the L adv (G).</p><formula xml:id="formula_3">L adv (D) = −E x,y min(0, −1 + D(x, y)) − E x min(0, −1 − D(x, G(x)))<label>(2)</label></formula><formula xml:id="formula_4">L adv (G) = −E x,y (D(G(x), y))<label>(3)</label></formula><formula xml:id="formula_5">L adv (G, D) = L adv (D) + λ adv (L adv (G))<label>(4)</label></formula><p>Here, In Eq. 2 and Eq. 3 the discriminators are first trained on the real fundus, x and real angiogram, y, and then trained on the real fundus, x and synthesized angiogram, G(x). We begin by batch-wise training the discriminators D1 f , D2 f , and D1 c , D2 c for a couple of iterations on randomly sampled data.</p><p>After that, we train the G c while keeping the weights of the discriminators frozen. In the same manner, we train the G f on a batch of random images while keeping weights of all the discriminators frozen.</p><p>The generators also incorporate the reconstruction loss and perceptual loss <ref type="bibr" target="#b33">[34]</ref> as shown in Eq. 5 and Eq. 6. By utilizing these losses we ensure the synthesized images contain more realistic color, contrast, and vascular structure. We also employ feature matching loss <ref type="bibr" target="#b9">[10]</ref> with all our discriminators and as given in Eq. 7.</p><formula xml:id="formula_6">L rec (G) = E x,y G(x) − y 2 (5) L perc (G) = E x,y k i=1 1 M F i vgg (y) − F i vgg (G(x)) (6) L f m (G, D n ) = E x,y k i=1 1 N D i n (x, y) − D i n (x, G(x)) (7)</formula><p>For Eq. 5, L rec is the reconstruction loss for a real angiogram, y, given a generated angiogram, G(x). We use this loss for both G f and G c so that the model can generate high-quality angiograms of different scales. In Eq. 6, L perc calculates the difference between real and fake angio features extracted by pushing both of successively in VGG19 architecture <ref type="bibr" target="#b34">[35]</ref>. Lastly, Eq. 7 is calculated by taking the features from intermediate layers of the discriminator by first inserting the real and fake angiograms consecutively. Here, M and N stands for the number of feature layers extracted from VGG19 and the discriminators consecutively.</p><p>By incorporating Eq. 4, 5, 6 and 7 we can formulate our final objective function as given in Eq. <ref type="bibr" target="#b7">8</ref>.</p><formula xml:id="formula_7">min G f ,Gc max D f ,Dc (L adv (G f , G c , D f , D c ))+ λ rec L rec (G f , G c ) + λ f m L f m (G f , G c , D f , D c ) + λ perc L perc (G f , G c )<label>(8)</label></formula><p>Here, λ adv , λ rec , λ perc and λ f m signifies loss weighting that are multiplied with their respective losses. The loss weighting dictates which networks to prioritize while training. For our architecture, more weight is given to the L adv (G), L rec , L perc , and thus we select bigger λ values for those.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In the next section, we detail our model experiments and evaluate our architecture based on qualitative and quantitative metrics. First, we elaborate on the structuring and preprocessing of our dataset in Sec. IV-A. Then detail our hyperparameter selection and tuning in Sec. IV-B. Next, we describe our adversarial training scheme in Sec. IV-C. Also, we compare our architecture with existing state-of-the-art generative models based on some qualitative evaluation metrics in Sec. IV-E. Lastly, in Sec. IV-D, we analyze the quantification done by experts, by distinguishing between real and synthesized angiograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We use the fundus and angiography data-set provided in <ref type="bibr" target="#b35">[36]</ref>. It consists of thirty image and twenty-nine pairs of the healthy and unhealthy fundus and angiogram images, collected from fifty-nine individual patients. Next, we clean the dataset by taking only seventeen pairs of images based on one-toone alignment between the fundus and angiogram pairs. These image pairs are either accurately aligned or almost aligned. The original image size is 576 × 720, but we take 50 overlapping crops of 512 × 512 sized samples from each. By doing so, we end up having 850 images in total for training. The fundus images are in RGB format, and angiograms are in a Gray-scale format. For testing, we take fourteen image pairs and crop four overlapping quadrants of the image to generate a test set of fifty-six test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Attention2AngioGAN training</head><p>Input: for d iter = 0 to max d iter do 5:</p><formula xml:id="formula_8">x i X, y i Y Output: G f , G C 1: Initialize hyper-parameters: max epoch, b, max d iter, ω f D , ω c D , ω f G , ω c G α f D , α c D , α f G , α c G , β f D , β c D , β f G , β c G , λ rec , λ per , λ f m ,</formula><formula xml:id="formula_9">L adv (D) ← D c (x c , y c ), D f (x f , y f ) 6: L adv (D) ← D c (x c , G c (x c )), D f (x f , G F (x f )) 7: L adv (G) ← G c (x c ), G F (x f ) 8: L adv (G, D) ← L adv (D) + λ adv (L adv (G)) 9: ω c D ← ω c D + Adam(D c , G c , ω c D , α c D , β c D )</formula><p>10: L rec (G c ) ← G(x c ), y c , <ref type="bibr" target="#b13">14</ref>:</p><formula xml:id="formula_10">ω f D ← ω f D + Adam(D f , G f , ω f D , α f D , β f D<label>)</label></formula><formula xml:id="formula_11">L rec (G f ) ← G(x f ), y f 15: L perc (G c ) ← F c vgg (y), F c vgg (G(x)) 16: L perc (G f ) ← F f vgg (y), F f vgg (G(x)) 17: ω c G ← ω c G + Adam(G c , ω c G , α c G , β c G )</formula><p>18:</p><formula xml:id="formula_12">ω f G ← ω f G + Adam(G f , ω f G , α f G , β f G ) Unfreeze ω c D , ω f D 19: L f m (D c ) ← D c n (x c , y c ), D f n (x c , G(x c ))</formula><p>20:</p><formula xml:id="formula_13">L f m (D f ) ← D f n (x f , y f ), D f n (x f , G(x f )) 21: ω c D ← ω c D + Adam(D c , G c , ω c D , α c D , β c D ) 22: ω f D ← ω f D + Adam(D f , G f , ω f D , α f D , β f D ) Freeze ω c D , ω f D 23: L adv (D) ← D c (x c , y c ), D f (x f , y f ) 24: L adv (D) ← D c (x c , G c (x c )), D f (x f , G F (x f )) 25: L adv (G) ← G c (x c ), G F (x f ) 26: L adv (G, D) ← L adv (D) + λ adv (L adv (G)) 27: ω c D ← ω c D + Adam(D c , G c , ω c D , α c D , β c D )</formula><p>28: For adversarial training, we used hinge loss <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b32">[33]</ref>. We picked λ adv = 10 (Eq. 4) and λ rec = 10, λ perc = 10, λ f m = 1 (Eq. 8). For optimizer, we used Adam <ref type="bibr" target="#b36">[37]</ref>, with learning rate α = 0.0002, β 1 = 0.5 and β 2 = 0.999. We train with mini-batches with batch size, b = 2 for 200 epochs. It took approximately 48 hours to train our model on NVIDIA P100 GPU.</p><formula xml:id="formula_14">ω f D ← ω f D + Adam(D f , G f , ω f D , α f D , β f D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training procedure</head><p>In this section, we elaborate on our detailed algorithm provided in Algorithm 1. To train our Attention2AngioGAN, we start by initializing all the hyper-parameters. Next, we sample a batch of the real fundus and angiogram x, y images. We train the real fundus and fake angiogram pairs with D f , D c . After that, we use G f , G c to synthesize fake angiograms and use the real fundus and fake angiograms, x, G(x) to train discriminators D f , D c . Following that, we calculate the adversarial loss, L adv (D, G), and update the weights. We freeze the weights of the discriminators. Next, we train the generators and calculate the L rec (G), L perc (G) losses and update both generator's weights. Subsequently, we unfreeze both discriminators weights, calculate the feature matching loss L f m (D) and update the discriminator weights. In the final stage, we freeze both discriminator's weights and jointly finetune all the discriminator and generators. We calculate the total loss by adding and multiplying with their relative weights. For testing, we save the snapshot of the model and its weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Evaluation</head><p>For assessing the performance of our network, we used 14 test samples and cropped four quadrants of the image with a size of 512×512. We conducted two sets of tests to evaluate our networks. First, for estimating the accurate visual representation without transforming the image. Next, for global and local changes due to transformation and distortion of the image. By doing so, we measured the network's ability to adjust to structural changes to the vascular patterns and formation of the retinal subspace. We used the GNU Image Manipulation Program (GIMP) <ref type="bibr" target="#b37">[38]</ref> for carrying out transformation and distortion on the images.</p><p>For the first experiment, we train three variants of our network and four other state-of-the-art image-to-image translation architecture using the same number of epochs and batches of images. Next, we evaluate them using the same test sample. A side by side comparison of the results is illustrated in <ref type="figure" target="#fig_5">Fig. 3</ref>. <ref type="figure" target="#fig_5">Fig. 3</ref> shows the global changes while column B &amp; D are zoomed-in to display local vascular structure and other local information. We use FM and PL to denote feature-matching and perceptual loss. By the looks of it, both of our models using the FM loss (with or without PL) produces vivid and convincing results. On the other hand, the visual result of our network without FM and PL produces distorted local structure due to not learning contrast and color of the optic disc using perceptual loss. Fundus2Angio and StarGANv2 also produce impressive results. However, if seen in the closedup versions in columns B &amp; D, we can witness the right upper portion of the optic discs contains fewer blood vessels compared to our ones. U-GAT-IT and Pix2PixHD also fail to generate rich venules, exudates, and protein buildup.</p><formula xml:id="formula_15">Column A &amp; C in</formula><p>In the second set of experiments, three transformations and two distortions were applied on the fundus images: 1) blurring to represent severe cataracts, 2) sharpening to represent dilated pupils, 3) signal noise to represent machine impedance during fundoscopy, 4) pinch, to visualize the pulled/pushed vascular formation, 5) whirl, for representing distortions caused by increased intraocular pressure (IOP). Improved robustness and adaption are represented by the generated angiograms similarity to the real FA image since these transformations and distortions may or may not affect the vascular structure of the retina. A side by side comparison of different architecture's predictions on these transformed images is illustrated in <ref type="figure" target="#fig_6">Fig. 4</ref>. As it can be observed from the image, the proposed architecture produces images very similar to the ground-truth (GT). The results under these global and local vascular changes applied to the fundus image.</p><p>In the case of blurred fundus images, our architecture with and without PL, is less affected compared to other state-of-theart models, as seen in (row 6 to 9 of column 1) of <ref type="figure" target="#fig_6">Fig. 4</ref>. The venular and cellular structures are better conserved as opposed to StarGANv2 and Pix2PixHD. For sharpened fundus, the angiogram produced by UGATIT and Fundus2Angio (row 7 and 8 of column 2) exhibits grainy artifacts around the blood vessels, which are not present in our model with and without PL. For noisy images, our result with and without PL, is still unaffected with this pixel-level modification. However, all other state-of-the-art models (row 6 to 9 of column 3) fail to generate thin and small venular formations by failing to extract local features from the retinal subspace. On the contrary, our model without PL and FM produces jittery motion artifacts and  <ref type="bibr" target="#b11">[12]</ref> 0.00118 0.05274 0.00235 0.05331 0.05539 0.05271 U-GAT-IT <ref type="bibr" target="#b10">[11]</ref> 0.00131 0.05610 0.00278 0.05533 0.05815 0.05719 Fundus2Angio <ref type="bibr" target="#b38">[39]</ref> 0.00184 0.05328 0.00272 0.05267 0.05278 0.04985 Pix2PixHD <ref type="bibr" target="#b9">[10]</ref> 0.00258 0.05613 0.00254 0.05788 0.06029 0.05838 1 PL = Perceptual Loss; FM = Feature-Matching Loss 2 FID: Lower is better; KID: Lower is better high contrast around the border of the optic disc for all these transformations.</p><p>For distortions like Pinch and Whirl, our experimental result with and without perceptual loss shows the versatility and reproducibility of the proposed network to uncover the changes in vascular structure as seen in <ref type="figure" target="#fig_6">Fig. 4</ref> (row 3 and 4 of column 4 and 5). Compared to ours, only StarGANv2 and U-GAT-IT maintains the flattening condition and manifestation of vascular changes but loses the overall smoothness in the process (row 6 to 7 of column 4 and 5). As seen in <ref type="figure" target="#fig_6">Fig. 4</ref> ours with and without PL network encodes the feature information of vessel structures and is much less affected by both kinds of contortion. The other architectures failed to generate microvessel structure due to IOP or vitreous changes as can be seen in <ref type="figure" target="#fig_6">Fig. 4</ref>.Contrarily, our model without any perceptual and feature-matching loss fails to encode this information and vascular changes. Consequently, For all kinds of transformation and distortion our model with and without perceptual triumphs over existing state-of-the-art image-to-image translation models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Quantitative Evaluations</head><p>For quantitative evaluation, we performed two experiments. In the first experiment we use the Frchet inception distance (FID) <ref type="bibr" target="#b12">[13]</ref> and Kernel Inception distance (KID) <ref type="bibr" target="#b13">[14]</ref> which has been used to evaluate similar style-transfer GANs <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We computed the FID and KID scores for different architectures on the generated FA image and original angiogram, including the five transformations and distortions. The results are reported in <ref type="table">Table.</ref> I. It should be noted that lower FID and KID score means better results.</p><p>From <ref type="table">Table.</ref> I, out of our three networks, the best FID is achieved for ours without PL. And it achieves the lowest scores among out of all other architecture, for both with and without distortions. For KID, our model with PL achieves the lowest score for four out of five types of distortions. Fundus2Angio scores lower KID for distorted images using whirl. Other than that, StarGAN-v2 achieves the same score as our network having a KID of 0.00235.</p><p>In the next experiment, we assess the quality of the synthesized angios by asking three expert ophthalmologists to identify fake angios among a collection of 50 balanced (50%, 50%) and randomly set of mixed angiograms. For this experiment, the exact number of fake and real images was not known by the experts. By not disclosing this information we tried to evaluate following criterion: 1) Correct fake and real angios found by the experts, where lower is better, 2) Incorrect fake and real angios missed by the experts, where higher is better and 3) The average precision representing the effective the identification is by the experts, where lower is better. The detailed results are shown in <ref type="table" target="#tab_0">Table II</ref>.</p><p>As it can be seen from <ref type="table" target="#tab_0">Table II</ref>, experts assigned 90% and of 88% of the fake angiograms as real, for images generated by two of our models. The result also shows that experts had difficulty in identifying fake images, while they easily identified real angiograms with 80% and 82% certainty. On average, the experts misclassified 55% and 53% of all images for two of our models consecutively. The average precision diagnosis of the experts are 47.1% and 48.2%. Consequently, our model with lower precision achieves the best result by fooling the experts to identify fake angios as real.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a new image-to-image translation architecture called Attention2AngioGAN. The architecture synthesizes high quality and vivid looking angiograms from fundus images without any expert intervention. Additionally, we illustrated its robustness, flexibility, and reproducibility by producing high-quality angiograms from transformed and distorted images, which imitates biological markers seen in real fundus images. As a result, the proposed network can be efficiently employed to generate precise FA images of patients developing disease overtime. This is best suited for disease progression monitoring to predict the development of diseases in vivo. We hope to extend this work for other areas of ophthalmological data modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The Proposed GAN architecture consist of two Generators [one Fine and one Coarse], four Discriminators [two Fine and two Coarse]. The fine discriminators take input image as the sample size while the coarse discriminators take input as half of the sample size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Individual blocks of our proposed GAN architecture consisting of (a) Encoder, (b) Decoder, (c) Attention block, (d) Residual Block for Generator and (e) Residual Block for Discriminator where K stands for kernel size, S is for stride and D is for Dilation rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>λ adv 2 :</head><label>2</label><figDesc>for e = 0 to max epoch do 3: Sample x f , x c , y f , y c , using batch-size b 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>12 :</head><label>12</label><figDesc>Sample x f , x c , y f , y c , using batch-size b 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>) 29 :</head><label>29</label><figDesc>Save weights and snapshot of G f , G c 30: L ← L adv + λ rec (L rec ) + λ f m (L f m ) + λ perc (L perc ) 31: end for B. Hyper-parameter tuning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Comparativie results of different Angiograms generated using different state-of-the-art architectures. Column (A) and (C) represents two samples of real fundus, real angio and predicted angio images. Whereas column (B) and (D) represents the red rectangle box to show zoomed in local venular structures corresponding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Angiogram generated from transformed and distorted Fundus images with natural changes, imaging errors and biological markers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TEST</head><label>I</label><figDesc>RESULTS FOR DIFFERENT ARCHITECTURES</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Frchet Inception Distance (FID)</cell><cell></cell><cell></cell></row><row><cell>Architecture</cell><cell>Orig.</cell><cell>Noise</cell><cell>Blur</cell><cell>Sharp</cell><cell>Whirl</cell><cell>Pinch</cell></row><row><cell>Ours + PL 1 + FM 2</cell><cell>24.6</cell><cell>21.6 (3.0↓)</cell><cell>30.0 (5.4↑)</cell><cell>25.6 (1.0↑)</cell><cell>40.0 (15.4↑)</cell><cell>24.9 (0.3↑)</cell></row><row><cell>Ours + FM 2</cell><cell>20.7</cell><cell>20.8 (0.1↑)</cell><cell>23.5 (2.8↑)</cell><cell>24.9 (4.2↑)</cell><cell>27.8 (7.1↑)</cell><cell>19.5 (1.2↓)</cell></row><row><cell>Ours</cell><cell>47.5</cell><cell>43.1 (4.4↓)</cell><cell>49.8 (2.3↑)</cell><cell>50.5 (3.5↑)</cell><cell>61.9 (14.5↑)</cell><cell>46.7 (0.8↓)</cell></row><row><cell>StarGAN-v2 [12]</cell><cell>27.7</cell><cell>35.1 (7.4↑)</cell><cell>32.6 (4.9↑)</cell><cell>27.4 (0.3↓)</cell><cell>32.7 (5.0↑)</cell><cell>26.7 (1.0↓)</cell></row><row><cell>U-GAT-IT [11]</cell><cell>24.5</cell><cell>26.0 (1.5↑)</cell><cell>30.4 (5.9↑)</cell><cell>26.8 (2.3↑)</cell><cell>33.0 (9.5↑)</cell><cell>29.1 (4.6↑)</cell></row><row><cell>Fundus2Angio [39]</cell><cell>30.3</cell><cell>41.5 (11.2↑)</cell><cell>32.3 (2.0↑)</cell><cell>34.3 (4.0↑)</cell><cell>38.2 (7.9↑)</cell><cell>33.1 (2.8↑)</cell></row><row><cell>Pix2PixHD [10]</cell><cell>42.8</cell><cell>53.0 (10.2↑)</cell><cell>43.7 (1.1↑)</cell><cell>47.5 (4.7↑)</cell><cell>45.9 (3.1↑)</cell><cell>39.2 (3.6↓)</cell></row><row><cell></cell><cell></cell><cell cols="3">Kernel Inception Distance (KID)</cell><cell></cell><cell></cell></row><row><cell>Architecture</cell><cell>Orig.</cell><cell>Noise</cell><cell>Blur</cell><cell>Sharp</cell><cell>Whirl</cell><cell>Pinch</cell></row><row><cell>Ours + PL 1 + FM 2</cell><cell>0.00087</cell><cell>0.05045</cell><cell>0.00235</cell><cell>0.05162</cell><cell>0.05390</cell><cell>0.04575</cell></row><row><cell>Ours + FM 2</cell><cell>0.00392</cell><cell>0.05390</cell><cell>0.00505</cell><cell>0.05301</cell><cell>0.05657</cell><cell>0.05341</cell></row><row><cell>Ours</cell><cell>0.00595</cell><cell>0.05237</cell><cell>0.00617</cell><cell>0.05298</cell><cell>0.05613</cell><cell>0.05419</cell></row><row><cell>StarGAN-v2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc>OF QUALITATIVE WITH UNDISCLOSED PORTION OF FAKE/REAL EXPERIMENT</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Results</cell><cell></cell><cell>Average</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Correct Incorrect Missed 1 Found 1 Precision 2</cell></row><row><cell>Ours + FM + PL</cell><cell>Fake Real</cell><cell>10% 80%</cell><cell>90% 20%</cell><cell>55%</cell><cell>45%</cell><cell>47.1%</cell></row><row><cell>Ours + FM</cell><cell>Fake Real</cell><cell>12% 82%</cell><cell>88% 18%</cell><cell>53%</cell><cell>47%</cell><cell>48.2%</cell></row><row><cell cols="4">1 Missed higher is better; Found lower is better</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2 Precision Lower is better</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Retinal fundus image analysis for diagnosis of glaucoma: a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Rajsingh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fluorescein and icg angiography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">St Louis: Mosby</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="800" to="808" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adverse reactions of fluorescein angiography: a prospective study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P C</forename><surname>Lira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L D A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V R B</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D C</forename><surname>Pessoa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arquivos brasileiros de oftalmologia</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="615" to="618" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fluorescein angiography and adverse drug reactions revisited: the lions eye experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Constable</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical &amp; experimental ophthalmology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Frequency of adverse systemic reactions after fluorescein angiography: results of a prospective study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Kwiterovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Schachat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Fine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1139" to="1142" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning identification of diabetic retinopathy from fundus images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gurudath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Celenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Disc-aware ensemble network for glaucoma screening from fundus image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2493" to="2501" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">158</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optic-net: A novel convolutional neural network for diagnosis of retinal diseases from optical tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Sabbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="964" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">U-gat-it: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10830</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Demystifying mmd gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sketchygan: Towards diverse and realistic sketch to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9416" to="9425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scribbler: Controlling deep image synthesis with sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse, smart contours to represent and edit images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3511" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Texturegan: Controlling deep image synthesis with texture patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8456" to="8465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognising panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1218</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5077" to="5086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-gan for object transfiguration in wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="164" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Geometric gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diabetic retinopathy grading by digital curvelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hajeb Mohammad Alipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Akhlaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and mathematical methods in medicine</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GIMP: GNU Image Manipulation Program. GIMP Team</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fundus2angio: A novel conditional gan architecture for generating fluorescein angiography images from retinal fundus photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Zuckerbrod</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05267</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
