<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MIT CSAIL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<email>iskweon77@kaist.ac.kr2taehyun@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our goal in this work is to train an image captioning model that generates more dense and informative captions. We introduce "relational captioning," a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in an image. Relational captioning is a framework that is advantageous in both diversity and amount of information, leading to image understanding based on relationships. Part-of-speech (POS, i.e. subject-object-predicate categories) tags can be assigned to every English word. We leverage the POS as a prior to guide the correct sequence of words in a caption. To this end, we propose a multi-task triple-stream network (MTTSNet) which consists of three recurrent units for the respective POS and jointly performs POS prediction and captioning. We demonstrate more diverse and richer representations generated by the proposed model against several baselines and competing methods. The code is available at https://github.com/Dong-JinKim/DenseRelationalCaptioning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human visual system has the capability to effectively and instantly collect a holistic understanding of contextual associations among objects in a scene <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">23]</ref> by densely and adaptively skimming the visual scene through the eyes, i.e. the saccadic movements. Such instantly extracted rich and dense information allows humans to have the superior capability of object-centric visual understanding. Motivated by this, in this work, we present a new concept of scene understanding, called dense relational captioning, that provides dense but selective, expressive, and relational representation in a human interpretable way, i.e., via captions.</p><p>Richer representation of an image often leads to numerous potential applications or performance improvements of subsequent computer vision algorithms <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>. In order to achieve richer object-centric understanding, Johnson et al. <ref type="bibr" target="#b11">[12]</ref> proposed a framework called DenseCap that gener-Small dog sitting on a motorcycle.</p><p>The man riding a red motorcycle.</p><p>The dog sitting behind the person.</p><p>Old man in the front of brown dog. The motorcycle is red in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Captioning</head><p>The man is wearing a black shirt.</p><p>The dog is brown and black. ates captions for each of the densely sampled local image regions. These regional descriptions facilitate both rich and dense semantic understanding of a scene in a form of interpretable language. However, the information in the image that we want to acquire includes not only the information of the object itself but also the interaction with other objects or the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple label Sentence</head><p>As an alternative way of representing an image, we focus on dense relationships between objects. In the context of human cognition, there has been a general consensus that objects and particular environments near the target object affect search and recognition efficiency. Understanding the relationships between objects clearly reveal object interactions and object-attribute combinations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Interestingly, we observe that the annotations done by humans on computer vision datasets predominantly contain relational forms; in Visual Genome <ref type="bibr" target="#b14">[15]</ref> and MS COCO <ref type="bibr" target="#b18">[19]</ref> caption datasets, most of the labels take the format of subject-predicate-object more so than subjectpredicate. Moreover, UCF101 <ref type="bibr" target="#b31">[31]</ref> action recognition dataset contains 85 actions out of 101 (84.2%) that are described in terms of human interactions with other objects or surroundings. These aspects tell us that understanding interaction and relationships between objects facilitate a major component in visual understanding of object-centric events.</p><p>In this regard, we introduce a novel captioning framework relational captioning that can provide diverse and dense representations from an image. In this task, we first exploit the relational context between two objects as a representation unit. This allows generating a combinatorial number of localized regional information. Secondly, we make use of captioning and its ability to express significantly richer concepts beyond the limited label space of object classes used in object detection tasks. Due to these aspects, our relational captioning expands the regime further along the label space both in terms of density and complexity, and provides richer representation for an image.</p><p>Our main contributions are summarized as follows. (1) We introduce relational captioning, a new captioning task that generates captions with respect to relational information between objects in an image. <ref type="bibr" target="#b1">(2)</ref> In order to efficiently train the relational caption information, we propose the multi-task triple-stream network (MTTSNet) that consists of three recurrent units trained via multi-task learning.</p><p>(3) We show that our proposed method is able to generate denser and more diverse captions by evaluating on our relational captioning dataset augmented from Visual Genome (VG) <ref type="bibr" target="#b14">[15]</ref> dataset. (4) We introduce several applications of our framework, including "caption graph" generation which contains richer and more diverse information than conventional scene graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work relates to two topics: image captioning and relationship detection. In this section, we categorize and review related work on these topics. Image captioning. By virtue of deep learning and the use of recurrent neural network (e.g. LSTM <ref type="bibr" target="#b8">[9]</ref>) based decoders, image captioning <ref type="bibr" target="#b24">[24]</ref> techniques have been extensively explored <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b41">41]</ref>. One of the research issues in captioning is the generation of diverse and informative captions. Thus, learning to generated diverse captions has been extensively studied recently <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34]</ref>. As one of the solutions, the dense captioning (DenseCap) task <ref type="bibr" target="#b11">[12]</ref> was proposed which uses diverse region proposals to generate localized descriptions, extending the conventional holistic image captioning to diverse captioning that can describe local contexts. Moreover, our relational captioning is able to generate even more diverse caption proposals than dense captioning by considering relations between objects.</p><p>Yang et al. <ref type="bibr" target="#b38">[38]</ref> improves the DenseCap model by incorporating a global image feature as context cue as well as a region feature of the desired objects with a late fusion. Motivated by this, in order to implicitly learn dependencies of subject, object and union representations, we incorporate a triple-stream LSTM for our captioning module. Visual relationship detection (VRD). Understanding visual relationships between objects have been an important concept in various tasks. Conventional VRD usually deals with predicting the subject-predicate-object (in short, subj-pred-obj). A pioneering work by Lu et al. <ref type="bibr" target="#b19">[20]</ref> formalizes the VRD task and provides a dataset, while addressing the subject (or object) and predicate classification models separately. On the other hand, similar to VRD task, scene graph generation (a task to generate a structured graph that contains the context of a scene) has also started to be explored <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b43">43]</ref>.</p><p>Although the VRD dataset is larger (100 object classes and 70 predicates) than Visual Phrases, it is still inadequate to handle the real world scale. The Visual Genome (VG) dataset <ref type="bibr" target="#b14">[15]</ref> for relationship detection consists of 31k predicate types and 64k object types giving the number of possible combinations of relationship triplets too diverse for the state-of-the-art VRD based models. This is because the labels consist of the various combinations of words (e.g. 'little boy,' 'small boy,' etc.) As a result, only the simplified version of VG relationship dataset has been studied. On the contrary, our method is able to generate relational captions by tokenizing the whole relational expressions into words, and learning from them.</p><p>While the recent state-of-the-art VRD <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b40">40]</ref> or scene graph generation works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b43">43]</ref> mostly use language priors to detect relationships, we directly learn the relationship as a descriptive language model. In addition, the expressions of traditional scene graph generation or VRD task are restricted to subj-pred-obj triplets, whereas the relational captioning is able to provide additional information such as attributes or noun modifiers by adopting free-form natural language expressions.</p><p>In summary, dense captioning facilitates a natural language interpretation of regions in an image, while VRD can obtain relational information between objects. Our work combines both axes, resulting in much denser and diverse captions than DenseCap. That is, given B region proposals in an image, we can obtain B(B−1) relational captions, whereas DenseCap returns only B captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-task Triple-Stream Networks</head><p>Our relational captioning is defined as follows. Given an input image, a bounding box detector generates various object proposals and a captioning module predicts combinatorial captions with POS labels describing each pair of objects. <ref type="figure">Figure 2</ref> shows the overall framework of the proposed relational captioning model, which is mainly composed of a localization module based on the region proposal network (RPN) <ref type="bibr" target="#b27">[27]</ref>, and a triple-stream RNN (LSTM <ref type="bibr" target="#b8">[9]</ref>) module for captioning. Our network supports end-to-end training  <ref type="figure">Figure 2</ref>: Overall architecture of the proposed multi-task triple-stream networks. Three region features (Union, Subject, Object) come from the same shared branch (Region Proposal Network), and for subject and object features, the first intermediate FC layer share weights (depicted in the same color).</p><p>with a single optimization step that allows joint localization, combination, and description with natural language. Given an image, RPN generates object proposals. Then, the combination layer takes a pair consisting of a subject and an object at a time. To take the surrounding context information into account, we utilize the union region of the subject and object regions, in a way similar to using the global image region as side information by Yang et al. <ref type="bibr" target="#b38">[38]</ref>. This feature of triplets (subject, object, union) are fed to the triple-stream LSTMs, where each stream takes its own purpose, i.e. subject, object, and union. Given this triplet feature, the triple-stream LSTMs collaboratively generate a caption and POS classes of each word. We describe these processes as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Region Proposal Networks</head><p>Our network uses fully convolutional layers of VGG-16 <ref type="bibr" target="#b30">[30]</ref> up to the final pooling layer (i.e. conv5 3) for extracting the spatial features via the bilinear ROI pooling <ref type="bibr" target="#b11">[12]</ref>. The object proposals are generated by localization layers. It takes the feature tensor, and proposes B regions (user parameter) of interest. Each proposed region has its confidence score, region feature of shape 512×7×7, and coordinates b=(x, y, w, h) of the bounding box with center (x, y), width w and height h. We process it into vectorized features (of shape D=512) using two fully-connected (FC) layers. This encodes the appearance of each region into a feature, called region code. Once the region codes are extracted, they are reused for the following processes.</p><p>To generate relational proposals, we build pairwise combinations of B region proposals, where in turn we get B(B−1) possible region pairs. We call this layer the combination layer. A distinctive point of our model with the previous dense captioning works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">38]</ref>, is that while the works regard each region proposal as an independent tar-get to describe and produce B number of captions, we consider their pairwise combinations B(B−1), which are much denser and explicitly expressible in term of relationships. Also, we can asymmetrically use each entry of a pair by assigning the roles of the regions, i.e. (subject, object).</p><p>Furthermore, motivated by Yang et al., where the global context of an image improves the captioning performance, we leverage an additional region, the union region b u of (subject, object). In addition, to provide relative spatial information, we append a geometric feature for the subject and object box pair, i.e. (b s , b o ) to the union feature before the FC layers. Given two bounding boxes b s and b o , the geometric feature r is defined similarly to <ref type="bibr" target="#b25">[25]</ref> as</p><formula xml:id="formula_0">r = xo−xs √ wshs , yo−ys √ wshs , woho</formula><p>wshs , ws hs , wo ho , bs bo bs bo ∈ R 6 . (1)</p><p>By concatenating the union feature with r which is passed through an additional FC layer, the shape of this feature is D+64. Then, the dimension of the union region code is reduced by the following FC layers. This stream of operations is illustrated in <ref type="figure">Fig. 2</ref>. The three features extracted from the subject, object, and union regions are fed to each LSTM described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Relational Captioning Networks</head><p>Relational caption generation takes the relational information of the object pairs into account. However, expressing the relationship in a sentence has been barely studied. Therefore, we design a new network that deals with relational captions, called the multi-task triple-stream network.</p><p>From the region proposal network, a triplet of region codes are fed as input to LSTM cells, so that a sequence of words (caption) is generated. In the proposed relational region proposal, a distinctive facet is to provide a triplet of region codes consisting of subject, object, and union regions, which virtually corresponds to the POS of a sentence  <ref type="figure">Figure 3</ref>: An illustration of the unrolled triple-stream LSTM. Our model consists of two major parts: triple-stream LSTM and a multi-task module. The multi-task module jointly predicts a caption word and its POS class (subj-pred-obj, illustrated as three cells colored according to the POS class), as well as the input vector for the next time step.</p><p>(subj-pred-obj). This correspondence between regions in a triplet and POS information leads to the following advantages: 1) input features can be adaptively merged depending on its POS and fed to the caption generation module, and 2) the POS prior on predicting a word can be effectively applied to caption generation. However, leveraging and processing these input cues are non-trivial. For the first advantage, in order to derive POS aware inference, we propose triple-stream networks, which are three separate LSTMs respectively corresponding to subjpred-obj. The outcomes of LSTMs are combined via concatenation. For the second advantage, during a word prediction, we jointly infer its POS class via multi-task inference. This POS class prediction acts as a prior for the word prediction of a caption during the learning phase. Triple-Stream LSTMs. Intuitively, the region codes of subject and object would be closely related to the subject and object related words in a caption, while the union and geometric features may contribute to the predicate. In our relational captioning framework, the LSTM modules must adaptively take input features into account according to which POS decoding stage it is for a caption.</p><p>As shown in <ref type="figure">Fig. 2</ref>, the proposed triple-stream LSTM module consists of three separate LSTMs, each of which is in charge of the subject, object and union region codes respectively. At each step, the triple-stream LSTMs generate three embedded representations separately, and a single word is predicted by consolidating the three processed representations. The embedding of the predicted word is distributed into all three LSTMs as inputs and is used to run the next step in a recursive manner. Thus in each step, each entry of the triplet input is used differently, which allows more flexibility than a single LSTM as used in traditional captioning models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">33]</ref>. In other words, the weights of the input cue features change at every recursive step according to which POS the word being generated belongs to. Multi-task with POS Classification. On top of this concatenation, we utilize the POS information to more effec-tively train the relational captioning model. Relational captioning generates a sequence of words in subj-pred-obj order, i.e. the order of POS. For each word prediction, in a multi-task module in <ref type="figure">Fig. 3</ref>, we also classify the POS class of the predicted word, so that it encourages the caption generation to follow the word order in the POS order.</p><p>When three representations for each POS are to be consolidated, one option can be to consolidate them in an early step, called early fusion. This results in a single LSTM with the fusion of the three region codes (e.g. concatenation of three codes). However, as reported by Yang et al. <ref type="bibr" target="#b38">[38]</ref>, this early fusion approach also shows lower performance than that of late fusion methods. In this regard, we adopt a late fusion for a multi-task module. The layer basically concatenates the representation outputs from the triple-stream LSTMs, but due to the recurrent multi-task modules, it is able to generate sophisticated representations.</p><p>We empirically observe that this multi-task learning with POS helps not only the shared representation to become richer but also guides the word predictions, and thus helps to improve the captioning performance overall. We hypothesize that the POS task provides distinctive information that may help learn proper representations from the triple-stream LSTMs. Since each POS class prediction tightly relies on respective representations from each LSTM, e.g. pred-LSTM closely related to pred of POS, the gradients generated from the POS classification would be back-propagated through the indices of the concatenated representation according to the class. By virtue of this, the multi-task triplestream LSTMs are able to learn the representation in such a way that it can predict plausible words for each time step. Therefore, our model can generate appropriate words according to the POS at a given time step. Loss functions. Training our relational captioning model can be mainly divided into captioning loss and detection loss. Specifically, the proposed model is trained to minimize the following loss function:</p><formula xml:id="formula_1">L = L cap + αL P OS + βL det + γL box ,<label>(2)</label></formula><p>where L cap , L P OS , L det , and L box denote captioning loss, POS classification loss, detection loss, and bounding box regression loss respectively. α, β, and γ are the balance parameters (we set them to 0.1 for all experiments). The first two terms are for captioning and the next two terms are for the region proposal. L cap and L P OS are crossentropy losses at every time step for each word and POS classification respectively. For each time step, L P OS measures a 3-class cross entropy loss. L det is a binary logistic loss for foreground/background regions, while L box is a smoothed L1 loss <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we provide the experimental setups, competing methods and performance evaluation of relational captioning with both quantitative and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Relational Captioning Dataset</head><p>Since there is no existing dataset for the relational captioning task, we construct a dataset by utilizing VG relationship dataset version 1.2 <ref type="bibr" target="#b14">[15]</ref> which consists of 85200 images with 75456/4871/4873 splits for train/validation/test sets respectively. We tokenize the relational expressions to form natural language expressions, and for each word, we assign the POS class from the triplet association.</p><p>However, VG relationship datasets show limited diversity in the words used. Therefore, by only using relational expressions to construct data, the captions generated from a model tends to be simple (e.g. "building-has-window"). Even though our model may enable richer concepts and expressions, if the training data does not contain such concepts and expressions, there is no way to actually see this. In order to validate the diversity of our relational captioner, we need to make our relational captioning dataset to have more natural sentences with rich expressions.</p><p>Through observation, we noticed that the relationship dataset labels lack attributes describing the subject and object, which are perhaps what enriches the sentences the most. Therefore, we utilize the attribute labels of VG data to augment existing relationship expressions. More specifically, we simply find the attribute that matches the subject/object of the relationship label and attach it to the subj/obj caption label. In particular, if an attribute label describes the same subject/object for a relationship label while associated bounding box overlaps enough, the label is considered to be matched to the subject/object in the relationship label. After this process, we obtain 15595 vocabularies for our relational captioning dataset (11447 vocabularies before this process). We train our caption model with this data, and report its result in this section. In addition, we provide a holistic image captioning performance and various analysis such as comparison with scene graph generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relational Dense Captioning: Ablation Study</head><p>Baselines. Since no direct work for relational captioning exists, we implement several baselines by modifying the most relevant methods, which facilitate our ablation study.</p><p>• Direct Union has the same architecture with Dense-Cap <ref type="bibr" target="#b11">[12]</ref>, but of which RPN is trained to directly predict union regions. The union region is used to generate captions by one LSTM.</p><p>• Union also resembles DenseCap <ref type="bibr" target="#b11">[12]</ref> and Direct union, but its RPN predicts individual object regions. The object regions are paired as (subject, object), and then a union region from each pair is fed to a single LSTM for captioning. Also, we implement two additional variants: Union (w/MTL) additionally predicts the POS classification task, and Union+Coord. appends the geometric feature to the region code of the union.</p><p>• Subj+Obj and Subj+Obj+Union models use the concatenated region features of (subject, object) and (subject, object, union) respectively and pass them through a single LSTM (early fusion approach). Also, Subj+Obj+Coord. uses the geometric feature instead of the region code of the union. Moreover, we evaluate the baselines, Subj+Obj+{Union,Coord} with POS classification (MTL loss).</p><p>• TSNet denotes the proposed triple-stream LSTM based model without a branch for POS classifier. Each stream takes the region codes of (subject, object, union + coord.) separately. MTTSNet denotes our final model, multi-task triple-stream network with POS classifier.</p><p>Evaluation metrics. Motivated by the evaluation metric suggested for dense captioning task <ref type="bibr" target="#b11">[12]</ref>, we suggest a new evaluation metric for relational dense captioning. We report the mean Average Precision (mAP) which measures both localization and language accuracy. As suggested by Johnson et al., we use METEOR score <ref type="bibr" target="#b5">[6]</ref> with thresholds {0, 0.05, 0.10.15, 0.2, 0.25} for language, and IOU thresholds {0.2, 0.3, 0.4, 0.5, 0.6} for localization. The AP values <ref type="figure">Figure 4</ref>: Example captions and region generated by the proposed model. We compare our result with the image captioner <ref type="bibr" target="#b33">[33]</ref> and the dense captioner <ref type="bibr" target="#b11">[12]</ref> in order to contrast the amount of information and diversity.</p><p>Recall METEOR #Caption Caption/Box Image Captioner (Show&amp;Tell) <ref type="bibr" target="#b33">[33]</ref> 23.55 8.66 1 1 Image Captioner (SCST) <ref type="bibr" target="#b28">[28]</ref> 24   <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b33">33]</ref> and a dense captioner <ref type="bibr" target="#b11">[12]</ref>.</p><p>obtained by all the pairwise combinations of language and localization thresholds are averaged to get the final mAP score. The major difference of our metric is that, for the localization AP, we measure for both the subject and object bounding boxes with respective ground truths. In particular, we only consider the samples with IOUs of both the subject and object bounding boxes greater than the localization threshold. For all cases, we use percentage as the unit of metric. In addition, we suggest another metric, called "image-level (Img-Lv.) recall." This measures the caption quality at the holistic image level by considering the bag of all captions generated from an image as a single prediction. Given only the aforementioned language thresholds for METEOR i.e. without box IOU threshold, we measure the recall of the predicted captions. The metric evaluates the diversity of the produced representations by the model for a given image. Also, we measure the average METEOR score for predicted captions to evaluate the caption quality.</p><p>Results. <ref type="table">Table 1</ref> shows the performance of the relational dense captioning task on relational captioning dataset. The second and third row sections (2-7 and 8-11th rows) show the comparison of the baselines with and without POS classification (w/MTL). In the last row, we show the performance of the state-of-the-art scene graph generator, Neural Motifs <ref type="bibr" target="#b43">[43]</ref>. Due to the different output structure, we compare with Neural Motifs trained with the supervision for relationship detection. Similar to the setup in DenseCap <ref type="bibr" target="#b11">[12]</ref>, we fix the number of region proposals before NMS to 50 for all methods for a fair comparison.</p><p>Among the results in the second row section (2-7th rows)</p><p>of <ref type="table">Table 1</ref>, our TSNet shows the best result suggesting that the triple-stream component alone is a sufficiently strong baseline over the others. On top of TSNet, applying the MTL loss (i.e., MTTSNet) improves overall performance, and especially improves mAP, where the detection accuracy seems to be dominantly improved compared to the improvement of the other metrics. This shows that triple-stream LSTM is the key module that most leverages the MTL loss across other early fusion approaches (see the third row section of the table). As another factor, we can see from <ref type="table">Table 1</ref> that the relative spatial information (Coord.) and union feature information (Union) improves the results. This is because the union feature itself preserves the spatial information to some extent from the 7 × 7 grid form of its activation. For Neural Motifs, other relational captioner baselines including our TSNet and MTTSNet perform favorably against Neural Motifs in all metrics. This is worth noting because handling free-form language generation which we aim to achieve is more challenging than the simple triplet prediction of scene graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Holistic Image Captioning Comparison</head><p>We also compare our approach with other image captioning frameworks, Image Captioner (Show&amp;Tell <ref type="bibr" target="#b33">[33]</ref> and SCST <ref type="bibr" target="#b28">[28]</ref>), and Dense Captioner (DenseCap <ref type="bibr" target="#b11">[12]</ref>), in a holistic image description perspective. In order to measure the performance of holistic image-level captioning for dense captioning methods, we use Img-Lv. Recall metric defined in the previous section (Recall). We compare them with two relational dense captioning methods, Union  <ref type="figure">Figure 5</ref>: Results of generating "caption graph" from our relational captioniner. In order to compare the diversity of the outputs, we also show the result of the scene graph generator, Neural Motifs <ref type="bibr" target="#b43">[43]</ref>.</p><p>and MTTSNet, denoted as Relational Captioner. For a fair comparison, for Dense and Relational Captioner, we adjust the number of region proposals after NMS to be similar, which is different from the setting in the previous section which fixed the number of proposals before NMS. <ref type="table" target="#tab_4">Table 2</ref> shows the image-level recall, METEOR, and additional quantities for comparison. #Caption denotes the average number of captions generated from an input image and Caption/Box denotes the average ratio of the number of captions generated and the number of boxes remaining after NMS. Therefore, Caption/Box demonstrates how many captions can be generated given the same number of boxes generated after NMS. By virtue of multiple captions per image from multiple boxes, the Dense Captioner is able to achieve higher performance than both of the Image Captioners. Compared with the Dense Captioner, MTTSNet as a Relational Captioner can generate an even larger number of captions given the same number of boxes. Hence, as a result of learning to generate diverse captions, the MTTSNet achieves higher recall and METEOR. From the performance of Union, we can see that it is difficult to obtain better captions than Dense Captioner by only learning to use the union of subject and object boxes, despite having a larger number of captions.</p><p>We show example predictions of our relational captioning model in <ref type="figure">Fig. 4</ref>. Our model is able to generate rich and diverse captions for an image. We also show a comparison with the traditional frameworks, image captioner <ref type="bibr" target="#b33">[33]</ref> and dense captioner <ref type="bibr" target="#b11">[12]</ref>. While the dense captioner is able to generate diverse descriptions than an image captioner by virtue of various regions, our model can generate an even greater number of captions from the combination of the bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Scene Graph</head><p>Motivated by scene graph, which is derived from the VRD task, we extend to a new type of a scene graph, which we call "caption graph." <ref type="figure">Figure 5</ref> shows the caption graphs generated from our MTTSNet as well as the scene graphs from Neural Motifs <ref type="bibr" target="#b43">[43]</ref>. For caption graph, we follow the same procedure as Neural Motifs but replace the relationship detection network into our MTTSNet. In both methods, we use ground truth bounding boxes to generate scene (and caption) graphs for fair comparison.</p><p>By virtue of being free form, our caption graph can have richer expression and information including attributes, whereas the traditional scene graph is limited to a closed set of the subj-pred-obj triplet. For example, in <ref type="figure">Fig. 5-(b,d)</ref>, given the same object 'person,' our model is able to distinguish the fine-grained category (i.e. man vs boy and man vs woman). In addition, our model can provide more status information about the object (e.g. standing, black), by virtue of the attribute contained in our relational captioning data. Most importantly, the scene graph can contain unnatural relationships (e.g. tree-on-tree in <ref type="figure">Fig. 5-(c)</ref>), because prior relationship detection methods, e.g. <ref type="bibr" target="#b43">[43]</ref>, predict object classes individually. In contrast, by predicting the full sentence for every object pair, relational captioner can assign a more appropriate word for an object by considering the relations, e.g. "Green leaf on a tree." Lastly, our model is able to assign different words for the same object by considering the context (the man vs baseball player in <ref type="figure">Fig. 5-(d)</ref>), whereas the scene graph generator can only assign one most likely class (man). Thus, our relational captioning framework enables more diverse interpretation of the objects compared to the traditional scene graph generation models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Additional Analysis</head><p>Vocabulary Statistics. In addition, we measure the vocabulary statistics and compare them among the frameworks. The types of statistics measured are as follows: 1) an average number of unique words that have been used to describe an image, and 2) an average number of words to describe each box. More specifically, we count the number of unique words in all the predicted sentences and present the average number per image or box. Thus, the metric measures the amount of information we can obtain given an image or a fixed number of boxes. The comparison is depicted in <ref type="table" target="#tab_6">Table 3</ref>. These statistics increase from Image Cap. to Scene Graph to Dense Cap. to Relational Cap. In conclusion, the proposed relational captioning is advantageous in diversity and amount of information, compared to both of the traditional object-centric scene understanding frameworks, scene graph generation and dense captioning. Sentence-based Image and Region-pair Retrieval. Since our relational captioning framework produces richer image representations than other frameworks, it may have benefits on the sentence based image or region-pair retrieval, which cannot be performed by scene graph generation or VRD models. To evaluate on the retrieval task, we follow the same procedure as in Johnson et al. <ref type="bibr" target="#b11">[12]</ref> with our relational captioning data. We randomly choose 1000 images from the test set, and from these chosen images, we collect 100 query sentences by sampling four random captions from 25 randomly chosen images. The task is to retrieve the correct image for each query by matching it with the generated captions. We compute the ratio of the number of queries, of which the retrieved image ranked within top k ∈ {1, 5, 10}, and the total number of queries (denoted as R@K). We also report the median rank of the correctly retrieved images across all 1000 test images (The random chance performance is 0.001, 0.005, and 0.01 for R@1, R@5, and R@10 respectively). The retrieval results compared with several baselines are shown in <ref type="table">Table 4</ref>. For baseline models Full Image RNN, Region RNN, and DenseCap, we display the performance measured from Johnson et al. <ref type="bibr" target="#b11">[12]</ref>. To be compatible, we followed the same procedure of running through random test sets 3 times to report the average results. Our R@1 R@5 R@10 Med Full Image RNN <ref type="bibr" target="#b12">[13]</ref> 0.10 0.30 0.43 13 Region RNN <ref type="bibr" target="#b7">[8]</ref> 0.18 0.43 0.59 7 DenseCap <ref type="bibr" target="#b11">[12]</ref> 0.27 0.53 0.67 5 RelCap (MTTSNet) 0.29 0.60 0.73 4 <ref type="table">Table 4</ref>: Sentence based image retrieval performance compared to previous frameworks. We evaluate ranking using recall at k (R@K, higher is better) and the median rank of the target image (Med, lower is better).</p><p>matching score is computed as follows. For every test image, we generate 100 region proposals from the RPN followed by NMS. In order to produce a matching score between a query and a region pair in the image, we compute the probability that the query text may occur from the region pair. Among all the scores for the region pairs from the image, we take the maximum matching score value as a representative score of the image. This score is used as the matching score between the query text and the image, and thus the images are sorted by rank based on these computed matching scores. As shown in <ref type="table">Table 4</ref>, the proposed relational captioner outperforms all baseline frameworks. This is meaningful because region pair based method is more challenging than a single region based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce relational captioning, a new notion which requires a model to localize regions of an image and describe each of the relational region pairs with a caption. To this end, we propose the MTTSNet, which facilitates POS aware relational captioning. In several sub-tasks, we empirically demonstrate the effectiveness of our framework over scene graph generation and the traditional captioning frameworks. As a way to represent imagery, the relational captioning can provide diverse, abundant, high-level and interpretable representations in caption form. In this regard, our work may open interesting applications, e.g., natural language based video summarization <ref type="bibr" target="#b2">[3]</ref> may be benefited by our rich representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overall description of the proposed relational captioning framework. Compared to traditional frameworks, our framework is advantageous in both interaction understanding and high-level interpretation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Relational Captioning 1 - 2 .</head><label>12</label><figDesc>The roof on yellow train. 5-2. Black wheel on a yellow train. 7-2. The window on a train. 9-2. Off light on yellow train. 2-4. Yellow train on old track. Scene Graph 1-2. building-on-train 5-2. wheel-on-train 7-2. window-on-train 9-2. light-on-train 2-4. train-on-track (a) Relational Captioning 5-6. Old man wearing blue hat. 7-3. Red pants on young man. 3-4. Standing boy wearing red hat. 1-2. The man wearing purple hat. Scene Graph 5-6. man-wearing-hat 7-3. pant-on-man 3-4. man-wearing-hat 1-2. man-wearing-hat (b) Relational Captioning 3-4 Green leaf on a tree. 1-2 White cap on standing man. 2-6 The man wearing blue pants. 2-8 Standing man wearing black shirt. 2-10 The man wearing white hat. Scene Graph 3-4 tree-on-tree 1-2 hat-on-man 2-6 man-wearing-short 2-8 man-wearing-shirt 2-10 man-has-helmet (c) Relational Captioning 7-8. The man wearing black helmet. 5-6. Sitting woman behind the stand. 1-2. Baseball player wearing helmet 1-4. The man wearing white pants. Scene Graph 7-8. man-wearing-helmet 5-6. man-behind-stand 1-2. man-wearing-helmet 1-4. man-wearing-pant (d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of the holistic level image captioning. We compare the results of the relational captioners with that of two image captioners</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Diversity comparison between image captioning, scene graph generation, dense captioning, and relational captioning. We measure the number of different words per image (words/img) and the number of words per bounding box (words/box).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Groupcap: Group-based image captioning with structured relevance and diversity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextually customized video summaries via natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contrastive learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The workshop on statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent fusion network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Youngjin Yoon, and In So Kweon. Disjoint multi-task learning between heterogeneous human-centric tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The organization of visually mediated actions in a subject without eye movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><forename type="middle">M</forename><surname>Michael F Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">D</forename><surname>Furneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilchrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocase</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="87" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">VIP-CNN: A visual phrase reasoning convolutional neural network for visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Weakly-supervised learning of visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speaking the same language: Matching machine to human captions by adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Captioning images with diverse objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Donghyeon Cho, and In So Kweon. Linknet: Relational embedding for scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dense captioning with joint inference and visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zoom-net: Mining deep feature interactions for visual relationship recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
