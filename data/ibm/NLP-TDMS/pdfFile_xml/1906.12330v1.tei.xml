<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Star Net for Generalized Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Haonan</surname></persName>
							<email>luhaonan@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
							<email>sethhuang@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Ye</surname></persName>
							<email>tianye25@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Xiuyan</surname></persName>
							<email>guoxiuyan1@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">AARC Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">AARC Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">AARC Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">AARC Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Star Net for Generalized Multi-Task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present graph star net (GraphStar), a novel and unified graph neural net architecture which utilizes message-passing relay and attention mechanism for multiple prediction tasks -node classification, graph classification and link prediction. GraphStar addresses many earlier challenges facing graph neural nets and achieves non-local representation without increasing the model depth or bearing heavy computational costs. We also propose a new method to tackle topic-specific sentiment analysis based on node classification and text classification as graph classification. Our work shows that "star nodes" can learn effective graph-data representation and improve on current methods for the three tasks. Specifically, for graph classification and link prediction, GraphStar outperforms the current state-of-the-art models by 2-5% on several key benchmarks. * Corresponding Author. Lu Haonan and Tian Ye are equal contributors Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relational data, consisted of nodes, sub-graphs and edges, has a vast range of applications from sentiment analysis, text classification to protein and enzyme interactions. Graphs have implicit, complex topological structures and relationships, and graph models capable of internalizing these structures have achieved state-of-the-art performances for many task <ref type="bibr" target="#b29">[30]</ref>. For data representation, graphs are more universal than conventional pairwise relationships, and machine learning approaches for graph analysis mainly focus on three primary tasks: node classification, graph classification and link prediction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Graph data models a set of subjects/ entities (nodes) and their inter-relationships (edges). In a simplest classification setting, the model attempts to predict unlabeled nodes by the surrounding labeled nodes. The graph domain is also categorized into spectral and non-spectral approaches. Spectral approaches often work with methods to represent the graph. Non-spectral approaches often conduct local-convolutions on the graph in the neighborhood <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. In order to capture the global state, a local conventional model has to increase its depth, but this may encounter an over-smoothing issue <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. On the other hand, <ref type="bibr" target="#b21">[22]</ref> tackles non-local representation to capture long-range dependencies, using fully-connected-nodes attention and aggregating all features of all positions as a weighted sum, resulting in a high computational complexity. Our model aims at capturing the global state without increasing the model depth and using fully-connected attention. The codes for essential experiments will be released at https://github.com/graph-star-team/graph_star. This site complies fully with the anonymous, double-blinded policies.</p><p>Graph Attention Networks, instead of using convolutional neural networks (CNN) as in <ref type="bibr" target="#b4">[5]</ref>, proposes a graph attention layer, which compared to CNNs can specify different weights to different nodes within a neighborhood <ref type="bibr" target="#b18">[19]</ref>. They adopt the multi-head attention mechanism <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1]</ref>, allowing for predictions of new-node and previously unseen graphs, a transductive-to-inductive approach. However, it does not allow for deep-neighborhood representation, which lessens the model capability to understand context. <ref type="bibr" target="#b21">[22]</ref> addresses this issue with a self-attention approach to capture global state without increasing the model depth.</p><p>Star Transformer <ref type="bibr" target="#b3">[4]</ref> changes the original fully-connected attention structure to one with a virtual message-passing relay, reducing the number of connections from quadratic to linear <ref type="bibr" target="#b3">[4]</ref>. The design has the potential of capturing long-range dependency, aggregating both local and non-local compositionality <ref type="bibr" target="#b3">[4]</ref>. Our work has shown improvement upon previous results while simplifying the model structure and improving the computational efficiency.</p><p>In this work, we introduce Graph Star Net (GraphStar), an inductive framework which extends the information boundary through relay nodes to aggregate global information. To our best knowledge, we have not seen another architecture which utilizes the virtual relay concept in graph modeling and aggregating local and non-local information. Our main contributions lie in addressing many previous limitations and unifying the approaches in multiple tasks for node classification, graph classification and link prediction simultaneously. We also proposed a new node-classification-based, topic-specific sentiment analysis method as well as graph-classification-based, text classification method. Compared with the previous node-classification-based transductive models such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>, our model can easily be used for unseen, new document predictions.</p><p>GraphStar utilizes the unique data representation via an information relay across the graph and trains the model to: (1) perform inductive tasks on previously unseen graph data; (2) aggregate both local and long-range information, making the model globally-aware, extracting high-level abstraction typically not represented in individual node features; (3) the relays serve as a hierarchical representation of the graphs and can be directly used for graph classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GraphStar Model Architecture</head><p>In this section, we present the star relay function and the updating process and describe the theoretical and implementation advantages. Generally, our approaches will be divided into three steps: star initialization, updating of the real nodes and the updating of the stars. In each layer of the model, the second and third steps occur in a cyclical manner layer-by-layer in the updating process, and our goal is to train a multi-purpose model to represent local and global compositionality for different prediction tasks. Our model setup primarily follows the landmark works of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> as well as <ref type="bibr" target="#b3">[4]</ref>. <ref type="figure">Figure 1</ref>: Graph Star Architecture. Left: Corresponding to Step 2, local compositionality updates for real nodes. Node i uses the representations of its neighbors, including itself, and the star in the layer t to get its own representation in layer (t+1). Right: Corresponding to Step 3, star global compositionality updates. To update the star, the model uses the updated representations of all nodes in the layer (t+1) and its own representation in layer t to get star's representation in (t+1). This star, connected to all nodes, is a natural hierarchical representation of the whole graph.</p><p>The input to our model is a graph G(A, F ), where A R N b ×N b is adjacency matrix and F R n×d is the feature matrix, and each node has d features, and N b is the number of nodes in a graph. We describe the graph with a set of learned features in each layer</p><formula xml:id="formula_0">H t = ( − → h 1 , − → h 2 , ..., − → h N b ), − → h t i R d , and F = ( − → f 1 , − → f 2 , ..., − → f N b ), − → f i R d</formula><p>is the initial representation of the node i and t being the layer number.</p><p>Step 1: Initial Representation of the Star</p><formula xml:id="formula_1">Let − → F mean = mean(F 0 ), F R d . We have the initial star representation − → S (0) = σ( i N b α init,i W init V − → f i ), where α init,i = exp(&lt; W init Q − → F mean , W init K − → f i &gt;) k N b exp(&lt; W init Q − → F mean , W init K − → f k &gt;) (1) &lt; − → a , − → b &gt; is the dot product of − → a , − → b , σ is the non-linear activation function and W init Q , W init K , W init V</formula><p>follow the standard Transformer setup <ref type="bibr" target="#b17">[18]</ref>.</p><p>Step 2: Real Node Update</p><p>Here we use multi-head attention to conduct real node update <ref type="bibr" target="#b18">[19]</ref>. Inspired by rGCN <ref type="bibr" target="#b12">[13]</ref>, we treat node-to-neighborhood under relation r R, node-to-star and node-to-self as three types of relations, with the first one corresponding to the number of relations in the graph. The node importance under relation r is controlled by attention coefficients. We define the self-self, self-star and self-neighbor under relation r as controlled by W V 0 , W V S and W rV 1 , respectively. The work presented in our paper is based on the special case R = 1, making it single-type relation link prediction. The node update can be represented by</p><formula xml:id="formula_2">h (t+1) i = N Head m σ( r R j N r i α m ijr W m(t) rV 1 h (t) j + α m is,r=s W m(t) V S S t + α m i0,r=0 W m(t) V 0 h t i ),<label>(2)</label></formula><p>where represents the concatenation of all multi-heads, N r i is the neighbors of node i under relation r R. N head is the number of heads in the multi-head attention setting. The multi-head outputs are then concatenated as the representation of t + 1 layer. Here we definê</p><formula xml:id="formula_3">m = r R k N r i exp(&lt; W m(t) Q h (t) i , W m(t) rV 1 h (t) k &gt;)+ exp(&lt; W m(t) Q h (t) i , W m(t) V S S (t) &gt;) + exp(&lt; W m(t) Q h (t) i , W m(t) V 0 h (t) i &gt;),<label>(3)</label></formula><p>and the attention coefficients can thus be represented by</p><formula xml:id="formula_4">α m ijr = exp(&lt; W m(t) Q h (t) i , W m(t) rV 1 h (t) j &gt;) m , α m is,r=s = exp(&lt; W m(t) Q h (t) i , W m(t) V S S (t) &gt;) m , α m i0,r=0 = exp(&lt; W m(t) Q h (t) i , W m(t) V 0 h (t) i &gt;) m .</formula><p>It should be noted that for all relations, we adopt a parameter sharing scheme W K * = W V * .</p><p>Step 3: Star Update</p><p>Following step 1 and step 2, after we got the real node representation at layer t + 1, we can now update the star nodes via</p><formula xml:id="formula_5">S (t+1) = N Head m σ( j N b α m s,j W m(t) V h (t+1) j + α m s,s W m(t) V S t ).<label>(4)</label></formula><p>We defineˆ</p><formula xml:id="formula_6">m s = k N b exp(&lt; W m Q S t , W m K h (t+1) k &gt;) + exp(&lt; W m(t) Q S t , W m K S t &gt;)</formula><p>, and the attention coefficients are thus</p><formula xml:id="formula_7">α m s,j = exp(&lt; W m Q S (t) , W m K h (t+1) j &gt;) m s , α m s,s = exp(&lt; W m Q S (t) , W m K S (t) &gt;) m s .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-Task Setup Node Classification</head><p>For node classification tasks, following standard graph neural setup described in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref> we take the GraphStar model's final layer output in (2) and apply the standard sof tmax activation function. We then minimize the cross-entropy loss conventional in classification model training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Classification</head><p>From the output of (4), in GraphStar, star and this virtual node are connected via other real nodes through attention mechanism, naturally forming hierarchical representation of the graph, meaning S ( t) is a natural representation of the graph's global state. We use the final layer output of the star for graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Predictions</head><p>Here we present the application of GraphStar in single-relation data sets. From <ref type="formula" target="#formula_2">(2)</ref> We treat the GraphStar model as an encoder for node embedding, and link prediction is a downstream task. A standard setup is to make predictions on subject, relation, object (s, r, o) as described in <ref type="bibr" target="#b12">[13]</ref>. The decoder is a score function f (s, r, o) :  <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13]</ref>, we train the model with negative sampling, and for a given positive example, we sample one negative sample through a randomized method to the subject or the object, with cross-entropy loss function.</p><formula xml:id="formula_8">R d × R ×R d → R.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We demonstrate the results for node classification, graph classification and link prediction tasks. We also developed several graph data sets based on standard text classification and sentiment analysis dataset IMDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Node Classification Tasks. For node classification, GraphStar is tested on three popular transductive learning benchmarks -Cora, Citeseer and PubMed, and one inductive learning benchmark -PPI.</p><p>Topic-Specific Sentiment Analysis. Here, we propose a new method to tackle sentiment analysis based on node classification and use IMDB-binary dataset as an example. This dataset was originally not a graph task; it is usually treated as a natural language processing task (NLP). It is a binary sentiment analysis consisting of 50,000 reviews labeled as either positive or negative.</p><p>Our innovation is to turn the pure NLP task into a graph data task based on document embedding. First, for IMBD, we use a pre-trained large Bert model to document encoding <ref type="bibr" target="#b0">[1]</ref>, and we treat every film review as a node in a graph. We then link the nodes (film reviews) which belong to the same topic and create a graph dataset. This approach is highly generalizable to most topic-specific classification tasks. <ref type="table">Table 1</ref>: Summary of Classification Datasets for Node Classification and Link Predictions.The notation |O+|:|O-| describes the ratio of positive and negative edges. More details regarding these datasets can be found in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref> Graph Classification task. For graph classification tasks, we use Enzymes, D&amp;D, Proteins and Mutag as our evaluation benchmarks. The description of the data is summarized in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Text Classification Tasks. For text classification, we use R8, R52, MR, 20 News Group and Ohsumed as our benchmarks. Different from previous text graph-based models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>, here we conduct the text classification as graph classification instead of node classifications. In a real life setting, a trained model should be inductive, which can be used to directly make predictions on new texts or documents. In the GraphStar setup, every document corresponds to one graph, and in the graph, every node is a word. If two words appear in the same sliding window, then these two words are linked via an edge. The same words with different context are treated as different nodes. In this way, although the initial word share the same feature, the outputs embedding of the words will be different. For 20 News Group, due to the text length, maximum being over 15000, same words are treated as the same node, and we truncate the text length at 3072. We divide the original training data into 90% of training data and 10% of validation data, leaving the test data out of the training cycles. For our experiments, we report the test data results. The summary of the data is in <ref type="table" target="#tab_2">Table 3</ref>.  Link Prediction. For link prediction, following <ref type="bibr" target="#b15">[16]</ref>, we use Cora, Citeseer, Pubmed as key benchmarks, and the data description is in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The experimental results are summarized in <ref type="table" target="#tab_3">Table 4</ref>, 5, 6, 7 and 8 for transductive node classification, inductive node classification, graph classification, graph text classification and link prediction. All reported results are based on 10 runs. We adopt the Adam optimizer, and the margins of error are provided in the parenthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Classification</head><p>For the transductive tasks, all tasks share the same parameters and are not optimized individually for different datasets, and the only variations are the initial learning rates and L 2 regularization settings. We use the attention coefficient dropout rate = 0.2. The number of GraphStar layers = 2, the number of attention head per layer N head = 8, hidden unit size per head is 16, and the hidden unit's dropout rate is 0.7. For the training, we keep the best model achieving the highest validation accuracy given the patience setting of 50 epochs. The initialization of the star nodes is based on the mean of raw features of the nodes. During training, for Cora, Citeseer and Pubmed, we apply the L2 regularization with ρ = 0.002, ρ = 0.004, and ρ = 0.0001, and the initial learning rates are set to be 0.001, 0.001 and 0.005, respectively. The initial learning rates for PPI and IMDB are 0.0002 and 0.0001, respectively. The L2 setting is ρ = 0 for both PPI and IMDB. For both data sets, the attention coefficient and the hidden unit dropout rates are both 0.2 with 3 layers. The number of hidden units per head is 128 for PPI and 256 for IMDB.</p><p>For transductive tasks, the GraphStar classification accuracy is measured against the results in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>. For inductive task the reported results are in <ref type="table" target="#tab_3">Table 4</ref>, based on <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>; the GraphStar model achieves better performance than the current state-of-the-art models for the PPI dataset.  <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>. Right: Results of inductive experiment in terms of micro-average F1 scores on the PPI dataset <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> . <ref type="table">Table 5</ref>: Results of inductive IMDB node classification experiments with reported results based on <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref> The IMDB sentiment analysis is reported in <ref type="table">Table 5</ref>. The GraphStar is shown to outperform the current state-of-the-art results after only 130 seconds of training time on V100 GPU.</p><p>Graph Classification / Graph-based Text Classification.</p><p>For graph classification, we report the current state-of-the-art results in <ref type="table" target="#tab_4">Table 6</ref>. For text classification, the current state-of-the-art results are reported and compared against our model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8]</ref> and reported in <ref type="table" target="#tab_5">Table 7</ref>.</p><p>For Enzymes, Proteins, D&amp;D and Mutag classification tasks, we use the attention coefficient dropout rate and the hidden unit dropout rate of 0.2. The number of GraphStar layers = 3, the number of attention head N head = 4. The learning rates are 0.0005 for all datasets. The L2 regularization is 0.0001, 0.001, 0.001, 0.0 and the hidden unit sizes per head are 16, 128, 16 and 16 for Enzymes, Proteins, D&amp;D and Mutag, respectively. For all datasets, we perform 10-fold cross-validation for model evaluation, and the resulting accuracy and margins are based on the 10 runs.</p><p>For the text classification tasks, we use the attention coefficient dropout rate = 0.3. The number of GraphStar layers = 3, the number of attention head N head = 4, hidden unit size per head is 128, and the hidden unit's dropout rate is 0.3. For the training, we keep the modes with the best validation accuracy given the patience setting of 20 epochs. During training, we apply the learning rate of 0.01 and L2 regularization with ρ = 0.002 for all datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction Task.</head><p>The evaluation metric for link prediction task is the average of the AUC and AP scores. The number of attention head N head = 4, the number of layer is 3, initial learning rate is 0.0002, and the L2 regularization setting is 0.0005. The same setting is applied to all datasets for link prediction except the hidden size per layer and dropout. The attention coefficient dropout is 0 for all datasets, and the hidden layer dropout rate is 0.2 for Cora and 0 for Citeseer and Pubmed. The hidden unit size per head is 512 for Cora and Citeseer, and 128 for Pubmed. For the training, we keep the model achieving the highest average of the AUC and AP scores on the validation set, given the patience setting of 300 epochs.</p><p>For link prediction, the <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> state-of-the-art results are reported. Link prediction performance is reported as the combined average of AUC and AP scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Results</head><p>In this section, we study the inductive node classification training size effect and the stability of graph classification. For PPI dataset, we gradually reduce the original training set graph size and give them to validation and test sets. The result is reported in <ref type="table" target="#tab_7">Table 9</ref>.</p><p>The result indicates that for PPI, GraphStar is particular effective such that, even if we reduce the training size drastically, the training accuracy can still achieve reasonable performance. We also observe the same effect in the IMDB dataset such that, by reducing the original training size of 25000 to 1997 and treating the 23003 as extra validation set, the model still maintains the accuracy of 94.0% on the test set.  Additionally, we also study the stability of 10-fold cross validation process for graph classification and present it in <ref type="figure" target="#fig_0">Figure 2</ref>. The process for Enzyme and Protein data sets are presented here, and as the graph indicates, the experiments are conducted on the 10-fold validation across epochs. All reported accuracy are based on experiment setups described above. The figure indicates that for graph classification, the test results can have high fluctuation for those datasets, and our best results, compared with the current state-of-the-art models, have considerably higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented graph star net (GraphStar) which can be used in three general graph tasks (node classification, graph classification and link prediction). GraphStar, without incurring heavy computation costs, is able to map the global state effectively. Lastly, we also propose novel approaches to conduct topic-specific sentiment analysis and text classification with graph models. GraphStar has achieved state-of-the-art performance across all tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Stability for test accuracy in 10-fold cross-validation training in graph classification. Left: Result for Enzymes dataset. Right: Result for Protein dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Graph Classification Datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of Text Classification Datasets for Graph Classification</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of transductive and inductive node classification experiments. Left: Results of transductive experiments on Cora, Citeseer and Pubmed datasets for multiple landmark model architectures</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Results of Graph Classification tasks. Left: Science datasets. Right: Text classification datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Results of graph text classification tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Results of link prediction experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Result of training effectiveness of GraphStar and training size reduction on PPI dataset</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09113</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Star-transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Supervised and semi-supervised text categorization using lstm for region embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02373</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semisupervised graph classification: A hierarchical graph perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Revisiting lstm networks for semi-supervised text classification via mixed objective function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-task graph autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phi</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
		</author>
		<idno>abs/1811.02798</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08090</idno>
		<title level="m">Graph capsule convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1902.07153</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised Data Augmentation. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05679</idno>
		<title level="m">Graph convolutional networks for text classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
