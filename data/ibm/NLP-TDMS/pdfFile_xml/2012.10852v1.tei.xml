<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Speech Enhancement Without A Real Visual Stream</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindhu</forename><forename type="middle">B</forename><surname>Hegde</surname></persName>
							<email>sindhu.hegde@research.iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Prajwal</surname></persName>
							<email>prajwal.k@research.iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>Mukhopadhyay</surname></persName>
							<email>radrabha.m@research.iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Namboodiri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
							<email>jawahar@iiit.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Bath</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Speech Enhancement Without A Real Visual Stream</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* The authors have contributed equally to the work</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we re-think the task of speech enhancement in unconstrained real-world environments. Current state-of-the-art methods use only the audio stream and are limited in their performance in a wide range of real-world noises. Recent works using lip movements as additional cues improve the quality of generated speech over "audio-only" methods. But, these methods cannot be used for several applications where the visual stream is unreliable or completely absent. We propose a new paradigm for speech enhancement by exploiting recent breakthroughs in speech-driven lip synthesis. Using one such model as a teacher network, we train a robust student network to produce accurate lip movements that mask away the noise, thus acting as a "visual noise filter". The intelligibility of the speech enhanced by our pseudo-lip approach is comparable (&lt; 3% difference) to the case of using real lips. This implies that we can exploit the advantages of using lip movements even in the absence of a real video stream. We rigorously evaluate our model using quantitative metrics as well as human evaluations. Additional ablation studies and a demo video on our website containing qualitative comparisons and results clearly illustrate the effectiveness of our approach. We provide a demo video which clearly illustrates the effectiveness of our proposed approach on our website: http://cvit.iiit.ac.in/research/projects/cvit-projects/ visual-speech-enhancement-without-a-real-visual-stream.</p><p>The code and models are also released for future research:</p><p>https://github.com/Sindhu-Hegde/ pseudo-visual-speech-denoising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Imagine calling your friend from inside a crowded public bus. She is unable to hear your plans for the evening due to the noise of the bus, wind, and the nearby moving vehicles. We are all constantly surrounded by noise that corrupts our speech. The problem of speech enhancement is thus quintessential, especially at a time when several workrelated meetings are happening over a phone call from our homes. But the applications of speech enhancement extend well beyond voice calls. For instance, separating human speech from the background music can be crucial for automatic subtitle/lyrics generation for movies and music. Further, speech enhancement can help the rising number of independent content creators filter the outdoor noises that are widely prevalent in their vlogs and short videos. Last but not least, enhancing historically important speeches will help us preserve our heritage for future generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Overview of Existing Approaches</head><p>The problem of speech enhancement has been studied for a long time, and various methods have already been proposed. Recently, several works <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b33">33]</ref> using deep learning have become popular, where noisy speech is enhanced using only the audio modality. However, all these works are applicable only in mild noise conditions (high SNR) and are known to produce artifacts in the generated speech. Most importantly, they often do not produce satisfactory results for unconstrained real-world applications such as those mentioned above. This is because various types of unseen noises degrade the input audio, the speakers and the recording systems go through unforeseen changes. A newer trend was introduced by works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> where lip movements are used as an additional modality for enhancement. These methods are more accurate than audio-only works in unconstrained settings with large amounts of noise levels. However, unlike audio-only works, audio-visual methods are adversely affected by rapid head motion, occlusions, lip going out of focus, or the audio and lips going out of sync <ref type="bibr" target="#b1">[2]</ref>. This significantly limits the applicability of these audiovisual methods, where most of the common applications discussed above cannot be handled due to the absence of a visual stream with clearly visible lip movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">A Visual Noise Filter</head><p>In this work, we introduce a new hybrid paradigm that brings together the best of both these branches of speech enhancement. We want a method that exploits the robustness and accuracy boost one can get with lip movements but also be able to function effectively in a wide range of applications where the visual stream is unreliable or absent. Thus, instead of relying on an actual video stream, we propose to use a lip synthesis model to generate a visual stream with lip movements for any given noisy audio. Using a carefully designed student-teacher training setup, we demonstrate that we can generate face images with lip movements that do not meaningfully represent the noise but accurately reflect the underlying speech component. Consequently, the generated images act as a visual noise filter for the down-stream speech enhancement model. In this way, unlike the existing audio-visual enhancement works, we are not constrained by the need for a video with clear visibility of lip movements. However, we can still utilize the improvements these works achieve with the help of pseudo lip movements <ref type="figure">(Figure 1</ref>). To the best of our knowledge, this is the first work to grasp the advantages of the visual stream, even in the presence of only audio. Our proposed approach yields significant improvements across all speech quality and intelligibility metrics and human evaluation studies. To summarize, the following are our major contributions:</p><p>• We propose a novel pseudo-visual speech enhancement model that is applicable in natural and high noise conditions. • We are the first to study the use of artificial lip movements from a lip synthesis model for speech enhancement. Our method is the first to effectively use the benefits of lip movements and still be applicable in situations where the visual information is unavailable or is corrupted. • Using a novel student-teacher setup, we show that we can train a lip synthesis model to generate accurate lip movements corresponding to the underlying speech in a noisy signal. In fact, the intelligibility of the speech enhanced by our pseudo-lip approach is comparable (&lt; 3% difference) to the case of using real lips. • We create and release a new standard human evaluation set consisting of real-world videos in unconstrained conditions with several types of noises. Future speech enhancement works can evaluate their perceptual quality on this set. We provide a demo video on our website, which clearly exhibits our approach's feasibility compared to the existing audio-only and audio-visual works. The code, trained models, and the evaluation benchmark are released publicly for future research 1 . The rest of the paper is organized as follows: In Section 2, we give an overview of the existing works in this space. We then elaborate on the proposed speech enhancement method in Section 3. The experimental setup, results and analysis are discussed in Sections 4 and 5. We perform several ablation studies in Section 6 and discuss various potential applications of our work in Section 7. We conclude our work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Audio-only Speech Enhancement</head><p>We first review the works using only the audio stream with no additional information for denoising of speech. Classical signal denoising techniques like the Wiener filtering <ref type="bibr" target="#b29">[29]</ref> became the first popular approach for speech enhancement. However, it was often ineffective in denoising speech in real-world situations as the wiener filter requires an estimate of the noise a priori. Like many other problems, deep learning models have become increasingly popular for speech enhancement in recent times. Initial works <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b30">30]</ref> used standard denoising auto-encoders and LSTM based approaches <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b35">35]</ref> for cleaning noisy speech. Feature-based loss functions have also been proposed in <ref type="bibr" target="#b11">[11]</ref>, while the most popular advancement came from models like <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19]</ref> using generative adversarial networks (GANs) which produce relatively higher quality speech from noisy audio segments.</p><p>Even though there has been significant progress in the last few years, speech enhancement models are still confined to being trained on datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref> that has been collected in constrained environments recorded by a selected set of speakers. The types of noises <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b34">34]</ref> that are synthetically added to the clean speech while training such models are also limited to a few types. Thus, these models often do not perform well in unconstrained natural settings as they fail to cope up with hundreds of speakers of different dialects and languages, the level and type of noise varying abruptly in a speech segment, etc. In this work, we aim to generate high-quality clean speech from the given noisy audio in such unconstrained conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Audio-visual Speech Enhancement</head><p>Since 2018, a new approach was introduced for denoising of speech. Works like <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">18]</ref> considered an additional visual stream of information by exploiting the lip movements of the speaker for extracting the clean speech. These methods not only perform well in unconstrained real- <ref type="figure">Figure 1</ref>. We propose a novel approach to enhance the speech by hallucinating the visual stream for any given noisy audio. In contrast to the existing audio-visual methods, our approach works even in the absence of a reliable visual stream, while also performing better than audio-only works in unconstrained conditions due to the assistance of generated lip movements. world settings, but they also show significant improvement in terms of metrics over other audio-only methods. However, they suffer from a major limitation that they work only on videos with a clear view of the speaker's lips. This requirement of a frontal, lip-synced video of the speaker prohibits these models from being used for a wide range of applications, where the visual stream is imperfect (profile views, lips going out of focus, video corruption, out-of-sync speech, motion blurs) or even completely absent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Understanding Speech and Lip Movements</head><p>Jointly understanding multiple modalities together has gained significant traction in recent times. Several recently published works <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b37">37]</ref> involve both audio and vision as modalities. Interpreting the spoken utterances from lip movements has attracted special attention in different works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b26">26]</ref>. The opposite task of generating lip motion for given speech segments has also become a popular area of research. The initial works in this space <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b31">31]</ref> were specifically trained for a single person with several hours of data. More recent works <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b27">27]</ref> can generate lip movements for any identity, any voice, and any language. Specifically, Wav2Lip <ref type="bibr" target="#b27">[27]</ref> is the current state-ofthe-art in "unconstrained lip syncing" which produces accurate lip motion for any given speech, but is inaccurate for noisy speech. In the next section, we discuss in detail how we can distill the knowledge of this network into a student network that learns to generate accurate lip movements for a given noisy speech segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pseudo-visual Stream for Enhancing Speech</head><p>How do we generate lip motion that is in sync with the clean speech component in given noisy audio?</p><p>Can we readily use the current lip synthesis models for noisy speech inputs? We find that the current stateof-the-art unconstrained speech-to-lip models <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b27">27]</ref> which work for arbitrary speakers, voices, and languages are highly inaccurate on noisy speech segments. This is understandable as these works were never aimed to tackle such cases. Moreover, these methods are speaker-independent and are designed to work on unconstrained videos by training on thousands of speakers with substantial variations in pose, expressions, backgrounds, etc. We found that it is not ideal to naively fine-tune the pre-trained lip synthesis model on noisy speech. This is simply because learning to lip-sync on highly noisy speech across such extreme variations in the visual data is a daunting task, yielding limited improvements in fine-grained lip shapes <ref type="table" target="#tab_0">(Table 1)</ref>. But, for our task at hand, we do not need a lip synthesis model on thousands of speakers; a single speaker is sufficient.</p><p>A single identity is all you need. We only need a sequence of accurate lip movements, preferably even just on a <ref type="figure">Figure 2</ref>. We train a novel student-teacher network for generating accurate lip movements for noisy speech segments. The teacher is a pre-trained lip synthesis network <ref type="bibr" target="#b27">[27]</ref> that generates accurate lip movements on a static face using clean speech as input. The student is trained to mimic the teacher's lip movements, but when given noisy speech as input. static image of a single identity where only the lips are moving in accordance with the speech. This is a relatively much easier task than the one before and is also well-aligned with our needs. If the only visual changes are in the lip shapes, the model will naturally focus on learning more accurate, fine-grained speech-lip correspondences. Note that we still need this identity-specific model to work for any speech in any voice and language. How do we train a lip synthesis model that can lip-sync just for a single identity image, but can handle any speech?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distilling lip motion knowledge for a single identity</head><p>We exploit the fact that the current state-of-the-art model, Wav2Lip <ref type="bibr" target="#b27">[27]</ref> can generate accurate lip motion for arbitrary static face images conditioned on any clean speech. Our core idea is to achieve this accuracy using noisy speech (harder part) as input, but on just a single identity (easier part). To do this, we train a student network to map the noisy speech inputs to lip motion on a single static face image. We employ Wav2Lip as the teacher network and use its predictions on the same identity image, but with clean speech inputs. This is illustrated in <ref type="figure">Figure 2</ref>.</p><p>A Visual Noise Filter: We hypothesize that since the only visual differences are in the lip shapes, the student network is forced to learn a strong correspondence between the underlying speech and the lip motion. Further, the student network cannot meaningfully represent noise in the generated images and is forced to represent only the speech components that the teacher network accurately indicates. Thus the images generated by the student network acts as a "visual noise filter" that manifests only the speech component for the down-stream speech enhancement network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training the Student Model</head><p>As described above, we would like to train a student model M by learning from a pre-trained lip-synthesis network L as a teacher. M is a simple encoder-decoder model that inputs a noisy speech segment and outputs a lip-synced mouth region of a pre-determined person. This is adapted from the Wav2Lip architecture <ref type="bibr" target="#b27">[27]</ref> by discarding the face identity branch, because we need M to generate lip movements only for a single identity image.</p><p>Noisy Speech Input to M : We feed a 0.2 second window of the noisy speech segment. We create this noisy speech segment (S input ) by mixing clean speech (S clean ) from the LRS3 <ref type="bibr" target="#b2">[3]</ref> with noise (S noise ) from the VGGSound <ref type="bibr" target="#b4">[5]</ref> dataset at one of the three signal-to-noise ratios (SNR) (0, 5 and 10 dB). As done in Wav2Lip <ref type="bibr" target="#b27">[27]</ref>, we use melspectrograms as the input speech representation.</p><p>Learning from a Lip Synthesis Teacher To train the model M , we need an accurate lip-synced ground-truth of a single target. We obtain this from L, a pre-trained lipsynced network, by feeding the clean speech S clean as the audio input. We use the audios present in the LRS3 <ref type="bibr" target="#b2">[3]</ref> dataset as our clean speech data. For our lip synthesis teacher L, we use Wav2Lip <ref type="bibr" target="#b27">[27]</ref>, a publicly available 2 stateof-the-art speech-to-lip synthesis model. As it is a speakerindependent model, we also need to feed an identity image. We choose a near-frontal face image of Taylor Swift on which the lips are morphed by Wav2Lip to match the clean speech inputs. The lip-synced output from Wav2Lip is accurate as the audio is clean. Further, the output is always of the same face image with only the lip and jaw regions changing while the rest of the face regions remain static. We use the lower half of the generated face output containing the Wav2Lip's prediction as ground-truth for our student model M . Thus, our new student network is trained to generate correct lip movements (matching the clean speech) given a noise-corrupted input of the same speech.</p><p>We train the student network to minimize the L1 loss between its predicted images and the lip-synced ground truth from Wav2Lip. We train this network for 150K iterations with a batch size of 64 on a single NVIDIA RTX 2080Ti GPU. Other hyper-parameters are the same as that of Wav2Lip <ref type="bibr" target="#b27">[27]</ref>. For our speech enhancement model in the next section, we use this trained student network to generate the lip motion given a noisy speech segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pseudo-visual Speech Enhancement</head><p>An overview of the proposed speech enhancement model is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. Our model takes both the pseudovisual stream and the noisy auditory stream as the input. Our proposed speech enhancement model. A pseudo-visual stream is generated for the given noisy audio, which acts as a visual noise filter. The enhancement model then ingests the noisy spectrogram along with the generated lip movements and outputs a mask for the clean speech.</p><p>For a given noisy audio, initially, we generate the lip movements as described in Section 3.1. These, along with the noisy input spectrograms are given to the visual encoder, and the speech encoder respectively as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. The speech decoder outputs a residual mask, which is added to the input spectrograms to filter the noise from the clean speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Audio representation:</head><p>We consider 1 second of noisy speech, S input as input to the enhancement model and extract linear spectrogram representation using short-time Fourier transform (STFT). Generating linear spectrograms allows us to directly invert them back to a waveform without the need for vocoders. To compute the STFT, we consider the window length of 25ms with a hop length of 10ms sampled at 16kHz. The computed STFT from the raw audio waveforms is a complex array of time-frequency representation, with a dimension of T s ×257. Here, T s is the number of STFT time steps, which corresponds to 100 in our experiments (1 second audio segment). We further decompose the complex STFT array into the magnitude and the phase components, and normalize them between [0, 1]. These components are concatenated along the frequency axis to form a representation of T s ×514 which acts as input to the speech encoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Visual representation:</head><p>The lip-sync student network generates 25 frames for one second of audio input. Using a visual encoder consisting of 12 layers of residual 2D-convolution blocks, we obtain a visual embedding for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Network architecture:</head><p>The speech enhancement model consists of the speech encoder and decoder networks along with a visual encoder to encode the pseudo-visual stream as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. The input noisy spectrogram is processed by the speech encoder which is a stack of 7 1D-convolution blocks with residual connections. We perform the convolutions along the temporal dimension, by considering the frequency component of the input spectrograms as channels. The output of the visual encoder module is up-sampled 4× using nearest-neighbor interpolation to match the spectrogram temporal dimension. We then combine the audio and the visual streams by concatenating the learned features of each stream along the channel dimension. This fused representation is then given to the speech decoder which is a stack of 14 1D convolution layers with residual connections. The decoder outputs a mask that is added to the input noisy spectrogram followed by a sigmoid activation to generate the enhanced speech spectrogram output. We minimize the L1 distance between the predicted spectrogram and the ground truth. Finally, the enhanced clean waveform is obtained by using inverse-STFT (ISTFT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We use the publicly available LRS3 dataset <ref type="bibr" target="#b2">[3]</ref> which consists of thousands of spoken sentences from TED videos. For training, we use the "pre-train" and "train-val" sets from the dataset which has around 430 hours of video data with 150K utterances. This is a challenging dataset that covers a large number of speakers (9K), thus encouraging the trained model to be speaker-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental setup</head><p>For evaluating in unconstrained settings, we create the following three synthetic test sets, each having three noise levels of 0db, 5db and 10db: (i) Test split of LRS3 <ref type="bibr" target="#b2">[3]</ref> + Noise from VGGSound <ref type="bibr" target="#b4">[5]</ref>, (ii) Test split of LRS3 <ref type="bibr" target="#b2">[3]</ref> + Noise from QUT-NOISE-TIMIT <ref type="bibr" target="#b5">[6]</ref> (unseen noise), (iii) Test split of LRS2 [1] + Noise from VGGSound <ref type="bibr" target="#b4">[5]</ref> (unseen speakers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation</head><p>We use the following standard speech enhancement evaluation metrics (higher is better) to evaluate our method. We compute Perceptual Evaluation of Speech Quality (PESQ) <ref type="bibr" target="#b28">[28]</ref> (−0.5 to 4.5), which measures the overall perceptual quality and short-time objective intelligibility measure (STOI) <ref type="bibr" target="#b32">[32]</ref> (0 to 1), which correlates with the intelligibility of speech. We also use objective measures such as the mean opinion score (MOS) prediction of the signal distortion (CSIG) (1 to 5), the MOS prediction of background noise (CBAK) (1 to 5), and the overall MOS prediction score (COVL) (1 to 5). In addition to evaluation based on these metrics, we also perform human evaluations on a newly curated real-world test set and report the MOS (range: 1 to 5) to analyze the real-world applicability of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluating the Lip-sync Network</head><p>We start by evaluating our lip-sync network that generates lip movements for a given noisy speech segment. It should be noted that this network is specifically designed for our speech enhancement task and not for the purpose of talking face generation. For comparison, we also fine-tune Wav2Lip <ref type="bibr" target="#b27">[27]</ref> with the same noisy speech samples used for training the student network. For evaluation, we use the synthetic test set consisting of speech from LRS3 data and noise from VGGSound as described in Section 4.2.</p><p>We then use this test set to benchmark (a) pre-trained Wav2Lip, (b) Wav2Lip trained on noisy data, and (c) Our student lip-sync network. Note that there are no original ground truth videos available for the kind of talking face videos our lip-sync model generates from a single face image. Thus, we use the LSE-D and LSE-C metrics used to evaluate lip-sync in Wav2Lip <ref type="bibr" target="#b27">[27]</ref>. A higher LSE-C indicates better overall audio-visual correlation. As we can see from <ref type="table" target="#tab_0">Table 1</ref>, our approach to train a lip-sync network specifically for this task with one face using Wav2Lip as a teacher outperforms other methods in terms of LSE-C, indicating a better overall audio-visual correspondence. The LSE-D of the student is close to the pre-trained Wav2Lip, but the confidence score, LSE-C of the pre-trained Wav2Lip is quite low. In Section 6.2, we also show the performance of our speech enhancement model when using other networks to generate the pseudo-visual stream. We now move on to evaluating our main pipeline for speech enhancement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Evaluation</head><p>We start by evaluating our method using the synthetic test sets created as described in Section 4.2. We present the metric scores of the input noisy data (without any denoising and enhancement) along with other speech enhancement methods. For fair comparison, we fine-tune the audioonly models, SEGAN <ref type="bibr" target="#b19">[19]</ref> and DFL <ref type="bibr" target="#b11">[11]</ref> on the same LRS3 training data. To further highlight the importance of our pseudo-visual stream, we implemented our own audio-only (AO) baseline, which is very similar to our model but without the visual encoder. Also, we compare our results with our implementation of the real-visual stream state-of-the-art audio-visual method <ref type="bibr" target="#b1">[2]</ref> to see how close our pseudo-visual approach is to the real visual-stream method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Our model is robust to various noise levels</head><p>The results on LRS3 test set mixed with noise from VG-GSound data at different noise levels is summarized in the first section of <ref type="table">Table 2</ref>. As we can observe, our pseudovisual model outperforms the audio-only approaches as well as our AO-baseline by a significant margin at all three noise levels. Also, it is very close to the real-visual stream method, which indicates that our model is effective in generating accurate lip movements. It is interesting to note that our model performs comparable to the real-visual stream approach even at higher noise level of 0db. This validates our claim that the generated pseudo-visual stream reflects the clean speech segment and thus is able to suppress the noise. Moreover, another important point to note is that our AO-baseline performs comparably to the existing audioonly methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Our model is robust to unseen noises</head><p>To further illustrate the generalization of our approach, we present the results on new, unseen noise type from QUT (city-street noise) dataset <ref type="bibr" target="#b5">[6]</ref> in the second section of Table 2. We can observe the robustness and the superior performance of our model even on unseen noise types. <ref type="table">Table 2</ref>. Quantitative comparison of different approaches. The first section contains clean speech from LRS3 <ref type="bibr" target="#b2">[3]</ref> test set mixed with VGGSound <ref type="bibr" target="#b4">[5]</ref> noises at different SNR levels. In the second section, we specifically evaluate the performance on "unseen noises" by mixing the LRS3 <ref type="bibr" target="#b2">[3]</ref> test set audios with the QUT <ref type="bibr" target="#b4">[5]</ref> city-street noises at different noise levels. Finally, in the third section, we evaluate specifically on "unseen speakers" by mixing the speeches of the unseen LRS2 <ref type="bibr" target="#b0">[1]</ref> test set speakers with VGGSound <ref type="bibr" target="#b4">[5]</ref> noises. Our method outperforms the audio-only approaches in all three sections and is comparable (&lt; 3% difference) to the real visual-stream method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Our model is robust to unseen speakers</head><p>We also perform an additional comparative study on unseen speakers from LRS2 test set as shown in the third section of <ref type="table">Table 2</ref>. In-line with the previous results on LRS3 test set, our method performs remarkably well in comparison with the audio-only approaches. The results clearly indicate that our method is robust to unseen speakers, which is also validated using our collected real-world test set. A sample spectrogram predicted by our model, along with the ground truth and the noisy input spectrograms are shown in <ref type="figure">Figure 4</ref>. We observe that our model is able to reconstruct accurate speech even from a highly noisy input. <ref type="figure">Figure 4</ref>. We can clearly see that our network is able to reconstruct clean speech (center spectrogram) which is very close to the ground-truth (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Human Evaluations</head><p>To evaluate the effectiveness of our work in real-world situations, we perform a rigorous human evaluation on a newly collected real-world evaluation set. This set comprises 50 real-world videos in unconstrained environments, which are originally degraded by different kinds of noises. The test set contains a wide variety of videos/audios such as people vlogging while riding a motorbike or sailing in rough oceans, interaction on camera in crowded airports and railway stations, and old heritage recordings. To the best of our knowledge, such an evaluation set is the first of its kind and can be used for perceptually evaluating the performance of future works on real-world examples.</p><p>As these samples are naturally corrupted by noise, and we do not have the ground truth clean speech, we depend on human evaluations for this dataset. For comparison, we also conduct a human evaluation on the subset containing 25 samples each from LRS2 and LRS3 datasets when mixed with noise from the VGGSound corpus. In <ref type="table" target="#tab_2">Table 3</ref>, we report the mean scores of 15 participants for different approaches on a scale of 1-5 based on: (A) Quality, and (B) Intelligibility. The participant group consists of almost equal male-female members spanning an age group of 22 -49 years. <ref type="table" target="#tab_2">Table 3</ref> shows that the speech generated by our model is preferred over the other methods, even in realworld conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Studies</head><p>In order to understand our design choices in our approach, we conduct ablation experiments. Unless specified, all the results are reported on the test set of LRS3 <ref type="bibr" target="#b2">[3]</ref> dataset when mixed with VGGSound <ref type="bibr" target="#b4">[5]</ref> at a noise level of 0db. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Model is invariant to the pseudo-lip identity</head><p>In <ref type="table" target="#tab_3">Table 4</ref>, we report the performance when different identities are used for the generation of lip movements. Our model is consistent across multiple pseudo-lip identities varied based on gender, age, ethnicity, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison of visual stream generators</head><p>Although Wav2Lip <ref type="bibr" target="#b27">[27]</ref> is the state-of-the-art in "unconstrained lip-syncing", we also compare other lip-sync models for the generation of pseudo-visual stream in <ref type="table" target="#tab_4">Table 5</ref>. All the methods other than our trained student lipsync model result in an inferior performance due to the inaccurate lip-shape generation. The main reason is that other methods are constrained and/or sensitive to noise in the speech, and generate wide open mouth shapes in the presence of noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Model's variation to speaker attributes</head><p>To evaluate the effect of speaker attributes such as gender, language, and accent on speech enhancement, we test our approach on diverse unseen clean speech sources mixed with VGGSound noises. For gender evaluation, we automatically classify the LRS3 test set into male and female speakers using a gender detection tool <ref type="bibr" target="#b25">[25]</ref>. We also test our model across different languages and accents using clean speech sourced from TTS datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">12]</ref>. The results (Table 6) clearly demonstrate that there is no distinctive variation in performance across gender of the speakers, but scores do vary across languages and accents, as the training data is mainly comprised of western-accented English. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Applications</head><p>Pseudo-visual speech enhancement opens up multiple application areas that were only being dominated by audioonly methods. The lack of clearly visible real lip movements is very common. For instance, in dynamic scenes like vlogs or press conferences, the camera constantly pans to other elements of the scene. This is even more common in the case of movies, and here, additional downstream applications such as automatic subtitle generation can benefit from speech enhancement. Recently, there is also an increased interest in improving the video call experience in works such as lip to speech synthesis <ref type="bibr" target="#b26">[26]</ref> and talking face generation <ref type="bibr" target="#b27">[27]</ref>. Psuedo-visual methods are very apt for this application, as it is not possible to expect a highquality visual stream consistently during video calls due to frequent network slowdowns and interruptions. As lip synthesis methods become more robust and realistic, we expect that pseudo-visual methods can improve further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we proposed a novel speech enhancement method that combines the diversity of applications of audioonly works while also exploiting the benefits of lip motion. We do this by generating a pseudo-visual stream for any given noisy input audio. We showed that it is possible to generate accurate and reliable lip motion that reflects the speech component in noisy audios. The generated lip movements serve as a visual noise filter, which assists the downstream enhancement model. Consequently, we showed significant gains over traditional audio-only speech enhancement works. Our approach adds a new dimension to the space of face and speech. With suitable modifications, a plethora of other audio-only algorithms like ASR can now be supplemented with additional generated visual information that was not available originally. We believe, exploring various problems with a similar approach could lead to further useful insights into this new space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Our proposed speech enhancement model. A pseudo-visual stream is generated for the given noisy audio, which acts as a visual noise filter. The enhancement model then ingests the noisy spectrogram along with the generated lip movements and outputs a mask for the clean speech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative Evaluation of the lip sync model. Directly using the pre-trained Wav2Lip model or fine-tuning it on noisy audios leads to poor results on noisy speech. Our student network leads to more clear audio-visual correspondence as denoted by LSE-C.</figDesc><table><row><cell>Model</cell><cell cols="2">LSE-C↑ LSE-D↓</cell></row><row><cell>Wav2Lip trained on noisy data</cell><cell>1.181</cell><cell>10.35</cell></row><row><cell>Pre-trained Wav2Lip</cell><cell>1.330</cell><cell>8.933</cell></row><row><cell>Lip-sync student (ours)</cell><cell>4.572</cell><cell>9.743</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.84 2.17 2.62 2.72 2.80 2.29 2.24 2.52 2.93 2.99 3.05 2.66 2.65 2.95 3.12 3.19 3.25 CSIG 2.31 2.10 2.82 3.02 3.18 3.25 2.79 2.67 3.22 3.26 3.32 3.39 3.15 3.17 3.36 3.45 3.51 3.56 CBAK 1.83 1.87 2.30 2.36 2.47 2.51 2.23 2.30 2.46 2.54 2.65 2.71 2.40 2.55 2.63 2.70 2.81 2.84 COVL 1.68 1.53 2.02 2.14 2.25 2.29 2.04 2.00 2.21 2.29 2.37 2.41 2.16 2.26 2.31 2.46 2.52 2.58</figDesc><table><row><cell></cell><cell>0db</cell><cell>5db</cell><cell>10db</cell></row><row><cell cols="4">Method Noisy [19] [11] AO Ours [2] Noisy [19] [11] AO Ours [2] Noisy [19] [11] AO Ours [2]</cell></row><row><cell cols="4">PESQ 1.93 1STOI 0.76 0.75 0.83 0.87 0.88 0.90 0.85 0.86 0.88 0.90 0.92 0.94 0.88 0.88 0.90 0.92 0.95 0.95</cell></row><row><cell>PESQ</cell><cell cols="3">1.86 1.77 2.14 2.54 2.65 2.73 2.26 2.14 2.54 2.83 2.92 3.01 2.67 2.61 2.90 3.05 3.12 3.21</cell></row><row><cell>CSIG</cell><cell cols="3">2.46 2.37 2.78 2.93 3.02 3.10 2.90 2.86 3.18 3.15 3.24 3.33 3.30 3.30 3.38 3.35 3.42 3.48</cell></row><row><cell>CBAK</cell><cell cols="3">1.55 1.89 2.10 2.31 2.43 2.46 1.94 2.23 2.30 2.49 2.59 2.63 2.42 2.53 2.59 2.62 2.73 2.77</cell></row><row><cell>COVL</cell><cell cols="3">1.69 1.68 1.95 2.04 2.12 2.20 2.02 1.99 2.06 2.20 2.29 2.34 2.14 2.20 2.30 2.35 2.41 2.45</cell></row><row><cell>STOI</cell><cell cols="3">0.75 0.77 0.80 0.84 0.87 0.89 0.85 0.86 0.87 0.89 0.91 0.93 0.87 0.90 0.91 0.91 0.94 0.95</cell></row><row><cell>PESQ</cell><cell cols="3">1.94 1.82 2.10 2.58 2.71 2.79 2.32 1.87 2.55 2.87 2.98 3.04 2.69 2.41 2.79 3.10 3.19 3.22</cell></row><row><cell>CSIG</cell><cell cols="3">2.55 2.29 2.80 3.07 3.16 3.23 3.01 2.85 3.16 3.21 3.32 3.38 3.23 3.13 3.33 3.36 3.44 3.49</cell></row><row><cell>CBAK</cell><cell cols="3">1.86 1.82 2.22 2.31 2.41 2.47 2.28 2.12 2.42 2.48 2.57 2.63 2.58 2.59 2.57 2.63 2.71 2.73</cell></row><row><cell>COVL</cell><cell cols="3">1.81 1.59 1.93 2.07 2.15 2.20 2.16 2.03 2.14 2.17 2.26 2.31 2.20 2.11 2.23 2.27 2.35 2.39</cell></row><row><cell>STOI</cell><cell cols="3">0.75 0.73 0.80 0.84 0.87 0.89 0.84 0.83 0.85 0.88 0.90 0.90 0.85 0.86 0.89 0.89 0.92 0.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Human evaluation of our model based on: (A) Quality and (B) Intelligibility.</figDesc><table><row><cell>Data</cell><cell cols="2">Metric Noisy [19] [11] AO Ours</cell></row><row><cell>LRS2</cell><cell>(A) (B)</cell><cell>1.23 2.02 2.38 3.45 3.73 2.81 3.05 3.27 3.96 4.12</cell></row><row><cell>LRS3</cell><cell>(A) (B)</cell><cell>1.25 2.14 2.49 3.52 3.84 2.85 3.12 3.36 3.98 4.25</cell></row><row><cell>Curated real</cell><cell>(A)</cell><cell>1.49 1.92 2.27 3.31 3.83</cell></row><row><cell cols="2">world samples (B)</cell><cell>2.66 2.91 3.08 4.01 4.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The performance of our model is consistent if the age/ethnicity of the pseudo-lip identity is varied.</figDesc><table><row><cell>Identity</cell><cell cols="3">PESQ CSIG CBAK COVL STOI</cell></row><row><cell>Taylor Swift</cell><cell>2.72 3.18</cell><cell>2.47</cell><cell>2.25 0.88</cell></row><row><cell cols="2">Paul McCartney 2.70 3.19</cell><cell>2.45</cell><cell>2.24 0.88</cell></row><row><cell cols="2">Morgan Freeman 2.71 3.16</cell><cell>2.42</cell><cell>2.21 0.87</cell></row><row><cell>Andrew Ng</cell><cell>2.72 3.18</cell><cell>2.46</cell><cell>2.23 0.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of different methods for generation of pseudo-visual stream. Our student lipsync network generates accurate lip movements, thus effectively enhances the noisy speech.</figDesc><table><row><cell cols="4">Lipsync models PESQ CSIG CBAK COVL STOI</cell></row><row><cell cols="2">Speech2Vid [13] 2.42 2.85</cell><cell>2.17</cell><cell>2.01 0.82</cell></row><row><cell>LipGAN [14]</cell><cell>2.54 2.98</cell><cell>2.21</cell><cell>2.05 0.85</cell></row><row><cell>Wav2Lip [27]</cell><cell>2.64 3.05</cell><cell>2.28</cell><cell>2.12 0.86</cell></row><row><cell>Ours</cell><cell>2.72 3.18</cell><cell>2.47</cell><cell>2.25 0.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Effect of speaker attributes such as gender, language and accent on model performance.</figDesc><table><row><cell>Attr.</cell><cell cols="4">Class PESQ CSIG CBAK COVL STOI</cell></row><row><cell>Gender</cell><cell>Female Male</cell><cell>2.75 3.29 2.80 3.15</cell><cell>2.42 2.43</cell><cell>2.35 0.89 2.24 0.89</cell></row><row><cell>Lang.</cell><cell cols="2">Hindi Bengali 2.39 2.86 2.46 2.45</cell><cell>2.41 2.45</cell><cell>1.94 0.89 2.14 0.86</cell></row><row><cell>Accent</cell><cell cols="2">Indian American 2.60 2.89 2.57 2.75</cell><cell>2.38 2.49</cell><cell>2.10 0.87 2.16 0.89</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">cvit.iiit.ac.in/research/projects/cvit-projects/ visual-speech-enhancement-without-a-real-visual-stream</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/Rudrabha/Wav2Lip</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lrs3-ted: a largescale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Resources for indian languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Baby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leela</forename><surname>Anju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nishanthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tts Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Text, Speech and Dialogue</title>
		<meeting>Text, Speech and Dialogue</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vggsound: A large-scale audio-visual dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="721" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The qut-noise-timit corpus for the evaluation of voice activity detection algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robbie</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring speech enhancement with generative adversarial networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5024" to="5028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<publisher>Avinatan Hassidim, William T. Freeman,</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to separate object sounds by watching unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Timit acoustic-phonetic continuous speech corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zue</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1992</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech denoising with deep feature losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>François</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The lj speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Ito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You said that? : Synthesising talking faces from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards automatic face-to-face translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K R</forename><surname>Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerin</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia, MM &apos;19</title>
		<meeting>the 27th ACM International Conference on Multimedia, MM &apos;19</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1428" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Obamanet: Photo-realistic lip-sync from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<idno>abs/1801.01442</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Alexandre de Brébisson, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech enhancement based on deep denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xugang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial networks for speech enhancement and noise-robust speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3642" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Audio-visual model distillation using acoustic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Sanguineti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">and Maja Pantic. End-to-end audiovisual fusion with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04343</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">and Maja Pantic. End-to-end multi-view lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00443</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end visual speech recognition for small-scale datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="421" to="427" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">cvlib -high level computer vision library for python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ponnusamy</surname></persName>
		</author>
		<ptr target="https://github.com/arunponnusamy/cvlib" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning individual speaking styles for accurate lip to speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>K R Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>K R Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia, MM &apos;20</title>
		<meeting>the 28th ACM International Conference on Multimedia, MM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andries P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Speech enhancement based on a priori signal to noise estimation. ICASSP &apos;96</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scalart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Filho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="629" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Listening with your eyes: Towards a practical visual speech recognition system using deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="95" to="96" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A short-time objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hybrid approach to combining conventional and deep learning techniques for single-channel speech enhancement and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zarar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2531" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Noisy speech database for training speech enhancement algorithms and tts models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Investigating rnn-based speech enhancement methods for noise-robust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The voice bank corpus: Design, collection and data analysis of a large regional accent speech database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Alignnet: A unifying approach to audio-visual alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
