<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Micro-Batch Training with Batch-Channel Normalization and Weight Standardization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Micro-Batch Training with Batch-Channel Normalization and Weight Standardization</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Micro-Batch Training</term>
					<term>Group Normalization</term>
					<term>Weight Standardization</term>
					<term>Batch-Channel Normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Batch Normalization (BN) has become an out-of-box technique to improve deep network training. However, its effectiveness is limited for micro-batch training, i.e., each GPU typically has only 1-2 images for training, which is inevitable for many computer vision tasks, e.g., object detection and semantic segmentation, constrained by memory consumption. To address this issue, we propose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to bring two success factors of BN into micro-batch training: 1) the smoothing effects on the loss landscape and 2) the ability to avoid harmful elimination singularities along the training trajectory. WS standardizes the weights in convolutional layers to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients; BCN combines batch and channel normalizations and leverages estimated statistics of the activations in convolutional layers to keep networks away from elimination singularities. We validate WS and BCN on comprehensive computer vision tasks, including image classification, object detection, instance segmentation, video recognition and semantic segmentation. All experimental results consistently show that WS and BCN improve micro-batch training significantly. Moreover, using WS and BCN with micro-batch training is even able to match or outperform the performances of BN with large-batch training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning has advanced the state-of-the-arts in many vision tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Many deep networks use Batch Normalization (BN) <ref type="bibr" target="#b2">[3]</ref> in their architectures because BN in most cases is able to accelerate training and help the models to converge to better solutions. BN stabilizes the training by controlling the first two moments of the distributions of the layer outputs in each mini-batch during training and is especially helpful for training very deep networks that have hundreds of layers <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Despite its practical success, BN has a shortcoming that it works well only when the batch size is sufficiently large, which prohibits it from being used in micro-batch training. Micro-batch training, i.e., the batch size is small, e.g., 1 or 2, is inevitable for many computer vision tasks, such as object detection and semantic segmentation, due to limited GPU memory. This shortcoming draws a lot of attentions from researchers, which urges them to design specific normalization methods for micro-batch training, such as Group Normalization (GN) <ref type="bibr" target="#b5">[6]</ref> and Layer Normalization (LN) <ref type="bibr" target="#b6">[7]</ref>, but they have difficulty matching the performances of BN in large-batch training <ref type="figure" target="#fig_0">(Fig. 1)</ref>.</p><p>In this paper, our goal is to bring the success factors of BN into micro-batch training but without relying on large batch sizes during training. This requires good understandings of the reasons of BN's success, among which we focus on two factors: 1) BN's smoothing effects: <ref type="bibr" target="#b7">[8]</ref> proves that BN makes the landscape of the corresponding optimization problem significantly smoother, thus is able to stabilize the training <ref type="bibr">•</ref>    <ref type="bibr" target="#b2">[3]</ref>, GN <ref type="bibr" target="#b5">[6]</ref>, our WS used with GN, and WS used with BCN on ImageNet and COCO. On ImageNet, BN and BCN+WS are trained with large batch sizes while GN and GN+WS are trained with 1 image/GPU. On COCO, BN is frozen for micro-batch training, and BCN uses its microbatch implementation. GN+WS outperforms both BN and GN comfortably and BCN+WS further improves the performances. process and accelerate the convergence speed of training deep neural networks.</p><p>2) BN avoids elimination singularities: Elimination singularities refer to the points along the training trajectory where neurons in the networks get eliminated. Eliminable neurons waste computations and decrease the effective model complexity. Getting closer to them will harm the training speed and the final performances. By forcing each neuron to have zero mean and unit variance, BN keeps the networks at far distances from elimination singularities caused by non-linear activation functions.</p><p>We find that these two success factors are not properly addressed by some methods specifically designed for microbatch training. For example, channel-based normalizations, e.g., Layer Normalization (LN) <ref type="bibr" target="#b6">[7]</ref> and Group Normalization (GN) <ref type="bibr" target="#b5">[6]</ref>, are unable to guarantee far distances from elimina-arXiv:1903.10520v2 [cs.CV] 9 Aug 2020 tion singularities. This might be the reason for their inferior performance compared with BN in large-batch training. To bring the above two success factors into micro-batch training, we propose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to improve network training. WS standardizes the weights in convolutional layers, i.e., making the weights have zero mean and unit variance. BCN uses estimated means and variances of the activations in convolutional layers by combining batch and channel normalization. WS and BCN are able to run in both largebatch and micro-batch settings and accelerate the training and improve the performances.</p><p>We study WS and BCN from both theoretical and experimental viewpoints. The highlights of the results are: 1) Theoretically, we prove that WS reduces the Lipschitz constants of the loss and the gradients. Hence, WS smooths loss landscape and improves training. 2) We empirically show that WS and BCN are able to push the models away from the elimination singularities. 3) Experiments show that on tasks where large-batches are available (e.g. ImageNet <ref type="bibr" target="#b8">[9]</ref>), GN <ref type="bibr" target="#b5">[6]</ref> + WS with batch size 1 is able to match or outperform the performances of BN with large batch sizes ( <ref type="figure" target="#fig_0">Fig. 1</ref>). 4) For tasks where only micro-batch training is available (e.g. COCO <ref type="bibr" target="#b9">[10]</ref>), GN + WS will significantly improve the performances <ref type="figure" target="#fig_0">(Fig. 1</ref>). 5) Replacing GN with BCN further improves the results in both large-batch and micro-batch training settings.</p><p>To show that our WS and BCN are applicable to many vision tasks, we conduct comprehensive experiments, including image classification on CIFAR-10/100 <ref type="bibr" target="#b10">[11]</ref> and Im-ageNet dataset <ref type="bibr" target="#b8">[9]</ref>, object detection and instance segmentation on MS COCO dataset <ref type="bibr" target="#b9">[10]</ref>, video recognition on Something-SomethingV1 dataset <ref type="bibr" target="#b11">[12]</ref>, and semantic image segmentation on PASCAL VOC <ref type="bibr" target="#b13">[13]</ref>. The experimental results show that our WS and BCN are able to accelerate training and improve performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Deep neural networks advance state-of-the-arts in many computer vision tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>. But deep networks are hard to train. To speed up training, proper model initialization strategies are widely used as well as data normalization based on the assumption of the data distribution <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>. On top of data normalization and model initialization, Batch Normalization <ref type="bibr" target="#b2">[3]</ref> is proposed to ensure certain distributions so that the normalization effects will not fade away during training. By performing normalization along the batch dimension, Batch Normalization achieves state-of-the-art performances in many tasks in addition to accelerating the training process. When the batch size decreases, however, the performances of Batch Normalization drop dramatically since the batch statistics are not representative enough of the dataset statistics. Unlike Batch Normalization that works on the batch dimension, Layer Normalization <ref type="bibr" target="#b6">[7]</ref> normalizes data on the channel dimension, Instance Normalization <ref type="bibr" target="#b26">[26]</ref> does Batch Normalization for each sample individually. Group Normalization <ref type="bibr" target="#b5">[6]</ref> also normalizes features on the channel dimension, but it finds a better middle point between Layer Normalization and Instance Normalization.</p><p>Batch Normalization, Layer Normalization, Group Normalization, and Instance Normalization are all activationbased normalization methods. Besides them, there are also weight-based normalization methods, such as Weight Normalization <ref type="bibr" target="#b27">[27]</ref> and Centered Weight Normalization <ref type="bibr" target="#b28">[28]</ref>. Weight Normalization decouples the length and the direction of the weights, and Centered Weight Normalization also centers the weights to have zero mean. Weight Standardization is similar, but removes the learnable weight length. Instead, the weights are standardized to have zero mean and unit variance, and then directly sent to the convolution operations. When used with GN, it narrows the performance gap between BN and GN.</p><p>In this paper, we study normalization from the perspective of elimination singularity <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref> and smoothness <ref type="bibr" target="#b7">[8]</ref>.</p><p>There are also other perspectives to understand normalization methods. For example, from training robustness, BN is able to make optimization trajectories more robust to parameter initialization <ref type="bibr" target="#b31">[31]</ref>. <ref type="bibr" target="#b7">[8]</ref> shows that normalizations are able to reduce the Lipschitz constants of the loss and the gradients, thus the training becomes easier and faster. From the angle of model generalization, <ref type="bibr" target="#b32">[32]</ref> shows that Batch Normalization relies less on single directions of activations, thus has better generalization properties, and <ref type="bibr" target="#b33">[33]</ref> studies the regularization effects of Batch Normalization. <ref type="bibr" target="#b34">[34]</ref> also explores length-direction decoupling in BN and WN <ref type="bibr" target="#b27">[27]</ref>. Other work also approaches normalizations from the gradient explosion issues <ref type="bibr" target="#b35">[35]</ref> and learning rate tuning <ref type="bibr" target="#b36">[36]</ref>. Our WS is also related to converting constrained optimization to unconstrained optimization <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>.</p><p>Our BCN uses Batch Normalization and Group Normalization at the same time for one layer. Some previous work also uses multiple normalizations or a combined version of normalizations for one layer. For example, SN <ref type="bibr" target="#b39">[39]</ref> computes BN, IN, and LN at the same time and uses AutoML <ref type="bibr" target="#b40">[40]</ref> to determine how to combine them. SSN <ref type="bibr" target="#b41">[41]</ref> uses SparseMax to get sparse SN. DN <ref type="bibr" target="#b42">[42]</ref> proposes a more flexible form to represent normalizations and finds better normalizations. Unlike them, our method is based on analysis and theoretical understandings instead of searching solutions through AutoML, and our normalizations are used together as a composite function rather than linearly adding up the normalization effects in a flat way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LIPSCHITZ SMOOTHNESS AND ELIMINATION SINGULARITIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We first describe Lipschitz Smoothness and Elimination</head><p>Singularities to provide the background of our analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lipschitz Smoothness</head><formula xml:id="formula_0">A function f : A → R m , A ∈ R n is L-Lipschitz [43] if ∀a, b ∈ A : ||f (a) − f (b)|| ≤ L||a − b||.<label>(1)</label></formula><p>A continuously differentiable function f is β-smooth if the gradient ∇f is β-Lipschitz, i.e.,</p><formula xml:id="formula_1">∀a, b ∈ A : ||∇f (a) − ∇f (b)|| ≤ β||a − b||.<label>(2)</label></formula><p>Many results show that training smooth functions using gradient descent algorithms is faster than training nonsmooth functions <ref type="bibr" target="#b44">[44]</ref>. Intuitively, gradient descent based training algorithms can be unstable due to exploding or vanishing gradients. As a result, they are sensitive to the selections of the learning rate and initialization if the loss landscape is not smooth. Using an algorithm (e.g. WS) that smooths the loss landscape will make the gradients more reliable and predictive; thus, larger steps can be taken and the training will be accelerated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Elimination singularities</head><p>Deep neural networks are hard to train partly due to the singularities caused by the non-identifiability of the model <ref type="bibr" target="#b30">[30]</ref>. These singularities include overlap singularities, linear dependence singularities, elimination singularities, etc. Degenerate manifolds in the loss landscape will be caused by these singularities, getting closer to which will slow down learning and impact model performances <ref type="bibr" target="#b29">[29]</ref>. In this paper, we focus on elimination singularities, which correspond to the points on the training trajectory where neurons in the model become constantly deactivated.</p><p>The original definition of elimination singularities is based on weights <ref type="bibr" target="#b30">[30]</ref>: if we use ω c to denote the weights that take the channel c as input, then an elimination singularity is encountered when ω c = 0. However, this definition is not suitable for real-world deep network training as most of ω c will not be close to 0. For example, in a ResNet-50 <ref type="bibr" target="#b1">[2]</ref> well-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, 1 L l min c∈l |ωc|1 avg c∈l |ωc|1 = 0.55, where L is the number of all the layers l in the network. Note that weight decay is already used in training this network to encourage weight sparsity. In other words, defining elimination singularities based on weights is not proper for networks trained in real-world settings.</p><p>In this paper, we consider elimination singularities for networks that use ReLU as their activation functions. We focus on a basic building element that is widely used in neural networks: a convolutional layer followed by a normalization method (e.g. BN, LN) and ReLU <ref type="bibr" target="#b45">[45]</ref>, i.e., X out = ReLU(Norm(Conv(X in ))).</p><p>(</p><p>When ReLU is used, ω c = 0 is no longer necessary for a neuron to be eliminatable. This is because ReLU sets any values below 0 to 0; thus a neuron is constantly deactivated if its maximum value after the normalization layer is below 0. Their gradients will also be 0 because of ReLU, making them hard to revive; hence, a singularity is created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WEIGHT STANDARDIZATION</head><p>In this section, we introduce Weight Standardization, which is inspired by BN. It has been demonstrated that BN influences network training in a fundamental way: it makes the landscape of the optimization problem significantly smoother <ref type="bibr" target="#b7">[8]</ref>. Specifically, <ref type="bibr" target="#b7">[8]</ref> shows that BN reduces the Lipschitz constants of the loss function, and makes the gradients more Lipschitz, too, i.e., the loss will have a better β-smoothness <ref type="bibr" target="#b43">[43]</ref>. We notice that BN considers the Lipschitz constants with respect to activations, not the weights that the optimizer is directly optimizing. Therefore, we argue that we can also standardize the weights in the convolutional layers to further smooth the landscape. By doing so, we do not have to worry about transferring smoothing effects from activations to weights; moreover, the smoothing effects on activations and weights are also additive. Based on these motivations, we propose Weight Standardization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Weight Standardization</head><p>Here, we show the detailed modeling of Weight Standardization (WS) ( <ref type="figure" target="#fig_1">Fig. 2</ref>). Consider a standard convolutional layer with its bias term set to 0:</p><formula xml:id="formula_3">y =Ŵ * x,<label>(4)</label></formula><p>whereŴ ∈ R O×I denotes the weights in the layer and * denotes the convolution operation. ForŴ ∈ R O×I , O is the number of the output channels, I corresponds to the number of input channels within the kernel region of each output channel. Taking <ref type="figure" target="#fig_1">Fig. 2</ref> as an example, O = C out and I = C in × Kernel Size. In Weight Standardization, instead of directly optimizing the loss L on the original weightsŴ , we reparameterize the weightsŴ as a function of W , i.e., W = WS(W ), and optimize the loss L on W by SGD:</p><formula xml:id="formula_4">W = Ŵ i,j Ŵ i,j = W i,j − µ Wi,· σ Wi,· ,<label>(5)</label></formula><formula xml:id="formula_5">y =Ŵ * x,<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">µ Wi,· = 1 I I j=1 W i,j , σ Wi,· = 1 I I j=1 W 2 i,j − µ 2 Wi,· + .<label>(7)</label></formula><p>Similar to BN, WS controls the first and second moments of the weights of each output channel individually in convolutional layers. Note that many initialization methods also initialize the weights in some similar ways. Different from those methods, WS standardizes the weights in a differentiable way which aims to normalize gradients during back-propagation. Note that we do not have any affine transformation onŴ . This is because we assume that normalization layers such as BN or GN will normalize this convolutional layer again, and having affine transformation will confuse and slow down training. In the following, we first discuss the normalization effects of WS to the gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparing WS with WN and CWN</head><p>Weight Normalization (WN) <ref type="bibr" target="#b27">[27]</ref> and Centered Weight Normalization (CWN) <ref type="bibr" target="#b28">[28]</ref> also normalize weights to speed up deep network training. Weight Normalization reparameterizes weights by separating the direction W W and length g:</p><formula xml:id="formula_7">W = g W W .<label>(8)</label></formula><p>WN is able to train good models on many tasks. But as shown in <ref type="bibr" target="#b46">[46]</ref>, WN has difficulty matching the performances of models trained with BN on large-scale datasets. Later, CWN adds a centering operation for WN, i.e.,</p><formula xml:id="formula_8">W = g W − W W − W .<label>(9)</label></formula><p>To compare with WN and CWN, we consider the weights for only one of the output channel and reformulate the corresponding weights output by WS in Eq. 5 aŝ</p><formula xml:id="formula_9">W = W − W W 2 − W 2 ,<label>(10)</label></formula><p>which removes the learnable length g from Eq. 9 and divides the weights with their standard deviation instead. Experiments in Sec. 8 show that WS outperforms WN and CWN on large-scale tasks <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE SMOOTHING EFFECTS OF WS</head><p>In this section, we discuss the smoothing effects of WS. Sec. 5.1 shows that WS normalizes the gradients. This normalization effect on gradients lowers the Lipschitz constants of the loss and the gradients as will be shown in Sec. 5.2, where Sec. 5.2.1 discusses the effects on the loss and Sec. 5.2.2 discusses the effects on the gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">WS normalizes gradients</head><p>For convenience, we set = 0 (in Eq. 7). We first focus on one output channel c. Let y c ∈ R b be all the outputs of channel c during one pass of feedforwarding and backpropagation, and x c ∈ R b,I be the corresponding inputs. Then, we can rewrite Eq. 5 and 6 aṡ</p><formula xml:id="formula_10">W c,· = W c,· − 1 I 1 1, W c,· ,<label>(11)</label></formula><formula xml:id="formula_11">W c,· =Ẇ c,· / 1 I 1,Ẇ •2 c,· ,<label>(12)</label></formula><formula xml:id="formula_12">y c = x cŴc,· ,<label>(13)</label></formula><p>where , denotes dot product and •2 denotes Hadamard power. Then, the gradients are In Eq. 14, to compute ∇Ẇ c,· L, ∇Ŵ c,· L is first subtracted by a weighted average of ∇Ŵ c,· L and then divided by σŴ c,· . Note that when BN is used to normalize this convolutional layer, as BN will compute again the scaling factor σ u , the effects of dividing the gradients by σŴ c,· will be canceled in both feedforwarding and back-propagation. As for the additive term, its effect will depend on the statistics of ∇Ŵ c,· L andŴ c,· . We will later show that this term will reduce the gradient norm regardless of the statistics. As for Eq. 15, it zero-centers the gradients fromẆ c,· . When the mean gradient is large, zero-centering will significantly affect the gradients passed to W c,· .</p><formula xml:id="formula_13">∇Ẇ c,· L = 1 σ Wc,· ∇Ŵ c,· L − 1 I Ŵ c,· , ∇Ŵ c,· L Ŵ c,· ,<label>(14)</label></formula><formula xml:id="formula_14">∇ Wc,· L = ∇Ẇ c,· L − 1 I 1 1, ∇Ẇ c,· L .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">WS smooths landscape</head><p>We will show that WS is able to make the loss landscape smoother. Specifically, we show that optimizing L on W has smaller Lipschitz constants on both the loss and the gradients than optimizing L onŴ .</p><formula xml:id="formula_15">Lipschitz constant of a function f is the value of L if f satisfies |f (x 1 ) − f (x 2 )| ≤ L x 1 − x 2 , ∀x 1 , x 2 .</formula><p>For the loss and gradients, f will be L and ∇ W L, and x will be W . Smaller Lipschitz constants on the loss and gradients mean that the changes of the loss and the gradients during training will be bounded more. They will provide more confidence when the optimizer takes a big step in the gradient direction as the gradient direction will vary less within the range of the step. In other words, the optimizer can take longer steps without worrying about sudden changes of the loss landscape and gradients. Therefore, WS is able to accelerate training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Effects of WS on the Lipschitz constant of the loss</head><p>Here, we show that both Eq. 14 and Eq. 15 are able to reduce the Lipschitz constant of the loss. We first study Eq. 14:</p><formula xml:id="formula_16">∇Ẇ c,· L 2 = 1 σ 2 Wc,· ∇Ŵ c,· L 2 + 1 I 2 Ŵ c,· , ∇Ŵ c,· L 2 Ŵ c,· ,Ŵ c,· − 2I .<label>(16)</label></formula><p>By Eq. 12, we know that Ŵ c,· 2 = I. Then,</p><formula xml:id="formula_17">∇Ẇ c,· L 2 = 1 σ 2 Wc,· ∇Ŵ c,· L 2 − 1 I Ŵ c,· , ∇Ŵ c,· L 2 .<label>(17)</label></formula><p>Since we assume that this convolutional layer is followed by a normalization layer such as BN or GN, the effect of 1/σ 2  Next, we study the effect of Eq. 15. By definition,</p><formula xml:id="formula_18">∇ Wc,· L 2 = ∇Ẇ c,· L 2 − 1 I 1, ∇Ẇ c,· L 2 .<label>(18)</label></formula><p>By Eq. 14, we rewrite the second term:</p><formula xml:id="formula_19">1 I 1, ∇Ẇ c,· L 2 = 1 I · σ 2 Wc,· 1, ∇Ŵ c,· L − 1 I Ŵ c,· , ∇Ŵ c,· L · 1,Ŵ c,· 2 .<label>(19)</label></formula><p>Since 1,Ŵ c,· = 0, we have</p><formula xml:id="formula_20">∇ Wc,· L 2 = ∇Ẇ c,· L 2 − 1 I · σ 2 Wc,· 1, ∇Ŵ c,· L 2 . (20)</formula><p>Summarizing the effects of Eq. 14 and 15 on the Lipschitz constant of the loss: ignoring 1/σ 2 Wc,· , Eq. 14 reduces it by 1 I Ŵ c,· , ∇Ŵ c,· L 2 , and Eq. 15 reduces it by 1 I 1, ∇Ŵ c,· L 2 . Although both Eq. 14 and 15 reduce the Lipschitz constant, their real effects depend on the statistics of the weights and the gradients. For example, the reduction effect of Eq. 15 depends on the average gradients onŴ . As for Eq. 14, note that 1,Ŵ c,· = 0, its effect might be limited whenŴ c,· is evenly distributed around 0. To understand their real effects, we conduct a case study on ResNet-50 trained on ImageNet to see which one of Eq. 11 and 12 has bigger effects or they contribute similarly to smoothing the landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Effects of WS on the Lipschitz constant of gradients</head><p>Before the Lipschitzness study on the gradients, we first show a case study where we train ResNet-50 models on ImageNet following the conventional training procedure <ref type="bibr" target="#b1">[2]</ref>. In total, we train four models, including ResNet-50 with GN, ResNet-50 with GN+Eq. 11, ResNet-50 with GN+Eq. 12 and ResNet-50 with GN+Eq. 11&amp;12. The training dynamics are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, from which we observe that Eq. 12 slightly improves the training speed and performances of models with or without Eq. 11 while the major improvements are from Eq. 11. This observation motivates us to study the real effects of Eq. 11 and 12 on the Lipschitz constant of the loss. To investigate this, we take a look at the values of</p><formula xml:id="formula_21">1 I Ŵ c,· , ∇Ŵ c,· L 2 , and 1 I 1, ∇Ŵ c,· L 2 during training.</formula><p>To compute the two values above, we gather and save the intermediate gradients ∇Ŵ c,· L, and the weights for the convolutionŴ c,· . In total, we train ResNet-50 with GN, Eq. 11 and 12 for 90 epochs, and we save the gradients and the weights of the first training iteration of each epoch. The right figure of <ref type="figure" target="#fig_2">Fig. 4</ref> shows the average percentages of 1</p><formula xml:id="formula_22">I Ŵ c,· , ∇Ŵ c,· L 2 , 1 I 1, ∇Ŵ c,· L 2 , and σ 2 Wc,· ∇ Wc,· L 2 . From the right figure we can see that 1 I Ŵ c,· , ∇Ŵ c,· L 2 is</formula><p>small compared with other two components (&lt; 0.02). In other words, although Eq. 12 decreases the gradient norm regardless of the statistics of the weights and gradients, its real effect is limited due to the distribution ofŴ c,· and ∇Ŵ c,· L. Nevertheless, from the left figures we can see that Eq. 12 still improves the training. Since Eq. 12 requires very little computations, we will keep it in WS.</p><p>From the experiments above, we observe that the training speed boost is mainly due to Eq. 11. As the effect of Eq. 12 is limited, in this section, we only study the effect of Eq. 11 on the Hessian of W c,· andẆ c,· . Here, we will show that Eq. 11 decreases the Frobenius norm of the Hessian matrix of the weights, i.e., ∇ 2</p><p>Wc,· L F ≤ ∇ 2Ẇ c,· L F . With smaller Frobenius norm, the gradients of W c,· are more predictable, thus the loss is smoother and easier to optimize.</p><p>We use H andḢ to denote the Hessian matrices of W c,· andẆ c,· , respectively, i.e.,</p><formula xml:id="formula_23">H i,j = ∂ 2 L ∂W c,i ∂W c,j ,Ḣ i,j = ∂ 2 L ∂Ẇ c,i ∂Ẇ c,j .<label>(21)</label></formula><p>We first derive the relationship between H i,j andḢ i,j :</p><formula xml:id="formula_24">H i,j =Ḣ i,j − 1 I I k=1 (Ḣ i,k +Ḣ k,j ) + 1 I 2 I p=1 I q=1Ḣ p,q .<label>(22)</label></formula><p>Note that</p><formula xml:id="formula_25">I i=1 I j=1 H i,j = 0.<label>(23)</label></formula><p>Therefore, Eq. 11 not only zero-centers the feedforwarding outputs and the back-propagated gradients, but also the Hessian matrix. Next, we compute its Frobenius norm:</p><formula xml:id="formula_26">H F = I i=1 I j=1 H 2 i,j = Ḣ F + 1 I 2 I i=1 I j=1Ḣ i,j 2 − 1 I I i=1 I j=1Ḣ i,j 2 − 1 I I j=1 I i=1Ḣ i,j 2 ≤ Ḣ F − 1 I 2 I i=1 I j=1Ḣ i,j 2 .<label>(24)</label></formula><p>As shown in Eq. 24, Eq. 11 reduces the Frobenius norm of the Hessian matrix by at least</p><formula xml:id="formula_27">I i=1 I j=1Ḣ i,j 2 /I 2 ,</formula><p>which makes the gradients more predictable than directly optimizing on the weights of the convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Connections to constrained optimization</head><p>WS imposes constraints to the weightŴ c,· such that</p><formula xml:id="formula_28">I i=1Ŵ c,i = 0, I i=1Ŵ 2 c,i = I, ∀c.<label>(25)</label></formula><p>Therefore, an alternative to the proposed WS is to consider the problem as constrained optimization and uses Projected Gradient Descent (PGD) to find the solution. The update rule for PGD can be written aŝ</p><formula xml:id="formula_29">W t+1 c,i = Proj Ŵ t c,i − · ∇Ŵ t c,i L<label>(26)</label></formula><p>where Proj(·) denotes the projection function and denotes the learning rate. To satisfy Eq. 25, Proj(·) standardizes its input. We can approximate the right hand side of Eq. 26 by minimizing the Lagrangian of the loss function L, which obtainŝ</p><formula xml:id="formula_30">W t+1 c,i ≈Ŵ t c,i − ∇Ŵ t c,i L − Ŵ c,· , ∇Ŵ c,· L Ŵ c,i (27) − 1 I 1, ∇Ŵ c,· L Different from Eq. 26, the update rule of WS iŝ W t+1 c,i =Proj W t c,i − · ∇ W t c,i L =Proj W t c,i − σ Wc,· ∇Ŵ c,i L − 1 I Ŵ c,· , ∇Ŵ c,· L Ŵ c,i − 1 I 1, ∇Ŵ c,· L − I 2 · σ Wc,· 1, Ŵ c,· , ∇Ŵ c,· L Ŵ c,· .<label>(28)</label></formula><p>Eq. 28 is more complex than Eq. 27, but the increased complexity is neglectable compared with training deep networks. For simplicity, Eq. 28 reuses Proj to denote the standardization process, despite that WS uses Stochastic Gradient Descent instead of Projected Gradient Descent to optimize the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">WS'S EFFECTS ON ELIMINATION SINGULARITIES</head><p>In this section, we will provide the background of BN, GN and LN, discuss the negative correlation between the performance and the distance to elimination singularities, and show LN and GN are unable to keep the networks away from elimination singularities as BN. Next, we will show that WS helps avoiding elimination singularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Batch-and channel-based normalizations and their effects on elimination singularities</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Batch-and channel-based normalizations</head><p>Based on how activations are normalized, we group the normalization methods into two types: batch-based normalization and channel-based normalization, where the batchbased normalization method corresponds to BN and the channel-based normalization methods include LN and GN.</p><p>Suppose we are going to normalize a 2D feature map X ∈ R B×C×H×W , where B is the batch size, C is the number of channels, H and W denote the height and the width. For each channel c, BN normalizes X by</p><formula xml:id="formula_31">Y ·c·· = X ·c·· − µ ·c·· σ ·c·· ,<label>(29)</label></formula><p>where µ ·c·· and σ ·c·· denote the mean and the standard deviation of all the features of the channel c, X ·c·· . Throughout the paper, we use · in the subscript to denote all the features along that dimension for convenience. Unlike BN which computes statistics on the batch dimension in addition to the height and width, channel-based normalization methods (LN and GN) compute statistics on the channel dimension. Specifically, they divide the channels to several groups, and normalize each group of channels, i.e., X is reshaped asẊ ∈ R B×G×C/G×H×W , and then:</p><formula xml:id="formula_32">Y bg··· =Ẋ bg··· − µ bg··· σ bg··· ,<label>(30)</label></formula><p>for each sample b of B samples in a batch and each channel group g out of all G groups. After Eq. 30, the outputẎ is reshaped asẊ and denoted by Y . Both batch-and channel-based normalization methods have an optional affine transformation, i.e.,</p><formula xml:id="formula_33">Z ·c·· = γ c Y ·c·· + β c .<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">BN avoids elimination singularities</head><p>Here, we study the effect of BN on elimination singularities.</p><p>Since the normalization methods all have an optional affine transformation, we focus on the distinct part of BN, which normalizes all channels to zero mean and unit variance, i.e., E y∈Y·c·· y = 0, E y∈Y·c·· y 2 = 1, ∀c.</p><p>As a result, regardless of the weights and the distribution of the inputs, it guarantees that the activations of each channel are zero-centered with unit variance. Therefore, each channel cannot be constantly deactivated because there are always some activations that are &gt; 0, nor underrepresented due to the channel having a very small activation scale compared with the others. We ask this question because this is similar to what happens in channel-normalized models. In the context of activation-based normalizations, BN completely resolve the issue of elimination singularities as each channel is zerocentered with unit variance. By contrast, channel-based normalization methods, as they do not have batch information, are unable to make sure that all neurons have zero mean and unit variance after normalization. In other words, there are likely some underrepresented channels after training if the model is using channel-based normalizations. Since BN represents the ideal case which has the furthest distance to elimination singularities, and any dissimilarity with BN will lead to lightly or heavily underrepresented channels and thus make the models closer to singularities, we use the distance to BN as the distance to singularities for activation-based normalizations. Specifically, in this definition, the model is closer to singularities when it is far from BN. <ref type="figure" target="#fig_3">Fig. 5</ref> shows that this definition is useful, where we study the relationship between the performance and the distance to singularities (i.e., how far from BN) caused by statistical differences. We conduct experiments on a 4-layer convolutional network, the results of which are shown in <ref type="figure" target="#fig_3">Fig 5.</ref> Each convolutional layer has 32 output channels, and is followed by an average pooling layer which down-samples the features by a factor of 2. Finally, a global average pooling layer and a fullyconnected layer output the logits for Softmax. The experiments are done on CIFAR-10 <ref type="bibr" target="#b10">[11]</ref>.</p><p>In the experiment, each channel c will be normalized to a pre-defined meanμ c and a pre-defined varianceσ c that are drawn from two distributions, respectively:</p><formula xml:id="formula_35">µ c ∼ N (0, σ µ ) andσ c = eσ c whereσ c ∼ N (0, σ σ ). (33)</formula><p>The model will be closer to singularities when σ µ or σ σ increases. BN corresponds to the case where σ µ = σ σ = 0. After gettingμ c andσ c for each channel, we compute</p><formula xml:id="formula_36">Y ·c·· = γ c σ c X ·c·· − µ ·c·· σ ·c·· +μ c + β c .<label>(34)</label></formula><p>Note thatμ c andσ c are fixed during training while γ c and β c are trainable parameters in the affine transformation. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the experimental results. When σ µ and σ σ are closer to the origin, the normalization method is more close to BN. When their values increase, we observe performance decreases. For extreme cases, we also observe training failures. These results indicate that although the affine transformation theoretically can find solutions that cancel the negative effects of normalizing channels to different statistics, their capability is limited by the gradient-based training. They show that defining distance to singularities as the distance to BN is useful. They also raise concerns about channel normalizations regarding their distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Statistics in Channel-based Normalization</head><p>Following our concerns about channel-based normalization and their distance to singularities, we study the statistical differences between channels when they are normalized by a channel-based normalization such as GN or LN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical differences in GN, LN and WS:</head><p>We train a ResNet-110 <ref type="bibr" target="#b1">[2]</ref> on CIFAR-10 <ref type="bibr" target="#b10">[11]</ref> normalized by GN, LN, with and without WS. During training, we keep record of the running mean µ r c and variance σ r c of each channel c after convolutional layers. For each group g of the channels that are normalized together, we compute their channel statistical difference defined as the standard deviation of their means divided by the mean of their standard deviations, i.e.,</p><formula xml:id="formula_37">StatDiff(g) = E c∈g (µ r c ) 2 − E c∈g µ r c 2 E c∈g σ c .<label>(35)</label></formula><p>We plot the average statistical differences of all the groups after every training epoch as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. By Eq. 35, StatDiff(g) ≥ 0, ∀g. In BN, all their means are the same, as well as their variances, thus StatDiff(g) = 0. As the value of StatDiff(g) goes up, the differences between channels within a group become larger. Since they will be normalized together as in Eq. 30, large differences will inevitably lead to underrepresented channels. <ref type="figure" target="#fig_6">Fig. 7</ref> plots 3 examples of 2 channels before and after normalization in  Eq. 30. Compared with those examples, it is clear that the models in <ref type="figure" target="#fig_4">Fig. 6</ref> have many underrepresented channels. Why GN performs better than LN: <ref type="figure" target="#fig_4">Fig. 6</ref> also provides explanations why GN performs better than LN. Comparing GN and LN, the major difference is their numbers of groups for channels: LN has only one group for all the channels in a layer while GN collects them into several groups. A strong benefit of having more than one group is that it guarantees that each group will at least have one neuron that is not suppressed by the others from the same group. Therefore, GN provides a mechanism to prevent the models from getting too close to singularities. <ref type="figure" target="#fig_4">Fig. 6</ref> also shows the statistical differences when WS is used. From the results, we can clearly see that WS makes StatDiff much closer to 0. Consequently, the majority of the channels are not underrepresented in WS: most of them are frequently activated and they are at similar activation scales. This makes training with WS easier and their results better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">WS helps avoiding elimination singularities</head><p>The above discussions show that WS helps keeping models away from elimination singularities. Here, we discuss why WS is able to achieve this. Recall that WS adds constraints to the weight W ∈ R O×I of a convolutional layer with O output channels and I inputs such that ∀c,</p><formula xml:id="formula_38">I i=1 W c,i = 0, I i=1 W 2 c,i = 1.<label>(36)</label></formula><p>With the constraints of WS, µ out c and σ out c become</p><formula xml:id="formula_39">µ out c = I i=1 W c,i µ in i , (σ out c ) 2 = I i=1 W 2 c,i (σ in i ) 2 ,<label>(37)</label></formula><p>when we follow the assumptions in Xavier initialization <ref type="bibr" target="#b24">[24]</ref>. When the input channels are similar in their statistics, i.e., µ in</p><formula xml:id="formula_40">i ≈ µ in j , σ in i ≈ σ in j , ∀i, j, µ out c ≈ µ in 1 I i=1 W c,i = 0,<label>(38)</label></formula><formula xml:id="formula_41">(σ out c ) 2 ≈ (σ in 1 ) 2 I i=1 W 2 c,i = (σ in 1 ) 2 .<label>(39)</label></formula><p>In other words, WS can pass the statistical similarities from the input channels to the output channels, all the way from the image space where RGB channels are properly normalized. This is similar to the objective of Xavier initialization <ref type="bibr" target="#b24">[24]</ref> or Kaiming initialization <ref type="bibr" target="#b25">[25]</ref>, except that WS enforces it by reparameterization throughout the entire training process, thus is able to reduce the statistical differences a lot, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>.</p><p>Here, we summarize this subsection. We have shown that channel-based normalization methods, as they do not have batch information, are not able to ensure a far distance from elimination singularities. Without the help of batch information, GN alleviates this issue by assigning channels to more than one group to encourage more activated neurons, and WS adds constraints to pull the channels to be not so statistically different. We notice that the batch information is not hard to collect in reality. This inspires us to equip channel-based normalization with batch information, and the result is Batch-Channel Normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">BATCH-CHANNEL NORMALIZATION</head><p>The previous section discusses elimination singularities and shows WS is able to keep models away from them. To fully address the issue of elimination singularities, we propose Batch-Channel Normalization (BCN). This section presents the definition of BCN, discusses why adding batch statistics to channel normalization is not redundant, and shows how BCN runs in large-batch and micro-batch training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Definition</head><p>Batch-Channel Normalization (BCN) adds batch information and constraints to channel-based normalization methods. Let X ∈ R B×C×H×W be the features to be normalized. Then, the normalization is done as follows. ∀c,</p><formula xml:id="formula_42">X ·c·· = γ b c X ·c·· −μ ĉ σ c + β b c ,<label>(40)</label></formula><p>where the purpose ofμ c andσ c is to make</p><formula xml:id="formula_43">E X ·c·· −μ ĉ σ c = 0 and E X ·c·· −μ ĉ σ c 2 = 1.<label>(41)</label></formula><p>Then,Ẋ is reshaped asẊ ∈ R B×G×C/G×H×W to have G groups of channels. Next, ∀g, b,</p><formula xml:id="formula_44">Y bg··· = γ c gẊ bg··· − µ bg··· σ bg··· + β c g .<label>(42)</label></formula><p>Finally,Ẏ is reshaped back to Y ∈ R B×C×H×W , which is the output of the Batch-Channel Normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Micro-batch BCN</head><p>Input: X ∈ R B×C×H×W , the current estimates ofμ c andσ 2 c , and the update rate r.</p><formula xml:id="formula_45">Output: Normalized Y . 1 Computeμ c ← 1 BHW b,h,w X b,c,h,w ; 2 Computeσ 2 c ← 1 BHW b,h,w X b,c,h,w −μ c 2 ; 3 Updateμ c ←μ c + r(μ c −μ c ); 4 Updateσ 2 c ←σ 2 c + r(σ 2 c −σ 2 c ); 5 NormalizeẊ ·c·· = γ b c X ·c·· −μ ĉ σ c + β b c ; 6 ReshapeẊ toẊ ∈ R B×G×C/G×H×W ; 7 NormalizeẎ bg··· = γ c gẊ bg··· − µ bg··· σ bg··· + β c g ; 8 ReshapeẎ to Y ∈ R B×C×H×W ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Large-and Micro-batch Implementations</head><p>Note that in Eq. 40 and 42, only two statistics need batch information:μ c andσ c , as their values depend on more than one sample. Depending on how we obtain the values ofμ c andσ c , we have different implementations for large-batch and micro-batch training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Large-batch training</head><p>When the batch size is large, estimatingμ c andσ c is easy: we just use a Batch Normalization layer to achieve the function of Eq. 40 and 41. As a result, the proposed BCN can be written as BCN(X) = CN(BN(X)).</p><p>Implementing it is also easy with modern deep learning libraries, which is omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Micro-batch training</head><p>One of the motivations of channel normalization is to allow deep networks to train on tasks where the batch size is limited by the GPU memory. Therefore, it is important for Batch-Channel Normalization to be able to work in the micro-batch training setting. Algorithm 1 shows the feed-forwarding implementation of the micro-batch Batch-Channel Normalization. The basic idea behind this algorithm is to constantly estimate the values ofμ c andσ c , which are initialized as 0 and 1, respectively, and normalize X based on these estimates. It is worth noting that in the algorithm,μ c andσ c are not updated by the gradients computed from the loss function; instead, they are updated towards more accurate estimates of those statistics.</p><p>Step 3 and 4 in Algorithm 1 resemble the update steps in gradient descent; thus, the implementation can also be written in gradient descent by storing the difference ∆μ c and ∆σ c as their gradients. Moreover, we set the update rate r to be the learning rate of trainable parameters.</p><p>Algorithm 1 also raises an interesting question: when researchers study the micro-batch issue of BN before, why not just using the estimates to batch-normalize the features? In fact, <ref type="bibr" target="#b47">[47]</ref> tries a similar idea, but does not fully solve the micro-batch issue: it needs a bootstrap phase to make the estimates meaningful, and the performances are usually  <ref type="bibr" target="#b47">[47]</ref> is that BCN has a channel normalization following the estimate-based normalization. This makes the previously unstable estimate-based normalization stable, and the reduction of Lipschitz constants which speeds up training is also done in the channel-based normalization part, which is also impossible to do in estimate-based normalization. In summary, channel-based normalization makes estimate-based normalization possible, and estimate-based normalization helps channel-based normalization to keep models away from elimination singularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Is Batch-Channel Normalization Redundant?</head><p>Batch-and channel-based normalizations are similar in many ways. Is BCN thus redundant as it normalizes normalized features? Our answer is no. Channel normalizations need batch knowledge to keep the models away from elimination singularities; at the same time, it also brings benefits to the batch-based normalization, including: Batch knowledge without large batches. Since BCN runs in both large-batch and micro-batch settings, it provides a way to utilize batch knowledge to normalize activations without relying on large training batch sizes. Additional non-linearity. Batch Normalization is linear in the test mode or when the batch size is large in training. By contrast, channel-based normalization methods, as they normalize each sample individually, are not linear. They will add strong non-linearity and increase the model capacity.</p><p>Test-time normalization. Unlike BN that relies on estimated statistics on the training dataset for testing, channel normalization normalizes testing data again, thus allows the statistics to adapt to different samples. As a result, channel normalization will be more robust to statistical changes and show better generalizability for unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTAL RESULTS</head><p>In this section, we will present the experimental results of using our proposed Weight Standardization and Batch-Channel Normalization, including image classification on CIFAR-10/100 <ref type="bibr" target="#b10">[11]</ref> and ImageNet <ref type="bibr" target="#b8">[9]</ref>, object detection and instance segmentation on COCO <ref type="bibr" target="#b9">[10]</ref>, video recognition on Something-SomethingV1 dataset <ref type="bibr" target="#b11">[12]</ref>, and semantic segmentation on PASCAL VOC <ref type="bibr" target="#b13">[13]</ref>.   images. It has 1000 categories, each has roughly 1300 training images and exactly 50 validation samples. <ref type="table" target="#tab_3">Table 1</ref> shows the top-1 error rates of ResNet-50 on ImageNet when it is trained with different normalization methods, including Layer Normalization <ref type="bibr" target="#b6">[7]</ref>, Instance Normalization <ref type="bibr" target="#b26">[26]</ref>, Group Normalization <ref type="bibr" target="#b5">[6]</ref> and Batch Normalization. From <ref type="table" target="#tab_3">Table 1</ref>, we can see that when the batch size is limited to 1, GN+WS is able to achieve performances comparable to BN with large batch size. Therefore, we will use GN+WS for micro-batch training because GN shows the best results among all the normalization methods that can be trained with 1 image per GPU. <ref type="table" target="#tab_5">Table 2</ref> shows our major experimental results of WS on the ImageNet dataset <ref type="bibr" target="#b8">[9]</ref>. Note that <ref type="table" target="#tab_5">Table 2</ref> only shows the error rates of ResNet-50 and ResNet-101. This is to compare with the previous work that focus on micro-batch training problem, e.g. Switchable Normalization <ref type="bibr" target="#b39">[39]</ref> and Group Normalization <ref type="bibr" target="#b5">[6]</ref>. We run all the experiments using the official PyTorch implementations of the layers except for SN <ref type="bibr" target="#b39">[39]</ref> which are the performances reported in their paper. This makes sure that all the experimental results are comparable, and our improvements are reproducible. <ref type="table" target="#tab_6">Table 3</ref> compares WS with other weight-based normalization methods including WN and CWN. To show the comparisons, we train the same ResNet-50 normalized by GN on activations with different weight-based normalizations. The code of WN uses the official PyTorch implementation, and the code of CWN is from the official implementation of their github. From the results, we can observe that these normalization methods have different effects on the performances of the models. Compared with WN and CWN, the proposed WS achieves lower top-1 error rate. <ref type="table" target="#tab_8">Table 4</ref> shows the individual effects of Eq. 11 and 12 on training deep neural networks. Consistent with <ref type="figure" target="#fig_2">Fig. 4</ref>, Eq. 11 is the major component that brings performance improvements. These results are also consistent with the theoretical results we have on the Lipschitz analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Image Classification on ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Weight Standardization</head><p>In <ref type="table" target="#tab_9">Table 5</ref>, we also provide the experimental results on ResNeXt <ref type="bibr" target="#b48">[48]</ref>. Here, we show the performance comparisons between ResNeXt+GN and ResNeXt+GN+WS. Note that GN originally did not provide results on ResNeXt. Without tuning the hyper-parameters in the Group Normalization   layers, we use 32 groups for each of them which is the default configuration for ResNet that GN was originally proposed for. ResNeXt-50 and 101 are 32x4d. We train the models for 100 epochs with batch size set to 1 and iteration size set to 32. As the table shows, the performance of GN on training ResNeXt is unsatisfactory: they perform closely to the original ResNets. In the same setting, WS is able to make training ResNeXt a lot easier. <ref type="figure">Fig. 8</ref> shows the training dynamics of ResNet-50 with GN, GN+WS and BCN+WS, and <ref type="table" target="#tab_11">Table 6</ref> shows the top-1 and top-5 error rates of ResNet-50 and ResNet-101 trained with different normalization methods. From the results, we observe that adding batch information to channel-based normalizations strongly improves their accuracy. As a result, GN, whose performances are similar to BN when used with WS, now is able to achieve better results than the BN baselines. And we find improvements not only in the final model accuracy, but also in the training speed. As shown in <ref type="figure">Fig. 8</ref>, we see a big drop of training error rates at each epoch. This demonstrates that the model is now farther from elimination singularities, resulting in an easier and faster learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Batch-Channel Normalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.3">Experiment settings</head><p>Here, we list the hyper-parameters used for getting all those results. For all models, the learning rate is set to 0.1 initially, and is multiplied by 0.1 after every 30 epochs. We use SGD to train the models, where the weight decay is set to 0.0001 and the momentum is set to 0.9. For ResNet-50 with BN or BN+WS, the training batch is set to 256 for 4 GPUs.  set the iteration size to 64, i.e., the gradients are averaged across every 64 iterations and then one step is taken. This is to ensure fair comparisons because by doing so the total numbers of parameter updates are the same even if their batch sizes are different. We train ResNet-50 with different normalization techniques for 90 epochs. For ResNet-101, we set the batch size to 128 because some of the models will use more than 12GB per GPU when setting their batch size to 256. In total, we train all ResNet-101 models for 100 epochs. Similarly, we set the iteration size for models trained with 1 image per GPU to be 32 in order to compensate for the total numbers of parameter updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Image Classification on CIFAR</head><p>CIFAR has two image datasets, CIFAR-10 (C10) and CIFAR-100 (C100). Both C10 and C100 have color images of size 32 × 32. C10 dataset has 10 categories while C100 dataset has 100 categories. Each of C10 and C100 has 50,000 images for training and 10,000 images for testing and the categories are balanced in terms of the number of samples. In all the experiments shown here, the standard data augmentation schemes are used, i.e., mirroring and shifting, for these two datasets. We also standardizes each channel of the datasets for data pre-processing. <ref type="table">Table 7</ref> shows the experimental results that compare our proposed BCN with BN and GN. The results are grouped into 4 parts based on whether the training is large-batch or micro-batch, and whether the dataset is C10 and C100. On C10, our proposed BCN is better than BN on largebatch training, and is better than GN (with or without  WS) which is specifically designed for micro-batch training. Here, micro-batch training assumes the batch size is 1, and RN110 is the 110-layer ResNet <ref type="bibr" target="#b1">[2]</ref> with basic block as the building block. The number of groups here for GN is min{32, (the number of channels)/4}. <ref type="table">Table 8</ref> shows comparisons with more recent normalization methods, Switchable Normalization (SN) <ref type="bibr" target="#b39">[39]</ref> and Dynamic Normalization (DN) <ref type="bibr" target="#b42">[42]</ref> which were tested for a variant of ResNet for CIFAR: ResNet-18. To provide readers with direct comparisons, we also evaluate BCN on ResNet-18 with the group number set to 32 for models that use GN. Again, all the results are organized based on whether they are trained in the micro-batch setting. Based on the results shown in <ref type="table">Table 7</ref> and 8, it is clear that BCN is able to outperform the baselines effortlessly in both large-batch and micro-batch training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Object Detection and Instance Segmentation</head><p>Unlike image classification on ImageNet where we could afford large batch training when the models are not too big, object detection and segmentation on COCO <ref type="bibr" target="#b9">[10]</ref> usually use 1 or 2 images per GPU for training due to the high resolution. Given the good performances of our method on ImageNet which are comparable to the large-batch BN training, we expect that our method is able to significantly improve the performances on COCO because of the training setting.    We use a PyTorch-based Mask R-CNN framework 1 for all the experiments. We take the models pre-trained on ImageNet, fine-tune them on COCO train2017 set, and test them on COCO val2017 set. To maximize the comparison fairness, we use the models we pre-trained on ImageNet instead of downloading the pre-trained models available online. We use 4 GPUs to train the models and apply the learning rate schedules for all models following the practice used in the Mask R-CNN framework our work is based on. We use 1X learning rate schedule for Faster R-CNN and 2X learning rate schedule for Mask R-CNN. For ResNet-50, we use 2 images per GPU to train the models, and for ResNet-101, we use 1 image per GPU because the models cannot fit in 12GB GPU memory. We then adapt the learning rates and the training steps accordingly. The configurations we run use FPN <ref type="bibr" target="#b51">[51]</ref> and a 4conv1fc bounding box head. All the training procedures strictly follow their original settings. <ref type="table" target="#tab_15">Table 9</ref> reports the Average Precision for bounding box (AP b ) and instance segmentation (AP m ) and <ref type="table" target="#tab_3">Table 10</ref> reports the Average Precision (AP) of Faster R-CNN trained with different methods. From the two tables, we can observe results similar to those on ImageNet. GN has limited performance improvements when it is used on more complicated architectures such as ResNet-101 and ResNet-101. But when we add WS to GN or use BCN, we are able to train the models much better. The improvements become more significant when the network complexity increases. Considering nowadays deep networks are becoming deeper and wider, having a normalization technique such as our WS will ease the training a lot without worrying about the memory and batch size issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Semantic Segmentation on PASCAL VOC</head><p>After evaluating BCN and WS on classification and detection, we test it on dense prediction tasks. We start with semantic segmentation on PASCAL VOC <ref type="bibr" target="#b13">[13]</ref>. We choose   on Something-SomethingV1 <ref type="bibr" target="#b11">[12]</ref>.</p><p>DeepLabV3 <ref type="bibr" target="#b53">[53]</ref> as the evaluation model for its good performances and its use of the pre-trained ResNet-101 backbone. <ref type="table" target="#tab_3">Table 11</ref> shows our results on PASCAL VOC, which has 21 different categories with background included. We take the common practice to prepare the dataset, and the training set is augmented by the annotations provided in <ref type="bibr" target="#b54">[54]</ref>, thus has 10,582 images. We take our ResNet-101 pretrained on ImageNet and finetune it for the task. Here, we list all the implementation details for easy reproductions of our results: the batch size is set to 16, the image crop size is 513, the learning rate follows polynomial decay with an initial rate 0.007. The model is trained for 30K iterations, and the multi-grid is (1, 1, 1) instead of <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4)</ref>. For testing, the output stride is set to 16, and we do not use multiscale or horizontal flipping test augmentation. As shown in <ref type="table" target="#tab_3">Table 11</ref>, by only changing the normalization methods from BN and GN to our BCN, mIoU increases by about 2%, which is a significant improvement for PASCAL VOC dataset. As we strictly follow the hyper-parameters used in the previous work, there could be even more room of improvements if we tune them to favor BCN or WS, which we do not explore in this paper and leave to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Video Recognition on Something-Something</head><p>In this subsection, we show the results of applying our method on video recognition on Something-SomethingV1 dataset <ref type="bibr" target="#b11">[12]</ref>. Something-SomethingV1 is a video dataset which includes a large number of video clips that show humans performing pre-defined basic actions. The dataset has 86,017 clips for training and 11,522 clips for validation.</p><p>We use the state-of-the-art method TSM <ref type="bibr" target="#b56">[55]</ref> for video recognition, which uses a ResNet-50 with BN as its backbone network. The codes are based on TRN <ref type="bibr" target="#b57">[56]</ref> and then adapted to TSM. The reimplementation is different from the original TSM <ref type="bibr" target="#b56">[55]</ref>: we use models pre-trained on ImageNet rather than Kinetics dataset <ref type="bibr" target="#b58">[57]</ref> as the starting points. Then, we fine-tune the pre-trained models on Something-SomethingV1 for 45 epochs. The batch size is set to 32 for 4 GPUs, and the learning rate is initially set to 0.0125, then divided by 10 at the 26th and the 36th epochs. The batch normalization layers are not fixed during training. With all the changes, the reimplemented TSM-BN achieves top-1/5 accuracy 44.30/74.53, higher than 43.4/73.2 originally reported in the paper.</p><p>Then, we compare the performances when different normalization methods are used in training TSM. <ref type="table" target="#tab_3">Table 12</ref> shows the top-1/5 accuracies of TSM when trained with GN, GN+WS, BN and BN+WS. From the table we can see that WS increases the top-1 accuracy about 2% for both GN and BN. The improvements help GN to cache up the performances of BN, and boost BN to even better accuracies, which roughly match the performances of the ensemble TSM with 24 frames reported in the paper. Despite that BCN improves performances of GN, it does not surpass BN. This shows the limitation of BCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this paper, we proposed two novel normalization methods, Weight Standardization (WS) and Batch-Channel Normalization (BCN) to bring the success factors of Batch Normalization (BN) into micro-batch training, including 1) the smoothing effects on the loss landscape and 2) the ability to avoid harmful elimination singularities along the training trajectory. WS standardizes the weights in convolutional layers and BCN leverages estimated batch statistics of the activations in convolutional layers. We provided theoretical analysis to show that WS reduces the Lipschitz constants of the loss and the gradients, and thus it smooths the loss landscape. By investigating normalization methods from the perspective of elimination singularities, we found that channel-based normalization methods, such as Layer Normalization (LN) and Group Normalization (GN) are unable to keep far distances from elimination singularities, caused by lack of batch knowledge. We showed that WS is able to alleviate this issue and BCN can further push models away from elimination singularities by incorporating estimated batch statistics channel-normalized models. Experiments on comprehensive computer vision tasks, including image classification, object detection, instance segmentation, video recognition and semantic segmentation, demonstrate 1) WS and BCN improve micro-batch training significantly, 2) WS+GN with batch size 1 is even able to match or outperform the performances of BN with large batch sizes, and 3) replacing GN by BCN leads to further improvement. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparing BN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Comparing normalization methods on activations (blue) and Weight Standardization (orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Training ResNet-50 on ImageNet with GN, Eq. 11 and 12. The left and the middle figures show the training dynamics. The right figure shows the reduction percentages on the Lipschitz constant. Note that the y-axis of the right figure is in log scale. norm is the reduction 1 I Ŵ c,· , ∇Ŵ c,· L 2 , which reduces the Lipschitz constant of the loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Model accuracy and distance to singularities. Larger circles correspond to higher performances. Red crosses represent failure cases (accuracy &lt; 70%). Circles are farther from singularities/closer to BN if they are closer to the origin.6.1.3 Statistical distance and its affects on performanceBN avoids singularities by normalizing each channel to zero mean and unit variance. What if they are normalized to other means and variances?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Means and standard deviations of the statistical differences (StatDiff defined in Eq. 35) of all layers in a ResNet-110 trained on CIFAR-10 with GN, GN+WS, LN, and LN+WS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Examples of normalizing two channels in a group when they have different means and variances. Transparent bars mean they are 0 after ReLU. StatDiff is defined in Eq. 35.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Huiyu</head><label></label><figDesc>Wang is a Ph.D. student in Computer Science at Johns Hopkins University, advised by Bloomberg Distinguished Professor Alan Yuille. He received M.S. in Electrical Engineering at University of California, Los Angeles in 2017 and B.S. in Information Engineering at Shanghai Jiao Tong University in 2015. He also spent wonderful summers at Google, Allen Institute for Artificial Intelligence(AI2), and TuSimple. His research interests are computer vision and machine learning. Chenxi Liu is a Ph.D. student at Johns Hopkins University, where his advisor is Bloomberg Distinguished Professor Alan Yuille. Before that, he received M.S. at University of California, Los Angeles and B.E. at Tsinghua University. He has also spent time at Facebook, Google, Adobe, Toyota Technological Institute at Chicago, University of Toronto, and Rice University. His research lies in computer vision and natural language processing. Wei Shen received his B.S. and Ph.D. degree both in Electronics and Information Engineering from the Huazhong University of Science and Technology, Wuhan, China, in 2007 and in 2012.From April 2011 to November 2011, he worked in Microsoft Research Asia as an intern. In 2012, he joined the School of Communication and Information Engineering, Shanghai University and served as an assistant and associate professor until Oct 2018. He is currently an Assistant Research Professor at the Department of Computer Science, Johns Hopkins University. His current research interests include computer vision, deep learning and biomedical image analysis. Alan Yuille received his B.A. in mathematics from the University of Cambridge in 1976, and completed his Ph.D. in theoretical physics at Cambridge in 1980. He then held a postdoctoral position with the Physics Department, University of Texas at Austin, and the Institute for Theoretical Physics, Santa Barbara. He then became a research scientists at the Artificial Intelligence Laboratory at MIT (1982-1986) and followed this with a faculty position in the Division of Applied Sciences at Harvard (1986-1995), rising to the position of associate professor. From 1995-2002 he worked as a senior scientist at the Smith-Kettlewell Eye Research Institute in San Francisco. From 2002-2016 he was a full professor in the Department of Statistics at UCLA with joint appointments in Psychology, Computer Science, and Psychiatry. In 2016 he became a Bloomberg Distinguished Professor in Cognitive Science and Computer Science at Johns Hopkins University. He has won a Marr prize, a Helmholtz prize, and is a Fellow of IEEE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>All authors are with the Department of Computer Science, Johns Hopkins University, Baltimore, MD, 21218. E-mail: {siyuan.qiao, hwang157, cxliu}@jhu.edu {shenwei1231, alan.l.yuille}@gmail.com</figDesc><table /><note>• Corresponding author: W. Shen Manuscript received April 19, 2005; revised August 26, 2015.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 3: Computation graph for WS in feed-forwarding and backpropagation.</figDesc><table><row><cell>. . .</cell><cell>x</cell><cell>(7)</cell><cell>y</cell><cell>. . .</cell><cell>L</cell></row><row><cell>(5)</cell><cell>(6)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">WẆŴ (9) (8)</cell><cell></cell><cell></cell></row></table><note>Fig. 3 shows the computation graph. Based on the equations, we observe that different from the original gradients ∇Ŵ c,· L which is back-propagated through Eq. 13, the gradients are normalized by Eq. 14 &amp; 15.W ,Ẇ andŴ are defined in Eq. 11, 12 and 13.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Wc, || Wc, || 2</figDesc><table><row><cell>Error Rate</cell><cell>30 40 50 60 70</cell><cell></cell><cell cols="3">ResNet50 Train GN GN+Eq.11 GN+Eq.12 GN+Eq.11&amp;12</cell><cell>Error Rate</cell><cell>30 40 50 60 70</cell><cell></cell><cell cols="2">ResNet50 Val</cell><cell cols="2">GN GN+Eq.11 GN+Eq.12 GN+Eq.11&amp;12</cell><cell>Percentage (%)</cell><cell>10 1 10 0</cell><cell>1 I Wc, , Wc, 1 I 1, Wc, 2</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 2</cell></row><row><cell></cell><cell>0</cell><cell>20</cell><cell>40 Epoch</cell><cell>60</cell><cell>80</cell><cell></cell><cell>0</cell><cell>20</cell><cell>40 Epoch</cell><cell cols="2">60</cell><cell>80</cell><cell></cell><cell>0</cell><cell>20</cell><cell>40 Epoch</cell><cell>60</cell><cell>80</cell></row></table><note>Wc,· will be canceled. Therefore, the real effect on the gradient</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Top-1 error rates of ResNet-50 on ImageNet. All models except BN are trained with batch size 1 per GPU. BN models are trained with batch size 64 per GPU.</figDesc><table><row><cell>Method</cell><cell>Top-1</cell><cell>Method</cell><cell>Top-1</cell></row><row><cell>LN [7]</cell><cell>27.22</cell><cell>LN+WS</cell><cell>24.60</cell></row><row><cell>IN [26]</cell><cell>29.49</cell><cell>IN+WS</cell><cell>28.24</cell></row><row><cell>GN [6]</cell><cell>24.81</cell><cell>GN+WS</cell><cell>23.72</cell></row><row><cell>BN [3]</cell><cell>24.30</cell><cell>BN+WS</cell><cell>23.76</cell></row><row><cell cols="4">not satisfactory. The underlying difference between micro-</cell></row><row><cell>batch BCN and</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>Error rates of ResNet-50 and ResNet-101 on ImageNet. ResNet-50 models with BN are trained with batch size 64 per GPU, and ResNet-101 models with BN are trained with 32 images per GPU. The others are trained with 1 image per GPU.</figDesc><table><row><cell>Backbone</cell><cell>WN</cell><cell>CWN</cell><cell>WS</cell><cell>Top-1</cell></row><row><cell>ResNet-50 + GN</cell><cell></cell><cell></cell><cell></cell><cell>24.81</cell></row><row><cell>ResNet-50 + GN</cell><cell></cell><cell></cell><cell></cell><cell>25.09</cell></row><row><cell>ResNet-50 + GN</cell><cell></cell><cell></cell><cell></cell><cell>24.23</cell></row><row><cell>ResNet-50 + GN</cell><cell></cell><cell></cell><cell></cell><cell>23.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Comparing Top-1 error rates between WS, WN and CWN on ImageNet. The backbone is a ResNet-50 normalized by GN and trained with batch size 1 per GPU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Comparing Top-1 error rates between WS ("-mean":</cell></row><row><cell cols="5">Eq. 11, and "/ div": Eq. 12) and its individual effects. The</cell></row><row><cell cols="5">backbone is a ResNet-50-GN trained with batch size 1 per GPU.</cell></row><row><cell>Method</cell><cell cols="2">GN [6]</cell><cell cols="2">GN+WS [6]</cell></row><row><cell>Batch Size = 1</cell><cell cols="4">Top-1 Top-5 Top-1 Top-5</cell></row><row><cell>ResNeXt-50 [48]</cell><cell>24.24</cell><cell>7.27</cell><cell>22.71</cell><cell>6.38</cell></row><row><cell cols="2">ResNeXt-101 [48] 22.86</cell><cell>6.51</cell><cell>21.80</cell><cell>6.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table /><note>ResNeXt-50 and ResNeXt-101 on ImageNet. All mod- els are trained with batch size 1 per GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6 :</head><label>6</label><figDesc>Top-1/5 error rates of ResNet-50, ResNet-101, and ResNeXt-50 on ImageNet. The test size is 224 × 224 with center cropping. All normalizations are trained with batch size 32 or 64 per GPU without synchronization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 7 TABLE 8 :</head><label>78</label><figDesc>Error rates of ResNet-18 on CIFAR-10 trained with SN<ref type="bibr" target="#b39">[39]</ref>, DN<ref type="bibr" target="#b42">[42]</ref>, and our BCN and WS. The results are grouped based on large/micro-batch training. The performances of BN, SN and DN are from<ref type="bibr" target="#b42">[42]</ref>. Micro-batch for BN, SN and DN uses 2 images per batch, while BCN uses 1.</figDesc><table><row><cell cols="5">: Error rates of a 110-layer ResNet [2] on CIFAR-</cell></row><row><cell cols="5">10/100 [11] trained with BN [3], GN [6], and our BCN and</cell></row><row><cell cols="5">WS. The results are grouped based on dataset and large/micro-</cell></row><row><cell cols="5">batch training. Micro-batch assumes 1 sample per batch while</cell></row><row><cell cols="5">large-batch uses 128 samples in each batch. WS indicates</cell></row><row><cell cols="3">whether WS is used for weights.</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>Micro</cell><cell>Method</cell><cell>Error</cell></row><row><cell>C10</cell><cell>RN18</cell><cell></cell><cell>BN</cell><cell>5.20</cell></row><row><cell>C10</cell><cell>RN18</cell><cell></cell><cell>SN</cell><cell>5.60</cell></row><row><cell>C10</cell><cell>RN18</cell><cell></cell><cell>DN</cell><cell>5.02</cell></row><row><cell>C10</cell><cell>RN18</cell><cell></cell><cell>BCN+WS</cell><cell>4.96</cell></row><row><cell>C10</cell><cell>RN18</cell><cell></cell><cell>BN</cell><cell>8.45</cell></row><row><cell>C10</cell><cell>RN18</cell><cell></cell><cell>SN</cell><cell>7.62</cell></row><row><cell>C10</cell><cell>RN18</cell><cell></cell><cell>DN</cell><cell>7.55</cell></row><row><cell>C10</cell><cell>RN18</cell><cell></cell><cell>BCN+WS</cell><cell>5.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 9 :</head><label>9</label><figDesc>Object detection and instance segmentation results on COCO val2017<ref type="bibr" target="#b9">[10]</ref> of Mask R-CNN<ref type="bibr" target="#b50">[50]</ref> and FPN<ref type="bibr" target="#b51">[51]</ref> with ResNet-50 and ResNet-101<ref type="bibr" target="#b1">[2]</ref> as backbone. The models are trained with different normalization methods, which are used in their backbones, bounding box heads, and mask heads.</figDesc><table><row><cell cols="2">Model GN WS BCN AP b AP b .5 AP b .75 AP b l AP b m AP b s</cell></row><row><cell>RN50</cell><cell>38.0 59.1 41.2 49.5 40.9 22.4</cell></row><row><cell>RN50</cell><cell>38.9 60.4 42.1 50.4 42.4 23.5</cell></row><row><cell>RN50</cell><cell>39.7 60.9 43.1 51.7 43.2 24.0</cell></row><row><cell>RN101</cell><cell>39.7 60.9 43.3 51.9 43.3 23.1</cell></row><row><cell>RN101</cell><cell>41.3 62.8 45.1 53.9 45.2 24.7</cell></row><row><cell>RN101</cell><cell>41.8 63.4 45.8 54.1 45.6 25.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 10 :</head><label>10</label><figDesc>Object detection results on COCO using Faster R-CNN<ref type="bibr" target="#b52">[52]</ref> and FPN with different normalization methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 11 :</head><label>11</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Comparisons of semantic segmentation performance</cell></row><row><cell cols="3">of DeepLabV3 [53] trained with different normalizations on</cell></row><row><cell cols="3">PASCAL VOC 2012 [13] validation set. Output stride is 16,</cell></row><row><cell cols="2">without multi-scale or flipping when testing.</cell><cell></cell></row><row><cell cols="3">Model #Frame GN BN WS BCN Top-1 Top-5</cell></row><row><cell>RN50</cell><cell>8</cell><cell>42.07 73.20</cell></row><row><cell>RN50</cell><cell>8</cell><cell>44.26 75.51</cell></row><row><cell>RN50</cell><cell>8</cell><cell>44.30 74.53</cell></row><row><cell>RN50</cell><cell>8</cell><cell>46.49 76.46</cell></row><row><cell>RN50</cell><cell>8</cell><cell>45.27 75.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 12 :</head><label>12</label><figDesc>Comparing video recognition accuracy of TSM<ref type="bibr" target="#b56">[55]</ref> </figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2488" to="2498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fründ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5843" to="5851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2017.622</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.622" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-014-0733-5</idno>
		<ptr target="https://doi.org/10.1007/s11263-014-0733-5" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298965</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298965" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unrealcv: Virtual worlds for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1221" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SORT: Second-Order Response Transform for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-scale spatially-asymmetric recalibration for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="509" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Knowledge distillation in generations: More tolerant teachers educate better students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single-shot object detection with enriched semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5813" to="5821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AIS-TATS)</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics (AIS-TATS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Centered weight normalization in accelerating training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skip connections eliminate singularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pitkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamics of learning near singularities in layered networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ozeki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="843" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An empirical analysis of deep network loss surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Branson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the importance of single directions for generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards understanding regularization in batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards a theoretical understanding of batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daneshmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Neymeyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10694</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A mean field theory of batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Theoretical analysis of auto ratetuning by batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Optimization algorithms on matrix manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Riemannian approach to batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5225" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Differentiable learning-to-normalize via switchable normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10779</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01246-5_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01246-52" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ssn: Learning sparse switchable normalization via sparsestmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03793</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Differentiable dynamic normalization for learning deep representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhanglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wenqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ruimao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lingyun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4203" to="4211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Introductory lectures on convex optimization: A basic course</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convex optimization: Algorithms and complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="231" to="357" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.icml2010.org/papers/432.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Comparison of batch normalization and weight normalization algorithms for the large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08145</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1945" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.05431" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-netw" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2011.6126343</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2011.6126343" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">He is currently a Ph.D. student at Johns Hopkins University, where he is advised by Bloomberg Distinguished Professor Alan Yuille</title>
	</analytic>
	<monogr>
		<title level="m">Baidu IDL as an intern. He interned at Adobe Inc. from</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
		<respStmt>
			<orgName>Siyuan Qiao received B.E. in Computer Science at Shanghai Jiao Tong University</orgName>
		</respStmt>
	</monogr>
	<note>He has also spent time at University of California, Los Angeles, and YITU Technology. His research interests are computer vision and deep learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
