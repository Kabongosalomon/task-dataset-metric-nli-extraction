<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Embarrassingly Shallow Autoencoders for Sparse Data *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
							<email>hsteck@netix.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Net ix Los Gatos</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Embarrassingly Shallow Autoencoders for Sparse Data *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender System</term>
					<term>Collaborative Filtering</term>
					<term>Autoencoder</term>
					<term>Neigh- borhood Approach</term>
					<term>Linear Regression</term>
					<term>Closed-Form Solution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Combining simple elements from the literature, we de ne a linear model that is geared toward sparse data, in particular implicit feedback data for recommender systems. We show that its training objective has a closed-form solution, and discuss the resulting conceptual insights. Surprisingly, this simple model achieves better ranking accuracy than various state-of-the-art collaborativeltering approaches, including deep non-linear models, on most of the publicly available data-sets used in our experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many recent improvements in collaborative ltering can be a ributed to deep learning approaches, e.g, <ref type="bibr">[5, 7-9, 13, 21, 25, 26]</ref>. Unlike in areas like computer vision, however, it was found that a small number of hidden layers achieved the best recommendation accuracy. In this paper, we take this to the extreme, and de ne a linear model without a hidden layer (see <ref type="figure" target="#fig_0">Figure 1</ref>). e (binary) input vector indicates which items a user has interacted with, and the model's objective (in its output layer) is to predict the best items to recommend to the user. is is done by reproducing the input as its output, as is typical for autoencoders. <ref type="bibr" target="#b0">1</ref> We hence named it Embarrassingly Shallow AutoEncoder (in Reverse order:</p><p>). is paper is organized as follows: we de ne in the next section, using simple elements from the literature. In Section 3.1, we derive the closed-form solution of its convex training objective. is has several implications: <ref type="bibr" target="#b0">(1)</ref> it reveals that the neighborhood-based approaches used in collaborative ltering are based on conceptually incorrect item-item similarity-matrices, while may be considered a principled neighborhood model, see Sections 3.2 and 4.3; (2) the code for training is comprised of only a few lines, see Section 3.3 and Algorithm 1; (3) if the model ts into memory, the wall-clock time for training can be several orders of magnitude less than for training a S <ref type="bibr" target="#b15">[16]</ref> model (see <ref type="bibr">Section 3.4)</ref>, which is the most similar model to . Apart from that, we surprisingly found that achieved competitive ranking accuracy, and even outperformed various deep, non-linear, or probabilistic models as well as neighborhood-based approaches on most of the publicly available data-sets used in our experiments (see <ref type="bibr">Section 5)</ref>. * is paper is published in the proceedings of ' e Web Conference' (WWW) 2019, under the Creative Commons A ribution 4.0 International (CC-BY 4.0) license. <ref type="bibr" target="#b0">1</ref> Note, however, that the proposed model does not follow the typical architecture of autoencoders, being comprised of an encoder and a decoder: one may introduce an implicit hidden layer, however, by a (full-rank) decomposition of the learned weightmatrix of this model. e self-similarity of each item is constrained to zero between the input and output layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL DEFINITION</head><p>Like in many recommender papers that use implicit feedback data, we assume that the data are given in terms of a sparse (typically binary 2 ) matrix X ∈ R |U |×|I | , regarding the sets of users U and items I, where | · | denotes the size of a set. A positive value (typically 1) in X indicates that the user interacted with an item, while a value of 0 indicates that no interaction has been observed.</p><p>e parameters of the model are given by the item-item weight-matrix B ∈ R |I |×|I | . Note that this is also similar to neighborhood-based approaches, see Section 4.3. In this weight matrix, self-similarity of an item in the input-layer with itself in the output layer is forbidden as to force the model to generalize when reproducing the input as its output (see <ref type="figure" target="#fig_0">Figure 1</ref>): hence the diagonal of the weight-matrix is constrained to zero, diag(B) = 0. is constraint is crucial, and is discussed in detail in the remainder of this paper. is constraint was rst introduced in the S model <ref type="bibr" target="#b15">[16]</ref>. e predicted score S u, j for an item j ∈ I given a user u ∈ U is de ned by the dot product</p><formula xml:id="formula_0">S u j = X u, · · B ·, j ,<label>(1)</label></formula><p>where X u, · refers to row u, and B ·, j to column j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL TRAINING</head><p>We use the following convex objective for learning the weights B:</p><formula xml:id="formula_1">min B ||X − X B|| 2 F + λ · ||B|| 2 F (2) s.t. diag(B) = 0<label>(3)</label></formula><p>Several comments are in order:</p><p>• We choose the square loss (|| · || F denotes the Frobenius norm) between the data X and the predicted scores S = X B over other loss functions because it allows for a closed-form solution (see next section). Training with other loss functions, however, might result in improved ranking-accuracy: in <ref type="bibr" target="#b12">[13]</ref>, it was observed that the multinomial likelihood resulted in be er ranking-accuracy than training with the logistic likelihood (log loss) or the Gaussian likelihood (square loss). Directly optimizing a (surrogate) ranking loss might result in further accuracy gains-however, at possibly increased computational costs. • We use L2-norm regularization of the weights B to be learned. e training objective hence has a single hyperparameter λ, to be optimized on a validation set. • e constraint of a zero diagonal, diag(B) = 0, is crucial as to avoid the trivial solution B = I (self-similarity of items), where I is the identity matrix. It was introduced in S [16].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Closed-Form Solution</head><p>In this section, we show that the constrained convex optimization problem for learning the weight matrix B in Eqs. 2 and 3 can be solved in closed form.</p><p>We start by including the equality constraint in Eq. 3 into the objective function in Eq. 2 by forming the Lagrangian</p><formula xml:id="formula_2">L = ||X − X B|| 2 F + λ · ||B|| 2 F + 2 · γ · diag(B), where γ = (γ 1 , ..., γ | I | )</formula><p>is the vector of Lagrangian multipliers. Its values will be chosen in Eq. 6 such that the constraint in Eq. 3 is ful lled. e constrained optimization problem in Eqs. 2 and 3 is solved by minimizing this Lagrangian. As a necessary condition, we hence set its derivative to zero, which yields the estimateB of the weight matrix a er re-arranging terms:</p><formula xml:id="formula_3">B = (X X + λI ) −1 · (X X − diagMat(γ )),</formula><p>where diagMat(·) denotes the diagonal matrix and I the identity matrix. De ning (for su ciently large λ)</p><formula xml:id="formula_4">P (X X + λI ) −1 ,<label>(4)</label></formula><p>this can be substituted into the previous equation:</p><formula xml:id="formula_5">B = (X X + λI ) −1 · (X X − diagMat(γ )) =P · (P −1 − λI − diagMat(γ )) = I −P · (λI + diagMat(γ )) = I −P · diagMat(γ )<label>(5)</label></formula><p>where we de ned the vectorγ λ ì 1 + γ in the last line, with ì 1 denoting a vector of ones. e values of the Lagrangian multipliers γ , and henceγ , are determined by the constraint diag(B) = 0. It follows from Eq. 5:</p><formula xml:id="formula_6">0 = diag(B) = ì 1 − diag(P) γ<label>(6)</label></formula><p>where denotes the elementwise product, and hence:</p><formula xml:id="formula_7">γ = ì 1 diag(P),</formula><p>where denotes the elementwise division (which is well-de ned given thatP is invertible). Substituting this into Eq. 5 immediately results in the closed-form solution:</p><formula xml:id="formula_8">B = I −P · diagMat ì 1 diag(P) .<label>(7)</label></formula><p>In other words, the learned weights are given by:</p><formula xml:id="formula_9">B i, j =      0 if i = j −P i ĵ P j j otherwise.<label>(8)</label></formula><p>is solution obviously obeys the constraint of a zero diagonal. e o -diagonal elements are determined by the matrixP (see Eq. 4), where the j th column is divided by its diagonal elementP j j . Note thatB is an asymmetric matrix in general, whileP is symmetric (see Eq. 4).</p><p>Eqs. 4 and 8 show that the su cient statistics for estimating B is given by the data Gram-matrix G X X , which is an item-item matrix. is is a consequence of using the square loss in Eq. 2, and is helpful for estimating B from sparse data X : if X is a sparse binary matrix, then G = X X is a co-occurrence matrix. e uncertainty of a co-occurrence count G i j is (approximately) determined by the standard deviation of the Poisson distribution, which is G i j . As long as the co-occurrence counts G i j are 'su ciently large', G and hence B can be estimated with small error. An interesting fact is that the entries of G = X X can be increased by two di erent mechanisms: (1) a denser X (due to users with increased activity), or (2) an increased number of users in X . e la er is particularly useful, as an increased sparsity of X can be compensated by an increased number of users. In other words, the problem that there possibly is only a small amount of data available for each user (i.e., data sparsity), does not a ect the uncertainty in estimating B if the number of users in the data matrix X is su ciently large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interpretation</head><p>In this section, we outline that the closed-form solution in Eq. 8 does not come as a complete surprise. To this end, let us consider the following special case throughout this section: let the training data X be an i.i.d. sample of |U| data points regarding a vector of |I| random variables x ∼ N (0, Σ) that follows a Gaussian distribution with zero mean and covariance matrix Σ ∈ R |I |×|I | . en the estimate of the covariance matrix isΣ = X X /|U|. If we further drop the L2-norm regularization in Eq. 4 and assume invertibility, thenP =Σ −1 is the estimate of the so-called precision (or concentration) matrix. Estimating the precision matrix from given data is a main objective in the area of graphical models (e.g., see <ref type="bibr" target="#b10">[11]</ref>).</p><p>It is well known <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> that the (univariate) conditional distribution of the random variable x j given the vector of all the other variables, denoted by x −j (x k ) k ∈I\{j } , is a Gaussian distribution with variance var(x j |x −j ) = 1/P j, j and mean</p><formula xml:id="formula_10">µ j |−j E[x j |x −j ] = −x −j · P −j, j /P j, j = x −j · B −j, j = x · B ., j</formula><p>where the dot in the rst line denotes the dot-product between the (row) vector x −j and the j th column of the precision matrix P = Σ −1 , omi ing the j th element. e second line follows from Eq. 8, and the last line from B j j = 0. Note that this is identical to the prediction rule of the model in Eq. 1. is shows that makes a principled point-prediction that user u will like item j conditional on the fact that the user's past interactions with all items are given by X u, . = x.</p><p>A more well-known fact (e.g., see <ref type="bibr" target="#b14">[15]</ref>) is that the absence of an edge between the random variables x i and x j in a Markov network corresponds to the conditional independence of the random variables x i and x j given the vector of all the other variables (x k ) k ∈I\{i, j } , which is also equivalent to a zero entry in the precision matrix. Note that this is di erent from a zero entry in the covariance matrix Σ, which signi es marginal independence of x i and x j .</p><p>is shows that the precision matrix is the conceptually correct similaritymatrix to be used, rather than the covariance matrix, which (or some rescaled variant thereof) is typically used in state-of-the-art neighborhood-based approaches (see Section 4.3).</p><p>Learning the graph structure in Markov networks corresponds to learning a sparse precision matrix from data. Approaches developed in that eld (e.g., see <ref type="bibr" target="#b18">[19]</ref> and references therein) might be useful for improved learning of a sparse matrixB. is is beyond the scope of this paper.</p><p>While the interpretation we outlined in this section is limited to the special case of normally distributed variables with zero mean, note that the derivation of Eq. 8 does not require that each column of the data matrix X has zero mean. In other words, in any de nition of the Gram matrix G X X may be used, e.g., G may be the co-occurrence matrix (if X is a binary matrix), proportional to the covariance matrix (if X is pre-processed to have zero mean in each column), or the correlation matrix (if each column of X is pre-processed to have zero mean and unit variance). A er training on these transformed matrices X and then transforming the predicted scores back to the original space (as de ned by the binary matrix X ), we found in our experiments that the di erences in the obtained ranking accuracies were of the order of the standard error, and we hence do not separately report these results in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Algorithm</head><p>e Python code of the resulting learning algorithm is given in Algorithm 1. Note that the training of requires only the itemitem matrix G = X X as input, instead of the user-item matrix X , and hence is particularly e cient if the size of G (i.e., |I| × |I|) is smaller than the number of user-item-interactions in X . In this case, the expensive computation of G = X X can be done on a big-data pre-processing system, prior to the actual model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Computational Cost</head><p>e computational complexity of Algorithm 1 is determined by the matrix inversion of the data Gram-matrix G X X ∈ R |I |×|I | , which is O(|I| 3 ) when using a basic approach, and about O(|I| 2.376 ) when using the Coppersmith-Winograd algorithm. Note that this is independent of the number of users as well as the number of user-item-interactions, as G can be computed in the pre-processing step.</p><p>is computational complexity is orders of magnitude lower than the cost of training a S model and its variants <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>: those approaches take advantage of the fact that the optimization problem regarding |I| items can be decomposed into |I| independent (and hence embarrassingly parallel) optimization problems involving |I| − 1 items each, due to the identity ||X − X B|| 2 F = j ∈I |X ., j − X B ., j | 2 2 . If each of the |I| independent problems is solved based on an item-item matrix, the total computational cost is hence O(|I|(|I| − 1) 2.376 ). Note that the computational cost of solving those |I| problems is larger by almost a factor of |I| than training , which requires only a single regression problem to be solved. In practice, however, S and its variants are trained on the user-item-interactions in X , which may incur additional In practice, the wall-clock time depends crucially on the fact if the number of items |I| is su ciently small such that the weight matrix ts into memory, so that the matrix inversion can be computed in memory. is was the case in our experiments in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>can be viewed as an autoencoder, as a modi ed version of S , and a neighborhood-based approach. We discuss each of the three related approaches in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep Learning and Autoencoders</head><p>While the area of collaborative ltering has long been dominated by matrix factorization approaches, recent years have witnessed a surge in deep learning approaches <ref type="bibr">[5, 7-9, 13, 21, 25, 26]</ref>, spurred by their great successes in other elds. Autoencoders provide the model architecture that ts exactly the (plain-vanilla) collaborative ltering problem. While various network architectures have been explored, it was found that deep models with a large number of hidden layers typically do not obtain a notable improvement in ranking accuracy in collaborative ltering, compared to 'deep' models with only one, two or three hidden layers, e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>, which is in stark contrast to other areas, like computer vision. A combination of deep and shallow elements in a single model was proposed in <ref type="bibr" target="#b4">[5]</ref>.</p><p>In contrast, has no hidden layer. Instead, the self-similarity of each item in the input and output layer is constrained to zero, see also <ref type="figure" target="#fig_0">Figure 1</ref>. As a consequence, the model is forced to learn the similarity of an item in the output layer in terms of the other items in the input layer. e surprisingly good empirical results of in Section 5 suggest that this constraint might be more e ective than using hidden layers with limited capacity as to force the model to generalize well to unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">S and Variants</head><p>While the S model <ref type="bibr" target="#b15">[16]</ref> has shown competitive empirical results in numerous papers, it is computationally expensive to train, e.g., see <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref> and Section 3.4. is has sparked follow-up work proposing various modi cations. In <ref type="bibr" target="#b11">[12]</ref>, both constraints on the weight matrix (non-negativity and zero diagonal) were dropped, resulting in a regression problem with an elastic-net regularization. While competitive ranking results were obtained in <ref type="bibr" target="#b11">[12]</ref>, in the experiments in <ref type="bibr" target="#b12">[13]</ref> it was found that its performance was considerably below par. e square loss in S was replaced by the logistic loss in <ref type="bibr" target="#b19">[20]</ref>, which entailed that both constraints on the weight matrix could be dropped, as argued by the authors. Moreover, the L1-norm regularization was dropped, and a user-user weight-matrix was learned instead of an item-item matrix. All these approaches take advantage of the fact that the optimization problem decomposes into independent and embarrassingly parallel problems. As discussed in the previous section, however, this is several orders of magnitudes more costly than training if the weight matrix ts into memory.</p><p>Most importantly, while those modi cations of S dropped the constraint of a zero diagonal in the weight matrix, it is retained in . In fact, we found it to be the most crucial property for achieving improved ranking accuracy (see Section 5). As we showed in Section 3.1, this constraint can be easily included into the training objective via the method of Lagrangian multipliers, allowing for a closed-form solution.</p><p>Compared to S <ref type="bibr" target="#b15">[16]</ref>, we dropped the constraint of non-negative weights, which we found to greatly improve ranking accuracy in our experiments (see <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_3">Figure 2</ref>). Moreover, we did not use L1-norm regularization for computational e ciency. We also did not nd sparsity to noticeably improve ranking accuracy (see <ref type="bibr">Section 5)</ref>. e learned weight matrixB of is dense. Also note that extensions to S like cSLIM <ref type="bibr" target="#b16">[17]</ref>, can be turned into an analogous extension of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Neighborhood-based Approaches</head><p>Numerous neighborhood-based approaches have been proposed in the literature (e.g., see <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and references therein). While model-based approaches were found to achieve be er ranking accuracy on some data sets, neighborhood-based approaches dominated on others, e.g., the Million Song Data Competition on Kaggle <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. Essentially, the co-occurrence matrix (or some modi ed variant) is typically used as item-item or user-user similarity matrix in neighborhood-based methods. ese approaches are usually heuristics, as the similarity matrix is not learned by optimizing an objective function (loss function or likelihood). More importantly, the closed-form solution derived in Eqs. 4 and 8 reveals that the inverse of the data Gram matrix is the conceptually correct similarity matrix, 3 see Section 3.2 for more details. is is in contrast to the typical neighborhood-based approaches, which use the data Gram-matrix without inversion. e use of the conceptually correct, inverse matrix in may explain the improvement observed in <ref type="table" target="#tab_1">Table 2</ref> compared to the heuristics used by state-of-the-art neighborhood approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, the proposed model is empirically compared to several state-of-the-art approaches, based on two papers that provided publicly available code for reproducibility of results <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>. Both papers together cover linear, non-linear, deep and probabilistic models, as well as neighborhood-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Set-up</head><p>We will only summarize the experimental set-ups used in these papers, and refer the reader to these papers for details. <ref type="bibr" target="#b12">[13]</ref>: is paper considers the following models:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Set-up in</head><p>• Sparse Linear Method (S ) <ref type="bibr" target="#b15">[16]</ref>. Besides the original model, also a computationally faster approximation (which drops the constraints on the weights) <ref type="bibr" target="#b11">[12]</ref> was considered, but its results were not found to be on par with the other models in the experiments in <ref type="bibr" target="#b12">[13]</ref>. • Weighted Matrix Factorization ( ) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, a linear model with a latent representation of users and items.</p><p>• Collaborative Denoising Autoencoder ( ) <ref type="bibr" target="#b24">[25]</ref>, a nonlinear model with one hidden layer.</p><p>• denoising autoencoder (M ) and variational autoencoder (M ) <ref type="bibr" target="#b12">[13]</ref>, both trained using the multinomial likelihood, which was found to outperform the Gaussian and logistic likelihoods. Best results were obtained in <ref type="bibr" target="#b12">[13]</ref> for the M and M models that were rather shallow 'deep models', namely with a 200-dimensional latent representation, as well as a 600dimensional hidden layer in both the encoder and decoder. Both models are non-linear, and M is also probabilistic.</p><p>ree data sets were used in the experiments in <ref type="bibr" target="#b12">[13]</ref>, and were preprocessed and ltered for items and users with a certain activity level, resulting in the following data-set sizes, see <ref type="bibr" target="#b12">[13]</ref> for details: 4 e evaluation in <ref type="bibr" target="#b12">[13]</ref> was conducted in terms of strong generalization, i.e., the training, validation and test sets are disjoint in terms of users. is is in contrast to weak generalization, where the training and test sets are disjoint in terms of the user-item interaction-pairs, but not in terms of users. Concerning evaluation in terms of ranking metrics, Recall@k for k ∈ {20, 50} as well as Normalized Discounted Cumulative Gain, NDCG@100 were used in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Summary of Set-up in <ref type="bibr" target="#b23">[24]</ref>: eir paper focuses on neighborhood-based approaches, and the authors publicly shared code <ref type="bibr" target="#b4">5</ref> regarding the experiments in their table 2 in <ref type="bibr" target="#b23">[24]</ref>, albeit only for the data-split where the training data was comprised of (at most) 30% of each user's interactions (and the remainder was assigned to the test data), which restricted our experimental comparison to this single split in <ref type="table" target="#tab_1">Table 2</ref>. ey used the MovieLens 10 Million (ML-10M) data <ref type="bibr" target="#b5">[6]</ref>, which was binarized in <ref type="bibr" target="#b23">[24]</ref> and is comprised of 69,878 users and 10,677 movies with 10 million interactions. eir evaluation was done in terms of weak generalization, and NDCG@10 was used as ranking metric for evaluation in <ref type="bibr" target="#b23">[24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Despite the simplicity of , we observed that obtained considerably be er ranking accuracy than any of the competing models on most of the data sets. is remarkable empirical result is discussed in detail in the following. <ref type="table" target="#tab_0">Table 1</ref> shows that achieved notably increased accuracy compared to S on all the data sets. is suggests that dropping the L1-norm regularization as well as the non-negativity constraint on the learned weights is bene cial. Our analysis indicates that the la er is especially important: as illustrated in <ref type="figure" target="#fig_3">Figure 2</ref> on the Net ix data (the histograms for ML-20M and MSD data look almost identical up to re-scaling, and are omi ed), the learned weights in are distributed around 0. Interestingly, it turns out that about 60% of the learned weights are negative on all the data sets in our experiments (regarding both papers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>). is indicates that it is crucial to learn also the dissimilarity (negative weights) between items besides their similarity (positive weights). Moreover, when we simply set the negative weights to zero (see ≥ 0 in <ref type="table" target="#tab_0">Table 1</ref>), which obviously is not the optimal non-negative solution, the resulting accuracy drops and is very close to the one of S . Apart from that, note that ≥ 0 is still quite dense (40% positive weights) compared to S , which indirectly indicates that the sparsity of S (due to L 1 -norm regularization) did not noticeably improve the ranking accuracy of S in our experiments. Regarding regularization, the optimal L2-norm regularization parameter (λ) for is about 500 on ML-20M, 1,000 on Net ix, and 200 on MSD. ese values are much larger than the typical values used for S , which o en are of the order of 1, see <ref type="bibr" target="#b15">[16]</ref>. Note that S additionally uses L1-norm regularization, and hence has much fewer (non-zero) parameters than . As expected based on Section 3.4, we also found the (wall-clock) training-time of to be smaller by several orders of magnitude compared to S : <ref type="bibr" target="#b12">[13]</ref> reports that parallelized grid search for S took about two weeks on the Net ix data, and the MSD data was 'too large for it to nish in a reasonable amount of time' <ref type="bibr" target="#b12">[13]</ref>. In contrast, training on the Net ix data took less than two minutes, and on the MSD data less than 20 minutes on an AWS instance with 64 GB RAM and 16 vCPUs in our experiments. ese times have to be multiplied by the number of di erent hyperparameter-values to be grid-searched. Note, however, that only has a single hyperparameter (regarding L2-norm regularization), while S has   two hyperparameters (concerning L1 and L2 norms) to be jointly optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to S :</head><p>Comparison to linear and deep non-linear models in <ref type="bibr" target="#b12">[13]</ref>: <ref type="table" target="#tab_0">Table 1</ref> shows that was consistently outperformed on only  <ref type="bibr" target="#b23">[24]</ref>: considerably improves over 'ii-SVD-500' <ref type="bibr" target="#b23">[24]</ref>.</p><p>reproduced from <ref type="bibr" target="#b23">[24]</ref>: ≥ 0</p><p>ii-SVD-500 item-item NDCG@10 0.6258 0.6199 0.6113 0.5957 0.5969 the ML-20M data, and only by a small margin by the best competing model (M ). On the Net ix and MSD data, obtained signi cantly be er ranking results than any of the competing linear, non-linear, deep or probabilistic models evaluated in <ref type="bibr" target="#b12">[13]</ref>. On the MSD data, even improved over the best competing model by 25%, 17% and 23% regarding Recall@20, Recall@50, and NDCG@100, respectively. is is consistent with the results of the Million Song Data Challenge on Kaggle <ref type="bibr" target="#b13">[14]</ref>, where neighborhoodbased approaches were found to vastly outperform model-based approaches <ref type="bibr" target="#b0">[1]</ref>. As discussed in Section 4.3, may also be viewed as a principled neighborhood approach.</p><p>As to explain 's relative improvements from ML-20M via Net ix to MSD data, various properties of the data sets may be considered. As shown by table 1 in <ref type="bibr" target="#b12">[13]</ref>, the number of user-item interactions, and the sparsity of the data sets do not appear well correlated with 's relative performance in <ref type="table" target="#tab_0">Table 1</ref>. Only the number of users correlates well with the improvements of over the competing models, which, however, appears to be spurious. e explanation can be understood in terms of the tradeo between recommending generally popular items vs. personally relevant items to each user, which is supported by two empirical ndings: <ref type="bibr" target="#b0">(1)</ref> we evaluated the popularity model in <ref type="table" target="#tab_0">Table 1</ref> as an additional baseline, where the items are ranked by their popularities (i.e., the number of users who interacted with an item). ese unpersonalized recommendations obviously ignore the personalized relevance to a user. <ref type="table" target="#tab_0">Table 1</ref> shows that this popularity model obtains be er accuracy on the ML-20M data than it does on the Net ix data, while its accuracy is considerably reduced on the MSD data. is suggests that good recommendations on the MSD data have to focus much more on personally relevant items rather than on generally popular items, compared to the ML-20M and Net ix data. (2) When counting how o en an item was recommended in the top-100 across all test-users, and then ranking the items by their counts, we obtained <ref type="figure" target="#fig_5">Figure 3</ref> for the MSD data: it shows that recommended long-tail items more o en than M did. In contrast, there was almost no di erence between the two approaches on either of the data sets ML-20M and Net ix ( gures omi ed due to page limit). e notable improvement of over the other models on the MSD data suggests that it is able to be er recommend personally relevant items on this data set. On the other hand, 's results on the ML-20M and Net ix data suggest that it is also able to make recommendations with an increased focus on popular items. We suspect that 's large number of parameters, combined with its constraint regarding self-similarity of items, provides it with su cient exibility to adapt to the various data sets. In contrast, the model architectures based on hidden layers with limited capacity seem to be unable to adapt well to the increased degree of personalized relevance in the MSD data.</p><p>Comparison to neighborhood-based approaches in <ref type="bibr" target="#b23">[24]</ref>: Considerable improvements were obtained in <ref type="bibr" target="#b23">[24]</ref> by rst predicting the scores for all user-item interactions with a neighborhoodbased approach ('item-item' in <ref type="table" target="#tab_1">Table 2</ref>) that was followed by a low-rank singular value decomposition of the predicted user-item score-matrix ('ii-SVD-500' in <ref type="table" target="#tab_1">Table 2</ref>): an increase in NDCG@10 by 0.0156 and 0.0144 compared to the baseline models 'item-item' and , respectively, as reproduced in <ref type="table" target="#tab_1">Table 2</ref>. In comparison, obtained increases of 0.0301 and 0.0289 over the baseline models 'item-item' and , respectively, see <ref type="table" target="#tab_1">Table 2</ref>. is is about twice as large an improvement as was obtained by the approach 'ii-SVD-500' proposed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Given the small size of this training data-set, a large L2-norm regularization was required (λ = 3000) for . Like in the previous experiments, also here about 60% of the learned weights were negative in , and se ing them to zero ( ≥ 0 in <ref type="table" target="#tab_1">Table  2</ref>) resulted in a small decrease in NDCG@10, as expected. In terms of wall-clock time, we found that training was about three times faster than computing merely the factorization step in the ii-SVD-500 approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We presented a simple yet e ective linear model for collaborative ltering, which combines the strengths of autoencoders and neighborhood-based approaches. Besides enabling e cient training (with savings of up to several orders of magnitude if the model ts into memory), the derived closed-form solution also shows that the conceptually correct similarity-matrix to be used in neighborhood approaches is based on the inverse of the given data Gram-matrix. In contrast, state-of-the-art neighborhood approaches typically use the data Gram-matrix directly. In our experiments, we found that allowing the learned weights to also take on negative values (and hence learn dissimilarities between items, besides similarities), was essential for the obtained ranking accuracy. Interestingly, the achieved ranking accuracy in our experiments was on par or even (notably) be er than those of various state-of-the-art approaches, including deep non-linear models as well as neighborhood-based approaches.</p><p>is suggests that models where the self-similarity of items is constrained to zero may be more e ective on sparse data than model architectures based on hidden layers with limited capacity. We presented a basic version as to illustrate the essence of the idea, and leave various modi cations and extensions for future work. Several practical extensions are outlined in <ref type="bibr" target="#b21">[22]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: e self-similarity of each item is constrained to zero between the input and output layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Training in Python 2 using numpy Input: data Gram-matrix G := X X ∈ R |I |×|I | , L2-norm regularization-parameter λ ∈ R + . Output: weight-matrix B with zero diagonal (see Eq. 8). dia Indices = numpy.diag indices(G.shape[0]) G[dia Indices] += λ P = numpy.linalg.inv(G) B = P / (-numpy.diag(P)) B[dia Indices] = 0 computational cost. is explains the vastly reduced training-times of observed in our experiments in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>MovieLens 20 Million (ML-20M) data [6]: 136,677 users and 20,108 movies with about 10 million interactions, • Net ix Prize (Net ix) data [2]: 463,435 users and 17,769 movies with about 57 million interactions, • Million Song Data (MSD) [3]: 571,355 users and 41,140 songs with about 34 million interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Histogram of the weights learned on Netflix data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>rec.-count in top 100)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>(green) recommends long-tail items more o en in the top-100, compared to M (dotted), on MSD data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ranking accuracy (with standard errors of about 0.002, 0.001, and 0.001 on the ML-20M, Netflix, and MSD data, respectively), following the experimental set-up in<ref type="bibr" target="#b12">[13]</ref>.</figDesc><table><row><cell cols="5">(a) ML-20M Recall@20 Recall@50 NDCG@100</cell></row><row><cell>popularity</cell><cell>0.162</cell><cell></cell><cell>0.235</cell><cell>0.191</cell></row><row><cell></cell><cell>0.391</cell><cell></cell><cell>0.521</cell><cell>0.420</cell></row><row><cell>≥ 0</cell><cell>0.373</cell><cell></cell><cell>0.499</cell><cell>0.402</cell></row><row><cell cols="3">results reproduced from [13]:</cell><cell></cell><cell></cell></row><row><cell>S</cell><cell>0.370</cell><cell></cell><cell>0.495</cell><cell>0.401</cell></row><row><cell></cell><cell>0.360</cell><cell></cell><cell>0.498</cell><cell>0.386</cell></row><row><cell></cell><cell>0.391</cell><cell></cell><cell>0.523</cell><cell>0.418</cell></row><row><cell>M</cell><cell>0.395</cell><cell></cell><cell>0.537</cell><cell>0.426</cell></row><row><cell>M</cell><cell>0.387</cell><cell></cell><cell>0.524</cell><cell>0.419</cell></row><row><cell>(b) Netflix</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>popularity</cell><cell>0.116</cell><cell></cell><cell>0.175</cell><cell>0.159</cell></row><row><cell></cell><cell>0.362</cell><cell></cell><cell>0.445</cell><cell>0.393</cell></row><row><cell>≥ 0</cell><cell>0.345</cell><cell></cell><cell>0.424</cell><cell>0.373</cell></row><row><cell cols="3">results reproduced from [13]:</cell><cell></cell><cell></cell></row><row><cell>S</cell><cell>0.347</cell><cell></cell><cell>0.428</cell><cell>0.379</cell></row><row><cell></cell><cell>0.316</cell><cell></cell><cell>0.404</cell><cell>0.351</cell></row><row><cell></cell><cell>0.343</cell><cell></cell><cell>0.428</cell><cell>0.376</cell></row><row><cell>M</cell><cell>0.351</cell><cell></cell><cell>0.444</cell><cell>0.386</cell></row><row><cell>M</cell><cell>0.344</cell><cell></cell><cell>0.438</cell><cell>0.380</cell></row><row><cell>(c) MSD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>popularity</cell><cell>0.043</cell><cell></cell><cell>0.068</cell><cell>0.058</cell></row><row><cell></cell><cell>0.333</cell><cell></cell><cell>0.428</cell><cell>0.389</cell></row><row><cell>≥ 0</cell><cell>0.324</cell><cell></cell><cell>0.418</cell><cell>0.379</cell></row><row><cell cols="3">results reproduced from [13]:</cell><cell></cell><cell></cell></row><row><cell>S</cell><cell cols="4">-did not nish in [13] -</cell></row><row><cell></cell><cell>0.211</cell><cell></cell><cell>0.312</cell><cell>0.257</cell></row><row><cell></cell><cell>0.188</cell><cell></cell><cell>0.283</cell><cell>0.237</cell></row><row><cell>M</cell><cell>0.266</cell><cell></cell><cell>0.364</cell><cell>0.316</cell></row><row><cell>M</cell><cell>0.266</cell><cell></cell><cell>0.363</cell><cell>0.313</cell></row><row><cell>0</cell><cell>10000</cell><cell>20000</cell><cell>30000</cell><cell>40000</cell></row><row><cell></cell><cell cols="2">item-ranks</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison to the neighborhood-approaches in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Non-binary matrices may also be used. arXiv:1905.03375v1 [cs.IR] 8 May 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In fact, inverse matrices are used in many areas, for instance, the inverse covariance matrix in the Gaussian density function or in the Mahalanobis distance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">e code regarding ML-20M in<ref type="bibr" target="#b12">[13]</ref> is publicly available at https://github.com/dawenl/vae cf. e authors kindly provided the code for the other two data sets upon request. 5 http://www.cs.toronto.edu/∼mvolkovs/SVD CF.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENTS I am very grateful to Tony Jebara and Nikos Vlassis for useful comments on an earlier dra , and especially to Dawen Liang for suggestions on related work with publicly available code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">E cient top-N Recommendation for very large scale binary rated Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aiolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Recommender Systems (RecSys)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">e Net ix Prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bennet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at SIGKDD-07, ACM Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Million Song Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference (ISMIR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical analysis of non-la ice data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistician</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="179" to="95" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
	<note>Issue 3</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (DLRS)</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems (DLRS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<title level="m">e MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Issue 4.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks with Top-k Gains for Session-based Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03847</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<title level="m">Session-based Recommendations with Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Implicit Feedback Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical Models</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">E cient Top-N Recommendation by Linear Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys Large Scale Recommender Systems Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Ho Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">2012. e Million Song Dataset Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<ptr target="http://www.kaggle.com/c/msdchallenge" />
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-dimensional graphs and variable selection with the Lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SLIM: Sparse Linear Methods for Top-N Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse Linear Methods with Side Information for Top-N Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Recommender Systems (RecSys)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One-Class Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lukose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<title level="m">Graphical Model Structure Learning with L1-Regularization. Ph.D. Dissertation. University of</title>
		<meeting><address><addrLine>British Columbia, Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the E ectiveness of Linear Models for One-Class Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Braziunas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13033</idno>
		<title level="m">Collaborative Filtering via High-Dimensional Regression</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unifying Nearest Neighbors Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Verstrepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goethals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Recommender Systems (RecSys)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">E ective Latent Models for Binary Feedback in Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative Denoising Auto-Encoders for top-N Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Web Search and Data Mining (WSDM)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Neural Autoregressive Approach to Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
