<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continual Occlusions and Optical Flow Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Neoral</surname></persName>
							<email>neoramic@fel.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Perception</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janšochman</forename></persName>
							<email>jan.sochman@fel.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Perception</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
							<email>matas@fel.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Machine Perception</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Continual Occlusions and Optical Flow Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two optical flow estimation problems are addressed: i) occlusion estimation and handling, and ii) estimation from image sequences longer than two frames. The proposed ContinualFlow method estimates occlusions before flow, avoiding the use of flow corrupted by occlusions for their estimation. We show that providing occlusion masks as an additional input to flow estimation improves the standard performance metric by more than 25% on both KITTI and Sintel. As a second contribution, a novel method for incorporating information from past frames into flow estimation is introduced. The previous frame flow serves as an input to occlusion estimation and as a prior in occluded regions, i.e. those without visual correspondences. By continually using the previous frame flow, ContinualFlow performance improves further by 18% on KITTI and 7% on Sintel, achieving top performance on KITTI and Sintel.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Optical flow is a two-dimensional displacement field describing the projection of scene motion between two images. Occlusions caused by scene motion contribute to the ill-posedness of optical flow estimation -at occluded pixels no visual correspondences exist. Classical non-CNN methods address this problem by using regularisation which extrapolates the flow from the surrounding non-occluded area. Current state-of-the-art CNN algorithms for optical flow use the correlation cost volume <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b16">16]</ref> to estimate the most likely correspondences. Their regularisation is only implicit and the network has to learn when to rely on the cost volume and when to extrapolate. In both cases, the occluded areas are processed the same way as non-occluded ones which leads to errors in the occluded areas as well as in the nearby non-occluded regions.</p><p>Approaches dealing with occlusions <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b0">1]</ref> usually first estimate initial forward and backward optical flows. Occlusions are found by a forward-backward consistency check and occlusion maps are then used for estimating of the final optical flow. The problem here is that occlusions affect the initial flow and thus the final output.</p><p>As our first contribution, we extend a current state-of-the-art CNN optical flow method <ref type="bibr" target="#b35">[35]</ref> by estimating the occluded areas first, without estimating the flow, and then passing the occlusion maps to the optical flow estimation network.</p><p>The correlation cost volume for flow estimation is re-used for occlusion estimation. Intuitively, the cost will be low in non-occluded areas with good correspondences and high in occluded regions. While preserving end-to-end trainability, we accurately estimate occlusions and significantly improve the estimated flow.</p><p>Optical flow estimation over more than two frames is a problem whose difficulty stems from the need for the pixels to be mapped to a reference coordinate system before loss evaluation. The mapping is defined by the unknown optical flow itself. Hence, it is difficult to apply temporal regularisation before the flow is known. A typical solution over three frames is to use the middle one as the reference defining the coordinate system and to compute the forward flow to the future frame and the backward flow to the past frame and to apply regularisation to these two flows. Published multi-frame approaches assume various motion constraints: constant rigid motion for three images <ref type="bibr" target="#b41">[41]</ref>, adaptive trajectory regularisation over five images <ref type="bibr" target="#b38">[38]</ref>, multi-frame subspace constrains <ref type="bibr" target="#b19">[19]</ref> and other complex motion models <ref type="bibr" target="#b12">[12]</ref> over the whole sequence.</p><p>We avoid modelling the motion regularity explicitly and let a CNN model learn the relations of the current and previous optical flows. The CNN is fed pairs of consecutive images together with the flow computed between the penultimate and last images. We solve the coordinate system mapping by bilinear warp <ref type="bibr" target="#b20">[20]</ref>. The proposed method is not limited to a fixed temporal horizon, the network uses previously estimated flows and thus, by recursion, all prior frames.</p><p>The two above-mentioned problems -occlusion estimation and the use of multiple frames -are related. Since there are no correspondences in occluded areas, optical flow cannot be estimated from the cost volume and the CNN is forced to use regularisation. Knowing the occlusions and given the previous flow, the network has prior information about the motion to be used when no correspondences are available. So, the last estimated flow is also fed into the occlusions estimation as it is a source of information about possible occlusions.</p><p>Finally, we add a specialised refinement network <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b29">29]</ref> to the proposed architecture. It has been shown to improve fine detail accuracy of the flow, which is confirmed by our experiments. We integrate this network with both occlusion estimation and temporal processing.</p><p>Contributions. We introduce integrated occlusion estimation, i.e. the algorithm does not operate on an occlusion-ignorant flow estimate, to the state-ofthe-art PWC-net <ref type="bibr" target="#b35">[35]</ref>. Second, we propose a novel method that implicitly uses all previous frames for optical flow estimation. Finally, we add refinement blocks with additional feature map inputs leading to improved spatial resolution of the final flow. ContinualFlow is state-of-the-art on several public benchmarks 1 : 1st place in Sintel <ref type="bibr" target="#b6">[6]</ref> 2 and 1st place in the KITTI'15 <ref type="bibr" target="#b27">[27]</ref> optical flow benchmark among Robust Vision Challenge (ROB) participants and 3rd over all optical flow methods 3 with a large margin in precision in occluded areas. Continual flow ranked 3rd in ROB <ref type="bibr" target="#b32">[32]</ref> for the optical flow category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Occlusion estimation and occlusion handling. Most optical flow methods detect occlusions as outliers of the correspondence field <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b1">2]</ref> or by a consistency check on the estimated forward and backward optical flows <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b8">8]</ref>. The optical flow is then extrapolated into the occluded areas. The shortcoming of such approaches is that the initial flow is already adversely affected by the occlusions. Other methods incorporate occlusion estimation directly into the energy minimisation <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b34">34]</ref> by truncating the data term, avoiding the problematic post-processing of already affected optical flow. The current best performing non-CNN method <ref type="bibr" target="#b17">[17]</ref> formulates optical flow estimation symmetrically -estimating the forward and backward flows, occlusions and dis-occlusions in a single joint optimisation.</p><p>Most of the current state-of-the-art CNN networks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35]</ref> do not explicitly deal with occlusions. The network in <ref type="bibr" target="#b26">[26]</ref> estimates the forward and backward flows independently and uses the forward-backward consistency check to estimate the occlusions. The estimated occlusions are then used for network training only. In LiteFlowNet <ref type="bibr" target="#b16">[16]</ref> an occlusion probability map is a function of brightness inconsistency between the reference frame and warped target frame. The occlusion probability map is used in a flow regularisation module.</p><p>To our best knowledge, no published CNN method estimates occlusions prior to optical flow estimation to improve the flow in the test phase.</p><p>Using multiple frames. Most methods that process more than two frames impose some kind of regularisation on the flow. Murray and Buxton <ref type="bibr" target="#b28">[28]</ref> introduced an approach that uses spatio-temporal smoothness term which regularises optical flow trajectory over multiple frames. However, the algorithm does not work well for large displacements. Black et al. <ref type="bibr" target="#b5">[5]</ref> extrapolate the flow from the previous frame as a starting point for the optimisation in the current frame. In Garg et al. <ref type="bibr" target="#b11">[11]</ref>, the motion regularisation was relaxed from several rigid motions into multi-frame subspace constraints allowing non-rigid motions. Multi-frame subspace constraints were used in <ref type="bibr" target="#b19">[19]</ref> over long trajectories. Its extension <ref type="bibr" target="#b12">[12]</ref> allows more complex motions using soft constraints between frames. An adaptive trajectory regularisation over five consecutive frames was used in <ref type="bibr" target="#b38">[38]</ref>, where optical flow was parametrised w.r.t. the central reference frame. Wulff et. al. <ref type="bibr" target="#b41">[41]</ref> use super-pixel segmentation and a rigid motion assumption over triplets of images. ProFlow <ref type="bibr" target="#b23">[23]</ref> uses three consecutive frames, a CNN regularises non-CNNestimated forward (I t → I t+1 ) and backward (I t−1 → I t ) optical flows.</p><p>While many non-CNN algorithms use more than two frames in some form, to our best knowledge, no CNN-based method using more frames has been published. Unlike the above-mentioned approaches, the proposed method trains the regularisation from data and does not need any hand-crafted approximations.</p><p>The refinement network. The last important component added to the proposed architecture is a specialised refinement network <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b29">29]</ref>. We confirm it improves accuracy of fine details of the flow. We integrate the network with both occlusion estimation and temporal processing.</p><p>The refinement network was introduced in <ref type="bibr" target="#b18">[18]</ref> for optical flow estimation as a part of an architecture specialised on optical flow fine detail refinement. The inputs to the network are the optical flow estimated by previous blocks, the brightness error of the warped image and the input images themselves. In <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b29">29]</ref>, it was shown that training the first flow estimation block and the refinement network sequentially leads to improvements in estimated optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ContinualFlow</head><p>The proposed ContinualFlow method builds on the state-of-the-art PWC-Net architecture <ref type="bibr" target="#b35">[35]</ref>. We extend the architecture by adding i) occlusion estimation blocks and use the estimated occlusions for flow estimation, ii) an refinement network to improve fine detail accuracy, and iii) temporal connections for utilising the previous flow for estimation of both the flow and the occlusions. The original PWC-Net <ref type="bibr" target="#b35">[35]</ref> is composed of two networks: a feature pyramid extractor and a coarse-to-fine optical flow decoder. The feature pyramid extractor takes as input two images I t and I t+1 and encodes them into a pyramid of feature vectors F s t and F s t+1 with gradually decreasing spatial resolution (indexed by s) and with increasing channel dimension. The decoder, in a coarse-to-fine manner, takes features from the corresponding resolution s, warps features F s t+1 using the up-sampled flow F s−1 t+1 estimated at a coarser iteration s − 1 (if not at the coarsest resolution) and builds a correlation cost volume -a volume of feature correlations over a limited displacement range. The cost volume is then fed to the optical flow estimator, which produces the current scale optical flow F s and the process is repeated for higher resolution. We refer the reader to the original paper for further details. We are using the version with DenseNet <ref type="bibr" target="#b15">[15]</ref> and a context network as described in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Occlusion Estimation</head><p>PWC-Net and many other state-of-the-art approaches rely on the correlation cost volume for estimation of the optical flow <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b26">26]</ref>. Apart from being useful for the flow estimation, it is also indicative of possible occlusions. Intuitively, when the cost for all displacements for some pixel is high, the pixel is likely occluded in the next frame. In order to utilise this information, we propose to connect the occlusions estimator directly after the cost volume computation, even before any flow is estimated as shown in <ref type="figure" target="#fig_1">Fig 1.</ref> The output of the occlusions estimator is then sent to the optical flow estimator together with the cost volume itself. This way the occlusion estimation does not rely on the imprecise flow estimation and the flow estimator benefits from the additional input. Same as the flow estimator, the occlusions estimator works in a coarse-to-fine manner with higher resolution estimators receiving also up-sampled flow estimate from the lower resolution.</p><p>In experiments, we use an occlusion estimator with five convolutional layers with D, D 2 , D 4 , D 8 and two output channels (occluded/not occluded maps), where D = 89 in our case (the number of correlation cost volume layers + 8). All layers use ReLU activation except for the last one, which uses soft-max.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Refinement Network</head><p>It was shown that a specialised refinement network which processes the output of the initial network boosts the precision of the flow estimate, especially the fine details recovery <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b29">29]</ref>. The refinement network takes several extra inputs, like the current estimate of the optical flow, image I t+1 warped back to time step t and brightness error between I t and the warped I t+1 , and produces a refined optical flow <ref type="bibr" target="#b18">[18]</ref>.</p><p>The refinement network used in ContinualFlow has the same architecture as the optical flow decoder, but without the DenseNet connections. The main difference is in the network inputs. Instead of using the input images and their warps as in <ref type="bibr" target="#b18">[18]</ref>, we use the features from the feature pyramid on the corresponding scale and their warps as a richer input representation. The input error channel for these features is computed as a sum of the L 1 distance and structure similarity (SSIM) <ref type="bibr" target="#b39">[39]</ref>. We applied the refinement two times, additional refinements did not improve the accuracy in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ContinualFlow Estimation over Image Sequence</head><p>We use temporal connections, which give the optical flow decoder, the occlusions decoder and the refinement network an additional input: the flow estimated in the previous time step (see the orange arrows in <ref type="figure" target="#fig_1">Fig 1 and Fig 2)</ref>. When processing sequences longer than two frames these connections allow the network to learn typical relations between the previous and current flows and use them in the current frame flow estimation.</p><p>However, as discussed in Sec 1, the coordinate systems in which the two flows are expressed differ and need to be transformed onto each other in order to apply the previous flow to the correct pixels in the current time step. Here we describe two such transformations, forward and backward warping, and we test them independently as well as in combination (concatenation of both) in Sec 4.</p><p>Forward warping transformation. Forward warping transforms the coordinate system from time step t − 1 using the optical flow F t−1 itself. The warped flowF t−1 is computed aŝ</p><formula xml:id="formula_0">F t−1 (x + round(F t−1 (x))) = F t−1 (x) ,<label>(1)</label></formula><p>for all pixel positions x. For positions to which the flow F t−1 maps more than once we preserve the larger of the mapped flows. This prioritises larger motions, thus faster moving objects. Although the experiments show usefulness of this warping, the main disadvantage of this approach is that the transformation is not differentiable. Thus, the training cannot propagate gradients through this step and relies on the shared weights only.</p><p>Backward warping transformation. Alternatively, the coordinate system could be transformed using the backward flow B t from frame t to frame t − 1. This requires an extra evaluation of the network, but then the warping is a direct application of the differentiable spatial transformer <ref type="bibr" target="#b20">[20]</ref>. Thus, in this case the gradients are propagated through the temporal connections during training. A disadvantage of this approach is the computationally expensive computation of the backward flow.</p><p>Combining forward and backward warping. It is possible to use both warpings at the same time. In ContinualFlow we combine forward warped previous flow, backward warped previous flow and backward flow by simply concatenating their outputs. The only difference becomes that the previous flow input has nine channels: three times two for the flow warps and a validity masks for each warp (set to zero if the measurement is not available, e.g. at the beginning of the sequence).</p><p>Multi-frame sequence initialisation. The network is fed a pair of input images and the previously estimated flow. For the first frame in the sequence, no previous flow estimation is available. We estimate the initial optical flow between the first and second frame twice. First, we mask out the temporal connection and, in the second estimation, we use the first estimate as a temporal input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Loss</head><p>The network is trained end-to-end with a weighted multi-task loss over the flow and occlusions estimators at all scales,</p><formula xml:id="formula_1">L = S s=1 α s L s F + α O S s=1 α s L s O ,<label>(2)</label></formula><p>where α s is the weight of individual scale s losses and α O is the occlusion estimation weight. The sums are over all S spatial resolutions. The flow estimator loss L F is the same as in PWC-Net, i.e. the end-point error</p><formula xml:id="formula_2">L s F = x γ(x)||F s (x) − F s gt (x)|| 2 ,<label>(3)</label></formula><p>where F s is the estimated optical flow at scale s, F s gt the corresponding groundtruth optical flow and γ is the valid ground-truth flow mask (one for valid flow and zero otherwise). The sum is over all pixel positions. As in <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b7">7]</ref> we adopted the weighted pixel-wise cross-entropy loss for occlusion map estimation</p><formula xml:id="formula_3">L s O = − w noc x: Ogt(x)=1 ρ(x) log Pr(O(x) = 1|X) − w occ x: Ogt(x)=0 ρ(x) log Pr(O(x) = 0|X) ,<label>(4)</label></formula><p>where Pr(O(x) = 1|X) is computed using soft-max σ(·) function on the occlusion estimator output O, O gt is the ground truth occlusion map, ρ the valid ground-truth occlusion mask used for masking out images without ground-truth occlusions, and w occ and w noc are the fractions of occluded and non-occluded ground truth pixels respectively. As suggested by <ref type="bibr" target="#b35">[35]</ref>, we modify this loss for the final fine-tuning on the most complex evaluation benchmark datasets. Here we change the L s F loss to the generalised Charbonnier loss (with q = 0.4, = 0.01 as in <ref type="bibr" target="#b35">[35]</ref>):</p><formula xml:id="formula_4">L s F = x γ(x) |F s (x) − F s gt (x)| + q .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Training details. The ContinualFlow network is trained using a curriculum learning approach <ref type="bibr" target="#b4">[4]</ref> starting from a dataset with less complex motions and increasing gradually the task complexity <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b35">35]</ref>. First, we train on FlyingChairs dataset <ref type="bibr" target="#b9">[9]</ref> using the training parameters introduced in <ref type="bibr" target="#b35">[35]</ref> and following the learning rate schedule from <ref type="bibr" target="#b18">[18]</ref>. We do not use rotation, scaling and translation augmentations. Since the FlyingChairs dataset contains only two frames sequences and no occlusion ground truth, we cannot train the full ContinualFlow model with temporal connections and the occlusion map estimation. Instead, we use it for pre-training the PWC-Net part of the ContinualFlow network. The network is trained for 1200k iteration and the learning rate 1e-4 is divided by 2 each 200k iteration, starting from 400k. Images in a batch of size eight are randomly cropped to 448 × 384 px. Next, the all parts of the ContinualFlow network are trained on the Fly-ingThings dataset <ref type="bibr" target="#b25">[25]</ref>. Since occlusion maps were not available for this dataset, we computed them using the available backward and forward ground truth flows and the object segmentation masks. The mask O t (x) is set to "occluded" for pixel x when the object labels L t (x) and L t+1 (F gt (x)) differ or the bi-directional consistency between backward and forward flows differs by more than one pixel. The network is trained for 500k iteration and the learning rate, set to 1e-4 for the first 200k iterations, is divided by 2 at that point and after 100k iterations. First, we train the network without the refinement. Then, only the refinement is trained while all other weights are fixed. Images in the batch of size four are randomly cropped to 768 × 384 px. After cropping, optical flow pointing out of the frame is labelled as occluded. Finally, the ContinualFlow is trained on data from six datasets: Driving <ref type="bibr" target="#b25">[25]</ref>, KITTI'15 <ref type="bibr" target="#b27">[27]</ref>, VirtualKITTI <ref type="bibr" target="#b10">[10]</ref>, Sintel <ref type="bibr" target="#b6">[6]</ref>, HD1K <ref type="bibr" target="#b22">[22]</ref> and the FlyingChairs small motions dataset <ref type="bibr" target="#b18">[18]</ref>. These datasets, except for FlyingChairs, contain sequences longer than two frames and are suitable for the training of temporal connections. We used the first image twice for the FlyingChairs dataset to obtain the same batch size for all input data, the loss on the estimate of the (zero) flow F 0,1 is not used. Dense occlusion maps are available only for the Sintel and Driving datasets. We set occlusion estimation loss to zero on the rest. The network is fine-tuned for 500k iteration and the learning rate, set to 1e-5 for the first 200k interactions, is divided by 2 at that point and after 100k iterations. Images in batches of size four are randomly cropped to 768 × 320 px. We sample images from all datasets uniformly.</p><p>We set weights for individual scales as in <ref type="bibr" target="#b35">[35]</ref>. Maximal displacement in the cost volume is set to four. The same scale weights are set to train the refinement network and for the occlusion map estimation. The occlusion estimation weight α O is set to 0.1. All experiments are trained with the ADAM optimiser <ref type="bibr" target="#b21">[21]</ref> and 0.0004 weight decay. All parts of the network are implemented in TensorFlow.</p><p>The ContinualFlow training has the same three phases as training of PWC-Net. Only when training the refinement network separately, there is an additional phase which updates only the refinement parameters as mentioned above. Con-tinualFlow without the refinement network has 9.6M parameters, 0.8M more than the PWC-Net. The refinement network adds 5.0M parameters, it is based on the PWC-Net-small architecture. ContinualFlow runs at 8 FPS on KITTIresolution of 1240x375 px.</p><p>In the following, we focus on the Robust Vision Challenge <ref type="bibr" target="#b32">[32]</ref>, where one trained model with the same parameters has to be evaluated on four individual benchmarks <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b2">3]</ref> instead of fine-tuning for each particular dataset independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head><p>In this section, we experimentally evaluate the individual contributions and design choices for the ContinualFlow network trained on FlyingChairs <ref type="bibr" target="#b9">[9]</ref> and finetuned on FlyingThings [25] as described above. Below, the term baseline refers to our TensorFlow implementation of PWC-Net. Unlike the PWC-Net settings <ref type="bibr" target="#b35">[35]</ref>, we trained the network without rotation, scaling and translation augmentation of input frames.</p><p>Occlusion map learning. <ref type="table">Table 1</ref> shows the results of optical flow estimation with and without occlusion learning. Temporal connections are not used. Application of the occlusion map estimator improves performance on all tested datasets not only in occluded regions but also in all non-occluded regions. <ref type="figure" target="#fig_3">Fig 3  shows</ref> example estimated occlusion maps.</p><p>The specialised refinement block improves results of the estimated optical flow as is shown in <ref type="bibr" target="#b18">[18]</ref>. <ref type="table">Table 1</ref> compares the optical flow estimation with and without the refinement block. No temporal connections are used. The refinement block improves the estimated optical flow, especially in occluded areas.</p><p>Influence of the coordinate warping methods. We evaluated the three approaches for warping the previous flow estimate introduced in section 3.3. Results for individual datasets are shown in <ref type="table">Table 1</ref>. Forward warping W f is beneficial for the KITTI dataset <ref type="bibr" target="#b27">[27]</ref> and the Sintel Clean dataset <ref type="bibr" target="#b6">[6]</ref>, while backward warping W b is more suitable for the complex Sintel Final sequences. The combination of both, W bf , is the most accurate on FlyingThings sequences <ref type="bibr" target="#b25">[25]</ref>. All evaluated variants use the occlusion estimator in the decoder and no refinement.</p><p>Temporal connection placement. We experimented with passing the warped optical flow from previous frame to different network components, thus creating different temporal connections. In one variant, only the refinement network received the previous frame flow estimates. In another variant, all temporal connections as depicted in <ref type="figure" target="#fig_2">Fig 2</ref> were used. <ref type="table">Table 1</ref> shows how feeding these connections with different warpings influences the estimated flow. The best results were obtained with temporal connections leading into both the decoder and refinement networks and the combination of forward and backward warpings.</p><p>Number of refinement blocks. <ref type="table">Table 1</ref> shows results for 1, 2, 3 and 5 stacked refinement networks. Stacking more than two refinement networks is not beneficial. Thus the final network architecture contains only two refinements. All evaluated variants use the occlusion estimator and warps the previously estimated optical flow using both warping methods in the first part of the network and the refinement.</p><p>Multi-frame sequences initialisation For the first frame in the sequence, no previous frame flow estimate is available to be passed to the temporal connections. Unsurprisingly, the estimation on the first frame is usually slightly worse than at the consecutive frames. We tested two initialisations of the first frame flow estimation: (i) no flow (zero displacements) instead of the previously estimated optical flow and, (ii) a two-pass initial estimation of the currently estimated optical flow as described in Section 3.3. We evaluated both approaches for an increased length of the sequence on different datasets. As shown in <ref type="table">Table 1</ref>, the two-pass initialisation leads to quicker convergence and is most beneficial for the first optical flow estimation in the sequence. <ref type="table">Table 1</ref>: Ablation study of ContinualFlow. The leftmost column codes the experiment configurations: occlusion estimator (+OC); refinement network (+R); temporal connection with forward warping (W f ), backward warping (W b ) and both warping methods (W bf ); previous flow input in the refinement (RW x ); and two pass (2pass) initialisation of the first frame of the sequence N frames long. Performance measure are the KITTI 3-pixel error metric (column Fl) and the end-point error (in pixels, all other columns) for background (bg), foreground (fg), occluded (occ), non-occluded (noc) and all (all) pixels. The best performance in bold. All models trained on FlyingChairs and fine-tuned on Fly-ingThings. See section 3 for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State of the Art</head><p>We start by noting that a single model was used for all benchmarks without further fine-tuning to individual datasets. We were not able to evaluate occlusions on public benchmarks since there is no benchmark available for occlusion map estimation. ContinualFlow achieves recall 0.87 and F1-score 0.83 for the validation split of FlyingThings <ref type="bibr" target="#b25">[25]</ref> and recall 0.72 and F1-score 0.48 for Sintel <ref type="bibr" target="#b6">[6]</ref>. Examples of estimated occlusion maps are shown in <ref type="figure" target="#fig_3">Fig 3.</ref> KITTI'15 optical flow benchmark <ref type="bibr" target="#b27">[27]</ref> results are reported in <ref type="table" target="#tab_1">Table 2</ref>. Fl refers to the KITTI evaluation metric -the percentage of pixels with end-point-error greater than 3 px. Our method ranked first among methods participating in the Robust Vision Challenge (ROB) and third for all optical flow estimation methods with score 10.03% on all evaluated pixels. We are interested in ROB Challenge since methods outside ROB fine-tune on each particular dataset, resulting in over-fitting, which we wanted to avoid.</p><p>Sintel. <ref type="figure">Fig 4 shows</ref> visual comparison with the closest competitors. Results of ROB participants on the Sintel dataset are reported in <ref type="table" target="#tab_2">Table 3</ref>. ContinualFlow ranked first on Sintel Final for the all pixels end-point-error evaluation. As we are focused on occlusion estimation and handling, we point out that ContinualFlow achieves best results for estimation in occluded areas with significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Vision Challenge.</head><p>A snapshot of the leaderboard 4 of optical flow Robust Vision Challenge <ref type="bibr" target="#b32">[32]</ref> is shown in <ref type="table" target="#tab_3">Table 4</ref>. ContinualFlow is built on our implementation of PWC-Net <ref type="bibr" target="#b35">[35]</ref>. While ContinualFlow did not achieve a better results in the ROB than the original PWC-Net, the experiments show that our contributions outperform the results of our baseline. The source code for PWC-Net was released by the authors just days before the ACCV submission deadline, so a direct comparison was possible only   through ROB vision challenge submissions, which are limited in number by the challenge rules. We did our best to follow the paper regarding the architecture, parameters and training. Later, when analysing the results, we found two main differences: i) Due to implementation issues, we omitted rotation and scaling data augmentations, which in retrospect could harm the performance significantly as suggested in <ref type="bibr" target="#b24">[24]</ref>. ii) Our implementation is in Tensorflow whereas the original implementation is in Caffe, so some of the suggested training parameter values may need to be fine-tuned for this framework. Still, the ablation study clearly shows the impact and significance of the novelties (occlusion estimation, feeding the previous estimate of optical flow as input).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The ContinualFlow network for optical flow estimation was introduced, with two novelties -occlusion estimation integrated in the optic flow computation and the use of the optic flow from the previous time instant, and, through recursion, of all prior flows. We showed that the two contributions improve performance, especially in occluded areas or areas close to motion discontinuities. In evaluation on standard dataset ContinualFlow is top ranked in Sintel and 3rd in KITTI.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig 1 showsa schematic of the PWC-Net with both the occlusions estimation blocks and temporal connections. Another diagram containing also the refinement network is shown inFig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>ContinualFlow -optical flow and occlusion decoder, which extends the PWC-Net<ref type="bibr" target="#b35">[35]</ref> flow decoder for occlusion estimation. The feature pyramid extractor (in blue) is a convolutional network which produces a feature pyramid given an input image. A correlation cost volume is computed on each scale from warped features from the second frame using up-sampled flow estimated at a coarser level of decoder. The cost volume is used to estimate occlusions in occlusion estimator (in magenta). The cost volume and the occlusion map are inputs to the optical flow estimator. For clarity, the diagram shows only three of the six levels of the ContinualFlow pyramid extractor. The output resolution is quarter of the input reference frame. Please, refer to the text for additional network details and inputs explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Block diagram of ContinualFlow. Feature extractors with shared weights compute a feature pyramid from the input images. Features are input to the optical flow and occlusion decoder and the refinement blocks. The decoder estimates the optical flow and the occlusion map from the input features and from the temporal connection -the warped optical flow from the previous time step. Optical flow and occlusion maps are finalised by the refinement blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Example estimated occlusion maps on the Sintel (final) dataset, our validation split. ContinualFlow estimates occlusions up to quarter resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>KITTI'15 optical flow benchmark results of Robust Vision Challenge participants as of June 7, 2018. Performance measured by the KITTI 3-pixel error metric (column Fl) and the end-point error (in pixels, all other columns) for background (bg), foreground (fg), occluded (occ), non-occluded (noc) and all (all) pixels. The best results in bold. Anonymous entries in time of paper submission are marked [anon]. Methods are sorted according to Fl-all, the default ranking for KITTI. 21.82 15.42 8.44 17.90 10.15 FF++ ROB [33] 15.32 19.27 15.97 7.82 15.33 9.18 ResPWCR ROB [anon] 16.63 16.18 16.55 10.10 12.23 10.49 AugFNG ROB [anon] 19.77 9.95 18.14 13.75 6.71 12.47 DMF ROB [40] 30.74 30.07 30.63 19.32 25.60 20.46</figDesc><table><row><cell></cell><cell cols="6">KITTI'15 occ (%) KITTI'15 noc (%)</cell></row><row><cell>Fl (%)</cell><cell>bg</cell><cell>fg</cell><cell>all</cell><cell>bg</cell><cell>fg</cell><cell>all</cell></row><row><cell cols="7">ContinualFlow ROB 8.54 17.48 10.03 5.90 14.99 7.55</cell></row><row><cell>LFNet ROB [anon]</cell><cell cols="6">11.18 10.20 11.01 6.14 6.87 6.27</cell></row><row><cell>PWC-Net ROB [35]</cell><cell cols="6">11.22 13.69 11.63 7.12 10.29 7.69</cell></row><row><cell>ProFlow ROB [23]</cell><cell>14.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Sintel benchmark results for Robust Vision Challenge participants. Performance measured the end-point error (EPE, in pixels) for matched (noc), unmatched (occ) and all (all) pixels. The best results in bold. Anonymous entries marked [anon]. Methods are sorted by EPE all, the default ranking for Sintel. ContinualFlow ROB 4.528 2.723 19.248 3.341 1.752 16.292 PWC-Net ROB [35] 4.903 2.454 24.878 3.897 1.726 21.637 ProFlow ROB [23] 5.015 2.659 24.192 2.709 1.013 16.549 AugFNG ROB [anon] 5.500 2.978 26.052 3.606 1.603 19.939 LFNet ROB [anon] 5.966 3.278 27.893 4.815 2.333 25.065 FF++ ROB [33] 6.496 2.990 35.057 3.953 1.148 26.836 ResPWCR ROB [anon] 6.530 3.849 28.371 5.674 3.138 26.380 DMF ROB [40] 7.475 3.575 39.245 5.368 1.742 34.899</figDesc><table><row><cell>Sintel Final</cell><cell></cell><cell>Sintel Clean</cell><cell></cell></row><row><cell>all noc</cell><cell>occ</cell><cell>all noc</cell><cell>occ</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Robust Vision Challenge. Performance measured by ranking of all metrics in individual datasets. The best results in bold. Anonymous entries marked [anon]. Methods are sorted by the Robust Vision Challenge rank.</figDesc><table><row><cell></cell><cell cols="4">Middlebury KITTI MPI Sintel HD1K</cell></row><row><cell>PWC-Net ROB [35]</cell><cell>2</cell><cell>4</cell><cell>2</cell><cell>1</cell></row><row><cell>ProFlow ROB [23]</cell><cell>1</cell><cell>6</cell><cell>1</cell><cell>4</cell></row><row><cell>ContinualFlow ROB</cell><cell>5</cell><cell>2</cell><cell>3</cell><cell>3</cell></row><row><cell>LFNet ROB [anon]</cell><cell>7</cell><cell>1</cell><cell>6</cell><cell>5</cell></row><row><cell>AugFNG ROB [anon]</cell><cell>9</cell><cell>3</cell><cell>4</cell><cell>2</cell></row><row><cell>FF++ ROB [33]</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>6</cell></row><row><cell>DMF ROB [40]</cell><cell>4</cell><cell>8</cell><cell>7</cell><cell>8</cell></row><row><cell>ResPWCR ROB [anon]</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>7</cell></row><row><cell>WOLF ROB [anon]</cell><cell>8</cell><cell>9</cell><cell>9</cell><cell>9</cell></row><row><cell>TVL1 ROB [30]</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>H+S ROB [14]</cell><cell>11</cell><cell>11</cell><cell>11</cell><cell>11</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As of the submission date, July 7, 2018.<ref type="bibr" target="#b1">2</ref> The "Final pass" category.<ref type="bibr" target="#b2">3</ref> Excluding scene flow methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">As of July 7, 2018.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research was supported by Toyota Motor Europe, CTU student grant SGS17/185/OHK3/3T/13 and the OP VVV MEYS project CZ.02.1.01/0.0/0.0/16 019/0000765 Research Center for Informatics.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnn-based patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3250" to="3259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s11263-010-0390-2</idno>
		<ptr target="https://doi.org/10.1007/s11263-010-0390-2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Curriculum learning. In: International conference on machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Robust dynamic motion estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dense multi-frame optic flow for non-rigid objects using subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A variational approach to video registration with subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep discrete flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mirrorflow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-frame correspondence estimation using subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<publisher>ANIPS</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The hci benchmark suite: Stereo and flow ground truth with uncertainties for urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krispin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gussefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahimimoghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jahne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Proflow: Learning to predict optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">What makes good synthetic training data for learning disparity and optical flow estimation? IJCV pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scene segmentation from visual motion using global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Buxton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tv-l1 optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Robust Vision Challenge team: Robust vision challenge</title>
		<ptr target="http://www.robustvision.net" />
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
	<note>Online; accessed 8</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wasenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
		<title level="m">Flowfields++: Accurate optical flow correspondences meet robust interpolation. ICIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Local layering for joint motion estimation and occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<title level="m">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint motion estimation and segmentation of complex scenes with label costs and occlusion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1878" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Modeling temporal coherence for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<title level="m">Optical flow in mostly rigid scenes. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bilateral filtering-based optical flow estimation with occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isnardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
