<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-02">2 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
							<email>deeptigp@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
							<email>trandu@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
							<email>hengwang@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
							<email>dhruvm@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-02">2 May 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current fully-supervised video datasets consist of only a few hundred thousand videos and fewer than a thousand domain-specific labels. This hinders the progress towards advanced video architectures. This paper presents an in-depth study of using large volumes of web videos for pre-training video models for the task of action recognition. Our primary empirical finding is that pre-training at a very large scale (over 65 million videos), despite on noisy social-media videos and hashtags, substantially improves the state-of-the-art on three challenging public action recognition datasets. Further, we examine three questions in the construction of weakly-supervised video action datasets. First, given that actions involve interactions with objects, how should one construct a verb-object pretraining label space to benefit transfer learning the most? Second, frame-based models perform quite well on action recognition; is pre-training for good image features sufficient or is pre-training for spatio-temporal features valuable for optimal transfer learning? Finally, actions are generally less well-localized in long videos vs. short videos; since action labels are provided at a video level, how should one choose video clips for best performance, given some fixed budget of number or minutes of videos?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is well-established <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref> that pre-training on large datasets followed by fine-tuning on target datasets boosts performance, especially when target datasets are small <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b49">50]</ref>. Given the well-known complexities in constructing large-scale fully-supervised video datasets, it is intuitive that large-scale weakly-supervised pre-training is vital for video tasks.</p><p>Recent studies <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b62">63]</ref> have clearly demonstrated that pre-training on hundreds of millions (billions) of noisy web images significantly boosts the state-of-the-art in object classification. While one would certainly hope that successes would carry over from images <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b62">63]</ref> to videos, action recognition from videos presents certain unique challenges that are absent from the image tasks.</p><p>First, while web images primarily face the challenge of label noise (i.e., missing or incorrect object labels), for videos in the wild, the challenges are two-fold: label noise and temporal noise due to the lack of localization of action labels. In real-world videos, a given action typically occupies only a very small portion of a video. In stark contrast, a typical web image is a particular moment in time, carefully selected by its creator for maximal relevance and salience.</p><p>Second, in prior work on images, labels were restricted to scenes and objects (i.e., nouns). However, action labels (eg: "catching a fish") are more complex, typically involving at least one verb-object pair. Further, even at large scale, many valid verb-object pairs may be observed rarely or never at all; for example, "catching a bagel" is an entirely plausible action, but rarely observed. Therefore, it is natural to inquire: is it more useful to pre-train on labels chosen from marginal distributions of nouns and verbs, do we need to pre-train on the observed portion of the joint distribution of (verb, noun) labels, or do we need to focus entirely on the target dataset's labels? How many such labels are sufficient for effective pre-training and how diverse should they be?</p><p>Third, the temporal dimension raises several interesting questions. By analogy to images, short videos should be better temporally-localized than longer videos; we investigate this assumption and also ask how localization affects pre-training. In addition, longer videos contain more frames, but short videos presumably contain more relevant frames; what is the best choice of video lengths when constructing a pre-training dataset?</p><p>Finally, we question whether pre-training on videos (vs images) is even necessary. Both frame-based models and image-based pre-training methods like inflation <ref type="bibr" target="#b11">[12]</ref> have been successful in action recognition. Is pre-training on video clips actually worth the increased compute, or, are strong image features sufficient?</p><p>In this work, we address all these questions in great detail. Our key aim is to improve the learned video feature representations by focusing exclusively on training data, which is complementary to model-architecture design. Specifically, we leverage over 65 million public, usergenerated videos from a social media website and use the associated hashtags as labels for pre-training. The label noise and temporal noise makes our training framework weakly-supervised. Unlike all existing fully-supervised video datasets <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr">65]</ref> which required expensive annotation, our training data is truly extensible to billion-scale without incurring any annotation costs. We effectively tackle the aforementioned challenges with label space and temporal noise, and demonstrate significant performance gains on various target tasks. Overall, we summarize our findings:</p><p>• Large-scale weak-supervision is extremely beneficial:</p><p>We show that large-scale video data, despite not providing strong supervision, tremendously helps models of varied capacities in learning better features. Our experiments clearly demonstrate that content diversity and scale outdo label and temporal noise. • Impact of data volume and model capacity: We report interesting findings on the effect of pre-training data size, data sampling strategies, model capacities, etc. For instance, we find that increasing the training data (Sec. 4.1.1) improves performance while increasing model capacity exhibits interesting behavior (Sec. 4.1.2) . • What is a suitable pre-training video label space? We systematically construct pre-training label sets that vary in cardinality and type (e.g., verbs, nouns, etc.), and study their effects of target tasks (Sec. 4.2). One key finding is that as in <ref type="bibr" target="#b46">[47]</ref>, pre-training labels that overlap the most with the target labels improve performance. • Short vs. long videos for pre-training? We study the effect of pre-training on short vs. long videos (Sec. 4.3.1) and show that (a) for a fixed video length budget (e.g., 400K minutes of total training video duration), it is beneficial to choose a large number of short videos as they supply succinct localized actions compared to fewer long videos, (b) for a fixed video budget (e.g., 5M ), choosing longer videos are beneficial as they offer diverse content. • Do we need pre-training on video data? We investigate the true value of pre-training using video data. We show that it is necessary to use videos as opposed to video frames or images followed by inflation <ref type="bibr" target="#b11">[12]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning from weak supervision: Given the known challenges in gathering exhaustive annotations for various image and video tasks, leveraging object labels and other meta information to supply weak supervision <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref> is a widely-adopted approach. Orthogonal to this strategy, our work investigates transfer learning benefits when networks are pre-trained on weakly-supervised data, i.e., data afflicted with label and temporal noise. While novel changes to architectures have been proposed <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62]</ref> to combat label noise, our experiments demonstrate that large-scale training of an existing video architecture <ref type="bibr" target="#b14">[15]</ref> makes it noise-resilient. Dataset sources: Many prior approaches use Internet as a natural source of content <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b68">69]</ref> and the associated search queries, hashtags, or user-comments as labels to construct datasets. Most large-scale video datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref> were constructed by first curating a label taxonomy, analyzing the text metadata surrounding YouTube or Flickr videos, followed by some manual cleaning of the non-visual labels. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48]</ref> analyzed movie scripts for automatic annotation of human actions for recognizing and localizing actions and actors. Our proposed work uses web-data and the associated text to supply weak supervision during pre-training.</p><p>Pre-training on large datasets: Since datasets for complex tasks such as object detection, segmentation and action recognition in videos are in smaller order of magnitude compared to ImageNet <ref type="bibr" target="#b17">[18]</ref>, pre-training on larger, auxiliary data followed by fine-tuning on target tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b66">67]</ref> is very popular. Indeed, inflation <ref type="bibr" target="#b11">[12]</ref> was proposed to exclusively leverage ImageNet instantiation by way of converting 2D filters to 3D, given that training 3D models is computationally expensive. In this work, we show that pre-training on video clips performs significantly better than pre-training on image/video frames followed by inflation (Sec. 4.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weak-supervision of video models</head><p>We harness millions of public videos from a social media website and use the associated hashtags as labels to supply weak supervisory signals while training video models. We construct and experiment with a variety of weaklysupervised datasets, which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Source datasets</head><p>To construct pre-training video datasets, we use several seed action label sets and gather videos that are related to these labels. Specifically, for a given seed action label "catching a fish," we first construct all possible meaningful phrases (i.e., relevant hashtags) from it by taking the original and stemmed versions of every word in the seed label and concatenating them in all possible permutations. As an example, relevant hashtags("catching a f ish") = {#catchingaf ish, #catchf ish, #f ishcatching, ...}. We then download public videos that are tagged with at least one of the hashtags from the set of relevant hashtags("catching a f ish") and associate them with the initial seed label. We use the seed labels as the final labels for videos during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Properties of the source datasets</head><p>Seed labels: As we describe in Sec. 4.2, we study the effect of pre-training on different types of labels by considering four different seed label sets. The resulting source datasets are summarized in <ref type="table" target="#tab_1">Table 1</ref>. We use a notation IG − source − size throughout this paper, where source indicates the seed label set used and size indicates the number of videos 1 . Our primary training set uses 400 action labels from Kinetics <ref type="bibr" target="#b70">[71]</ref> as seed labels, resulting in IG − Kinetics dataset comprising 359 labels 2 . We also consider (a) the 1428 hashtags that match the 1000 synsets from ImageNet-1K <ref type="bibr" target="#b17">[18]</ref>, thereby constructing an IG − Noun dataset 3 , (b) the 438 verbs from Kinetics and VerbNet <ref type="bibr" target="#b63">[64]</ref>, thus an IG − Verb dataset, and (c) all possible concatenations of the 438 verbs and the 1428 nouns from the above two seed label sets. We identify over 10, 653 such meaningful combinations 4 , thus constructing an IG − Verb + Noun dataset. More details on dataset construction are provided in the supplementary material. We want to reiterate that no manual annotation was involved in constructing these datasets implying that there is a large amount of noise in the data. Long tail distribution: Distribution of videos in all our source datasets is heavily long-tailed. To mitigate its effect on model training, we adopt the square root sampling approach <ref type="bibr" target="#b48">[49]</ref> when constructing pre-training data in our experiments as it proved to be most beneficial for images <ref type="bibr" target="#b46">[47]</ref>. Diversity: Unlike all benchmark datasets which contain short videos of localized actions, video lengths in our source datasets range from 1 to 60 seconds. Thus, the labeled action can occur anywhere in a video, resulting in a large amount of temporal noise, an aspect we study in Sec. 4.3.</p><p>Given that we cannot make our datasets or the exact hashtags used public (as was the case with <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b46">47]</ref>), we acknowledge that it is not possible for other research groups to reproduce our results at this time. Despite this limitation, we hope that the reader finds value in our wide-range of findings on various aspects of large-scale video pre-training </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Target datasets</head><p>Next, we describe the target datasets used in experiments. Kinetics [65]: Kinetics is a multi-class dataset with~246K training videos (400 human action labels). We report performance on the 20K validation videos. EPIC-Kitchens <ref type="bibr" target="#b16">[17]</ref>: EPIC-Kitchens is a multi-class egocentric dataset with~28K training videos associated with 352 noun and 125 verb classes. For our ablation studies, following <ref type="bibr" target="#b5">[6]</ref> we construct a validation set of unseen kitchen environments. We evaluate our best pre-trained model on validation, standard seen (S1: 8047 videos), and unseen (S2: 2929 videos) kitchens test datasets.</p><p>Something-Something-v1 <ref type="bibr" target="#b28">[29]</ref> is a multi-class dataset of 86K training videos and 174 fine-grained actions. We report results on the 11, 522 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video deduplication:</head><p>We devise a pipeline to deduplicate videos in the source datasets that may overlap with any from the target dataset. To err on the side of caution, we adopt an aggressive low-precision high-recall strategy and remove any potential duplicates (eg: we removed~29K videos from the IG-Kinetics dataset). Details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pre-training Setup</head><p>Models: R(2+1)D-d <ref type="bibr" target="#b14">[15]</ref> 5 is the fundamental architecture used for pre-training, where d denotes model depth = {18, 34, 101, 152}. As in <ref type="bibr" target="#b29">[30]</ref>, we construct models of depth &gt; 34 by replacing simple temporal blocks with bottleneck blocks for computational feasibility. We direct the reader to the supplementary material for details.</p><p>Loss Function: Our pre-training datasets are multi-label since multiple hashtags may be associated with any given video. The authors of <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47]</ref> have observed that perlabel sigmoid outputs with logistic loss do not work well for noisy labels. Hence, we follow a simple strategy of randomly assigning one of the associated hashtags to each video thereby formulating a multi-class problem, and use softmax activations with cross-entropy loss.</p><p>Training Details: Video frames are down-sampled to a resolution of 128 × 171 and each video clip is generated by cropping a random patch of size 112 × 112 from a frame. Video clips of either 8 or 32 frames are used in our experiments, and temporal jittering is also applied to the input. Synchronous stochastic gradient descent (SGD) is used to train our models on 128 GPUs across 16 machines using caffe2 <ref type="bibr" target="#b9">[10]</ref>. When 32 frames per input video clip are considered, each GPU processes 6 videos at a time (due to memory constraints), while 16 videos are processed at a time when 8 frames per video clip are considered. Batch normalization (BN) is applied to all convolutional layers and the statistics <ref type="bibr" target="#b33">[34]</ref> are computed on each GPU. All pretraining experiments process 490M videos in total. Learning rate is set following the linear scaling procedure proposed in <ref type="bibr" target="#b27">[28]</ref> with a warmup. An initial learning rate of 0.192 is used which is divided by 2 at equal steps such that the total number of learning rate reductions is 13 over the course of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we study various aspects of large-scale weakly-supervised video pretraining. We first describe our evaluation setup and then report our extensive analysis on three aspects: (a) effect of scale, e.g., model capacity and pre-training data size, (b) design of the pre-training label space, and (c) temporal properties of videos. We also pretrain on benchmark datasets such as Sports-1M <ref type="bibr" target="#b38">[39]</ref>, Kinetics [65] as competitive baselines.</p><p>Evaluation Setup: As in <ref type="bibr" target="#b46">[47]</ref>, we consider two scenarios:</p><p>• Full-finetuning (full-ft) approach involves bootstrapping with a pre-trained model's weights and training endto-end on a target dataset. We do a grid search for best the hyper-parameters (learning rate etc.) on validation data constructed by randomly holding out (10%) of training data. The hyper-parameters used for each experiment and target dataset are in the supplementary material. Full-ft approach has the disadvantage that it can potentially mask the absolute effect of pre-training for large target datasets. • Fully-connected (fc-only) approach involves extracting features from the final fc layer of a pre-trained model and training a logistic regressor on each target dataset. This approach evaluates the strength of the learned features without changing the network parameters. For multi-class target datasets our loss function is a L2regularized logistic regressor and we report accuracy. For multi-label datasets, we use a per-label sigmoid output followed by logistic loss and report mAP. During testing, center crops of 10 clips uniformly sampled from each test video are considered, and the average of these 10 clip predictions are used to obtain the final video-level prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effect of large-scale 4.1.1 Amount of pre-training data</head><p>To understand this question, we pre-train on different amounts of training data by constructing different data subsets -IG-Kinetics-{500K, 1M, 5M, 10M, 19M, 65M }. R(2+1)D-34 models are independently trained on these data subsets on the exact same labels, with an input of 8-frames per video and evaluated on Kinetics ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>) and EPIC-Kitchens ( <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>).</p><p>As in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b62">63]</ref>, we observe that performance improves log-linearly with training data size indicating that more pretraining data leads to better feature representations. For Kinetics, with full-ft approach, pre-training using 65M videos gives a significant boost of 7.8% compared to training from scratch (74.8% vs. 67.0%). With increase in training data, performance gains are even more impressive when using fc-only approach, which achieves an accuracy of 73.0% with 65M training videos, thus closely matching the accuracy from full-ft approach (74.8%). On EPIC-Kitchens, using IG-Kinetics-65M yields an improvement of 3.8% compared to using Kinetics for pre-training (16.1% vs. 12.3%). Compared with Kinetics, on EPIC-Kitchens, there is a larger gap in the performance between full-ft and fc-only approaches. This may be due to a significant domain difference in the pre-training and target label space.</p><p>These plots indicate that despite the dual challenge of label and temporal noise, pre-training using millions of web videos exhibit excellent transfer learning performance.</p><p>Data Sampling: Web data typically follows a Zipfian (long tail) distribution. When using only a subset of such data for pre-training, a natural question to ask is, if there are better ways to choose a data subset beyond random sampling. We design one such approach where we retain all videos from tail classes and only sub-sample head classes. We refer to this scheme as tail-preserving sampling. <ref type="figure" target="#fig_0">Figure 1</ref> (c) compares random and tail-preserving sampling strategies for Kinetics and reports performance obtained via fc-only approach. We observe that the tailpreserving strategy does consistently better and in fact, the performance saturates around 10M − 19M data points. Hence, for all future experiments, we adopted tailpreserving sampling strategy when needed. <ref type="table" target="#tab_2">Table 2</ref> reports the capacity of different video models and their effect on transfer learning performance. Specifically, we use IG-Kinetics-65M to pre-train 4 different R(2+1)Dd models, where d = {18, 34, 101, 152} with input cliplength 32. On Kinetics, we observe that increasing model capacity improves the overall performance by 3.9%. In comparison, when training from scratch, the accuracy improves only by 2.7%. Interestingly, on EPIC-Kitchens, pretraining either using IG-Kinetics-65M or Kinetics (referred to as baseline) yield similar gains with the increase in model capacity. Unlike in <ref type="bibr" target="#b46">[47]</ref> where the transfer learning performance was observed to be bottlenecked by capacity, we see a saturation in performance when going from d = 101 to d = 152 <ref type="bibr" target="#b5">6</ref> . Given that R(2+1)D-152 has higher GFLOPS  compared to the largest image model in <ref type="bibr" target="#b46">[47]</ref>, we believe that our model may be bottlenecked by the amount of pretraining data. Thus, using more than 65M training videos may further boost the accuracy. Additionally, inability to do long-range temporal reasoning beyond 32 frames (due to memory constraints) may also be leading to this behavior. These questions are interesting to explore in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Effect of model capacity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploring the pre-training label space</head><p>Web videos and the associated (noisy) hashtags are available in abundance; hence it is natural to question: what constitutes a valuable pre-training label space for achieving superior transfer learning performance and how to construct one? Since hashtags are generally composed of nouns, verbs, or their combinations, and vary greatly in their frequency of occurrence, it is important to understand the trade-offs of different pre-training label properties (eg: cardinality and type) on transfer learning. In this section, we study these aspects in great detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Effect of the nature of pre-training labels</head><p>To study the type of pre-training labels that would help target tasks the most, as mentioned in Sec. 3.1, we systematically construct label sets that are verbs, nouns, and their combinations. Specifically, we use IG-Kinetics-19M, IG-Verb-19M, IG-Noun-19M, and IG-Verb+Noun-19M as our pre-training datasets. We use R(2+1)D-34 with cliplength of 32 for training. <ref type="figure" target="#fig_1">From Fig. 2</ref>, we may observe that for each target dataset, the source dataset whose labels overlap the most with it yield maximum performance. For instance, for Kinetics we see an improvement of at least 5.5%, when we use IG-Kinetics-19M for pre-training, compared to other pre-training datasets <ref type="figure" target="#fig_1">(Fig. 2(a)</ref>). Pretraining on IG-Noun benefits the noun prediction task of EPIC-Kitchens the most while IG-Verb significantly helps the verb prediction task (at least 1.2% in both cases, <ref type="figure" target="#fig_1">Fig.  2(b)</ref> and (c)). We found an overlap of 62% between IG-Verb and the verb labels and 42% between IG-Noun and the noun labels in EPIC-Kitchens. Pre-training on Sports-1M performs poorly across all target tasks, presumably due to its domain-specific labels.</p><p>Given that actions in EPIC-Kitchens are defined as verbnoun pairs, it is reasonable to expect that IG-Verb+Noun is the most well-suited pre-training label space for EPIC-Kitchens-actions task. Interestingly, we found that this was not the case ( <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>). To investigate this further, we plot the cumulative distributions of the number of videos per label for all four pre-training datasets in <ref type="figure" target="#fig_2">Fig. 3</ref>. We observe that though IG-Verb+Noun captures all plausible verb-noun combinations leading to a very large label space, it is also heavily skewed (and hence sparse) compared to other datasets. This skewness in the IG-Verb+Noun label space is perhaps offsetting its richness and diversity as well as the extent of its overlap with the EPIC-Kitchens action labels. Thus, for achieving maximum performance gains, it may be more effective to choose those pre-training labels that most overlap with the target label space while making sure that label distribution does not become too skewed. Understanding and exploiting the right trade-offs between these two factors is an interesting future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effect of the number of pre-training labels</head><p>In Sec. 4.1.1, we study how varying the number of pretraining videos for a fixed source label space effects the transfer learning performance. In this section, we investigate the reverse scenario, i.e., vary the number of pretraining labels while keeping the number of videos fixed. We consider IG-Verb+Noun as our candidate pre-training dataset due to a large number (10, 653) of labels. We randomly 7 sub-sample different number of labels from the full label set all the way to 675 labels and fix the number of videos in each resulting dataset to be 1M . We did not have enough training videos, i.e., at least 1M for fewer than 675 labels. Label sampling is done such that the smaller label    space is a subset of the larger one. R(2+1)D-34 is used for pre-training with a clip-length of 8. <ref type="figure" target="#fig_4">Figure 4</ref> (a) shows performance on Kinetics. We may observe that using full-ft, there is an improvement of~1% until 1350 labels, following which the performance saturates. For fc-only approach, the improvement in accuracy is 9% before it saturates at 2700 labels. This suggests that the relatively fewer action labels in Kinetics (400) may not require a highly diverse and extensive pre-training label space such as IG-Verb+Noun. However, a large image label space ( 17K hashtags) was proven <ref type="bibr" target="#b46">[47]</ref> to be effective for highly diverse target image tasks (e.g., ImageNet-5k). Hence, we believe that to reap the full benefits of a large pre-training video label space, there is a need for more diverse benchmark video datasets with large label space.</p><p>Next, to understand the effect when the number of pretraining labels are fewer than the target labels (i.e, &lt;400 for Kinetics), we consider IG-Kinetics as our pre-training dataset and vary the number of labels from 20 to 360. Pretraining data size is again fixed to 1M . From <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>, we may observe a log-linear behavior as we vary the number of labels. There is a significant drop in the performance when using fewer labels even in the full-ft evaluation setting. This indicates that pre-training on a small label space that is a subset of the target label space hampers performance.</p><p>In summary, while using fewer pre-training labels hurts performance ( <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>), increasing the diversity through a simple approach of combining verbs and nouns ( <ref type="figure" target="#fig_4">Fig. 4 (a)</ref>) does not improve performance either. Thus, this analysis highlights the challenges in label space engineering, especially for video tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Exploring the temporal dimension of video</head><p>We now explore the temporal aspects of videos over long and short time scales. As mentioned in Sec. 3.1, our dataset inherently has large amounts of temporal noise as video lengths vary from 1 -60 seconds and no manual cleaning was undertaken. While short videos are better localized, longer videos can potentially contain more diverse content. First, we attempt to understand this trade-off between temporal noise and visual diversity. Second, we address a more fundamental question of whether video clipbased pre-training is needed at all or is frame-based pretraining followed by inflation <ref type="bibr" target="#b11">[12]</ref> is sufficient. The latter has an advantage of being very fast and more scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effect of temporal noise</head><p>To study this, we construct 3 datasets from IG-Kinetics: (i) short-N: N videos of lengths between 1 -5 seconds.</p><p>(ii) long-N: N videos of lengths between 55 -60 seconds. (iii) long-center-N: N videos (4 second long) constructed from the center portions of videos from long-N. We ensure that the temporal dimension is the only factor that varies by keeping the label space and distribution (videos per label) fixed across these 3 datasets. Temporal jittering is performed for all these datasets during pretraining. Also, note that the exact same number of videos are seen while training on all the datasets. We now consider the following two scenarios. Fixed number of videos budget (F1): A natural question that arises is: given a fixed budget of videos, what temporal property should guide our selection of pre-training videos? To answer this, we fix the total number of unique videos to 5M and consider short-5M, long-5M, and long-center-5M datasets. Note that both short-5M and long-center-5M have similar per-video duration (i.e., 4 seconds on average), but long-center-5M has greater temporal noise, since short videos are presumably more temporally localized than any given portion of longer videos. Between short-5M and long-5M, while short-5M has better temporal localization, long-5M may have greater content diversity From  observe that short-5M performs significantly better than long-center-5M suggesting that short videos do provide better temporal localization. Also, long-5M performs better than short-5M by 3.2% indicating that more diverse content in longer videos can mask the effect of temporal noise. Thus, for a fixed total number of videos, longer videos may benefit transfer learning than short videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fixed video time budget (F2):</head><p>If storage or bandwidth is a concern, it is more practical to fix the total duration of videos, instead of the total number. Given this fixed budget of video hours, should we choose short or long videos? To answer this, we consider short-5M, long-center-5M and long-500K datasets, all with similar total video hours. From <ref type="table" target="#tab_3">Table 3</ref>, we observe that short-5M significantly outperforms long-500K. This indicates that diversity and/or temporal localization introduced by using more short videos is more beneficial than the diversity within fewer long videos. Thus, for a fixed video duration budget, choosing more short videos yields better results. long-center-5M and long-500K perform similarly, indicating that on average, a fixed central crop from a long video contains similar information to a random crop from a long video. short-5M outperforms long-center-5M, consistent with the claim that short videos do indeed have better temporal localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Frame-vs. clip-based pre-training:</head><p>Although we have shown substantial gains when using clipbased R(2+1)D models for large-scale weakly supervised pre-training, it is computationally more intensive than 2D (image) models. Moreover, techniques such as inflation <ref type="bibr" target="#b11">[12]</ref> efficiently leverage pre-trained image models by converting 2D filters to 3D and achieve top-performance on benchmark datasets. Given these, we want to understand the key value in pre-training directly on weakly-supervised video clips vs. images. Towards this end, we first construct an image variant of the IG-Kinetics dataset (suffixed by −Images in <ref type="table" target="#tab_5">Table  4</ref>), following the procedure described in Sec. 3.1. We pretrain an 18 layer 2D deep residual model (R2D) <ref type="bibr" target="#b29">[30]</ref> from scratch on different types of 2D data (image/single video frames). We then inflate <ref type="bibr" target="#b11">[12]</ref> this model to R3D 8 <ref type="bibr" target="#b14">[15]</ref> and perform full-finetuning with a clip-length of 8 on Kinetics.</p><p>From the inflation-based models in <ref type="table" target="#tab_5">Table 4</ref>, we may observe that, pre-training on ImageNet achieves an improvement of 0.9% compared to training R3D from scratch, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with state-of-the-art</head><p>In this section, we compare R(2+1)D-34 and R(2+1)D-152 models pre-trained on IG-Kinetics-65M with several state-of-the-art approaches on 3 different target datasets. For the results reported in this section alone, we follow <ref type="bibr" target="#b11">[12]</ref> to perform fully-convolutional prediction for a closely-fair comparison with other approaches. Specifically, the fullyconnected layer in R(2+1)D is transformed into a 1 × 1 × 1 convolutional layer (while retaining learned weights), to allow fully-convolutional evaluation. Each test video is scaled to 128 × 171, then cropped to 128 × 128 (a full center crop). We also report results from using another frame scaling approach (indicated as SE in <ref type="table">Tables 5 -7)</ref>, where each (train / test) video's shortest edge is scaled to 128, while maintaining its original aspect ratio, followed by a full center crop.</p><p>We note that each approach being compared varies greatly in terms of model architectures, pre-training datasets (ImageNet vs. Sports-1M), amount and type of input data (RGB vs flow vs audio, etc.), input clip size, input frame size, evaluation strategy, and so on. We also note that many prior state-of-the-art models use complex, optimized network architectures compared to ours. Despite these differences, our approach of pre-training on tens of millions of videos outperforms all existing methods by a substantial margin of 3.6% when fully-finetuned on Kinetics ( <ref type="table">Table 5</ref>). Further, instead of uniformly sampling 10 clips, we used SC-Sampler <ref type="bibr" target="#b3">[4]</ref> and sampled 10 salient clips from test videos and achieved a top-1 accuracy of 82.8%.</p><p>In <ref type="table" target="#tab_1">Table 16</ref>, we report the performance on the validation <ref type="bibr" target="#b5">[6]</ref>, seen (S1), and unseen (S2) test datasets that are part of the EPIC-Kitchens Action Recognition Challenge  <ref type="table">Table 5</ref>. Comparison with the state-of-the-art on Kinetics. SE: short edge scaling. <ref type="bibr" target="#b0">[1]</ref>. Since the training dataset of EPIC-Kitchens consists of only~20K videos, for stronger baselines, we pre-train separate R(2+1)D-34 models on Kinetics and Sports-1M and fine-tune on EPIC-Kitchens. We also report the topperforming method from the challenge website <ref type="bibr" target="#b0">[1]</ref> at the time of this manuscript submission. From <ref type="table" target="#tab_1">Table 16</ref>, we may observe that, on unseen kitchens (S2), R(2+1)D-152 pre-trained on IG-Kinetics-65M improves the top-1 accuracy on verbs and nouns by 8.9% and 9.1% compared to R(2+1)D-34 pre-trained on Kinetics; and a 7.3% boost on actions compared to R(2+1)D-34 pre-trained on Sports-1M. Similar substantial gains hold for seen (S1) and validation datasets. We note that we process only 32 RGB frames of the input video (no optical flow), at a much lower resolution (128 × 128) compared to the state-of-the-art, which is an ensemble model. Finally, we report the performance <ref type="table">(Table 7)</ref> on the validation data of Something-V1 <ref type="bibr" target="#b28">[29]</ref>, a challenging dataset with fine-grained classes. Using only RGB as input, pretraining with IG-Kinetics-65M achieves a top-1 accuracy of 51.6%, an improvement of 2.1% over state-of-the-art [73] 9 (49.5%). Compared to other approaches that use only RGB as input <ref type="bibr" target="#b69">[70]</ref>, our approach yields a boost of 3.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this work, we explored the feasibility of large-scale, noisy, weakly-supervised pre-training with tens of million of videos. Despite the presence of significant noise in label space and temporal localization, our pre-trained models learn very strong feature representations. These models are able to significantly improve the state-of-the-art action recognition results on the popular Kinetics [65], a recently introduced EPIC-Kitchens <ref type="bibr" target="#b16">[17]</ref>, and Something-something <ref type="bibr" target="#b28">[29]</ref> datasets. All of our large-scale pre-trained models show significant gains over Kinetics and Sports-1M, the de facto pre-training datasets in the literature. Our ablation studies address many important questions related to scale, label space, and temporal dimension, while also raising other interesting questions.</p><p>Our study of label spaces found that sampling from the joint distribution of verb-noun pairs performs relatively <ref type="bibr" target="#b8">9</ref> This number is achieved using RGB+flow and an ensemble of models.   <ref type="table">Table 7</ref>. Comparison with the state-of-the-art on Something-V1 <ref type="bibr" target="#b28">[29]</ref>. IG-Kin.</p><p>refers to IG-Kinetics. SE: short edge scaling.</p><p>poorly; this is presumably due to the skewed distribution and suggests that solving low-shot learning at scale is an important area of investment. Data augmentation can also help here: social media offers a nearly unlimited supply of unlabeled video, and non-parametric approaches like Knearest neighbors could be employed to enhance sparse labels. Optimizing label space granularity in the face of data sparsity is another worth-while direction; this may require finding algorithmic ways to construct hashtag taxonomies for videos. Future research should also invest in creating new public benchmarks with larger and richer label spaces. Label spaces of current target datasets are small; they do not reflect the value of large-scale pre-training. Finally, our analysis of temporal localization raises more questions. Our experiments clearly show the competing benefits of both good temporal localization in short videos, and greater diversity in long videos. Understanding this trade-off more rigorously may lead to intelligent data con-struction strategies that can leverage the best of both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>A.1 Dataset construction IG − Verb − Noun dataset: When constructing this dataset, we combine the canonicalized forms of verbs and nouns and consider this as the class label. We construct the set relevant hashtags by considering all possible forms (with and without canonicalization) of the verbs and nouns. For example, for the verb 'burning' and noun 'candle,' relevant hashtags = {burncandle, candlesburning, candleburning, burncandles, burningcandle, burningcandles, candlesburn, candleburn}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Video deduplication</head><p>We devise a pipeline to deduplicate the source content that may overlap with any of the target dataset videos. Towards this end, we first decode videos at 16 fps and scale to a resolution of 112 × 112. Next, we employ the Census Transform technique <ref type="bibr" target="#b71">[72]</ref> to extract a 64 dimensional perframe global feature vector for each videos across all source and target datasets. Following this, we use locality sensitive hashing <ref type="bibr" target="#b65">[66]</ref> to measure the amount of overlap (percentage of feature match w.r.t. source video length) between each source and target video. Census Transform is robust to scale thereby allowing us to identify scenarios where the target video is a scaled version of a source video. Furthermore, operating on features at a frame-level also help tackle scenarios where a small portion of the target video may occur anywhere in a source video.</p><p>For each video from the three target datasets, we compute a percentage of overlap with each video in the source dataset. Using this approach, we identify about 20K videos from IG-Kinetics-65M dataset that potentially overlap (with varying amounts) with the three target datasets we consider: Kinetics, EPIC-Kitchens, and Somethingsomething-v1. Upon manual inspection, some of videos from the source dataset that overlap with a high match percentage are truly duplicates. Nevertheless, to err on the side of caution, we adopt an aggressive low-precision high-recall strategy and exclude all 20K videos from IG-Kinetics-65M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Model architectural details</head><p>We note that there is a slight variation in the architecture between models of depths 18 and 34 when compared to models of depth &gt; 34. Specifically, for computation feasibility, as described in <ref type="bibr" target="#b29">[30]</ref>, we use bottleneck blocks consisting of 1 × 1 × 1 convolution before and after the factorized convolutions for R(2+1)D-101 and R(2+1)D-152.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Hyper parameters</head><p>Here, we report the hyper parameters we use for all our full-finetuning experiments. The total training length is the number of times we iterate over an entire target dataset through the network. To set the learning rate (LR), we follow the linear scaling rule with gradual warmup described in <ref type="bibr" target="#b27">[28]</ref> when the target dataset is Kinetics. We did not use warmup for EPIC-Kitchens and Something-something-v1. In a distributed environment, the final LR is scaled according to the total number of nodes and the number of GPUs per node (16 and 4 respectively in our experiments). The LR is also decayed from the initial value by the specified factor at equally spaced steps; the total number LR decay steps is given in the LR steps column. We use a weight decay of 0.0001 and LR decay of 0.1 for all experiments. We report the hyper parameters when target dataset is Kinetics in <ref type="table" target="#tab_1">Tables 8 -12, EPIC-Kitchens in Tables 14 -16</ref>, and Something-something-v1 in <ref type="table" target="#tab_1">Table 17</ref>. Since we train two separate models for the verb and noun tasks of EPIC-Kitchens, we report the LR used for them separately.  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20]</ref> IG-Kinetics-19M (Video) 188 0.01 40 <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40]</ref>    <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> IG-Kinetics-1M 27 0.0005/0.0005 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> IG-Kinetics-5M 27 0.0005/0.0001 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> IG-Kinetics-10M 27 0.00025/0.0001 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> IG-Kinetics-19M 27 0.00025/0.0001 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> IG-Kinetics-65M 27 0.00025/0.0001 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> Kinetics 27 0.00025/0.0001 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref>   <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> R(2+1)D- <ref type="bibr">34 27</ref> 0.000025/0.00025 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> R(2+1)D-101 27 0.000025/0.0001 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> R(2+1)D-152 27 0.00005/0.0001 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref>   <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> R(2+1)D-34; Sports-1M 27 0.0025/0.0025 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> R(2+1)D-34; IG-Kinetics 27 0.000025/0.00025 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref> R(2+1)D-152; IG-Kinetics 27 0.00005/0.0001 <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9]</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustrating the effect of increasing the number of pre-training videos. For Kinetics, we train a R(2+1)D-34 model from scratch as baseline, while for EPIC-Kitchens, we pre-train R(2+1)D-34 on Kinetics as baseline (indicated in orange). Random sampling was used for experiments reported in (a) and (b). X-axis is in log-scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Top-1 accuracy on Kinetics and (b)-(d) mAP on the three Epic-Kitchens tasks after fc-only finetuning, when different source label sets are used (indicated in the legend). The results indicate that target tasks benefit the most when their labels overlap with the source hashtags. Best viewed in color. of videos IG -N oun (1428 labels) IG -Verb (438 labels) IG -Verb+ N oun (10653 labels) IG -Kinetics (359 labels)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Cumulative distribution of the number of videos per label for the 4 pretraining datasets discussed in Sec. 4.2.1. The x-axis is normalized by the total number of labels for each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Top-1 accuracy on Kinetics when pre-training on different number of labels. Note that the source datasets used in panels (a) and (b) are different, hence the results are not comparable. X-axis is log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the weakly-supervised datasets constructed for pre-training. that we describe in Sec. 4.</figDesc><table><row><cell>Pre-training dataset</cell><cell>Total #Videos</cell><cell>#Labels</cell></row><row><cell>IG-Kinetics</cell><cell>65M</cell><cell>359</cell></row><row><cell>IG-Noun</cell><cell>19M</cell><cell>1428</cell></row><row><cell>IG-Verb</cell><cell>19M</cell><cell>438</cell></row><row><cell>IG-Verb+Noun</cell><cell>19M</cell><cell>10653</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Kinetics</cell><cell cols="2">Epic-Kitchens</cell></row><row><cell>Models</cell><cell cols="2">GFLOPS # params</cell><cell>full-ft</cell><cell cols="2">baseline full-ft</cell><cell>baseline</cell></row><row><cell>R(2+1)D-18</cell><cell>83</cell><cell>33M</cell><cell>76.0</cell><cell>69.3</cell><cell>20.8</cell><cell>14.8</cell></row><row><cell>R(2+1)D-34</cell><cell>152</cell><cell>64M</cell><cell>78.2</cell><cell>69.6</cell><cell>22.4</cell><cell>15.2</cell></row><row><cell>R(2+1)D-101</cell><cell>176</cell><cell>86M</cell><cell>79.1</cell><cell>71.7</cell><cell>24.9</cell><cell>17.1</cell></row><row><cell>R(2+1)D-152</cell><cell>252</cell><cell>118M</cell><cell>79.9</cell><cell>72.0</cell><cell>23.7</cell><cell>17.8</cell></row></table><note>. Performance when pre-trained models of varied capacities are fully- finetuned on Kinetics (top-1 accuracy) and Epic-Kitchens (mAP). For EPIC- Kitchens, as a baseline, we use a model pre-trained on Kinetics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 ,</head><label>3</label><figDesc>we may</figDesc><table><row><cell></cell><cell>long-5M</cell><cell>long-500K</cell><cell>short-5M</cell><cell>long-center-5M</cell></row><row><cell>F1 F2</cell><cell>60.6 -</cell><cell>-50.6</cell><cell>57.4</cell><cell>51.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Video top-1 accuracy when R(2+1)D-34 is pre-trained on 4 different short and long video datasets, followed by fc-only finetuning on Kinetics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Understanding the benefit of using images vs. videos for pre-training.</figDesc><table><row><cell>Input dataset</cell><cell>Pre-training</cell><cell>Pre-train</cell><cell>FT</cell><cell>Top-1</cell></row><row><cell></cell><cell>Input</cell><cell>model</cell><cell>model</cell><cell></cell></row><row><cell>ImageNet</cell><cell>Image</cell><cell>R2D-18</cell><cell>R3D-18</cell><cell>66.5</cell></row><row><cell>IG-Kinetics-19M-Images</cell><cell>Image</cell><cell>R2D-18</cell><cell>R3D-18</cell><cell>67.0</cell></row><row><cell>IG-Kinetics-250M-Images</cell><cell>Image</cell><cell>R2D-18</cell><cell>R3D-18</cell><cell>67.0</cell></row><row><cell>IG-Kinetics-19M</cell><cell>Video frame</cell><cell>R2D-18</cell><cell>R3D-18</cell><cell>67.5</cell></row><row><cell>Kinetics</cell><cell>Video clip</cell><cell>R3D-18</cell><cell>R3D-18</cell><cell>65.6</cell></row><row><cell>IG-Kinetics-19M</cell><cell>Video clip</cell><cell>R3D-18</cell><cell>R3D-18</cell><cell>71.7</cell></row><row><cell cols="5">while pre-training on IG-Kinetics-19M-Images yields a</cell></row><row><cell cols="5">modest boost of 0.5% over ImageNet. Training on ran-</cell></row><row><cell cols="5">dom video frames from IG-Kinetics-19M gives a further</cell></row><row><cell cols="5">improvement of 0.5% over weakly-supervised image pre-</cell></row><row><cell cols="5">training and an overall boost of 1.0% over ImageNet. To</cell></row><row><cell cols="5">make sure that this marginal improvement is not because of</cell></row><row><cell cols="5">pre-training on only 19M weakly-supervised noisy images,</cell></row><row><cell cols="5">we pre-train using IG-Kinetics-250M-Images but find no</cell></row><row><cell cols="5">further improvements. Finally, pre-training R3D directly</cell></row><row><cell cols="5">using video clips achieves an accuracy of 71.7%, a signif-</cell></row><row><cell cols="5">icant jump of 4.2% over the best inflated model (67.5%).</cell></row><row><cell cols="5">This clearly indicates that effectively modeling the temporal</cell></row><row><cell cols="5">structure of videos in a very large-scale pre-training setup is</cell></row><row><cell>extremely beneficial.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Comparison with the state-of-the-art approaches on Epic-Kitchens dataset. IG-Kin. refers to IG-Kinetics. SE: short edge scaling.</figDesc><table><row><cell>Method; pre-training</cell><cell>top-1</cell><cell>top-5</cell><cell>Input type</cell></row><row><cell>NL I3D + Joint GCN [68]</cell><cell>46.1</cell><cell>76.8</cell><cell>RGB</cell></row><row><cell>S3D-G [70]</cell><cell>48.2</cell><cell>78.7</cell><cell>RGB</cell></row><row><cell>ECOEnLite [73]</cell><cell>46.4</cell><cell>-</cell><cell>RGB</cell></row><row><cell>ECOEnLite [73]</cell><cell>49.5</cell><cell>-</cell><cell>RGB + flow</cell></row><row><cell>R(2+1)D-34; Kinetics</cell><cell>45.2</cell><cell>74.1</cell><cell>RGB</cell></row><row><cell>R(2+1)D-34; Sports-1M</cell><cell>45.7</cell><cell>74.5</cell><cell>RGB</cell></row><row><cell>Ours: R(2+1)D-34; IG-Kin.</cell><cell>49.7</cell><cell>77.5</cell><cell>RGB</cell></row><row><cell>Ours: R(2+1)D-34; IG-Kin.; SE</cell><cell>49.9</cell><cell>77.5</cell><cell>RGB</cell></row><row><cell>Ours: R(2+1)D-152; IG-Kin.</cell><cell>51.0</cell><cell>79.0</cell><cell>RGB</cell></row><row><cell>Ours: R(2+1)D-152; IG-Kin.; SE</cell><cell>51.6</cell><cell>78.8</cell><cell>RGB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Hyper parameters for the ablation study on studying the effect of amount of pre-training data (Sec. 4.1.1) on transfer learning performance. Target dataset: Kinetics; Model: R(2+1)D-34; input video clip-length=8. batch size per GPU: 16. Data sampling strategy: Either random or tail-preserving.</figDesc><table><row><cell>Model</cell><cell>Total</cell><cell>LR per</cell><cell>warmup</cell><cell>LR</cell></row><row><cell></cell><cell>length</cell><cell>GPU</cell><cell></cell><cell>steps</cell></row><row><cell>R(2+1)D-{18,34,101,152}</cell><cell>58</cell><cell>0.00003</cell><cell>16</cell><cell>[16,16]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Hyper parameters for the ablation study on studying the effect of model</figDesc><table><row><cell cols="5">capacity (Sec. 4.1.2) on transfer learning performance. Target dataset: Kinetics; Pre-</cell></row><row><cell cols="5">training data: IG-Kinetics-65M. Model: R(2+1)D; input video clip-length=32. batch</cell></row><row><cell>size per GPU: 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of labels</cell><cell>Total</cell><cell>LR per</cell><cell>warmup</cell><cell>LR</cell></row><row><cell></cell><cell>length</cell><cell>GPU</cell><cell></cell><cell>steps</cell></row><row><cell>{675,1350, 2700, 5400, 10563}</cell><cell>58</cell><cell>0.0003</cell><cell>16</cell><cell>[16,16]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Hyper parameters for the ablation study on studying the effect of the</figDesc><table><row><cell cols="5">number of pre-training labels (Sec. 4.2.2) on transfer learning performance. Source</cell></row><row><cell cols="5">label space: IG-Verb+Noun. Target dataset: Kinetics; Model: R(2+1)D-34; input</cell></row><row><cell cols="2">video clip-length=8. batch size per GPU: 16.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of labels</cell><cell>Total</cell><cell>LR per</cell><cell>warmup</cell><cell>LR</cell></row><row><cell></cell><cell>length</cell><cell>GPU</cell><cell></cell><cell>steps</cell></row><row><cell>{20, 45, 90, 180, 359}</cell><cell>58</cell><cell>0.0003</cell><cell>16</cell><cell>[16,16]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .</head><label>11</label><figDesc>Hyper parameters for the ablation study on studying the effect of the number of pre-training labels (Sec. 4.2.2) on transfer learning performance. Source label space: IG-Kinetics. Target dataset: Kinetics; Model: R(2+1)D-34; input video clip-length=8. batch size per GPU: 16.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 .</head><label>12</label><figDesc>Hyper parameters for the ablation study on studying frame vs. clip-based pre-training (Sec. 4.3.2) on transfer learning performance. Target dataset: Kinetics</figDesc><table><row><cell>Method; Pre-training</cell><cell>Total</cell><cell>LR per</cell><cell cols="2">warmup LR</cell></row><row><cell></cell><cell>length</cell><cell>GPU</cell><cell></cell><cell>steps</cell></row><row><cell>R(2+1)D-34; Sports-1M</cell><cell>58</cell><cell>0.00003</cell><cell>16</cell><cell>[16,16]</cell></row><row><cell>R(2+1)D-34; IG-Kinetics</cell><cell>58</cell><cell>0.00003</cell><cell>16</cell><cell>[16,16]</cell></row><row><cell>R(2+1)D-152; IG-Kinetics</cell><cell>58</cell><cell>0.00003</cell><cell>16</cell><cell>[16,16]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 .</head><label>13</label><figDesc>Hyper parameters to compare with the state-of-the-art methods (reported in Sec. 4.4). Target dataset: Kinetics; Input video clip-length=32. batch size per GPU: 6.</figDesc><table><row><cell>Source dataset</cell><cell>Total</cell><cell>LR per GPU</cell><cell>LR</cell></row><row><cell></cell><cell>length</cell><cell>(Verb / Noun)</cell><cell>steps</cell></row><row><cell>IG-Kinetics-500K</cell><cell>27</cell><cell>0.0005/0.0005</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 .</head><label>14</label><figDesc>Hyper parameters for the ablation study on studying the effect of amount of pre-training data (Sec. 4.1.1) on transfer learning performance. Target dataset: EPIC-Kitchens; Model: R(2+1)D-34; input video clip-length=8. batch size per GPU: 16. Data sampling strategy: Either random or tail-preserving.</figDesc><table><row><cell>Model</cell><cell>Total</cell><cell>LR per GPU</cell><cell>LR</cell></row><row><cell></cell><cell>length</cell><cell></cell><cell>steps</cell></row><row><cell>R(2+1)D-18</cell><cell>27</cell><cell>0.00005/0.00025</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 .</head><label>15</label><figDesc>Hyper parameters for the ablation study on studying the effect of model capacity (Sec. 4.1.2) on transfer learning performance. Target dataset: EPIC-Kitchens; Pre-training data: IG-Kinetics-65M. Model: R(2+1)D; input video clip-length=32. batch size per GPU: 6.</figDesc><table><row><cell>Method; Pre-training</cell><cell>Total</cell><cell>LR per GPU</cell><cell>LR</cell></row><row><cell></cell><cell>length</cell><cell>(Verb /Noun)</cell><cell>steps</cell></row><row><cell>R(2+1)D-34; Kinetics</cell><cell>27</cell><cell>0.0025/0.0025</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 16 .</head><label>16</label><figDesc>Hyper parameters to compare with the state-of-the-art methods (reported in Sec. 4.4). Target dataset: EPIC-Kitchens; Input video clip-length=32. batch size per GPU: 6.</figDesc><table><row><cell>Method; Pre-training</cell><cell>Total</cell><cell>LR per</cell><cell>LR</cell></row><row><cell></cell><cell>length</cell><cell>GPU</cell><cell>steps</cell></row><row><cell>R(2+1)D-34; Kinetics</cell><cell>16</cell><cell>0.001</cell><cell>[6,6]</cell></row><row><cell>R(2+1)D-34; Sports-1M</cell><cell>16</cell><cell>0.001</cell><cell>[6,6]</cell></row><row><cell>R(2+1)D-34; IG-Kinetics</cell><cell>16</cell><cell cols="2">0.00025 [6,6]</cell></row><row><cell>R(2+1)D-152; IG-Kinetics</cell><cell>16</cell><cell>0.00013</cell><cell>[12]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 17 .</head><label>17</label><figDesc></figDesc><table /><note>Hyper parameters to compare with the state-of-the-art methods (reported in Sec. 4.4). Target dataset: Something-something-v1; Input video clip-length=32. batch size per GPU: 6.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use IG − source as notation whenever we refer to pre-training data source alone throughout this paper.<ref type="bibr" target="#b1">2</ref> For the remaining 41 labels, we could not find sufficient videos (i.e., at least 50 per label) using the approach detailed in Sec. 3.1.<ref type="bibr" target="#b2">3</ref> We get 1428(&gt; 1000) total hashtags because multiple hashtags may map to the same synset.<ref type="bibr" target="#b3">4</ref> Note that this is far fewer than 438 * 1428 =~600k, as we discarded those concatenations which are not associated with at least 50 videos.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Source code: https://github.com/dutran/R2Plus1D</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For EPIC-Kitchens, we even observe a performance drop.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Random sampling also makes sure that we remove uniformly from head and tail classes and long-tail issue with IG-Verb+Noun does not affect the observations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We chose to inflate to R3D because it was not immediately obvious how to inflate a 2D model to R(2+1)D given that it factorizes 3D convolution to 2D spatial and 1D temporal<ref type="bibr" target="#b14">[15]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">EPIC-Kitchens Action Recognition Challenge</title>
		<ptr target="https://competitions.codalab.org/competitions/20115#results.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">SCSampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04289</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1503.01817</idno>
		<title level="m">The new data and new challenges in multimedia research. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06157</idno>
		<title level="m">Object level visual reasoning in videos</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding actors and actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Caffe2 : A new lightweight, modular, and scalable deep learning framework</title>
		<ptr target="https://caffe2.ai/.4" />
	</analytic>
	<monogr>
		<title level="m">Caffe2 Team</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08050</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neil: Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epickitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hello! my name is buffy-automatic naming of characters in tv video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning object categories from internet image searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferrable knowledge for semantic segmentation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00352</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">What makes imagenet good for transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep classifiers from image tags in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Community-Organized Multimodal Mining: Opportunities for Novel Solutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14.2" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep hand: How to train a cnn on 1 million hand images when your data is continuous and weakly labelled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<title level="m">Space-time interest points. ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning visual n-grams from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7144</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Weaklysupervised learning of visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">From image-level to pixellevel labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Harvesting image databases from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Transfer learning by ranking for weakly supervised object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00873</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV- TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">VerbNet : A Computational Lexical Resource for Verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verbnet</surname></persName>
		</author>
		<ptr target="https://verbs.colorado.edu/verbnet/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Hashing for similarity search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2927</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01810</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Annotating images by mining image search results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-the-shelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03805</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">ECO: efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09066</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
