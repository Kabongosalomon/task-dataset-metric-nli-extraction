<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-aware Deep Feature Compression for High-speed Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018">2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwon</forename><surname>Choi</surname></persName>
							<email>jychoi@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ASRI, ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung</forename><forename type="middle">Jin</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Personal Robotics Lab</orgName>
								<orgName type="institution" key="instit1">EEE</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Fischer</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Personal Robotics Lab</orgName>
								<orgName type="institution" key="instit1">EEE</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASRI, ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuewang</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASRI, ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyeoup</forename><surname>Jeong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASRI, ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Demiris</surname></persName>
							<email>y.demiris@imperial.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Personal Robotics Lab</orgName>
								<orgName type="institution" key="instit1">EEE</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ASRI, ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-aware Deep Feature Compression for High-speed Visual Tracking</title>
					</analytic>
					<monogr>
						<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
						<imprint>
							<date type="published" when="2018">2018</date>
						</imprint>
					</monogr>
					<note>Preprint version; final version available at Published by: IEEE</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new context-aware correlation filter based tracking framework to achieve both high computational speed and state-of-the-art performance among real-time trackers. The major contribution to the high computational speed lies in the proposed deep feature compression that is achieved by a context-aware scheme utilizing multiple expert auto-encoders; a context in our framework refers to the coarse category of the tracking target according to appearance patterns. In the pre-training phase, one expert auto-encoder is trained per category. In the tracking phase, the best expert auto-encoder is selected for a given target, and only this auto-encoder is used. To achieve high tracking performance with the compressed feature map, we introduce extrinsic denoising processes and a new orthogonality loss term for pre-training and fine-tuning of the expert autoencoders. We validate the proposed context-aware framework through a number of experiments, where our method achieves a comparable performance to state-of-the-art trackers which cannot run in real-time, while running at a significantly fast speed of over 100 fps.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The performance of visual trackers has vastly improved with the advances of deep learning research. Recently, two different groups for deep learning based tracking have emerged. The first group consists of online trackers which rely on continuous fine-tuning of the network to learn the changing appearance of the target <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. While these trackers result in high accuracy and robustness, their computational speed is insufficient to fulfil the real-time requirement of online tracking. The second group is composed of correlation filter based trackers utilising raw deep convolutional features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>. However, these features are designed to represent general objects contained in large datasets such as ImageNet <ref type="bibr" target="#b26">[27]</ref> and therefore are of high dimensionality. As the computational time for the This plot compares the performance and computational speed of the proposed tracker (TRACA) with previous state-of-the-art trackers using the CVPR2013 dataset <ref type="bibr" target="#b35">[36]</ref>. TRACA shows comparable performance with the best performing non real-time trackers, while running at a fast speed of over 100 fps.</p><p>correlation filters increases with the feature dimensionality, trackers within the second group do not satisfy the real-time requirement of online tracking either.</p><p>In this work, we propose a correlation filter based tracker using context-aware compression of raw deep features, which reduces computational time, thus increasing speed. This is motivated by the observation that a lower dimensional feature map can sufficiently represent the single target object which is in contrast to the classification and detection tasks using large datasets that cover numerous object categories. Compression of high dimensional features into a low dimensional feature map is performed using autoencoders <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. More specifically, we employ multiple auto-encoders whereby each auto-encoder specialises in a specific category of objects; these are referred to as expert auto-encoders. We introduce an unsupervised approach to find the categories by clustering the training samples according to contextual information, and subsequently train one expert auto-encoder per cluster. During visual tracking, an appropriate expert auto-encoder is selected by a contextaware network given a specific target. The compressed feature map is then obtained after fine-tuning the selected expert auto-encoder by a novel loss function considering the orthogonality of the correlation filters. The compressed feature map contains reduced redundancy and sparsity, which increases accuracy and computational efficiency of the tracking framework. To track the target, correlation filters are applied to the compressed feature map. We validate the proposed framework through a number of self-comparisons and show that it outperforms other trackers using raw deep features while being notably faster at a speed of over 100 fps (see <ref type="figure" target="#fig_0">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Online deep learning based trackers: Recent trackers based on online deep learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref> have outperformed previous low-level feature-based trackers. Wang et al. <ref type="bibr" target="#b33">[34]</ref> proposed a framework simultaneously utilising shallow and deep convolutional features to consider detailed and contextual information of the target respectively. Nam and Han <ref type="bibr" target="#b23">[24]</ref> introduced a novel training method which avoids overfitting by appending a classification layer to a convolutional neural network that is updated online. Tao et al. <ref type="bibr" target="#b28">[29]</ref> utilised a Siamese network to estimate the similarities between the target's previous appearance and the current candidate patches. Yun et al. <ref type="bibr" target="#b38">[39]</ref> suggested a new tracking method using an action decision network which can be trained by a reinforcement learning method with weakly labelled datasets. However, trackers based on online deep learning require frequent fine-tuning of the networks, which is slow and prohibits real-time tracking. David et al. <ref type="bibr" target="#b14">[15]</ref> and Bertinetto et al. <ref type="bibr" target="#b0">[1]</ref> proposed pre-trained networks to quickly track the target without online fine-tuning, but the performance of these trackers is lower than that of the stateof-the-art trackers.</p><p>Correlation filter based trackers: The correlation filter based approach for visual tracking has become increasingly popular due to its rapid computation speed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. Henriques et al. <ref type="bibr" target="#b15">[16]</ref> improved the tracking performance by extending the correlation filter to multi-channel inputs and kernel-based training. Danelljan et al. <ref type="bibr" target="#b7">[8]</ref> developed a new correlation filter that can detect scale changes of the target. Ma et al. <ref type="bibr" target="#b21">[22]</ref> and Hong et al. <ref type="bibr" target="#b18">[19]</ref> integrated correlation filters with an additional long-term memory system. Choi et al. <ref type="bibr" target="#b4">[5]</ref> proposed a tracker with an attentional mechanism exploiting previous target appearance and dynamics.</p><p>Correlation filter based trackers showed state-of-theart performance when deep convolutional features were utilised <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref>. Danelljan et al. <ref type="bibr" target="#b6">[7]</ref> extended the regularised correlation filter <ref type="bibr" target="#b8">[9]</ref> to use deep convolutional features. Danelljan et al. <ref type="bibr" target="#b9">[10]</ref> also proposed a novel correlation filter to find the target position in the continuous domain to incorporate features of various resolutions. Ma et al. <ref type="bibr" target="#b25">[26]</ref> estimated the target position by fusing the response maps obtained from convolutional features of various resolutions. However, even though each correlation filter works fast, raw deep convolutional features have too many channels to be handled in real-time. A first step towards decreasing the feature space was made by Danelljan et al. <ref type="bibr" target="#b5">[6]</ref> by considering the linear combination of raw deep features, however the method still cannot run in real-time, and the deep feature redundancy was not fully suppressed.</p><p>Multiple-context deep learning frameworks: Our proposed tracking framework benefits from the observation that the performance of deep networks can be improved using contextual information to train multiple specialised deep networks. Indeed, there are several works utilizing such a scheme. Li et al. <ref type="bibr" target="#b19">[20]</ref> proposed a cascaded framework detecting faces through multiple neural networks trained by samples divided according to the degree of their detection difficulty. Vu et al. <ref type="bibr" target="#b31">[32]</ref> integrated the head detection results from two neural networks, one specialising in local information and the other one in global information. Neural networks specialising in local and global information have also been utilised in the saliency map estimation task <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>. In crowd density estimation, many works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref> have increased their performance by using multiple deep networks with different receptive fields to cover various scales of crowds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The proposed TRAcker based on Context-aware deep feature compression with multiple Auto-encoders (TRACA) consists of multiple expert auto-encoders, a context-aware network, and correlation filters as shown in <ref type="figure">Fig. 2</ref>. The expert auto-encoders robustly compress raw deep convolutional features from VGG-Net <ref type="bibr" target="#b2">[3]</ref>. Each of them is trained according to a different context, and thus performs context-dependent compression (see Sec. 3.1). We propose a context-aware network to select the expert auto-encoder best suited for the specific tracking target, and only this auto-encoder is running during online tracking (see Sec. 3.2). After initially adapting the selected expert auto-encoder for the tracking target, its compressed feature map is utilised as an input of correlation filters which track the target online. We introduce the general concept of correlation filters in Sec. 3.3 and then detail the tracking processes including the initial adaptation and the online tracking in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Expert Auto-encoders</head><p>Architecture: Auto-encoders have shown to be suitable for unsupervised feature learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. They offer a way to learn a compact representation of the input while retaining the most important information to recover the input given the compact representation. In this paper, we propose to use a set of N e expert auto-encoders of the same structure,  <ref type="figure">Figure 2</ref>. Proposed algorithm scheme. The expert auto-encoder is selected by the context-aware network and fine-tuned once by the ROI patch at the initial frame (I <ref type="bibr" target="#b0">(1)</ref> ). For the following frames, we first extract the ROI patch (I (t) ) centred at the previous target position. Then, a raw deep convolutional feature (X) is obtained through VGG-Net, and is compressed by the fine-tuned expert auto-encoder. The compressed feature (Z ) is used as the feature map for the correlation filter, and the target's position is determined by the peak position of the filter response. After each frame, the correlation filter is updated online by the newly found target's compressed feature.</p><p>each covering a different context. The inputs to be compressed are raw deep convolutional feature maps obtained from one of the convolution layers in VGG-Net <ref type="bibr" target="#b2">[3]</ref>. To achieve a high compression ratio, we stack N l encoding layers which are followed by N l decoding layers in the auto-encoder. The l-th encoding layer f l is a convolutional layer working as f l : R w×h×c l → R w×h×c l+1 , thus reducing the channel dimension c l of the input to latent channel dimension c l+1 while preserving the resolution of the feature map. The output of f l is provided as input to f l+1 such that the channel dimension c decreases as the feature maps pass through the encoding layers. More specifically, in our proposed framework one encoding layer reduces the channel dimension in half, i.e. c l+1 = c l /2 for l ∈ {1, · · · , N l }. By denoting the (N l − k + 1)-th decoding layer by g k in the adverse way of f l , g k : R w×h×c k+1 → R w×h×c k expands the input channel dimension c k+1 into c k to restore the original dimension c 1 of X at the last layer of the decoder, where k ∈ {1, · · · , N l }. Then, the auto-encoder AE can be expressed as AE(X) ≡ g 1 (· · · (g N l (f N l (· · · (f 1 (X))))) ∈ R w×h×c1 for a raw convolutional feature map X ∈ R w×h×c1 , and the compressed feature map in the auto-encoder is defined as Z ≡ f N l (· · · (f 1 (X))) ∈ R w×h×c N l +1 . All convolution layers are followed by the ReLU activation function, and the size of their convolution filters is set to 3 × 3.</p><p>Pre-training: The pre-training phase for the expert autoencoders is split into three parts, each serving a distinct purpose. First, we train the base auto-encoder AE o using all training samples to find context-independent initial compressed feature maps. Then, we perform contextual cluster-ing on the initial compressed feature maps of AE o to find N e context-dependent clusters. Finally, these clusters are used to train the expert auto-encoders initialised by the base auto-encoder with one of the sample clusters.</p><p>The purpose of the base auto-encoder is twofold: Using the context-independent compressed feature maps to cluster the training samples and finding good initial weight parameters from which the expert auto-encoders can be fine-tuned. The base auto-encoder is trained by raw convolutional feature maps {X j } m j=1 with a batch size m. The X j is obtained as the output from a convolutional layer involved in VGG-Net <ref type="bibr" target="#b2">[3]</ref> fed by randomly selected training images I j from a large image database such as ImageNet <ref type="bibr" target="#b26">[27]</ref>.</p><p>To make the base auto-encoder more robust to appearance changes and occlusions, we use two denoising criteria which help to capture distinct structures in the input distribution (illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>). The first denoising criterion is a channel corrupting process where a fixed number of feature channels is randomly chosen and the values for these channels is set to 0 (while the other channels remain unchanged), which is similar to the destruction process of denoising autoencoders <ref type="bibr" target="#b30">[31]</ref>. Thus all information for these channels is removed and the auto-encoder is trained to recover this information. The second criterion is an exchange process, where some spatial feature vectors of the convolutional feature are randomly interchanged. Since the receptive fields of the feature vectors cover different regions within an image, exchanging the feature vectors is similar to interchanging regions within the input image. Thus, interchanging feature vectors that cover the background region and target region  respectively leads to a similar effect as the background occluding the target. Therefore, the auto-encoders are trained to be robust against occlusions. We denote {X j } m j=1 as the mini-batch after performing the two denoising processes. Then, the base auto-encoder AE o can be trained by minimising the distance between the input feature map X j and its output AE o (X j ) with the noisy sampleX j .</p><p>However, when we only consider the distance between the input and the final output of the base auto-encoder, we frequently observed an overfitting problem and unstable training convergence. To solve these problems, we design a novel loss based on a multi-stage distance which consists of the distances between the input and the outputs obtained by the partial auto-encoders. The partial auto-</p><formula xml:id="formula_0">encoders {AE i (X)} N l i=1</formula><p>contain only a portion of the encoding and decoding layers of their original auto-encoder AE(X), while the input and output sizes match that of the original auto-encoder, i.e. AE 1 (X) = g 1 (f 1 (X)), AE 2 (X) = g 1 (g 2 (f 2 (f 1 (X)))), · · · when AE(X) = g 1 (· · · (g N l (f N l (· · · (f 1 (X)))))). Thus, the loss based on the multi-stage distance can be described as:</p><formula xml:id="formula_1">Lae = 1 m m j=1 N l i=1 Xj − AE o i (Xj) 2 2 ,<label>(1)</label></formula><p>where AE o i (X) is the i-th partial auto-encoder of AE o (X), and recall that m denotes the mini batch size.</p><p>Then, we cluster the training samples {I j } N j=1 according to their respective feature maps compressed by the base auto-encoder, where N denotes the total number of training samples. To avoid overfitting of the expert auto-encoders due to a too small cluster size, we introduce a two-step clustering algorithm which avoids small clusters.</p><p>In the first step, we find 2N e samples which are chosen randomly from the feature maps compressed by the base auto-encoder (note that this is twice the amount of desired clusters). We repeat the random selection 1000 times and find the samples which have the largest Euclidean distance amongst them as initial centroids. Then, all training samples are clustered by k-means clustering with k = 2N e using the compressed feature maps. In the second step, among the resulting 2N e centroids, we remove the N e centroids of the clusters with the smallest number of included samples. Then, N e centroids remain, and we cluster the training samples again using these centroids, which results in N e clusters including enough samples to avoid the overfitting problem. We denote the cluster index for I j as d j ∈ {1, ..., N e }.</p><p>The d-th expert auto-encoder AE d is then found by finetuning the base auto-encoder using the training samples with contextual cluster index d. The training process (including the denoising criteria) differs from the base auto-encoder only in the training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context-aware Network</head><p>Architecture: The context-aware network selects the expert auto-encoder which is most contextually suitable for a given tracking target. We adopt a pre-trained VGG-M model <ref type="bibr" target="#b2">[3]</ref> for the context-aware network since it contains a large amount of semantic information from pre-training on ImageNet <ref type="bibr" target="#b26">[27]</ref>. Given a 224 × 224 RGB input image, our context-aware network consists of three convolutional layers {conv1, conv2, conv3} followed by three fully connected layers {fc4, fc5, fc6}, whereby {conv1, conv2, conv3, fc4} are identical to the corresponding layers in VGG-M. The weight parameters of fc5 and fc6 are initialised randomly with zero-mean Gaussian distribution. fc5 is followed by a ReLU function and contains 1024 output nodes. Finally fc6 has N e output nodes and is combined with a softmax layer to estimate the probability for each of the expert auto-encoders to be suited for the tracking target.</p><p>Pre-training: The context-aware network takes a training sample I j as input and outputs the estimated probabilities of that sample belonging to cluster index d j . It is being trained by a batch {I j , d j } m j=1 of image/cluster-index pairs where m is the mini-batch size for the context-aware network. We fix the weights of {conv1, conv2, conv3, fc4}, and train the weights for {fc5, fc6} by minimising the multi-class loss function L pr using stochastic gradient descent, where</p><formula xml:id="formula_2">Lpr = 1 m m j=1 H(dj, h(Ij)),<label>(2)</label></formula><p>H denotes the cross-entropy loss, and h(I j ) is the predicted cluster index of I j by the context-aware network h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Correlation Filter</head><p>Before detailing the tracking process of TRACA, we briefly introduce the functionality of conventional correlation filters using a single-channel feature map. Based on the property of the circulant matrix in the Fourier domain [?], correlation filters can be trained quickly which leads to high-performing trackers under low computational load <ref type="bibr" target="#b15">[16]</ref>. Given the vectorised single-channel training feature map z ∈ R wh×1 and the vectorised target response map y obtained from a 2-D Gaussian window with size w × h and variance σ 2 y as in <ref type="bibr" target="#b15">[16]</ref>, the vectorised correlation filter w can be estimated by:</p><formula xml:id="formula_3">w = F −1 z y z z * + λ ,<label>(3)</label></formula><p>whereŷ andẑ represent the Fourier-transformed vector of y and z respectively, z * is the conjugated vector of z, denotes an element-wise multiplication, F −1 stands for an inverse Fourier transform function, and λ is a predefined regularisation factor. For the vectorised single-channel test feature map z ∈ R wh×1 , the vectorised response map r can be obtained by:</p><formula xml:id="formula_4">r = F −1 w z * .<label>(4)</label></formula><p>Then, after re-building a 2-D response map R ∈ R w×h from r, the target position is found from the maximum peak position of R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Tracking Process</head><p>To track a target in a scene, we rely on a correlation filter based algorithm using the compressed feature map of the expert auto-encoders as selected by the context-aware network. We describe the initial adaptation of the selected expert auto-encoder in Sec. 3.4.1 followed by a presentation of the correlation filter based tracking algorithm in Sec. 3.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Initial Adaptation Process</head><p>The initial adaptation process contains the following parts. We first extract a region of interest (ROI) including the target from the initial frame, and the expert auto-encoder suitable for the target is selected by the context-aware network. Then, the selected expert auto-encoder is fine-tuned using the raw convolutional feature maps of the training samples augmented from the ROI. When we obtain the compressed feature map from the fine-tuned expert auto-encoder, some of its channels represent background objects rather than the target. Thus, we introduce an algorithm to find and remove the channels which respond to the background objects.</p><p>Region of interest extraction: The ROI is centred around the target's initial bounding box, and is 2.5 times bigger than the target's size to cover the area nearby. We then resize the ROI of width W and height H to 224 × 224 in order to match the expected input size of the VGG-Net. This results in the resized ROI I (1) ∈ R 224×224×3 for the rgb domain. For grey-scale images, the grey value is replicated three times to obtain I <ref type="bibr" target="#b0">(1)</ref> . The best expert auto-encoder for the tracking scene is selected according to the contextual information of the initial target using the context-aware network h, and we can denote this auto-encoder as AE h(I <ref type="bibr" target="#b0">(1)</ref> ) .</p><p>Initial sample augmentation: Even though we use two denoising criteria as described earlier, we found that the compressed feature maps of the expert auto-encoders show a deficiency for targets which become blurry or are flipped. Thus, we augment I <ref type="bibr" target="#b0">(1)</ref> in several ways before fine-tuning the selected auto-encoder. To tackle the blurriness problem, four augmented images are obtained by filtering I <ref type="bibr" target="#b0">(1)</ref> with Gaussian filters with variances {0.5, 1.0, 1.5, 2.0}. Two more augmented images are obtained by flipping I (1) around the vertical and horizontal axes respectively. Then, the raw convolutional feature maps extracted from the augmented samples can be represented by {X</p><formula xml:id="formula_5">(1) j } 7 j=1</formula><p>. Fine-tuning: The fine-tuning of the selected autoencoder differs from the pre-training process for the expert auto-encoders. As there is a lack of training samples, the optimisation rarely converges when applying the denoising criteria. Instead, we employ a correlation filter orthogonality loss L ad which considers the orthogonality of the correlation filters estimated from the compressed feature map of the expert auto-encoder, where L ad is defined as:</p><formula xml:id="formula_6">L ad = 7 j=1 N l i=1    X (1) j −AEi(X (1) j ) 2 2 + λΘ c i+1 k,l=1 Θ(w jik , w jil )    ,<label>(5)</label></formula><p>where Θ(u, v) = (u · v) 2 /( u 2 2 v 2 2 ) and w jik defines a vectorised correlation filter estimated by Eq.(3) using the vectorised k-th channel of the compressed feature map f i (· · · (f 1 (X (1) j ))) from the selected expert auto-encoder. The correlation filter orthogonality loss allows increasing the interaction among the correlation filters as estimated from the different channels of the compressed feature maps. The fine-tuning is performed by minimising L ad using stochastic gradient descent. The differentiation of L ad is described in Appendix A of the supplementary material.</p><p>Background channel removal: The compressed feature map Z ∀ can be obtained from the fine-tuned expert autoencoder. Then, we remove the channels within Z ∀ which have large responses outside of the target bounding box. Those channels are found by estimating the channel-wise ratio of foreground and background feature responses. First, we estimate the channel-wise ratio of the feature responses for channel k as</p><formula xml:id="formula_7">ratio k = vec(Z k,∀ bb ) 1 / vec(Z k,∀ ) 1 ,<label>(6)</label></formula><p>where Z k,∀ is the k-th channel feature map of Z ∀ and Z k,∀ bb is obtained from Z k,∀ by setting the values out of the target bounding box to 0 while the other values are untouched. Then, after sorting all channels according to ratio k in descending order, only the first N c channels of the compressed feature map are utilised as input to the correlation filters. We denote the resulting feature map as Z ∈ R S×S×Nc , where S is the feature size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Online Tracking Sequence</head><p>Correlation filter estimation &amp; update: We first obtain the resized ROI for the current frame t using the same method as in the initial adaptation, i.e. the resized ROI is centred at the target's centre and its size is 2.5 times the target's size and resized to 224 × 224. After feeding the resized ROI to the VGG-Net, we obtain the compressed feature map Z (t) ∈ R S×S×Nc by inserting the raw deep convolutional feature map of the VGG-Net into the adapted expert auto-encoder.</p><p>Then, using Eq.(3), we estimate independent correlation filters w k,(t) for each feature map Z k,(t) , where Z k,(t) denotes the k-th channel of Z (t) . Following <ref type="bibr" target="#b15">[16]</ref>, we suppress background regions by multiplying each Z k,(t) with cosine windows of the same size. For the first frame, we can estimate the correlation filtersw k, <ref type="bibr" target="#b0">(1)</ref> with Eq.(3) given by Z k, <ref type="bibr" target="#b0">(1)</ref> . For the following frames (t &gt; 1), the correlation filters are updated as follows:</p><formula xml:id="formula_8">w k,(t) = (1 − γ)w k,(t−1) + γw k,(t) ,<label>(7)</label></formula><p>where γ is an interpolation factor. Tracking: After estimating the correlation filter, we need to find the position [x t , y t ] of the target in frame t. As we assume that [x t , y t ] is close to the target position in the previous frame [x t−1 , y t−1 ] , we extract the tracking ROI from the same position as the ROI for the correlation filter estimation of the previous frame. Then, we can obtain the compressed feature map Z (t) for tracking using the adapted expert auto-encoder. Inserting Z (t) andw k,(t−1) to Eq.(4) then provides the channel-wise response map R k,(t) (we apply the multiplication of cosine windows in the same manner as for the correlation filter estimation).</p><p>We then need to combine R k,(t) to the integrated response map R (t) . We use a weighted averaging scheme, where we use the validation score s k as weight factor with</p><formula xml:id="formula_9">s k = exp −λ s R k,(t) − R k,(t) o 2 2 ,<label>(8)</label></formula><p>and R k,(t) o = G(p k,(t) , σ 2 ) S×S is a 2-D Gaussian window with size S×S and variance σ 2 y centred at the peak point p k,(t) of R k,(t) . Then, the integrated response map is calculated as:</p><formula xml:id="formula_10">R (t) = Nc k=1 s k R k,(t) .<label>(9)</label></formula><p>Following <ref type="bibr" target="#b4">[5]</ref>, we find the sub-pixel target position p (t) by interpolating the response values near the peak position. Finally, the target position [x t , y t ] is found as:</p><formula xml:id="formula_11">[x t , y t ] = [x t−1 , y t−1 ] + round([W, H] p (t) /S). (10)</formula><p>Scale changes: To handle scale changes of the target, we extract two additional ROI patches scaled from the previous ROI patch size with scaling factors 1.015 and 1.015 −1 respectively in the tracking sequence. The new target scale is chosen as the scale where the respective maximum value of the response map (from the scaled ROI) is the largest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full occlusion handling:</head><p>To handle full occlusions, a re-detection algorithm is adopted. The overall idea is to introduce a so-called re-detection correlation filter which is not being updated and applied to the position of the target at the time where an occlusion has been detected. A full occlusion is assumed when a sudden drop of the maximum response value R</p><formula xml:id="formula_12">(t) max ≡ max(R (t) ) is detected as described by R (t) max &lt; λ reR (t−1) max withR (t) max = (1 − γ)R (t−1) max + γR (t)</formula><p>max andR 0 max = R 1 max (note that this is the same γ as in Eq. <ref type="formula" target="#formula_8">(7)</ref>). If that condition is fulfilled, the correlation filter at time (t − 1) is used as re-detection correlation filter. During the next N re frames, the target position as determined by the re-detection correlation filter is being used if the maximum value of the re-detection filter's response map is larger than the maximum value of the response map obtained by the normal correlation filter. Furthermore,w k,(t) are replaced by the ones of the re-detection correlation filter, and the target scale is reset to the scale when the occlusion was detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation</head><p>The feature response after the second convolution layer (conv2) of VGG-M <ref type="bibr" target="#b2">[3]</ref> was given to the auto-encoders as raw convolutional feature input. The number of expert autoencoders was set to N e = 10, and their depth to N l = 2. The mini-batch size for all auto-encoders was set to m = 10. The learning rate for the base auto-encoder was set to 10 −10 , and for expert auto-encoders to 10 −9 . The base auto-encoder was trained for 10 epochs, and the expert auto-encoders were fine-tuned for 30 epochs. The proportions for the two extrinsic denoising processes were set to 10% respectively. For training the context-aware network, the mini-batch size and the learning rate were set to m = 100 and 0.01, respectively. The weight for the orthogonality loss term was set to λ Θ = 10 3 , and the reduced channel dimension after the background channel removal was N c = 25. The parameters for the correlation filter based tracker were set to σ g = 0.05, λ = 1.0, and γ = 0.025, and λ s = 50. The parameters for full occlusion handling, λ re and N re , were experimentally determined to 0.7 and 50 using scenes with occlusions.</p><p>We used MATLAB and MatConvNet <ref type="bibr" target="#b29">[30]</ref> to implement the proposed framework. The computational environment had an Intel i7-2700K CPU @ 3.50GHz, 16GB RAM, and an NVIDIA GTX1080 GPU. The computational speed was 101.3 FPS in the CVPR2013 dataset <ref type="bibr" target="#b35">[36]</ref>. We release the source code along with the attached experimental results 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset</head><p>The classification image samples included in VOC2012 <ref type="bibr" target="#b11">[12]</ref> were used to pre-train the expert auto- (c) Evaluation plots on TPAMI2015 <ref type="figure">Figure 4</ref>. Evaluation results. TRACA showed the best performance within the self-comparison, and the state-of-the-art performance amongst real-time trackers in CVPR2013 <ref type="bibr" target="#b35">[36]</ref> and TPAMI2015 <ref type="bibr" target="#b36">[37]</ref> datasets. The numbers within the legend are the average precisions when the centre error threshold equals 20 pixels (top row), or the area under the curve of the success plot (bottom row).</p><p>encoders and the context-aware network. To evaluate the proposed framework, we used the CVPR2013 <ref type="bibr" target="#b35">[36]</ref> (51 targets, 50 videos) and TPAMI2015 <ref type="bibr" target="#b36">[37]</ref> (100 targets, 98 videos) datasets, which contain the ground truth of the target bounding box at every frame. These datasets have been frequently used <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref> as they include a large variety of environments to evaluate the general tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Measure</head><p>As performance measure, we used the average precision curve of one-pass evaluation (OPE) as proposed in <ref type="bibr" target="#b35">[36]</ref>. The average precision curve was estimated by averaging the precision curves of all sequences, which was obtained using two sources: location error threshold and overlap threshold. As representative scores of trackers, the average precisions when the location error threshold equals 20 pixels and the area under the curve of the success plot were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Self-comparison</head><p>To analyse the effectiveness of TRACA, we compare TRACA with nine variants: no orth.-TRACA, pretrain-TRACA, clean-TRACA, dropout-TRACA, random-TRACA, l 2 -TRACA, label-TRACA, simple-TRACA, and single-TRACA. In no orth.-TRACA, the weight factor λ Θ for the orthogonality loss term is set to zero. Pretrain-TRACA skipped the initial adaptation step and directly utilised the pretrained expert auto-encoder. Clean-TRACA used the expert auto-encoders which were pre-trained without any extrinsic denoising process. Dropout-TRACA adopted a dropout scheme instead of the proposed dimension corrupting process, while keeping the feature vector exchange process. Random-TRACA randomly selected the suitable expert autoencoder. l 2 -TRACA selected the best suitable expert autoencoder according to the smallest l 2 generation error esti-  <ref type="bibr" target="#b21">[22]</ref> 84.8% 21.6 N SCT <ref type="bibr" target="#b3">[4]</ref> 84.5% 40.0 N MEEM <ref type="bibr" target="#b39">[40]</ref> 81.4% 19.5 N SiamFC <ref type="bibr" target="#b0">[1]</ref> 80.9% 86.0 Y KCF <ref type="bibr" target="#b15">[16]</ref> 74.2% 120.5 N DSST <ref type="bibr" target="#b7">[8]</ref> 74.0% 25.4 N Non Real-time ECO <ref type="bibr" target="#b5">[6]</ref> 93.0% 8.0 Y ADNet <ref type="bibr" target="#b38">[39]</ref> 90.3% 2.9 Y C-COT <ref type="bibr" target="#b9">[10]</ref> 89.9% 0.5 N MUSTer <ref type="bibr" target="#b18">[19]</ref> 86.5% 3.9 N FCNT <ref type="bibr" target="#b33">[34]</ref> 85.6% 3.0 Y D-SRDCF <ref type="bibr" target="#b6">[7]</ref> 84.9% 0.2 N mated from the initial target. Label-TRACA utilised 20 class labels of the pre-training dataset (VOC2012 <ref type="bibr" target="#b11">[12]</ref>) as the contextual clusters. The expert auto-encoders of simple-TRACA were trained and fine-tuned by minimising the Euclidean distance between their inputs and final outputs, i.e. without using the multi-stage distance. Single-TRACA utilised the compressed feature map of the base auto-encoder 2 .</p><p>The results of the comparison with these nine trackers are shown in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="figure">Fig. 4 (a)</ref>. The results of random-TRACA and l 2 -TRACA show decreased performance which reflects the importance of the context-aware network. In the result of pretrain-TRACA, the performance was reduced by 6.6% when the expert auto-encoder was not adapted initially. The initial adaptation ignoring the orthogonality loss term (no orth.-TRACA) further decreased the performance by 1% compared to pretrain-TRACA. When the extrinsic denoising processes were not applied, the tracking performance reduced dramatically (14.3%) according to the result of clean-TRACA. Similarly, as shown in the result of dropout-TRACA, the proposed dimension corrupting process made the expert auto-encoders more robust than a dropout scheme (11.3% performance reduction). When the multi-stage distance was not used, the performance was reduced by 13.4% as shown in the result of simple-TRACA. Single-TRACA showed a dramatic reduction in the tracking performance (13.6%), which demonstrates that the multiplecontext scheme was effective to compress the raw deep convolutional feature for a specific target. Finally, the tracking performance was reduced dramatically in label-TRACA (13.2%), which shows that clustering in feature space is beneficial when training the expert auto-encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-art Trackers</head><p>The results of the state-of-the-art methods, including ECO <ref type="bibr" target="#b5">[6]</ref>, ADNet <ref type="bibr" target="#b38">[39]</ref>, ACFN <ref type="bibr" target="#b4">[5]</ref>, C-COT <ref type="bibr" target="#b9">[10]</ref>, SiamFC <ref type="bibr" target="#b0">[1]</ref>, FCNT <ref type="bibr" target="#b33">[34]</ref>, D-SRDCF <ref type="bibr" target="#b6">[7]</ref>, SCT <ref type="bibr" target="#b3">[4]</ref>, LCT <ref type="bibr" target="#b21">[22]</ref>, and DSST <ref type="bibr" target="#b7">[8]</ref> were obtained from the authors. In addition, the results of MUSTer <ref type="bibr" target="#b18">[19]</ref>, MEEM <ref type="bibr" target="#b39">[40]</ref>, and KCF <ref type="bibr" target="#b15">[16]</ref> were estimated using the authors' implementations <ref type="bibr" target="#b2">3</ref> .</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, the precision scores of the algorithms on the CVPR2013 dataset are presented along with the computational speed and whether they make use of a GPU. <ref type="figure">Fig. 4 (b-c)</ref> compares the performances of the real-time trackers, where TRACA demonstrates state-of-the-art performance in both the CVPR2013 and TPAMI2015 datasets while running at over 100 fps. Some qualitative results are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Further Analysis</head><p>The context in the proposed framework refers to a coarse category of the compressed feature maps encoding the target <ref type="bibr" target="#b2">3</ref> For fair comparison, the computational time was estimated by the same computer as TRACA and included the image resizing time.  <ref type="figure">Figure 6</ref>. Top-5 images for each contextual cluster. Each column represents one context category and consists of the five samples within Caltech256 <ref type="bibr" target="#b12">[13]</ref> that have the highest scores of the contextaware network for this category. The results confirm that the contextual clusters represent the category of appearance patterns. object appearance. To illustrate the context in practice, we extracted the five samples with the highest scores within the context-aware network for each contextual category using Caltech256 <ref type="bibr" target="#b12">[13]</ref>. As shown in <ref type="figure">Fig. 6</ref>, the contextual clusters categorise the samples according to appearance patterns.</p><p>In Appendix B, we evaluate the impact of the chosen target layer of VGG-Net and the number of contextual clusters on the proposed framework. In Appendix C, we analyse the correlation matrix among various computer vision datasets, which was obtained by estimating the correlation among the histograms of the results from the context-aware network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a new visual tracking framework based on context-aware deep feature compression using multiple auto-encoders was proposed. Our main contribution is to introduce a context-aware scheme which includes expert autoencoders specialising in one context, and a context-aware network which is able to select the best expert auto-encoder for a specific tracking target. This leads to a significant speedup of the correlation filter based tracker utilising deep convolutional features. Our experiments lead to the compelling finding that our framework achieves a high-speed tracking ability of over 100 fps while our framework maintains a competitive performance compared to the state-of-the-art trackers. We expect that embedding our context-aware deep feature compression scheme will be integrated with other trackers utilising raw deep features, which will increase their robustness and computational efficiency. In addition, the scheme can be utilised as a way to avoid the overfitting problem in other computer vision tasks where only few training samples are available, such as in image k-shot learning and image domain adaptation. As a future work, we will jointly train the expert auto-encoders and the context-aware network to potentially further increase the performance due to the correlation between the contextual clustering and the feature compression.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of computational efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Channel corrupting process (b) Feature vector exchange process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Extrinsic denoising criteria. To increase robustness of the compressed feature map in the pre-training, two extrinsic denoising criteria are applied to the raw deep feature map which is the input of the auto-encoder. (a) In the channel corrupting process, some randomly selected channels are set to zero. (b) In the exchange process, randomly chosen feature vectors are interchanged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results. The used sequences are Lemming, Couple, Jumping, FaceOcc2, CarDark, and Soccer from the left-top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>https://sites.google.com/site/jwchoivision/</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="5">Precision plots of OPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Precision plots of OPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Precision plots of OPE</cell></row><row><cell>Precision</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TRACA [0.898] l2-TRACA [0.877] random-TRACA [0.845] pretrain-TRACA [0.832] no orth.-TRACA [0.822] dropout-TRACA [0.785] label-TRACA [0.766] simple-TRACA [0.764] single-TRACA [0.762] clean-TRACA [0.754]</cell><cell>Precision</cell><cell cols="2">1 0.4 0.6 0.8 0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TRACA [0.898] ACFN [0.860] LCT [0.848] SCT [0.845] MEEM [0.814] SiamFC [0.809] KCF [0.742] DSST [0.740]</cell><cell>Precision</cell><cell>0.8 0.3 0.4 0.5 0.6 0.7 0.2 0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TRACA [0.816] ACFN [0.802] LCT [0.763] SCT [0.768] MEEM [0.773] SiamFC [0.772] KCF [0.699] DSST [0.687]</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Location error threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Location error threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Location error threshold</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="5">Success plots of OPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="5">Success plots of OPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="6">Success plots of OPE</cell></row><row><cell></cell><cell></cell><cell cols="3">TRACA [0.652]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Success rate</cell><cell>0 0.2 0.4 0.6 0.8</cell><cell cols="4">l2-TRACA [0.646] random-TRACA [0.625] pretrain-TRACA [0.623] no orth.-TRACA [0.619] dropout-TRACA [0.585] label-TRACA [0.586] simple-TRACA [0.580] single-TRACA [0.569] clean-TRACA [0.563]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Success rate</cell><cell>0.2 0.4 0.6 0.8 0</cell><cell cols="3">TRACA [0.652] ACFN [0.607] LCT [0.628] SCT [0.595] MEEM [0.565] SiamFC [0.607] KCF [0.517] DSST [0.554]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Success rate</cell><cell>0.2 0.4 0.6 0.8 0</cell><cell cols="3">TRACA [0.603] ACFN [0.575] LCT [0.563] SCT [0.534] MEEM [0.529] SiamFC [0.583] KCF [0.480] DSST [0.518]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell></cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Overlap threshold</cell></row><row><cell></cell><cell></cell><cell cols="9">(a) Self-comparison on CVPR2013</cell><cell></cell><cell></cell><cell></cell><cell cols="9">(b) Evaluation plots on CVPR2013</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on the CVPR2013 dataset<ref type="bibr" target="#b35">[36]</ref> </figDesc><table><row><cell></cell><cell>Algorithm</cell><cell>Pre. score</cell><cell cols="2">Mean FPS GPU</cell></row><row><cell></cell><cell>TRACA</cell><cell>89.8%</cell><cell>101.3</cell><cell>Y</cell></row><row><cell></cell><cell>l2-TRACA</cell><cell>87.7%</cell><cell>101.2</cell><cell>Y</cell></row><row><cell></cell><cell>random-TRACA</cell><cell>84.4%</cell><cell>99.5</cell><cell>Y</cell></row><row><cell>Proposed</cell><cell>pretrain-TRACA no orth.-TRACA dropout-TRACA label-TRACA</cell><cell>83.2% 82.2% 78.5% 76.6%</cell><cell>98.8 101.2 97.5 97.2</cell><cell>Y Y Y Y</cell></row><row><cell></cell><cell>simple-TRACA</cell><cell>76.4%</cell><cell>94.1</cell><cell>Y</cell></row><row><cell></cell><cell>single-TRACA</cell><cell>76.2%</cell><cell>100.9</cell><cell>Y</cell></row><row><cell></cell><cell>clean-TRACA</cell><cell>75.4%</cell><cell>92.9</cell><cell>Y</cell></row><row><cell></cell><cell>ACFN [5]</cell><cell>86.0%</cell><cell>15.0</cell><cell>Y</cell></row><row><cell></cell><cell>LCT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Real-time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For fair comparison, we train the base auto-encoder for 20 epochs in the case of single-TRACA.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual tracking using attention-modulated disintegration and integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attentional correlation filter network for adaptive visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ECO: efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative scale space tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1561" to="1575" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stacked convolutional denoising auto-encoders for feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1017" to="1027" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Caltech Technical Report</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2096" to="2109" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-store tracker (MUSTer): a cognitive psychology inspired approach to object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning of higher-order image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards perspectivefree object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oñoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Context-aware CNNs for person head detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">STCT: Sequentially training convolutional networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Actiondecision networks for visual tracking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meem: Robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Singleimage crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
