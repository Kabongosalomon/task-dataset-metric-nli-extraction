<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REGION-ADAPTIVE TEXTURE ENHANCEMENT FOR DETAILED PERSON IMAGE SYNTHESIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbo</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Center</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Center</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Center</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Center</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">REGION-ADAPTIVE TEXTURE ENHANCEMENT FOR DETAILED PERSON IMAGE SYNTHESIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-image generation</term>
					<term>pose transfer</term>
					<term>texture enhancement</term>
					<term>adaptive normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to produce convincing textural details is essential for the fidelity of synthesized person images. Existing methods typically follow a "warping-based" strategy that propagates appearance features through the same pathway used for pose transfer. However, most fine-grained features would be lost due to down-sampling, leading to over-smoothed clothes and missing details in the output images. In this paper we presents RATE-Net, a novel framework for synthesizing person images with sharp texture details. The proposed framework leverages an additional texture enhancing module to extract appearance information from the source image and estimate a fine-grained residual texture map, which helps to refine the coarse estimation from the pose transfer module. In addition, we design an effective alternate updating strategy to promote mutual guidance between two modules for better shape and appearance consistency. Experiments conducted on DeepFashion benchmark dataset have demonstrated the superiority of our framework compared with existing networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this paper, we study the problem of synthesizing images of a person's appearance under novel poses, which is commonly known as human pose transfer <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. The problem is first introduced in <ref type="bibr" target="#b0">[1]</ref>, where a transfer model receives a source image that provides conditional appearance constraints, and is expected to transfer the person's appearance to new poses. Human pose transfer forms the core of many real-world applications, including interactive fashion design, creative media production, and many other human-centered tasks.</p><p>This work was done when Lingbo Yang is working at DAMO Academy, Alibaba as a research intern. Siwei Ma is the corresponding author. This work was supported by the National Natural Science Foundation of China (61632001) and High-performance Computing Platform of Peking University, which are gratefully acknowledged. ሚ ሚ <ref type="figure">Fig. 1</ref>. Overview of our proposed RATE-Net. The pose transfer module first estimates a coarse outputĨ co under the target pose. In addition, it also provides regional guidance for the texture enhancing module, which then synthesizes a residual map R t to refine the coarse output.</p><p>In a typical person image, the bulk of the area is occupied by clothes with rich textural patterns, which are more likely to attract a viewer's attention. Therefore, the ability to synthesize realistic texture details is crucial for the performance and human perception of a transfer model. In the area of image-to-image (I2I) translation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, the U-net architecture <ref type="bibr" target="#b5">[6]</ref> has achieved remarkable success in preserving finegrained visual details through skip-connections. For human pose transfer task, however, such technique is not suitable due to the structural deformation of the human body under different poses. Siarohin et. al.proposed a modified version called deformable skip-connection <ref type="bibr" target="#b1">[2]</ref>, where feature maps extracted from the source image are warped onto the corresponding target regions according to pose correspondences.</p><p>The deformable skip-connections <ref type="bibr" target="#b1">[2]</ref> encapsulates the two most distinctive features of the so-called "warping-based" methods: (1) A pixel-wise (part-wise) correspondence map estimated from the source pose to the target pose; (2) A mechanism for warping features from the source image to corresponding target regions according to the estimated mapping. However, most existing methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> propose to channel the appearance features through the same pathway used for pose transfer, where the loss of fine-grained textural information would be inevitable due to down-sampling operations. This often leads to over-smoothed clothes and distorted facial landmarks that severely degrade the visual quality of synthesized images. In addition, estimating pixelwise mapping is extremely time-consuming and often requires additional dense annotations, making it intractable for many interactive designing and editing applications.</p><p>Instead, we seek another path of enhancing appearance details of pose-transferred images by synthesizing new textures that match the style of the input source image. To this end, we propose a novel Region-Adaptive Texture Enhancing Network (RATE-Net), where an additional texture enhancing module is introduced to estimate a residual map for detail refinement upon coarse estimations by the pose transfer module. The architecture of our framework is shown in <ref type="figure">Fig 1,</ref> where the source image is utilized in two ways: For the pose transfer module, we estimate a feature map roughly aligned with target pose, which later serves as the spatial guidance for the texture enhancing module. In addition, we also extract the style and textural information from the source image into compact codes, and inject the codes into the residual enhancing map through adaptive normalization layers <ref type="bibr" target="#b10">[11]</ref>. To help strengthen the mutual guidance between two modules, we also design an alternate training strategy to further improve the overall performance. Extensive experiments conducted on the challenging DeepFashion <ref type="bibr" target="#b11">[12]</ref> benchmark dataset demonstrate the superiority of the proposed framework against recent warping-based methods. In summary, our contributions are twofold:</p><p>• We propose RATE-Net, a novel enhancing based solution that utilize the style and texture information from the input source image for better textural refinement upon coarse images. The network utilizes the source image for both label map estimation and texture/style control, which leads to better pose transfer results compared with warping-based methods.</p><p>• We design an effective training strategy to maximize the mutual guidance between two modules, where pose transformation mapping can be further refined through the style-aware loss function computed over enhanced images, thus helps to preserve the integrity of human body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Human Pose Transfer is first described in PG2 <ref type="bibr" target="#b0">[1]</ref> where the goal is to transfer the appearance of a person from the source image into new poses. Existing works typically focus on warping the textures from the source image to the target image based on the warping transformation estimated between the corresponding poses. DSC <ref type="bibr" target="#b1">[2]</ref> segments the human skeleton into rigid parts and approximates the warping function with piece-wise affine transformation. Some recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> further estimate pixel-level feature warping flow by leveraging dense keypoint annotations, such as DensePose <ref type="bibr" target="#b12">[13]</ref> and SMPL <ref type="bibr" target="#b13">[14]</ref>. However, estimating dense annotations and pixel-wise warping flow are typically computationally expensive, and the corresponding groundtruth annotations are more difficult to collect. PATN <ref type="bibr" target="#b2">[3]</ref> proposed a lightweight network to gradually transfer a person's pose through several cascaded pose-attentional transfer blocks. However, most appearance details of the input image would be lost due to the down-sampling operation, which often leads to inferior results when dealing with person images with texture-rich clothes. Instead, we propose a region adaptive texture enhancing network which is better at capturing and re-synthesizing fine-grained details. Furthermore, it only relies on 2D skeletons for pose annotation and can be trained in an end-to-end fashion.</p><p>Adaptive Normalization provides a new mechanism to inject guidance information into the main image generation pathway. It is typically implemented as an affine transformation over normalized feature responses, with parameters inferred from external data. AdaIN <ref type="bibr" target="#b10">[11]</ref> was first proposed in style transfer task, and later used in other tasks such as high resolution face image synthesis <ref type="bibr" target="#b14">[15]</ref> and few-shot I2I translation <ref type="bibr" target="#b15">[16]</ref>. SPADE <ref type="bibr" target="#b16">[17]</ref> further expands the mean and deviation parameters from vectors to 3-tensors, thus incorporating spatial attention into semantic controlled image synthesis. This idea is also useful for few-shot video synthesis <ref type="bibr" target="#b17">[18]</ref>. Inspired by these works, we incorporate the pose guidance into our appearance encoding network to capture fine-grained visual information from texture-rich regions for detail enhancement in synthesized images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE NETWORK ARCHITECTURE</head><p>The proposed RATE-Net contains two modules: A pose transfer module that generates a coarse image under the target pose, and a texture enhancing module that estimates a residual map to fill in more appearance details onto the coarse image. The overall architecture is illustrated in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations</head><p>We first introduce some notations. The network takes two inputs, a source image I s and a target pose P t , and tries to generate a new image I t containing the person in I s under the pose P t . An 18 × 2 array of keypoint coordinates is estimated for both the source and target image, denoted as P s and P t , respectively. During the training, the generator G is fed with paired images (I s , I t ) of the same person under different poses along with the corresponding pose heatmaps (P s , P t ), which can be estimated and cached before training. The output of the network,Ĩ t = G(I s ; P s , P t ), is compared with groundtruth image I t for losses. Below we further dissect the generator G and elaborate on the two modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose Transfer Module</head><p>As shown in <ref type="figure">Fig. 1</ref>, the input of the pose transfer module consists of a source image I s and a pair of target poses (P s , P t ), which are concatenated along the channel dimension. The network consists of several convolutional down-sampling layers, followed by a series of pose-attentional transfer blocks <ref type="bibr" target="#b2">[3]</ref> that help to warp the contents of the source image onto the target pose P t in a progressive manner. The output feature map F t is then fed into the up-sampling layers to recover a coarse estimationĨ co = G P (I s ; P s , P t ) of the target image I t .</p><p>It should be emphasized that although our pose-transfer module shares a similar network architecture as in <ref type="bibr" target="#b2">[3]</ref>, the purpose and training strategy are completely different. In particular, we DO NOT aim to directly warp the fine-grained textures from the source image into the target pose, as most of the textural information would be lost due to the downsampling operation. Instead, we utilize the pose-transfer network to acquire a reasonable content feature map F t under the target pose, which provides regional guidance for the preceding texture enhancing module by hinting "where to add more texture details". In this way, our framework can synthesize new textures with a separate network and build the final pose-transferred image in a coarse-to-fine manner, which greatly improves the training stability and the performance of our network. Ablation studies are presented to validate our claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Texture Enhancing Module</head><p>The texture enhancing module aims to recover fine-grained visual details from the source image I s , and enhance the coarse estimationĨ co by synthesizing a region-aware residual texture map R t = G t (I s ; F t ) under the guidance of posealigned content feature map F t . We reuse the source image I s to extract texture codes z t with an encoder containing several convolutional down-sampling layers, residual blocks and an average pooling operation. To inject the textural code into the content feature map, we utilize the Adaptive Instance Normalization <ref type="bibr" target="#b10">[11]</ref> that originates from style transfer tasks. In particular, z t now plays the role of "style guide" which controls the pattern and granularity of textures in respective regions. The final output is acquired by adding the residual map R onto the coarse estimation:</p><formula xml:id="formula_0">I t =Ĩ co + R t = G P (I s ; P s , P t ) + G t (I s ; F t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discriminators</head><p>We leverage the design in <ref type="bibr" target="#b2">[3]</ref> that uses two discriminators to differentiate real and fake samples both in terms of shape and appearance consistency. The shape discriminator D S evaluates input image/pose pairs for shape consistency R S = D S (Ĩ t ; P t ), and the appearance discriminator D A compares the appearance consistency between the synthesized image and the source image R A = D A (Ĩ t ; I S ). Unlike <ref type="bibr" target="#b2">[3]</ref> that multiplies the scores, we train the two discriminators separately so that both criterion can be individually analyzed and optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE TRAINING STRATEGY</head><p>It is clear from <ref type="figure">Fig. 1</ref> that the two modules of our proposed framework are mutually dependent: The texture enhancement module relies on the pose-aligned guidance map F t to put textures on the right places, and the losses computed over texture-enhanced images are back-propagated to the pose transfer module, which helps improve the accuracy of the estimated guidance map. Therefore, we figure that using an alternate updating strategy should be useful for promoting the mutual guidance between two modules and achieve the best overall performance. Concretely, for each input batch, we first update the pose transfer module with a loss function L 1 defined over coarse estimationĨ co before performing an end-toend fine-tuning step to update two modules together with another texture-aware loss L 2 defined over final outputĨ t . The discriminator is then updated for K steps, where we empirically found K = 3 leads to a nice balance between training speed and discriminative capability. This concludes a "1-1-3" training cycle of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Loss Functions</head><p>As discussed in section 3.2, the pose transfer module is not designed for conveying fine-grained texture information, and the estimated coarse output can be expected to lack visual realism. Therefore it's unnecessary to enforce discriminative loss in L 1 . Instead, we simply adopt the following formulation:</p><formula xml:id="formula_1">L 1 = λ recon L recon + λ per L per</formula><p>where L recon is the pixelwise L1 loss, and L per is the perceptual loss in <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_2">L per = 1 CHW l w l φ l (I t ) − φ l (Ĩ co ) 1</formula><p>Here φ is a pretrained VGG-19 network and l denotes the layer index. In practice we found it effective to sample from different layers and use the weighted average loss to balance the perceptual consistency across different scales.</p><p>For loss function in step 2, we further add style loss and adversarial loss terms upon L 1 , leading to the full loss function as follows: where L sty is the Gram-matrix based style loss <ref type="bibr" target="#b18">[19]</ref> and L GAN is the addition of losses in D S and D A , which is also used for discriminator updating. Readers can find more details in the corresponding literature.</p><formula xml:id="formula_3">L 2 = λ recon L recon + λ per L per + λ sty L sty + λ GAN L GAN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement the proposed framework in PyTorch. Both modules include 3 down-sampling layers, and the pose transfer module contains 9 cascaded pose transfer blocks. LeakyReLU is used after convolution and normalization layers with 0.2 negative slope. The dimension of the texture codes is set to 128, and the impact of different lengths are further analyzed in ablation study. We use the Rectified Adam optimizer <ref type="bibr" target="#b19">[20]</ref> for better stability and performance at convergence. The full training involves 40K alternating cycles leading to a total of 200K iterations. The learning rate is set to 1e − 4 for all networks, and is fixed for the first 10K cycles before linearly drops to 0. The weight λ recon is set to 10 and other weights are set to 5.  <ref type="table">Table 1</ref>. Performance against other baseline methods, the highest and the second highest value for each metric is shown in bold / underline format. Up arrow means higher score is preferred, and vice versa.</p><formula xml:id="formula_4">Models SSIM ↑ IS ↑ FID ↓ LPIPS ↓ UV-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we compare our framework with several state-of-the-art methods to demonstrate the superiority of our framework. Furthermore, we perform a thorough ablation study to verify the efficacy of our main contributions. Dataset We validate our proposed framework on the Inshop Clothes Retrieval Benchmark of DeepFashion <ref type="bibr" target="#b11">[12]</ref> containing about 50K images of fashion models in texture-rich clothes under various poses. The images are in 256 × 256 resolution and contain clean background. We adopt Openpose <ref type="bibr" target="#b20">[21]</ref> to estimate an 18-point skeleton for each image, and convert it into a 18-channel heatmap as in <ref type="bibr" target="#b1">[2]</ref>. We use the train/test pairs in <ref type="bibr" target="#b2">[3]</ref>, where 101,966 pairs are randomly selected for training and 8,570 pairs for testing. The partition guarantees that the persons appeared in training and testing sets do not overlap, making it more reliable to validate the generalization ability of our model.</p><p>Evaluation Metrics For human pose transfer task, we aim to evaluate both the statistical fidelity and the perceptual quality of generated images. To this end, we adopt the Structural Similarity (SSIM) <ref type="bibr" target="#b21">[22]</ref> and Inception Score (IS) <ref type="bibr" target="#b22">[23]</ref> to account for the model's performance in both perspectives. However, a recent study <ref type="bibr" target="#b23">[24]</ref> has pointed out that IS is theoretically flawed and cannot always provide useful guidance when comparing models. To more reliably evaluate the perceptual quality of generated images, we introduce two more supervised metrics: FID <ref type="bibr" target="#b24">[25]</ref> and LPIPS <ref type="bibr" target="#b25">[26]</ref>. Note that both metrics utilize a pretrained network to convert the images into feature space, and compute the distance between image features with respect to both the global distribution and each pair of samples. We believe that supervised perceptual metrics can better reflect the perceptual fidelity of our proposed model than the unsupervised IS metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with Previous Works</head><p>We compare our proposed framework with several representative works: DSC <ref type="bibr" target="#b1">[2]</ref>, UV-Net <ref type="bibr" target="#b26">[27]</ref>, SPT <ref type="bibr" target="#b27">[28]</ref> and PATN <ref type="bibr" target="#b2">[3]</ref>. All the tests are carried out on the same set of testing pairs in <ref type="bibr" target="#b2">[3]</ref>. <ref type="table">Table 1</ref>   <ref type="table">Table 2</ref>. Quantitative ablation study results, the highest value for each metric is shown in bold format.</p><p>posed framework against recent state-of-the-art methods in terms of perceptual quality, while the SSIM and IS scores are also comparable. To further analyze the impact of the texture enhancing module, we also evaluate the quality of coarse estimations without residual map. As observed in <ref type="table">Table 1</ref>, although the SSIM score slightly drops after enhancement, the perceptual fidelity is significantly improved, with 20% gain on FID and 10% gain on LPIPS. Furthermore, although the network backbone is highly similar, our pose transfer module still performs considerably better than the original PATN implementation in perceptual fidelity, which can verify the efficacy of our texture enhancing module for refining the estimation of the guidance map F t .</p><p>In addition, we also showcase the qualitative result for some challenging examples with large pose transition and texture-rich garments, As shown in <ref type="figure" target="#fig_0">Fig 2,</ref> our method is more faithful to the input pose and appearance condition than other baselines and possesses more realistic visual details, especially like clothing textures and hair waves. Also, the gender bias issue <ref type="bibr" target="#b1">[2]</ref> on DeepFashion dataset is partly resolved as shown in the third row, where all the other methods output a female image by mistake. Moreover, it can be observed that the texture enhancing module is effective for recovering finegrained visual details lost in pose transfer module and consistently improving the visual quality of synthesized human images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>In this section, we perform an ablation study to analyze the effect of different parts of our proposed framework on the final performance. For each part, we set up a corresponding baseline by either removing this part from the whole framework or changing the key parameters.</p><p>PB Only We remove the texture enhancing module and train the pose-transfer module directly in an end-to-end fashion with loss function L 2 computed over coarse estimatioñ I co . Notice that this setting slightly differs from the PATN <ref type="bibr" target="#b2">[3]</ref> baseline as the loss function has an additional style loss term, and the weights are slightly modified.</p><p>PB Fix To further investigate the efficacy of our proposed training scheme, we initialize the parameters of the posetransfer module using pretrained models in <ref type="bibr" target="#b2">[3]</ref>, and keep it fixed during training. In this way, the interaction between two modules is broken, and detailed style loss and condi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Input</head><p>Ground Truth PB Only PB Fix Texture64dim Full <ref type="figure">Fig. 3</ref>. Qualitative results of ablation methods. Please zoom in for details.</p><p>tional adversarial losses cannot be back-propagated into the pose transfer module. Texture64dim We construct the corresponding baseline by reducing the length of the textural code to 64. In this way, the textural information extracted from source image is reduced, which could result in less informative textures and more severe artifacts.</p><p>Full This is the full version of our proposed framework used for comparing with other SOTA methods.</p><p>We report the quantitative performances of all four methods on the DeepFashion dataset in <ref type="table">Table 2</ref>. As observed, our full method has a clear advantage over other ablation methods, especially in terms of perceptual distance metrics like FID and LPIPS, which we believe has highlighted the importance of textural enhancement in human pose transfer task. It is also noteworthy that increasing the length of texture code can significantly boost the performance, and we speculate that this could lead to a useful method for determining the intrinsic dimension of the latent texture space, which is the length of the texture code at which point the model's performance stops improving. We didn't evaluate our model at higher code lengths because the memory cost had become intolerable. However, we believe that it's still possible to further improve the performance by using longer texture codes.</p><p>To better visualize the impact of different components, we showcase some representative examples in <ref type="figure">Fig. 3</ref>. As observed, our full framework is capable of synthesizing much finer textural details than all other methods. In addition, our framework also better preserves the integrity of human body (as observed in the second and the third row), which to our belief indicates the increasing accuracy of the pose-aligned feature map F t due to more localized guidance provided by the loss function L 2 defined over texture enhanced images. This further justifies our claim of mutual guidance between two modules and the proposed alternate training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We presents RATE-Net, a novel framework for synthesizing person images with sharp texture details. Instead of simply warping the patches from the source image with the risk of losing fine-grained texture details, we proposed to synthesize new textures with additional texture enhancing module that helps add more visual details to the coarse pose transfer results. Furthermore, an effective training strategy is proposed to alternately update the two modules for better overall performance. Compared with previous works, our framework can synthesize human images with much finer details, and can better preserve the style and appearance of the source image. We believe the idea behind our framework can inspire other related topics in semantic image synthesis as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative results of the proposed method against several competitive baselines. Some images have been cropped for visualization purposes. Please zoom in for details.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="406" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deformable gans for posebased human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3408" to="3416" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Stéphane Lathuilière, and Nicu Sebe</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Progressive pose attention transfer for person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI. 2015, Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Riza Alp Güler, and Iasonas Kokkinos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Dense pose transfer</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Soft-gated warping-gan for poseguided person image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dense intrinsic appearance flow for human pose transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Liquid warping GAN: A unified framework for human motion imitation, appearance transfer and novel view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1703.06868</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SMPL: a skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A stylebased generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>abs/1812.04948</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatiallyadaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Few-shot videoto-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1603.08155</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="586" to="595" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8857" to="8866" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised person image generation with semantic parsing transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2357" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
