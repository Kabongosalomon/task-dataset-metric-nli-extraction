<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrating Egocentric Localization for More Realistic Point-Goal Navigation Agents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Datta</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Oregon State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Georgia Tech</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Integrating Egocentric Localization for More Realistic Point-Goal Navigation Agents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>embodied navigation</term>
					<term>Point-Goal navigation</term>
					<term>visual odom- etry</term>
					<term>localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work has presented embodied agents that can navigate to pointgoal targets in novel indoor environments with near-perfect accuracy. However, these agents are equipped with idealized sensors for localization and take deterministic actions. This setting is practically sterile by comparison to the dirty reality of noisy sensors and actuations in the real world -wheels can slip, motion sensors have error, actuations can rebound. In this work, we take a step towards this noisy reality, developing point-goal navigation agents that rely on visual estimates of egomotion under noisy action dynamics. We find these agents outperform naive adaptions of current point-goal agents to this setting as well as those incorporating classic localization baselines. Further, our model conceptually divides learning agent dynamics or odometry (where am I?) from task-specific navigation policy (where do I want to go?). This enables a seamless adaption to changing dynamics (a different robot or floor type) by simply re-calibrating the visual odometry model -circumventing the expense of re-training of the navigation policy. Our agent was the runner-up in the PointNav track of CVPR 2020 Habitat Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Impressive progress has been made in training agents to navigate inside photo-realistic, 3D simulated environments <ref type="bibr" target="#b0">[1]</ref>. One of the most fundamental and widely studied of these tasks is Point-Goal navigation (PointNav) <ref type="bibr" target="#b1">[2]</ref>. In PointNav, an agent is spawned in a never-before-seen environment and asked to navigate to target coordinates specified relative to the agent's start position. No map is provided. The agent must navigate from egocentric perception and stop within a fixed distance of the target to be successful. Recent work <ref type="bibr" target="#b2">[3]</ref> has developed agents that can perform this task with nearperfect accuracy -not only achieving success 99.6% of the time but also following near-shortest paths from start to goal while doing so, all in novel environments without a map! While commendable, this result is not without some caveats. The standard PointNav setting (as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>) provides agents with perfect localization, which is referred to as an idealized 'GPS+Compass' sensor in that literature. Moreover, all agent actions are executed without any error or noise. Unfortunately, having accurate and precise localization information in unseen and previously unmapped indoor environments is unrealistic given current technology. Likewise, agent actuation is inherently noisy due to constraints such as imperfections in physical parts, variations in the environment, (e.g. a worn spot on the carpet that provides less friction), and the chaotic result of collisions with objects.</p><p>In addition to being unrealistic, these assumptions also shortcut the hard problems that a real robot must learn to solve this task. For example, oracle localization provided by the environment enables agents to continuously update the relative coordinates to the goal location -trivializing the problem of identifying when the goal has been reached.</p><p>In this work, we lift these assumptions and explore the problem of PointNav under noisy actuation and without oracle localization (no 'GPS+Compass' sensor). We develop models with internal visual  <ref type="figure">Figure 1</ref>: We lift assumptions of access to perfect localization via an idealized GPS+Compass sensor and noise-free agent actuations for Point-Goal navigation. Our proposed agent integrates learned visual odometry predictions to derive an estimate of its location in the trajectory. We train and evaluate our agent in environments with both noise-free and noisy actuations. odometry modules that estimate egomotion (changes in position and heading) from consecutive visual observations. This allows the agent to maintain a noisy, but up-to-date estimate of pose by integrating the per-action egomotion estimates along its trajectory ( <ref type="figure">Fig. 1)</ref>. Overall, this agent is provided the point-goal location in relative coordinates once at the start of each episode and must rely entirely on its own estimations to navigate effectively. We analyze our agent in two settings: without and with actuation noise. The former matches experimental protocols of prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>; the latter is our contribution and more realistic.</p><p>In both cases, we show that our agent significantly outperforms an entire range of highly competitive baselines. In particular, we compare against a "dead-reckoning" baseline that relies on a fixed mapping from actions to odometry readings to estimate pose. In the absence of actuation noise, our approach involving learning as opposed to memorization of odometry estimates outperforms deadreckoning with a relative improvement of 68% in SPL (0.51 vs. 0.30). Note that dead-reckoning serves as a very strong baseline for noiseless actuation: an agent equipped with knowledge about the environment dynamics can maintain an accurate estimate of its location in a seemingly straightforward manner. However, our empirical results show that dead-reckoning estimates are poor due to collisions, leaving the agent with no avenues to recover from even a single faulty localization. We also compare to and outperform adaptions of current PointNav agents that are re-trained to operate without localization information. On account of being monolithic neural policies trained end-to-end for the task, these adaptions, in principle, could learn to implicitly integrate egomotion. However, our approach, that involves an explicit module for predicting and integrating egomotion, outperforms these adaptions with relative improvements of over 466% (SPL=0.51 v/s SPL=0.09).</p><p>In the noisy setting, we again outperform dead-reckoning and adaptions of current PointNav agents with relative improvements of 194% and 114%, respectively. In addition to that, we show that having a conceptual separation between the learning of agent dynamics (via the odometry module) and task-specific navigation policy allows for a seamless transfer of the latter from noiseless to noisy actuation environments. We show this via a simple fine-tuning of the learned odometry model (while keeping the underlying policy frozen) to adapt to the noisy actuations in the new setting. This is akin to a 'hot-swappable' navigation policy that can be incorporated with different egomotion models, thereby circumventing the expensive re-training of the former. This is highly significant because prior work <ref type="bibr" target="#b2">[3]</ref> requires billions of frames of experience, hundreds of GPUs, and over 6 months of GPU-time to train a near-perfect navigation policy. Our 'hot-swapping' approach allows us to seamlessly leverage that work in noisy actuation settings with zero (re-)training of the policy.</p><p>To summarize our contributions, to the best of our knowledge, we develop the first approach for PointNav in realistic conditions of noisy agent actuations and no localization (GPS+Compass sensor) input. A straightforward modification of our proposed agent was the runner-up for the PointNav track of the CVPR 2020 Habitat Challenge. We view this as a step towards one of the grand goals of the community -making navigation agents more suitable for deployment in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Goal-Driven Navigation. The topic of goal-driven embodied navigation has witnessed exciting progress <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Point-Goal navigation <ref type="bibr" target="#b1">[2]</ref>, being one of the most fundamental among these task, has also been the subject of several prior works. Point-Goal navigation approaches can be broadly categorized into two parts. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> take inspiration from a classical decomposition of the navigation problem into building map representations and path-planning. More recently, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> study end-to-end training of neural policies using RL. Among the above, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref> either assume known agent egomotion or perfect localization. Furthermore, <ref type="bibr" target="#b9">[10]</ref> assumes access to a noisy variant of the agents pose obtained by manually adding noise (sampled from real-world experiments on Lo-CoBot <ref type="bibr" target="#b11">[12]</ref>) to the oracle sensor. Similarly, <ref type="bibr" target="#b12">[13]</ref> uses a noisy variant of dead-reckoning to estimate agent location while building allocentric spatial maps for exploration. In contrast to these works, our agents operate in a more realistic setting which doesnt assume any form of localization information whatsoever and has noisy transition dynamics. Furthermore, we show that our approach of integrating visual odometry predictions to estimate localization outperforms dead-reckoning baselines.</p><p>The motivations of our work are also closely related to <ref type="bibr" target="#b13">[14]</ref> where the authors train an agent to navigate to 8-image panoramic targets without an explicit map or compass and using imitation learning on expert trajectories. In contrast, we work with a different task wherein our agents navigate to 3D goal locations as opposed to a panoramic image of the target in an unseen environment. Likewise, the authors in <ref type="bibr" target="#b14">[15]</ref> show that binaural audio signals in indoor environments can serve as an alternate form of localization in the absence of GPS+Compass. In contrast, our method doesn't rely on the introduction of any additional modalities beyond vision to estimate localization.</p><p>Localization and Visual Odometry. Our work is also related to the large body of work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> on localization and visual odometry. Most similar to our approach for training our agent's odometer is <ref type="bibr" target="#b17">[18]</ref> that estimates relative camera pose from image sequences. However, unlike these models that are trained specifically for the end-task of odometry estimation, our goal is to integrate the visual odometry model into the specific downstream task of Point-Goal navigation. In this context, our egomotion estimator is used in an active, embodied set-up wherein the egocentric localization estimates are used by the agent to select actions that in turn affect the egomotion.</p><p>Reducing the Sim2Real Gap. Our motivation to move towards more realistic PointNav settings is also aligned with <ref type="bibr" target="#b18">[19]</ref>. The authors attempt to find the optimal simulation parameters such that improvements in simulation also translate to improvements in reality for Point-Goal navigation (with GPS and compass). In order to run in-reality experiments, they mounted a high-precision (and costly) LIDAR sensor to the robot in order to provide location information -on-board IMUs, motor encoders, and wheel rotation counters proved too noisy. In contrast, our proposed approach performs navigation without the need for any expensive localization sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Preliminaries. We start by describing the Point-Goal navigation task with oracle localization and current agent architectures for the task, before describing the details of our approach.</p><p>In Point-Goal navigation <ref type="bibr" target="#b1">[2]</ref>, an agent is spawned at a random pose in an unseen environment and asked to navigate to goal coordinates specified relative to its start location. At every step of the episode, the agent receives RGB-D observation inputs via its visual sensors and a precise estimate of its location in the trajectory (relative to the start of the episode) via an idealized GPS+Compass sensor. Using these inputs for the current step, the agent performs an action by predicting a distribution over a discrete action space -{move-forward, turn-left, turn-right, stop}.</p><p>Prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> has trained recurrent neural policies for Point-Goal navigation end-to-end using RL. At a high-level, these recurrent policies (modelled as a 2-layer LSTM) typically predict actions based on the (ResNet-50) encoded representation of the current visual observation, its previous action, the goal coordinates for the episode and the noise-free localization estimate from the GPS+Compass sensor. Note that given the information about (a) the goal coordinates and (b) its current location both with respect to the start of the episode, it is straightforward for the agent to derive a noise-free estimate of the goal coordinates relative to its current state at every point in time.</p><p>Point-Goal Navigation without Oracle Localization. Our proposed approach removes the oracle localization information (GPS+Compass sensor) and instead equips agents with an odometry module that is responsible for predicting per-action egomotion estimates. Access to such an odometer allows the agent to integrate its egomotion estimates over the course of its trajectory -thereby deriving a potentially erroneous substitute of the localization information. Taking inspiration from existing literature in visual odometry, we train our odometry model to regress to the change in pose between two sequential observations. In the sections that follow, we describe the details of our visual odometry model, the dataset collection protocol that was followed to collect data for training the odometer and its integration with the agent policy.</p><p>Visual Odometry Model. We design our odometry model as a CNN that takes visual observation pairs (in our case, depth maps) as input and outputs an estimate of the relative pose change between the two states. We characterize the pose change as the positional offset (∆x, ∆y, ∆z) and change in heading/yaw (∆θ) of the second state with respect to the first ( <ref type="figure">Fig. 6 (a)</ref> shows the agent coordinate system). More concretely, let I t and I t+1 be the depth images corresponding to the agent states at time t and t + 1, respectively. Both frames are depth concatenated and passed through a series of 3 convolutional layers: (Conv 8×8, ReLU, Conv 4×4, ReLU, Conv 3×3, ReLU). Subsequent fc-layers generate the flattened 512-d embedding for the depth map pair, followed by predictions for ∆x, ∆y, ∆z, and ∆θ (egomotion deltas). The egomotion CNN is trained to regress to the ground-truth egomotion deltas by minimizing the smooth-L1 loss.</p><p>Egomotion Dataset. To train our odometry model, we require a dataset comprising of observation pairs and the corresponding ground-truth egomotion between the two states defined by the observation pairs. These constitute the inputs and regression targets for the odometry CNN respectively.</p><p>In order to collect this dataset, we adopt the follwing protocol. We take a Point-Goal navigation agent that has been trained with oracle localization and use its policy to unroll trajectories. Then, we sample pairs of source (src) and target (tgt) agent states from within those trajectories uniformly at random. For each (src, tgt) pair of states, we record (a) the corresponding visual observations (v t and v t+1 ) and (b) the ground-truth 6-DoF camera pose from the simulator. The latter comprises of a translation vector, t ∈ IR 3 , denoting the agent's location in world coordinates and a rotation matrix, R ∈ SO(3), representing a transformation from agent's current state to the world. Therefore, for a given pair of (src, tgt) states, we can obtain our egomotion transformation between (src, tgt) as:</p><formula xml:id="formula_0">T tgt→src = T −1 src→world · T tgt→world , where T a→b = R a→b t a→b 0 1 .</formula><p>In the absence of noise in agent actuations, we would, in principle, expect to obtain a deterministic mapping between agent actions and odometry readings. However, in practice, the agent occasionally suffers displacements along x while executing move-forward (see <ref type="figure">Fig. 6</ref>(a) for agent coordinate system). This happens due to collisions with the environment and in such cases, the agent's displacement along z i.e. its heading is also different than the standard setting of 0.25m. turn-left and turn-right always correspond to 10 • (or 0.17 radians).</p><p>We sample a total of 1000 (src, tgt) pairs from each of the 72 scenes in the Gibson training split <ref type="bibr" target="#b0">[1]</ref> for a total of 72k pairs. Within each scene, we balance out the sampling of the 1000 points by collecting an equal number of points from each of the 122 trajectories in that scene. Since our dataset has been collected by sampling from unrolled agent trajectories, the distribution of actions is representative of the type of actions an agent might take while navigating -58%, 21% and 21% of the dataset corresponds to move-forward, turn-left and turn-right actions respectively.</p><p>Integrating Egomotion Estimation with the Agent. Having described the odometry model and the dataset that it's trained on, we now discuss how it integrates into the agent architecture. Recall (from Sec. 3) that, owing to the availability of perfect localization, the Point-Goal navigation agents from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> can derive the relative goal coordinates at every state. In the absence of GPS+Compass, our proposed agent uses the odometry model to first estimate its location in the trajectory and subsequently, predicts the relative goal coordinates as follows.</p><p>At every step, the egomotion predictions from the odometry CNN are first converted to a 4 × 4 transformation matrix that represents a transformation of coordinates between the agent's previous and current states in the trajectory (using the equation above). The transformation is then applied to the relative goal coordinate predictions from the previous time step to project them on to the move_forward = 0, = -0.25, = 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Δx</head><p>Δz Δθ   Noisy actuations. Real-world robot locomotion is far from deterministic due to the presence of actuation noise. In order to model such noisy agent actions, we leverage noise models derived from real-world benchmarking of an actual, physical robot by the authors in <ref type="bibr" target="#b18">[19]</ref>. Specifically, we use both linear and rotational noise models from LoCoBot <ref type="bibr" target="#b19">[20]</ref>. The former is a bi-variate Gaussian (with a diagonal covariance matrix) that models egomotion errors along the z and x axis whereas the latter is a uni-variate Gaussian modeling egomotion errors in the agent's heading. For any given action, we perform truncated sampling and add noise from both the linear and rotational noise models.</p><formula xml:id="formula_1">Policy h t−1 h t loc t a t−1 (i) no-localization a t I t g ep Policy h t−1 h t a t−1 (ii) aux-loss goal/ego a t I t g ep h t−1 h t a t−1 (iii) gt-localization Policy a t I t g ep a t−1 Policy h t−1 h tl oc t ego t loc t−1 CNN (v) ego-localization (iv) dead-reckoning a t I t g ep a t−1 Policy h t−1 h tl oc t loc t−1 Left : 10 ∘ Right : − 10 ∘ Fwd : 0.25m</formula><p>We collect a version of the egomotion dataset under the noisy actuation setting by following the same protocol as described in Sec. 3. Under this setting, the agent suffers non-zero egomotion deltas along all degrees of freedom for all action types. Please refer to the Supplementary document for details regarding the parameters of actuation noise models and a comparison of the distributions of per-action egomotion deltas in the dataset across noiseless and noisy actuation set-ups.</p><p>Training. The odometry CNN is pre-trained on the egomotion dataset and kept frozen. We train the agent's navigation policy using DD-PPO [3] -a distributed, decentralized and synchronous implementation of the Proximal Policy Optimization (PPO) algorithm.</p><p>Let d t be the geodesic distance to target at time step t. Furthermore, let s denote the terminal success reward obtained at the end of a successful episode (with I success being the indicator variable denoting episode success) and λ be a slack-reward to encourage efficient exploration. Then, the reward r t obtained by the agent at time t is given by: r t = s . I success success reward</p><formula xml:id="formula_2">+ (d t−1 − d t ) reward shaping + λ slack reward</formula><p>. We set s = 1.0, λ = −0.01 for all our experiments. We train all our agents for a maximum of 60M frames. We initialize the weights of the visual encoder with those from an agent that has been trained (using ground-truth localization) for 2.5B frames. Check Supplement for additional training details.</p><p>4 Baselines no-localization: This is a naive adaption of the Point-Goal agents from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> to our setting with no localization information. The policy network for the agents in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> take the previous action, visual observations, episode goal and oracle GPS+Compass as inputs at every step (Sec. 3). For adapting this model to our setting, we drop the GPS+Compass input (keeping the other 3 unchanged). We train this agent with the same reward settings and losses, as described in Sec 3 (and consistent with <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>). The performance of this baseline tells us how well an agent would do if it is trained for the Point-Goal navigation task using the state-of-the-art approach <ref type="bibr" target="#b2">[3]</ref> but without any localization information whatsoever to guide its navigation.</p><p>aux-loss (aux-loss-goal + aux-loss-ego): These agents are similar in architectural set-up to the no-localization baseline. However, they are trained with additional auxiliary losses that encourage them to predict information pertinent to their localization. At every step, the policy network is additionally trained to predict, from its hidden state, either the goal coordinates relative to its current state (aux-loss-goal) or the relative change of pose between its previous and current states (aux-lossego). Note that these baselines have access to the GPS+Compass sensor during training, just as our approach does to train our odometry module. But, they use this information indirectly as auxiliary losses rather than directly as localization information. Both our approach and these baselines do not use the oracle GPS+Compass information at test time. Comparing our approach to these baselines demonstrates the value of an explicit odometry module as opposed to relying on a monolithic neural policy agent to learn that it should infer localization-related information from auxiliary signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dead-reckoning:</head><p>The agent derives localization estimates using a static look-up table that maps actions to associated odometry readingsmove-forward: displacement of 0.25m along heading, turn-left/turn-right: 10 • on-spot rotation. A comparison with this baseline answers the question is it really necessary to learn the odometry estimates instead of naively memorizing them?</p><p>classic: We also compare with a classic robotic navigation pipeline that has modular components for map creation, localizing the agent on the map, planning a path, selecting a waypoint on the planned path and moving the agent along the predicted waypoint. Such pipelines have been extensively used in robotics with several choices available for each component. Following <ref type="bibr" target="#b0">[1]</ref>, we leverage prior work <ref type="bibr" target="#b20">[21]</ref> that proposes a complete implementation of such a pipeline that can be readily deployed in simulation. <ref type="bibr" target="#b20">[21]</ref> uses ORB-SLAM2 <ref type="bibr" target="#b21">[22]</ref> as the agent localization module. We use the same set of hyperparameters as reported in the original work (refer to the Supplementary for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ego-localization:</head><p>Finally, this is our approach wherein the agent uses a trained odometry model to derive localization estimates for navigation using a neural policy.</p><p>As a benchmark for upper-bound performance, we also report numbers for the agent that navigates in the presence of 'oracle' localization (gt-localization) during both training and test episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Findings</head><p>Environment. We use the Habitat simulator <ref type="bibr" target="#b0">[1]</ref> with the Gibson dataset of 3D scans <ref type="bibr" target="#b22">[23]</ref> for our experiments. We leverage the human-annotated ratings (on a scale of 1-5) provided by <ref type="bibr" target="#b0">[1]</ref> for mesh reconstructions and only use high-quality environments (≥ 4 rating) for our experiments. For a given scene, we use the dataset of Point-Goal navigation episodes from <ref type="bibr" target="#b0">[1]</ref>. Overall, we work with 8784 episodes across 72 scenes in train and 994 episodes across 14 scenes in val.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics.</head><p>A navigation episode is deemed successful if the agent calls stop within a distance of 0.2m from the target location. We measure success rate as the fraction of successful episodes. Following prior work on Point-Goal navigation, we also report performance on the Success Weighted by Path Length (SPL) metric <ref type="bibr" target="#b1">[2]</ref>, averaged across all episodes in the validation split.</p><p>Owing to their reliance on a binary indicator of episode success, both success rate and SPL do not provide any information about episodes that fail. Consider the scenario <ref type="figure" target="#fig_4">(Fig 3(c)</ref>) wherein an agent follows a path that closely resembles the shortest path between start and goal locations, but prematurely calls stop right at the 0.2m success perimeter boundary. Both success and SPL for the episode are 0. Although the agent managed to navigate reasonably well, its performance gets harshly ignored from the overall statistics. Hence, relying on success and SPL as the sole metrics (as done by prior work) often leads to an incomplete picture of the agents overall performance. We observe that this problem is even more acute in set-ups without GPS+Compass where noisy pose estimates directly affects the agent's decision to call stop at the right distance relative to the goal.</p><p>To get a more holistic estimate of the agent's performance, we also report the geodesic distance to target upon episode termination (geo d T). In addition to that, we propose a new metric called Soft-SPL. Let d init and d T denote the (geodesic) distances to target upon episode start and termination. The SoftSPL for an episode is defined as: SoftSPL = 1 − dT dinit · s max(s,p) where s and p are the lengths of the shortest path and the path taken by the agent. SoftSPL replaces the binary success term in SPL with a "soft" value that is indicates the progress made by the agent towards the goal ("progress" can be negative if the agent ends up farther away from the goal than where it started).</p><p>We now present navigation results for our proposed model and compare with baselines in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How crucial is localization for PointNav?</head><p>We answer this by comparing the upper-bound performance of the agent trained and evaluated with oracle localization (GT-localization) with an agent that has been trained and evaluated to navigate without any form of localization input (the no-localization baseline). We see a drastic drop in both SPL (0.866 to 0.096) and Success (0.948 to 0.099) as the localization input is taken away. This makes it seem like the baseline method is completely failing. However, we observe that these baselines are able to navigate reasonably well, they are just not reaching the exact goal location (SoftSPL of 0.726 for no-localization vs. 0.865 for gt-localization). Therefore, to get a meaningful measure of performance, we compare SoftSPL numbers, wherever appropriate in the text that follows.</p><p>Next, we also compare the dead-reckoning agent with memorized odometry estimates with the nolocalization baseline. We see that the former outperforms the latter (SPL=0.096, SoftSPL=0.726 vs. SPL=0.303, SoftSPL=0.797). This shows that having some form of localization (albeit, erroneous) is essential for navigation. Our proposed agent with learnt odometry prediction) is able to further outperform dead-reckoning with 67.6% relative improvements in SPL (0.508 vs. 0.303).</p><p>How privileged is an agent with access to oracle localization at test time? By comparing the gt-localization agent with our ego-localization approach, we find that our agent succeeds less often (SPL=0.508, Success=0.535 vs. SPL=0.866, Success=0.948). However, it is able to make progress towards the goal by a degree that reasonably matches that of the "oracle" agent (SoftSPL=0.813 v/s SoftSPL=0.865). This can be explained by the fact that our agent, with noisy estimates of its location thinks that it has reached the goal and calls stop prematurely (geo d T=0.959 v/s geo d T=0.388).</p><p>How well can we do by simply memorizing odometry estimates? Recall that, in the absence of any noise in the agent's actuations, memorized egomotion estimates will be incorrect only during collisions. Our agent consistently outperforms dead-reckoning across all navigation metrics in the noiseless settings -with 2%, 67.6%, 72% and 8.4% relative improvements in SoftSPL, SPL, Success and geo d T, respectively. This points to the utility of learning egomotion from visual inputs.</p><p>Transferring policies to noisy actuations. We shift our attention to the noisy actuation setting now. For this setting, we present results for the following set-ups: (a) old-policy: where we evaluate policies trained in noiseless actuations without any fine-tuning, (b) re-trained policy: where the policies are re-trained and evaluated in the noisy set-up.</p><p>As one would expect, when policies trained in a noiseless actuation setting are transferred to environments with noisy actions, navigation performance suffers. For the no-localization, gt-localization and our ego-localization agents, the SoftSPLs drop from 0.726, 0.865 and 0.813 to 0.491, 0.666 and 0.280 (old-policy setting for no-localization, gt-localization and ego-localization in Tab. 1), respectively. It is interesting to note that this performance drop affects agents with imperfect localization estimates worse than those with oracle localization. This is because, for the former, noisy actuations become all the more challenging -the agent doesn't end up where it would expect its actions to lead to, and there is no corrective signal that can allow for a potential re-calibration towards the goal.  Next, we investigate ways to retrain our agents to adapt to noise in the actuations. Recall that our proposed agent offers a decoupling between learning dynamics (odometry) and navigation (policy). Taking advantage of this, we treat the policy as a "swappable" component that can be used with a different odometry model re-calibrated to noisy actuations. We first fine-tune the odometer on the noisy version of the egomotion dataset (Sec 3), followed by using the fine-tuned odometer with the frozen policy (the retrained-odom setting in Tab. 1). We see significant performance gains in doing so -a relative improvement of 76.43% in SoftSPL (0.280, old-policy vs. 0.494, retrained-odom).</p><p>In the absence of a separation between dynamics and policy, the only way to adapt the nolocalization and gt-localization agents to the noisy setting is via an expensive re-training of the policy. Doing so leads to performance gains across both sets of agents -relative improvements of 4.88% and 0.75% in SoftSPL, respectively (old-policy v/s re-trained policy). Moreover, doing the analogous re-training of our agent's policy (with the re-trained odometer) i.e. re-trained-odom+policy, leads us to out-perform all baseline agents in noisy actuation settings as well (SoftSPL=0.576 for our proposed agent vs. 0.515, 0.407 for no-localization and dead-reckoning, respectively).</p><p>Comparisons with classic approaches. In the noiseless setting, our learnt visual odometry estimates lead to better PointNav agents than using ORB-SLAM2 <ref type="bibr" target="#b21">[22]</ref> for localization -with 39.2%, 6.3% and 18.9% relative improvements in SoftSPL, SPL and geo d T, respectively. With noisy actuations, the baseline has a higher success rate (SPL=0.267 vs. 0.047). However, it takes much longer to reach the goal (SoftSPL=0.301 vs. 0.576). Note that the high success but poor SPL of this baseline is simply a statement about its heuristic path-planning aspect, and not about the performance of SLAM-based localization. Specifically, the baseline uses a set of hand-coded heuristics to select and move towards a waypoint on the planned-path. Every-so-often (with a probability of 0.1), the agent executes a randomly sampled action (an avenue to recover, in case the agent gets stuck). Therefore, this baseline agent does make progress towards the goal, but in a suboptimal number of steps on account of the above heuristics-based/occasionally-random sampling of actions.</p><p>Habitat Challenge 2020 Our submission, comprising of a simple modification of our proposed agent, to the PointNav track of Habitat Challenge 2020, was ranked #1 on the Test-Challenge leaderboard with respect to SoftSPL (0.596) and distance to goal (1.824) and #2 with respect to SPL (0.146). Please check the Supplementary document for details regarding the challenge configuration settings, modifications to our approach and a snapshot of the leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We develop Point-Goal navigation agents that can operate without explicit localization information from the environment (no GPS+Compass sensor) in the presence of noisy transition dynamics. Our agent, with learnt visual odometry modules, demonstrates performance improvements over naive adaptions of existing agents to our setting as well as strong (learnt and traditional) baselines and emerges as runners-up in the Habitat Challenge 2020. We also show that such a separation between learning the dynamics of the environment (via the odometer) and learning to navigate (via the policy) allows for a straight-forward transfer of our agent from noiseless to noisy actuation settings -circumventing the time and resource intensive training of near-perfect navigation policies as in prior work <ref type="bibr" target="#b2">[3]</ref>. We view this as a step towards one of the grand goals of the community -making navigation agents more suitable for deployment in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>The Georgia Tech effort was supported in part by NSF, AFRL, DARPA, ONR YIPs, ARO PECASE, Amazon and Siemens. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">LoCoBot noise models</head><p>For our experiments with noisy actuations, we source noise models that were collected in the real world by measuring the accuracies of position controllers (implementations of low-level control that the robot uses to get to a desired pose) on a physical robot. The authors in <ref type="bibr" target="#b11">[12]</ref> experimented with implementations of 3 different controllers on both LoCoBot and LoCoBot-Lite [20] -(1) DWA, (2) Proportional Controller and (3) ILQR (refer to <ref type="bibr" target="#b11">[12]</ref> for a description of the controllers). Trials in the real world were conducted to quantify the difference in the commanded v/s achieved state of the robot by using each of the above controllers. <ref type="bibr" target="#b11">[12]</ref> reports that the errors were generally lower for LocoBot and ILQR performed the best among the controllers. Hence, we source the noise models derived from the LoCoBot-ILQR trials for experiments in our paper.</p><p>To re-cap, we model both translational and rotational noise in the agent's actuations. Translational noise is measured along z (the direction of agent motion) and x (the direction perpendicular to motion) on the ground plane and modelled as a bi-variate Gaussian (with a diagonal co-variance matrix). Rotational noise is measured along rotation around y and modelled as a uni-variate Gaussian.</p><p>Also recall that, for any given action, we add noise to both the agent's location as well as heading (by sampling from the respective noise distributions) in order to simulate say, an agent turning a bit while attempting to move forward (or, vice-versa). We use different sets of translational and rotational noise models for (a) linear motion (i.e. move-forward) and rotational motion (i.e. turn-left and turn-right). We now present the parameters of the noise models. The mean and variance for the uni-variate rotational noise model are: µ = 0.023 and σ = 0.012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">DD-PPO Training</head><p>Recall that we use DD-PPO <ref type="bibr" target="#b2">[3]</ref> to train our agent policies. Following <ref type="bibr" target="#b2">[3]</ref>, we force the rollout collection by all DD-PPO workers to end (and model optimization to start) after 80% (pre-emptive threshold) of the workers have finished collecting rollouts. This is done in order to avoid delays due to "stragglers and leads to better scaling across multiple GPUs. We use PPO with Generalized Advantage Estimation <ref type="bibr" target="#b23">[24]</ref>. We set the discount factor γ to 0.99, the GAE parameter τ to 0.95 and the clip parameter to 0.2 (along with a linear decay for the clip parameter with the number of PPO updates). We use the Adam optimizer with an initial learning rate of 2.5e-4 and with a linear decay. We clip gradients norms to a max of 0.5 before policy updates. These hyper-parameters are consistent with the experimental settings in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Hyperparameters for the classic navigation baseline</head><p>The classic navigation baseline (Sec 4 in the main paper) builds a map of size 400 × 400 where each grid/cell in the map corresponds to 0.1m ×0.1m dimensions of physical space. The mapper estimates an occupancy map of the environment from depth maps and camera intrinsics. At any given step, all points in the depth map are projected into an egocentric point cloud, followed by a thresholding operation where only points within a depth range of 0.1m to 4m are retained. The point cloud is then transformed to global scene coordinates with the help of pose estimates from the ORB-SLAM2 localization module. This is followed by projecting all points that lie within a range of [0.3, 1] times the camera height (1.25m) onto the ground plane. To create a 2-D obstacle map, any map cell with at least 128 projected points is treated as an obstacle. Given the obstacle map generated by the mapper, the SLAM-estimated pose and the target goal location, the planner finds a shortest path from the agent's location to the goal via the D* Lite algorithm <ref type="bibr" target="#b24">[25]</ref>. Given that planned path, the baseline selects a waypoint along the path that lies at a distance of at least 0.5m from the agent's current state. If the agent's heading is within a range of 15 • from the direction of this waypoint, it executes a move-forward action. Otherwise, it rotates (turn-left or turn-right) towards the direction of the waypoint. Every so often (with a probability 0.1), a random action (among move-forward, turn-left and turn-right) is executed. <ref type="figure" target="#fig_6">Fig 4.</ref> shows some qualitative examples of data points from our egomotion dataset that has been collected in the noiseless actuation setting. For each of the 3 action classes in our dataset (move-forward, turn-left and turn-right), we show randomly sampled data points. Recall (from Sec. 3 in the main paper) that each data point in our dataset consists of RGB frames and depth maps corresponding to the source (src) and target (tgt) agent states as well as the associated ground-truth relative pose change between src and tgt. Since there's no noise in the agent actuations, the agent always turns by exactly 10 • (or, 0.17 radians) when executing turn-left or turn-right (and doesn't suffer any displacements in the ground plane while doing so). Similarly, a move-forward doesn't lead to any change in the agent's heading. The right-most example under the move-forward action presents an instance where the agent suffers a collision (with a wall) while attempting to move forward. As a result, the agent also has some non-zero displacement along the x-axis (the direction perpendicular to the agent's heading in the ground plane). Note that the displacement along the z-axis (the agent's heading) is also different from 0.25m (the default actuation specification for the move-forward action). or turn-right), we record non-zero pose changes across x, z as well as θ (that deviates from the default actuation specifications for the actions). <ref type="figure">Fig. 6</ref> shows a comparison of the per-action egomotion distribution between the noiseless and noisy actuation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Qualitative examples from the Egomotion dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Standalone evaluation of the visual odometry model</head><p>Recall that, the odometry model that our proposed agent is equipped with is pre-trained on the egomotion dataset and then kept frozen during the training/evaluation of the agent policy. In Sec. 5 of the main paper, we report results for our agent's navigation performance (using our odometer to derive localization estimates). In addition to that, in this section, we also report numbers for a standalone evaluation of our visual odometry model on the task of regressing to the ground-truth relative pose estimates between source and target agent states.</p><p>We perform such an evaluation of our odometry model under two settings -novel observation pairs from previously-seen environments (val-seen) and novel observation pairs from unseen environments (val-unseen). We generate the former by sampling data points from scenes in the training split (and ensuring there is no overlap with the train set of data points) and the latter by generating data points from scenes in the val split. For both splits (val-seen and val-unseen), the data collection protocol remains the same, as described in Sec. 3 in the main paper.  <ref type="figure">Figure 6</ref>: A comparison of the distribution of per-action egomotion deltas between noiseless and noisy actuations settings of our egomotion dataset. For noiseless actuations, turn actions always correspond to a 10 • turn. Forward actions result in a displacement of 0.25m along the agent's heading with the exception of collisions where the agent potentially also suffers some displacement along x. In the noisy setting, for every action, we sample and add noise from LoCoBot <ref type="bibr" target="#b19">[20]</ref> to all 3 degrees of freedom.  We also report the distribution of egomotion prediction errors factored along x, y, z and yaw (θ) for the val-unseen split in <ref type="figure" target="#fig_10">Fig 7.</ref> Error here refers to the difference in the predicted and the ground-truth values for the x, y, z and yaw components of the relative pose. Note that although the task of regressing to the relative pose change, given visual observation pairs seems straightforward (as evidenced by the low loss values on val), navigating using these integrated egomotion estimates from the odometry model still remains a challenging task (as seen by the difference in performance between gt-localization and ego-localization in Sec. 5 of the paper). This is due to the fact that although the visual odometry model is able to deliver locally accurate predictions of egomotion, integration of these estimates along trajectories leads to accumulation of errors ( <ref type="figure" target="#fig_4">Fig.  3</ref> from the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Large-scale training</head><p>The results reported in the paper (Sec. 5) correspond to training our agents (and baselines) for up to 60M frames of experience. We also experiment with large-scale training of our agents for up </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7">Habitat Challenge 2020</head><p>We submitted a straightforward modification of our proposed (ego-localization) agent to the Point-Goal navigation (PointNav) track of Habitat Challenge 2020 (organized as part of the Embodied AI Workshop at CVPR 2020). In addition to noisy agent actuations and the absence of a GPS+Compass sensor, the configuration settings of the challenge also include the following:</p><p>• visual sensing noise (noisy RGB-D observation maps)</p><p>• a change in the agent's sliding dynamics. As per the simulator settings used in the experiments reported in the paper (also consistent with prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>), the agent slides along walls and boundaries of obstacles during collisions. This design choice was inspired by game engines where such sliding behavior allows for a smoother player control. However, this behavior is not realistic a real-world robot would bump into obstacles and simply stop upon colliding. • wider turn angles (30 • vs. 10 • ) • physical agent dimensions and camera configuration parameters (spatial resolution, fieldof-view and camera height) set to match the settings in LoCoBot.</p><p>For the purposes of our challenge submissions, we replaced the 3-layer CNN encoder of our visual odometry model with a ResNet18 based encoder and trained the odometer on a version of the egomotion dataset collected under the challenge settings. We found that using a higher capacity ResNet18 encoder in the odometer was necessary to outperform baselines in the presence of noise in the depth maps. We also re-train our agent policy under the new challenge settings.</p><p>The challenge results are shown in <ref type="figure" target="#fig_11">Fig. 8</ref>. Our submission (under the team name, "ego-localization") was ranked #1 on the Test-Challenge leaderboard with respect to SoftSPL (0.596) and distance to goal (1.824) and #2 with respect to SPL (0.146).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2009.03231v1 [cs.CV] 7 Sep 2020 GPS+Compass + Noiseless actuations No GPS+Compass + Noisy actuations (Our setting) a t ∈ { move-forward, turn-left, turn-right, stop } { move-forward, turn-left, turn-right, stop }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Egomotion dataset (noiseless actuations) (c) Egomotion dataset (noisy actuations) (d) Architectures for baselines and our proposed agent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>coordinates (wrt. initial pose) loc t ,loc t : oracle and estimated localization ego t : egomotion prediction (a) Agent coordinate system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>(a) shows the agent coordinate system. We also show samples of data points from the egomotion dataset collected under (b) noiseless and (c) noisy actuation settings. Each data point consists of visual observations (RGB-D maps) along with ground-truth egomotion (∆θ represents a rotation around Y) between two states defined by the observations. (d) We show architectures for our proposed (ego-localization) and baseline agents.coordinate system defined by the agent's current state. This generates the relative goal coordinate input for the current step of the policy network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Top-down visualization of our ego-localization agent trajectories. In all cases, the shortest path, is shown in green. (a), (b) demonstrate successful trajectories where the agent's predicted pose (red) closely follows its actual trajectory (blue). In (c), the agent's pose predictions deviates substantially from its trajectory causing it to incorrectly call stop at the success boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Linear motion. For linear motion (i.e. move-forward action), the mean and co-variance for the translational bi-variate Gaussian noise model are:µ = [µ z = 0.014, µ x = 0.009] Σ = diag(0.006, 0.005)The mean and variance for the uni-variate rotational noise model are: µ = 0.008 and σ = 0.004.Rotational motion. Similarly, for rotational motion (i.e. turn-left, turn-right), the mean and co-variance for the translational bi-variate Gaussian noise model are: µ = [µ z = 0.003, µ x = 0.003] Σ = diag(0.002, 0.003)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of samples from the egomotion dataset collected in a noiseless actuation setting. We group the data points according to the actionmove-forward, turn-left and turn-right. For every data point, we show the RGB image frames, depth maps and the relative pose change between the source and target agent states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 Figure 5 :</head><label>55</label><figDesc>presents qualitative examples from the noisy actuation version of the egomotion dataset. Note that irrespective of the type of action -translational (move-forward) or rotational (turn-left Visualization of samples from the egomotion dataset collected in a noisy actuation setting. We group the data points according to the actionmove-forward, turn-left and turn-right. For every data point, we show the RGB image frames, depth maps and the relative pose change between the source and target agent states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>For</head><label></label><figDesc>the odometry model trained on noiseless agent actuations, the Smooth-L1 loss on the val-unseen and val-seen splits are 0.56e-4 and 0.46e-4, respectively. The val-unseen split provides a comparatively more challenging set-up than val-seen due to the added complexity of previously-unseen environments (in addition to evaluation of novel observation pairs) and this is reflected in the trends in the Smooth-L1 loss values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>We present the egomotion prediction errors, broken down as errors along x, y, z and yaw for the odometry model trained in (a) noiseless and (b) noisy actuation settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Leaderboard for the challenge phase of the PointNav track in the CVPR 2020 Habitat Challenge. The leaderboard is sorted by SPL on the left and SoftSPL on the right. to 300M frames of experience (across 128 GPUs) in noisy actuation settings. At 300M frames, our proposed agent ego-localization (SoftSPL=0.517, SPL=0.034, geo d T=2.535) is still able to outperform the no-localization (SoftSPL=0.411, SPL=0.005, geo d T=3.127) baseline. The "oracle" gt-localization navigator's performance at 300M is as follows: (SoftSPL=0.542, SPL=0.594, geo d T=0.831).</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Habitat: A Platform for Embodied AI Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On Evaluation of Embodied Navigation Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06757</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="https://aihabitat.org/challenge/2019/" />
		<title level="m">Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. CVPR 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Embodied question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2054" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural modular control for embodied question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11181</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Embodied question answering in photorealistic environments with point cloud perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6659" to="6668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cognitive mapping and planning for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2616" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Modular visual navigation using active neural mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08360</idno>
		<title level="m">Neural map: Structured memory for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Alwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08236</idno>
		<title level="m">Pyrobot: An open-source robotics framework for research and benchmarking</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning exploration policies for navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyMWn05F7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning your way without map or compass: Panoramic target driven visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Watkins-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Waytowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09295</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schissler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V A</forename><surname>Gari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ithapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11474</idno>
		<title level="m">Audio-visual embodied navigation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Planet-photo geolocation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="37" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2043" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06321</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Locobot: An open source low cost robot</title>
		<ptr target="https://locobot-website.netlify.com/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Benchmarking classic and learned navigation in complex 3D environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10915</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gibson env: Realworld perception for embodied agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/gibson_material/Agreement%20GDS%2006-04-18.pdf" />
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Gibson dataset license agreement</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Likhachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dˆ* Lite</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
