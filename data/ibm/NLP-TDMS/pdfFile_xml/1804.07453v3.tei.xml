<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Junliang</forename><forename type="middle">Xing</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jianru</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-View adaptation</term>
					<term>skeleton</term>
					<term>action recognition</term>
					<term>RNN</term>
					<term>CNN</term>
					<term>consistent !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based human action recognition has recently attracted increasing attention thanks to the accessibility and the popularity of 3D skeleton data. One of the key challenges in action recognition lies in the large variations of action representations when they are captured from different viewpoints. In order to alleviate the effects of view variations, this paper introduces a novel view adaptation scheme, which automatically determines the virtual observation viewpoints over the course of an action in a learning based data driven manner. Instead of re-positioning the skeletons using a fixed human-defined prior criterion, we design two view adaptive neural networks, i.e., VA-RNN and VA-CNN, which are respectively built based on the recurrent neural network (RNN) with the Long Short-term Memory (LSTM) and the convolutional neural network (CNN). For each network, a novel view adaptation module learns and determines the most suitable observation viewpoints, and transforms the skeletons to those viewpoints for the end-to-end recognition with a main classification network. Ablation studies find that the proposed view adaptive models are capable of transforming the skeletons of various views to much more consistent virtual viewpoints. Therefore, the models largely eliminate the influence of the viewpoints, enabling the networks to focus on the learning of action-specific features and thus resulting in superior performance. In addition, we design a two-stream scheme (referred to as VA-fusion) that fuses the scores of the two networks to provide the final prediction, obtaining enhanced performance. Moreover, random rotation of skeleton sequences is employed to improve the robustness of view adaptation models and alleviate overfitting during training. Extensive experimental evaluations on five challenging benchmarks demonstrate the effectiveness of the proposed view-adaptive networks and superior performance over state-of-the-art approaches. The source code is available at https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition. Fig. 1: Skeleton representations of the same posture captured from different viewpoints (different camera position and angle) are very different. distractions, and variations of viewpoints [2], [10], [33], [57]. Biological observations from the early seminal work of Johansson [18] suggest that humans are capable of recognizing actions from the motion of just a few joints of the human body, even without appearance information. The prevalence of cost-effective depth cameras such as Microsoft Kinect [59], Intel RealSense [1], dual camera devices, and the advance of powerful techniques for human pose estimation</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>H UMAN action recognition is an important research area in computer vision with extensive studies and rapid developments in the past decades. It has a wide range of applications, such as visual surveillance, human-computer interaction, video indexing/retrieval, human-computer interaction, game control, video summary and video understanding <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b50">[51]</ref> According to the types of input data, human action recognition can be categorized into RGBbased and 3D skeleton-based approaches. RGB-based human action recognition has been studied extensively. 3D skeleton-based human representation, where a human body is represented by the locations of human key joints in the 3D space, has recently attracted increasing attention. It has been demonstrated that RGB-based and skeleton-based approaches for human action recognition complement each other <ref type="bibr" target="#b33">[34]</ref>. As high level representations, skeletons have the merits of being robust to appearances, surrounding from depth <ref type="bibr" target="#b40">[41]</ref> make 3D skeleton data easy to obtain. Like many previous works discussed in the survey paper <ref type="bibr" target="#b9">[10]</ref>, we focus on skeleton-based action recognition in this work.</p><p>For human action recognition, one of the main challenges is the large diversity of viewpoints of the captured human action data. There are two major reasons for large view variations. First, in a practical scenario, the viewpoints of the cameras are flexible and different viewpoints result in large differences in skeleton representations even for the same scene. Second, the actor could conduct an action in different orientations. Moreover, he/she may dynamically change his/her orientations as time goes on. As illustrated in <ref type="figure">Fig. 1</ref>, the skeleton representations of the same posture are rather different when captured from different viewpoints. In practice, the variation of the observation viewpoints makes action recognition very challenging <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The viewpoints of the testing samples may have never been seen in those of the training samples and the recognition performance could be significantly degraded. Additionally, to handle diverse viewpoints, a larger model is usually needed in comparison with the case with consistent viewpoints. However, a larger model is prone to be overfitting and is more difficult to train. Some attempts have been made in previous works to overcome the view variation problem for robust action recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b59">[60]</ref>. However, most of these works are designed for RGB-based action recognition. They either require training data under many views or their designed view-invariant feature representations do not provide satisfactory performance due to the information loss. For skeleton-based human action recognition, the investigation of view invariance remains under-explored.</p><p>There are very few attempts in previous skeleton-based action recognition works which consider the effect of view variations. A pre-processing treatment is usually employed to make the skeleton data invariant to the absolute location and body orientation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b61">[62]</ref>. The original 3D coordinates are transformed to representations under a person-centric coordinate system by placing the body center at the origin and aligning the body plane of skeleton to be parallel to the (x, y)-plane. Such a pre-processing strategy can partially alleviate the view variation problem. However, it introduces some drawbacks. The human-defined strategy may not be flexible enough to handle various cases, considering that human body is not rigid. Besides, those processing methods are not explicitly designed with the target of optimizing action recognition in mind but are instead based on priori knowledge, which reduces the space for exploiting optimal viewpoints. How to design a system which learns optimized viewpoints for action recognition that alleviates the impact of viewpoint diversity without involving much human effort is still an under-explored problem, warranting more investigation.</p><p>In this work, we intend to address the view variation problem to achieve high performance for skeleton-based action recognition. Instead of pre-processing the 3D skeletons based on human defined criteria to reduce view variations, we propose a view adaptation scheme which automatically determines the observation viewpoint within the network for each sample. This enables the classification module to "see" the skeleton representation under the new viewpoint for efficient recognition. Note that the change of the viewpoint of the camera is equivalent to the transformation of the skeleton to a new coordinate system. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we design an end-to-end view adaptive neural network. It consists of a main classification network and a view adaptation subnetwork. The view adaptation subnetwork automatically determines the appropriate virtual viewpoints based on the input skeletons. The skeletons represented in the new observation viewpoints are then fed to the main classification network for easier action recognition. With the objective of maximizing the recognition performance, the entire network, including the view adaptation subnetwork and the main classification network, is trained from end to end to encourage the view adaptation subnetwork to learn and determine suitable virtual viewpoints. To demonstrate the effectiveness of our proposed view adaptation mechanism, we apply it to both the recurrent neural network (VA-RNN) and the convolutional neural network (VA-CNN), respectively. One can also fuse the classification scores of these two networks to provide the final prediction , which will be referred to as the two-stream scheme VA-fusion. Our main contributions are summarized below:</p><p>• We propose a self-regulated view adaptation scheme which re-positions the observation viewpoints adaptively to facilitate better action recognition from skeleton data. This emancipates the human energy spent on designing complex pre-processing criteria to handle various cases. • We design two view adaptive neural networks, VA-RNN and VA-CNN. For the VA-RNN, we integrate an RNN-based view adaptation module into an LSTM classification network for end-to-end learning. For the VA-CNN, we integrate a CNN-based view adaptation module into a CNN classification network for endto-end learning. In each stream, the view adaptation module automatically determines the "best" observation viewpoints during recognition. • We perform extensive ablation studies. The effectiveness of the view adaptation subnetworks is demonstrated in extensive experiments. The influence of parameters is analyzed. Moreover, we demonstrate the gain is brought by our view adaptation module rather than the simple increase of layers. We visualize the transformed skeletons from both VA-RNN and VA-CNN networks to better understand why our models work well. Failure cases are discussed. • To enhance the "power" of our view adaptation subnetworks, view enriching by data augmentation is performed by randomly rotating the skeletons during training. Experimental results demonstrate that data augmentation improves the robustness of our view adaptation models.</p><p>Based on the above model innovations and technical contributions, we have presented a very effective skeleton based action recognition system which automatically regulates the skeletons to more consistent viewpoints while maintaining the continuity of an action.</p><p>It should be mentioned that this paper is an extension of our previous conference paper <ref type="bibr" target="#b57">[58]</ref>. As an extension, we validate the effectiveness of the view adaptation schemes on both the recurrent neural network, and convolutional neural network. A two-stream scheme by fusing the scores of the two view adaptive networks provides much better performance. We introduce view enriching on the samples during training to further enhance the robustness of the view adaptation model to the view variations. We also conduct the extensive experimental analysis on the network designs in terms of the parameter determinations. Moreover, we conduct experiments on five challenging datasets and experimental results show that our proposed scheme consistently achieves significant gains on the various datasets, demonstrating the effectiveness of our proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">View Invariant RGB-based Action Recognition</head><p>In reality, human actions can be captured from arbitrary camera viewpoints. This makes it difficult to develop efficient action recognition techniques. For RGB-based action recognition, researchers have paid much attention to this issue and proposed various view-invariant approaches <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b59">[60]</ref>. One category of approaches requires as many views as possible for training to get a 'panorama' model <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b52">[53]</ref>. For example, the 3D histogram of Oriented Gradients based Bag of Words model <ref type="bibr" target="#b49">[50]</ref> is learned from data of all viewpoints to provide robustness to view changes. In practice, it is expensive to capture videos from abundant views. Another category of approaches designs view-invariant feature representations <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref> like self-similarity descriptors <ref type="bibr" target="#b18">[19]</ref> or descriptions based on trajectory curvature <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Usually, the descriptors, which are presented in another domain, lose some information of the original video sequences. There is also a category of approaches that employs knowledge transfer-based models <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. They find a view independent latent space in which features from different views are directly comparable. It requires significant human effort to find the view independent latent space in the design. Considering the different domains of color videos and skeleton sequences, the approaches designed for color videos cannot be directly extended to skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Viewpoints in Skeleton-based Action Recognition</head><p>For skeleton-based action recognition, the study of viewpoint influences is under-explored. To be view-invariant, the commonly used strategies are to perform pre-processing of skeletons <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Unfortunately, frame-level pre-processing, where each frame is transformed to the body center with the upperbody orientation aligned, usually results in the partial loss of relative motion information. For example, the action of walking becomes walking in the same place and the action of dancing with body rotating becomes dancing with body facing a fixed orientation. Sequence-level pre-processing performs the same transformation on all frames with the same parameters determined from the first frame, in which case the motion is invariant to the initial body position and orientation ,and the motion information is preserved. However, since the human body is non-rigid, the definition of the body plane by the joints of "hip", "shoulder", "neck" is not always suitable for the purpose of orientation alignment <ref type="bibr" target="#b45">[46]</ref>. For example, after the alignment of such a defined body plane, a person who is bending over will have his/her legs upward. Wang et al. <ref type="bibr" target="#b45">[46]</ref> use only the up-right pose frames in a sequence to determine the body plane by averaging the rotation transformation. However, a sequence may not contain an up-right pose.</p><p>Rather than struggling to define complex criteria to handle the non-rigid body, we leverage content-dependent view adaptation models to automatically learn and determine the suitable viewpoints, and transform the skeletons to representations under those views for each sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">RNN for Skeleton-based Action Recognition</head><p>In earlier works, hand-crafted features are used for action recognition from the skeleton <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b54">[55]</ref>. With the advance of deep learning, many recent works leverage Recurrent Neural Networks to recognize human actions from raw skeleton input, where the feature learning and temporal dynamic modeling are accomplished by the neural networks. Du et al. <ref type="bibr" target="#b5">[6]</ref> propose an end-to-end hierarchical RNN for action recognition. It splits the whole human body into five parts with each part fed into different subnetworks and the output of the subnetworks are fused hierarchically. In the part aware LSTM model <ref type="bibr" target="#b37">[38]</ref>, Amir et al. propose to split a cell in an LSTM into part-based cells, with each cell learning long-term context representations for each individual part rather than for the entire body. Zhu et al. <ref type="bibr" target="#b61">[62]</ref> propose to automatically explore the co-occurrence of discriminative skeleton joints in an LSTM network based on group sparse regularization. To explicitly explore both the spatial and temporal relationships among joints, the spatial-temporal LSTM network extends the LSTM architecture to two concurrent domains, i.e., the temporal domain and the spatial domain <ref type="bibr" target="#b26">[27]</ref>. A trust gate is introduced to reduce the influence of noisy joints. To further exploit the discriminative powers of different joints and frames, a spatial-temporal attention model <ref type="bibr" target="#b41">[42]</ref> further introduces the attention mechanism into the network to enable it to selectively focus on discriminative joints of the skeleton within one frame, and pay different levels of attention to the outputs at different time instances. Similarly, Liu et al. <ref type="bibr" target="#b28">[29]</ref> use both global contextual information and local information to selectively focus on informative joints. Li et al. <ref type="bibr" target="#b24">[25]</ref> combine tree-like hierarchy RNNs with action category hierarchy to distinguish easy-tell actions in the low levels of networks and hard-tell actions in the high levels of networks. Different from the above works, we enhance the recognition performance from a new perspective. We leverage RNNs to automatically determine the virtual observation viewpoints and thus the new skeleton representations for efficient action recognition. Note that most previous works take the center and orientation aligned skeletons as input to the RNNs, by using the human defined alignment criteria which are not flexible and not effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ConvNet for Skeleton-based Action Recognition</head><p>Considering the strong capability of convolutional neural networks for classification, several recent works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b48">[49]</ref> attempt to convert a skeleton sequence to 2D image(s) and then leverage convolutional neural networks for classification. Some works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref> assign the values of the x, y and z axes in 3D coordinates to three channels of an image, with the frame ids corresponding to different rows (or columns) and joint ids corresponding to different columns (or rows). Normalization of the coordinate values to the range of 0-255 is performed based on dataset statistics <ref type="bibr" target="#b3">[4]</ref> or sequence statistics <ref type="bibr" target="#b22">[23]</ref>. Instead of using the absolute coordinate values, He et al. <ref type="bibr" target="#b19">[20]</ref> use the relative positions between the joints and the reference joints (e.g. left shoulder, right shoulder, left hip and right hip) to generate multiple images. Some works <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b48">[49]</ref> use the 2D projection maps from the trajectories of joints to different orthogonal planes as the images. Liu et al. <ref type="bibr" target="#b29">[30]</ref> represent a 5D space (3D coordinates, time label, joint label) as a 2D coordinate space and a 3D color space. Thus 10 images obtained from different assignments of the 5D space are fed to 10 ConvNets for classification, respectively. The results from the 10 models are fused as the final prediction.</p><p>Most of the above works focus on how to map a skeleton sequence to image(s) but ignore the challenge caused by view variations of the skeleton data. In contrast, we leverage ConvNets to automatically determine the virtual observation viewpoints and thus the new skeleton maps for efficient CNN-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIEW ADAPTATION MODELING</head><p>The skeleton representations under different views are very different even for the same action. The intra-class differences caused by the view variations may be even larger than inter-class differences. The diversity of capturing viewpoints makes human action recognition challenging.</p><p>To eliminate the influences of viewpoints, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, we propose an end-to-end neural network architecture which automatically re-observes a skeleton sequence from new virtual viewpoints before action recognition. It consists of a main classification network and a view adaptation subnetwork. The view adaptation module automatically determines the virtual observation viewpoints and outputs a set of transform parameters T t for each time t (or T for a sequence). The input skeleton representation is transformed to representations under the new viewpoints for classification by the main classification network. The For the t th frame, the observation coordinate system is at a new position which is obtained from a translation of d t and a rotation of α t , β t , γ t radians anticlockwise around the X-axis, Y -axis, and Z-axis, respectively, corresponding to the global coordinate system. The skeleton can then be represented under this observation coordinate system O t . entire network is end-to-end trained to optimize the classification performance.</p><p>In the following subsection, we formulate the problem of observation viewpoint adaptation transformation, which is the role played by the view adaptation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Captured raw 3D skeletons are representations corresponding to the camera coordinate system (global coordinate system), with the coordinate origin located at the position of the camera sensor. To be insensitive to the initial position of an action and to facilitate our study, for a sequence, we translate the global coordinate system to the body center of the first frame as our new global coordinate system O. Note that the input skeleton V t for our system as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref> is the skeleton representation under the new global coordinate system.</p><p>In TV or movie shooting, one can choose to observe an action from suitable viewpoints over time to better understand the scene and better tell a story. Thanks to the availability of the 3D skeletons captured from a certain view, it is similarly possible to set up a movable virtual camera and observe the action from new observation viewpoints as illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. With the skeleton at frame t re-observed from the movable virtual camera viewpoint (observation viewpoint), the skeleton is transformed to a representation under the movable virtual camera coordinate system, which is also referred to as the observation coordinate system O t .</p><p>Given a skeleton sequence S under the global coordinate system O, the j th skeleton joint on the t th frame is denoted</p><formula xml:id="formula_0">as v t,j = [x t,j , y t,j , z t,j ] T , where t ∈ (1, · · · , T ),</formula><p>j ∈ (1, · · · , J), T denotes the total number of frames in a sequence, J denotes the total number of skeleton joints in a frame. The set of joints in the t th frame is denoted as</p><formula xml:id="formula_1">V t = {v t,1 , · · · , v t,J }.</formula><p>For the t th frame, we assume the movable virtual camera is placed at a suitable viewpoint, with the corresponding observation coordinate system obtained from a translation by d t ∈ R 3 , and a rotation of α t , β t , γ t radians anticlockwise around the X-axis, Y -axis, and Z-axis, respectively, of the global coordinate system. We denote the set of transformation parameters as</p><formula xml:id="formula_2">T t = {α t , β t , γ t , d t }. Therefore, the rep- resentation of the j th skeleton joint v t,j = [x t,j , y t,j , z t,j ] T of the t th frame under the observation coordinate system O t is v t,j = [x t,j , y t,j , z t,j ] T = R t (v t,j − d t ).<label>(1)</label></formula><p>R t is represented as</p><formula xml:id="formula_3">R t = R x t,α R y t,β R z t,γ ,<label>(2)</label></formula><p>where R x t,α , R y t,β , R z t,γ , denote the coordinate transformation matrixes for rotating the original coordinate system around the X-axis by α t radians, the Y -axis by β t radians, and the Z-axis by γ t radians anticlockwise, respectively, which are defined as</p><formula xml:id="formula_4">R x t,α =   1 0 0 0 cos(α t ) sin(α t ) 0 − sin(α t ) cos(α t )   ,<label>(3)</label></formula><formula xml:id="formula_5">R y t,β =   cos(β t ) sin(β t ) 0 − sin(β t ) cos(β t ) 0 0 0 1   ,<label>(4)</label></formula><formula xml:id="formula_6">R z t,γ =   cos(γ t ) 0 − sin(γ t ) 0 1 0 sin(γ t ) 0 cos(γ t )   .<label>(5)</label></formula><p>Note that all the skeleton joints in the t th frame share the same transformation parameters, i.e., α t , β t , γ t , d t . This is because changing viewpoints is a rigid motion. Given these transformation parameters, the skeleton representation V t = {v t,1 , · · · , v t,J } under the new observation coordinate can be obtained from <ref type="bibr" target="#b0">(1)</ref>. The viewpoints can vary over time for different frames. The key problem becomes how to determine the viewpoints of the movable virtual camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VIEW ADAPTIVE NEURAL NETWORKS</head><p>We design two view adaptive neural networks based on RNN and CNN, named VA-RNN and VA-CNN, respectively. As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, the VA-RNN (as shown on the top), consists of a RNN-based view adaptation subnetwork for transforming the skeletons to new representations under the suitable observation viewpoints, and a main LSTM network for recognizing actions from the transformed skeletons. The VA-CNN (as shown on the bottom) consists of a CNN-based view adaptation subnetwork, and a main convolutional network (ConvNet). Each network is trained end-to-end by optimizing the classification performance. Alternatively, we can fuse the scores from the two networks to provide a fused prediction, denoted as the VA-funsion scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">View Adaptive Recurrent Neural Network (VA-RNN)</head><p>As illustrated by the top branch in <ref type="figure" target="#fig_1">Fig. 3</ref> as marked by VA-RNN, we use a view adaptation subnetwork to automatically learn and determine the observation viewpoints, i.e., with transformation parameters of α t , β t , γ t , d (as discussed in section 3.1), and use a main LSTM network to learn the temporal dynamics and features from the view-adapted skeleton data for action recognition, from end to end. View Adaptation Subnetwork. The adaptation of observation viewpoint can be considered as the re-positioning of the movable virtual camera, which is characterized by the translation and rotation of this virtual camera (observation coordination system). At a time slot corresponding to the t th frame, with a skeleton V t as input, two branches of LSTM subnetworks are utilized to learn the rotation parameters α t , β t , γ t to obtain the rotation matrix R t , and the translation vector d t .</p><p>The branch of rotation subnetwork for learning rotation parameters consists of an LSTM layer, and a fully connected (FC) layer. The parameters related to rotation are obtained with</p><formula xml:id="formula_7">[α t , β t , γ t ] T = W r h r t + b r ,<label>(6)</label></formula><p>where h r t ∈ R N ×1 is the vector of the hidden output of the LSTM layer with N denoting the number of LSTM neurons, W r ∈ R 3×N and b r ∈ R 3×1 denote the weight matrix and offset vector of the FC layer, respectively. With the parameters of rotation learned, the rotation matrix R t is obtained using <ref type="bibr" target="#b1">(2)</ref>.</p><p>The branch of translation subnetwork for learning translation parameters consists of an LSTM layer, and a FC layer. The translation vector d t is calculated as</p><formula xml:id="formula_8">d t = W d h d t + b d ,<label>(7)</label></formula><p>where h d t ∈ R N ×1 is the hidden output vector of its LSTM layer, W d ∈ R 3×N and b d ∈ R 3×1 denote the weight matrix and offset vector of the FC layer.</p><p>Under the observation viewpoint of the t th frame, the representation of the skeleton V t is then obtained through <ref type="bibr" target="#b0">(1)</ref>.</p><p>Main LSTM Network. The LSTM network is capable of modeling long-term temporal dynamics and automatically learning feature representations. Similar to the designs in <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b61">[62]</ref>, we build a main LSTM network by stacking three LSTM layers, followed by one FC layer with a SoftMax classifier. The number of neurons of the FC layer is equal to the number of action classes.</p><p>End-to-End Training. The entire network is end-to-end trainable. We use cross-entropy loss as the training loss <ref type="bibr" target="#b41">[42]</ref>. The gradients of loss flow back not only within each subnetwork, but also from the main LSTM network to the view adaptation subnetwork. Let us denote the loss backpropagated to the output of the view adaptation subnetwork as v t,j ∈ R 1×3 , where j ∈ (1, · · · , J) and J is the number of joints in a frame. The loss back-propagated to the output of the branch for determining the translation vector</p><formula xml:id="formula_9">of d t is dt = −J v t,j R t ,<label>(8)</label></formula><p>Similarly, the loss back-propagated to the output of the branch for determining the rotation parameters can be obtained. For example, the loss back-propagated to the output of α t is</p><formula xml:id="formula_10">αt = v t,j ∂R t ∂α t j=J j=1 (v t,j − d t ).<label>(9)</label></formula><p>With the end-to-end training feasible, the view adaptation model is guided to select the suitable observation viewpoints for enhancing the recognition accuracy.</p><p>Our proposed system automatically chooses the suitable observation viewpoints based on the contents, rather than using human predefined criteria, and the view adaptation model is optimized for high accuracy recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">View Adaptive Convolution Neural Network (VA-CNN)</head><p>As illustrated by the bottom branch in <ref type="figure" target="#fig_1">Fig. 3</ref> as marked by VA-CNN, a skeleton sequence is mapped to an image map, referred to as skeleton map, to facilitate the spatiotemporal dyanmics modeling by ConvNet. We use a view adaptation subnetwork constructed by convolution layers and a fully connected layer to learn and determine the sequence-level observation viewpoint, i.e., with transform parameters of α, β, γ, d (as discussed in section 3.1 with subscript removed). A main ConvNet follows to explore the spatial and temporal relations and performs the feature extraction from the view-adapted skeleton map for the action recognition, from end to end.</p><p>Mapping Skeletons to Image. Similar to that in <ref type="bibr" target="#b3">[4]</ref>, we transform a skeleton sequence into an image, with columns representing different frames while rows representing different joints. The 3D coordinate values for X, Y , and Z are treated as the three channels of an image.</p><p>Considering the differences of values in 3D skeleton and image, similar to <ref type="bibr" target="#b3">[4]</ref>, we normalize the pixel values to be within 0-255 by</p><formula xml:id="formula_11">u t,j = f loor(255 × v t,j − c min c max − c min ),<label>(10)</label></formula><p>where v t,j denotes the 3D coordinates of the j th joint of the t th frame in a skeleton sequence, u t,j denotes the corresponding pixel value of the normalized image map, c max and c min are the maximum and minimum of all the joint coordinates in the training dataset respectively, c min = [c min , c min , c min ] T , f loor is the greatest integer function. View Adaptation Subnetwork. Observed from a new observation viewpoint, the skeleton representation of the j th joint in the t th frame v t,j is transformed to v t,j according to <ref type="bibr" target="#b0">(1)</ref>. Correspondingly, the pixel value of the skeleton map under the new observation viewpoint is approximated as</p><formula xml:id="formula_12">u t,j = 255 × v t,j − c min c max − c min (11) = R t,j u t,j + 255 × R t,j (c min −d t,j )−c min c max − c min .<label>(12)</label></formula><p>Note that <ref type="formula" target="#formula_2">(12)</ref> is derived from <ref type="formula" target="#formula_2">(1)</ref> and <ref type="formula" target="#formula_2">(10)</ref>. The CNN-based view adaptation network is designed to learn and determine the observation viewpoint of each skeleton sequence and performs the transform on the skeleton map. We build the view adaptation subnetwork by stacking some convolutional layers and a fully connected layer to regress the transformation parameters, i.e., α, β, γ for R t,j , and d t . Based on these parameters and (12), a transform layer transforms each pixel in the skeleton map to a new representation in the observation viewpoint. Thus, a new skeleton map corresponding to new observation viewpoint is obtained.</p><p>Note that we have tried regressing frame level parameters T t = {α t , β t , γ t , d t }, which corresponds to 6 × T parameters for a skeleton map with a width of T -pixels, and regressing sequence level parameters T = {α, β, γ, d}, which corresponds to 6 parameters for a skeleton map. Even though the frame level parameters are more flexible and powerful in theory, we find in practice, the design with sequence level parameters provide superior performance for the ConvNets. The reason might be that fewer parameters are easy to learn.</p><p>Main ConvNet. With the transformed skeleton map as input, we can use an existing ConvNet, e.g., ResNet <ref type="bibr" target="#b10">[11]</ref> and AlexNet <ref type="bibr" target="#b21">[22]</ref>, for classification.</p><p>End-to-End Training. As shown by the bottom branch in <ref type="figure" target="#fig_1">Fig. 3</ref>, similarly, the view adaptation subnetwork, and the main ConvNet is end-to-end trainable, with the view determination optimized by minimizing the classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Two Stream Fusion (VA-fusion)</head><p>Similar to the fusion strategy in <ref type="bibr" target="#b47">[48]</ref>, we can take a weighted fusion method to combine the scores of the VA-RNN stream and VA-CNN stream to obtain the final score. Considering the performance gap between the two streams, we give more credit to the VA-CNN stream by setting its weight to 4 and that of the VA-RNN stream to 1, which is experimentally determined. We have tried learning the weights but it does not outperform the simple fusion approach.</p><p>Note that we do not investigate more complicated fusion scheme since that is not the focus of this work. We try to allow each stream to be as independent as possible, which can facilitate the design choices in practice. The users can choose the scheme which better meets the requirements, such as speed, hardware, and storage among VA-RNN, VA-CNN, and VA-fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Implementation and Training</head><p>Model Architecture. For VA-RNN, we build our model using recurrent neural networks with LSTM. We use 100 neurons for each LSTM layer. For the main LSTM network, we stack three LSTM layers. For the view adaptation subnetworks, we only use one LSTM layer followed by one fully connected layer to learn the transformation parameters.</p><p>For VA-CNN, we build our model using convolutional neural networks. For our main ConvNet, similar to <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we use ResNet-50 with pretrained parameters from Ima-geNet for classification. The view adaptation subnetwork is constructed by stacking two convolutional layers and one fully connected layer. Note that the two convolutional layers are both followed by a batch normalization layer and a Relu activation layer. A max-pooling layer is used to reduce the resolution after the second convolutional layer and finally a fully connected (FC) layer is used to regress the parameters related to view transformation. We set the number of kernels to 128 for all these convolution layers. For each convolutional layer, we set the kernel size to 5 and stride to 2.</p><p>View Enriching by Data Argumentation. All the data have a limited range of capturing viewpoints. This is common especially in practical application scenarios. To enhance the "power" of our view adaptation model, at the sequence level, we perform view enriching by rotating the skeleton around the X, Y , and Z axes by some degrees during training procedure. This is expected to alleviate the overfitting problem especially on small datasets, and reinforce the view adaptation model.</p><p>Model Training. We train each stream of view adaptive neural network by minimizing the cross-entropy loss endto-end. Similar to that in <ref type="bibr" target="#b47">[48]</ref>, we fuse the two streams by weighted averaging the scores and obtain the classification probability after the SoftMax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>We evaluate the effectiveness of our proposed view adaptation frameworks on five benchmark datasets, including the NTU RGB+D dataset <ref type="bibr" target="#b37">[38]</ref>, the SYSU Human-Object Interaction dataset <ref type="bibr" target="#b11">[12]</ref>, the UWA3D dataset <ref type="bibr" target="#b34">[35]</ref>, the Northwestern-UCLA dataset <ref type="bibr" target="#b46">[47]</ref>, and the SBU Kinect Interaction dataset <ref type="bibr" target="#b55">[56]</ref>.</p><p>In the following, we first describe the datasets and our experiment settings in Section 5.1. For the ablation studies in Section 5.2, the effectiveness of the proposed view adaptation model is analyzed and demonstrated. We also make comparison with other view-invariant strategies. The influence of parameters are analyzed. To further understand the view adaptation model, we perform analysis through visualization and some failure cases are discussed. Section 5.3 presents the performance comparisons with the stateof-the-art approaches on the five datasets respectively. The results demonstrate that our scheme consistently achieves the best performance on all datasets. Some comparative analyses of VA-RNN and VA-CNN are presented in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Experimental Settings</head><p>Datasets. NTU RGB+D Dataset (NTU) <ref type="bibr" target="#b37">[38]</ref>. This Kinect captured dataset is currently the largest dataset with RGB+D videos and skeleton data for human action recognition, with 56880 video samples. It contains 60 different action classes including daily, mutual, and health-related actions. Each subject has 25 joints. The various setups of cameras, capturing views, and different facing orientations of the subjects, result in a great diversity of sample viewpoints. There are two standard evaluations, i.e., Cross-Subject (CS), where the 40 subjects are split into the training and testing groups, and Cross-View (CV), where the samples of cameras 2 and 3 are used for training and those of camera 1 for testing. It is a challenging dataset for action recognition because of the large amount of videos, various subjects, and the difference of camera views.</p><p>SYSU 3D Human-Object Interaction Set (SYSU) <ref type="bibr" target="#b11">[12]</ref>. This Kinect captured dataset contains 12 actions performed by 40 subjects. It has 480 sequences in total. Each subject has 20 joints. This dataset is challenging for high similarity among activities.</p><p>UWA3D Multiview Activity II Dataset (UWA3D) <ref type="bibr" target="#b34">[35]</ref>. This Kinect captured dataset contains 30 actions performed by 10 different subjects. The videos are captured from 4 different views: front view (V1), left side view (V2), right side view (V3), and top view (V4). It has 1075 sequences in total. This dataset is challenging because of the diversity of viewpoints, self-occlusion, and high similarity among activities.</p><p>Northwestern-UCLA dataset (N-UCLA) <ref type="bibr" target="#b46">[47]</ref>. This Kinect captured dataset contains 1494 videos of 10 actions. These actions are performed by 10 subjects, repeated 1 to 6 times. There are three views. Each subject has 20 joints.</p><p>SBU Kinect Interaction Dataset (SBU) <ref type="bibr" target="#b55">[56]</ref>. This Kinect captured dataset is an interaction dataset with each action performed by two subjects. It contains 282 sequences of 8 classes. Each subject has 15 joints.</p><p>Note that both the SYSU and SBU dataset are captured by a single camera with one primary viewpoint. However, different subjects may perform actions at different locations with different distances to the camera and orientations.</p><p>Experimental Settings. For VA-RNN, We set the batch size to 256 for the NTU dataset and 32 for other datasets in considering the small sizes of the other datasets. For the view adaptation subnetwork, we initialize the fully connected layer parameters to zeros for efficient training. Dropout <ref type="bibr" target="#b42">[43]</ref> with a probability of 0.5 is used to alleviate overfitting. Gradient clipping similar to <ref type="bibr" target="#b43">[44]</ref> is used by enforcing a hard constraint on the norm of the gradient (not exceeding 1) to avoid the gradient explosion problem. Adam <ref type="bibr" target="#b20">[21]</ref> is adapted to train all networks, and the initial learning rate is set to 0.005 for all datasets.</p><p>For VA-CNN, we set the batch size to 32. For the view adaptation subnetwork, we initialize the fully connected layer parameters to zeros for efficient training. Adam <ref type="bibr" target="#b20">[21]</ref> is adapted to train all networks, and the initial learning rate is set to 0.0001 for all datasets. Skeleton maps are resized to 224×224.</p><p>In the UWA3D and N-UCLA datasets, at the sequence level, we rotate the skeleton around the X, Y , and Z axes by some degrees which are generated randomly from -90 to 90 during training procedure. Considering that the ranges of view variation on other datasets are smaller, at the sequence level, we rotate the skeleton around the X, Y , and Z axes by some degrees which are generated randomly from -17 to 17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Comparison with Other Pre-processing Strategies</head><p>Some approaches use human defined criteria to pre-process the skeletons to reduce the challenges caused by view variations <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b61">[62]</ref>. We make comparison on the effectiveness of those strategies and our view adaptation model. Considering that the NTU RGB+D dataset is currently the largest dataset and is representative, we perform our in-depth analyses on this dataset under the framework of recurrent neural networks and show the results in <ref type="table" target="#tab_0">Table 1</ref>. VA-RNN is our proposed view adaptation scheme under the RNN framework, which automatically changes the observation viewpoints in the network. In comparison, S-trans+RNN is our baseline scheme without enabling the view adaptation model, i.e., the switch s rota and s trans are both off, i.e. V t = V t . Note that the input V t is the same as that of our view adaptation schemes, where the global coordinate system is moved to the body center of the first frame for the entire sequence to be insensitive to the initial position (see section 3.1). We refer to this pre-processing as sequence level translation, i.e., S-trans.</p><p>From <ref type="table" target="#tab_0">Table 1</ref>, we observe that the proposed view adaptation scheme outperforms the S-trans+RNN by 3.4% and 5.3% in accuracy for the CS and CV settings, respectively. We notice that rotation-only adaptation VA-rota+RNN seems to be more effective than translation-only adaptation VA-trans+RNN. That is because most actions in this dataset are performed without shifting positions during the occurrence.</p><p>One may wonder about the performance when using the pre-processed skeletons, based on the widely used human defined processing criteria, before inputing them to the main RNN Network. Such pre-processings follow human defined rules to determine the viewpoints. We denote the pre-processing based schemes as C+RNN, where C indicates the pre-processing strategy, e.g., F-trans+RNN. The 3 rd to 9 th rows show the performance of schemes using different pre-processing strategies. F-trans means performing frame level translation to have the body center at the coordinate system origin for each frame. S-rota is the sequence level rotation with the rotation parameters calculated from the first frame, which is to fix the X-axis to be parallel to the vector from "left shoulder" to "right shoulder", Y -axis to be parallel to the vector from "spline base" to "spine", and Z-axis corresponding to the new X × Y . Similarly, F-rota is the frame wise rotation. For S/F-rota(w.r.t. shoulder), only the rotation pre-processing to make the X-axis to be parallel to the vector from "left shoulder" to "right shoulder" is performed at sequence/frame level. F-trans&amp;F-rota means both F-trans and F-rota are performed, which is similar to the pre-processing in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>. The scheme Raw+RNN in the 2 nd row denotes a scheme which uses the original skeleton without any pre-processing as the input to the Main RNN Network. Note that for 3D skeletons, the distance of a subject to the camera does not influence the scale of the skeletons. Therefore, the scaling operation is not considered in our framework. From <ref type="table" target="#tab_0">Table 1</ref>, we have the following observations and conclusions. (1) Our final scheme significantly outperforms the commonly used pre-processing strategies. In comparison with F-trans&amp;F-rota+RNN <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, our scheme achieves an improvement of 5.3% and 3.7% in accuracy for the CS and CV settings, respectively. In comparison with S-trans&amp;S-rota+RNN, our scheme achieves an improvement of 3.0% and 2.2% in accuracy. (2) Frame wise pre-processing is inferior to the sequence level pre-processing, because the former loses more information, e.g., the motion across frames. (3) Being insensitive to the initial position of an action, S-trans+RNN significantly outperforms Raw+RNN, the scheme with raw skeletons as input. (4) Some humandefined pre-processings, such as S-trans&amp;S-rota, S-trans&amp;Frota, S-trans&amp;S-rota(w.r.t. shoulder) and S-trans&amp;F-rota(w.r.t. shoulder) achieve superior results to those without rotation processing on the CV setting. Because such pre-processings can reduce the diversity of viewpoints and alleviate the viewpoint mismatch problem between the training and testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Influence of Data Augmentation</head><p>Data augmentation by increasing the viewpoint diversity during the training procedure can alleviate the viewpoint mismatch problem between training and testing sets. It strengthens the capability of both baseline scheme S-trans and our VA scheme. The VA module benefits from data augmentation by "seeing" more views during training to learn how to make transformation for various views. We will show the effectiveness of data augmentation on both baseline scheme S-trans and the proposed view adaptation model VA in <ref type="table" target="#tab_1">Table 2</ref>. S-trans+RNN/CNN and VA-RNN/CNN denote the baseline schemes and the proposed view adaptation schemes without data augmentation, respectively. S-trans+RNN/CNN(aug.) and VA+RNN/CNN(aug.) denote the schemes with data augmentation(aug.).</p><p>We can see that, for the CNN-based networks, data augmentation improves the performance by 0.4% and 1.3% on the CS and CV settings of NTU dataset, respectively. For the CV setting, the viewpoints of the testing data are different from those of the training data. Thus, data augmentation by increasing the viewpoints can bring larger gain on the CV setting, which makes some unseen testing views seen during the training process. For the UWA3D and N-UCLA datasets which are under CV setting, the view differences are large where there is even a top view while the other views are captured by cameras located at nearly horizontal positions. Data augmentation enables the training process to "see" the testing views and thus it even brings gains of 9.8-11.8%. Data augmentation mainly addresses the mismatch between training viewpoints and testing viewpoints by increasing the diversity of training viewpoints.</p><p>In addition, with the help of data augmentation, VA-RNN(aug.) and VA-CNN(aug.) improve the performance significantly in comparison with VA-RNN and VA-CNN, especially for the UWA3D and N-UCLA datasets. For the UWA3D and N-UCLA datasets, data augmentation brings gains of 8.1% and 4.9% on VA-CNN. The main reason is that the VA-RNN and VA-CNN models are not able to transform the skeleton sequence of testing set to good learned view when the view of training and testing sets mismatch with each other significantly. However, thanks to the data augmentation, the VA-RNN(aug.) and VA-CNN(aug.) models could "see" plenty of views during training and be capable of transforming skeletons of both training and testing sets to suitable learned view.</p><p>Data augmentation is an efficient and necessary technique for both the baseline and the proposed view adaptation schemes. Hereafter, our experiments are all conducted with data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Effectiveness of the View Adaptation Model</head><p>We will compare our proposed VA model with the two powerful baselines in <ref type="table">Table 3</ref>. S-trans&amp;S-rota+RNN and S-trans&amp;S-rota+CNN are the baseline schemes with both human defined translation and rotation pre-processing. Note that for S-trans&amp;S-rota, data augmentation by rotating the skeleton sequence to increase the view diversity should not be performed since the purpose of the rotation preprocessing is to align the viewpoints. S-trans+RNN(aug.) and S-trans+CNN(aug.) are another type of baseline schemes with only translation pre-processing, and data augmentation is performed.</p><p>View consistence by view adaptation versus preprocessing. From <ref type="table">Table 3</ref>, we find that, in comparison with the human defined rotation pre-processing, the view adaptation model consistently achieves superior performance. The human defined pre-processing strategy is not optimized for recognition performance. Human body is non-rigid and the definition of rotation criteria is not always suitable for the purpose of orientation alignment. Our scheme leverages the network to automatically determine the suitable viewpoints, trained by optimizing the classification accuracy.</p><p>View adaptation versus data augmentation. From Table 3, VA-RNN(aug.) and VA-CNN(aug.) outperform S-trans+RNN(aug.) and S-trans+CNN(aug.) for all datasets. 3: Accuracy (%) comparisons of two types of powerful baseline schemes, i.e., the schemes with sequence translation pre-processing strategy (S-trans+RNN(aug.) and S-trans+CNN(aug.)), the schemes with sequence translation and rotation pre-processing strategy (S-trans&amp;S-rota+RNN and S-trans&amp;S-rota+CNN), and our schemes with view adaptation. Note for the two types of baseline schemes, we mark the better one by underline.  Viewpoint is a distractor rather than a discriminative characteristic for action recognition. Therefore, for a network, when there is no mismatch between training and testing viewpoints (e.g., after data augmentation), it should be more challenging to deal with diverse viewpoints than only dealing with consistent viewpoints. Our view adaptation scheme VA-RNN intends to transform the diverse viewpoint to a consistent viewpoint to alleviate the difficulty. The learned consistent viewpoint is beneficial to learning discriminative features.</p><p>In conclusion, our proposed VA-RNN(aug.) and VA-CNN(aug.) consistently achieve the best performance in comparison with two powerful baseline schemes for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Influence of Network Parameters</head><p>VA module is the subnetwork of the recognition network. Therefore, VA-RNN and VA-CNN have more parameters than the corresponding base networks. One may wonder whether the gains are brought by the increased number of parameters or the proposed view adaptation modules. There are two ways to increase the model size of a network: (1) Stacking more RNN or CNN layers; (2) Increasing the number of LSTM neurons for RNN-based network or convolutional kernels for CNN-based network. Note that we use the ResNet as our CNN-based backbone network with pre-trained parameters from ImageNet. Therefore, we have not changed the number of convolutional kernels. Instead, we show the results when changing the number of LSTM neurons of each RNN layer for RNN-based networks. We will discuss these two ways as follows. Stacking more layers. <ref type="table" target="#tab_4">Table 4</ref> shows comparisons between our proposed view adaptation models and the corresponding main classification networks with different number of layers. Each LSTM layers contains 100 neurons. For the RNN-based networks, as the increase of the number of LSTM layers, the performance initially increases but drops after 3 layers. Stacking LSTM layers simply would not achieve better performance significantly. However, our proposed VA-RNN(aug.), which includes 5 LSTM layers (3 LSTM layers for the main network and 2 LSTM layers for the VA subnetwork), outperforms the baseline scheme with 5 or 6 LSTM layers by around 3.5% and 4.5% for the CS and CV settings, respectively. For the CNN-based networks, we use ResNet of different layers as our backbone networks and find that a deeper network does not bring obvious gain. In comparison, our scheme with 53 layers outperforms the baseline scheme with 152 layers by 0.5% and 0.9% for the CS and CV settings, respectively. Increasing the number of LSTM neurons. <ref type="figure" target="#fig_3">Fig. 5</ref> shows comparisons between our proposed view adaptation models and the corresponding main classification networks for RNN-based networks with different number of LSTM neurons. The baseline models contains 3 LSTM layers and the VA model contains 5 LSTM layers. We set the number of neurons of the three models, which include VA-RNN(aug.), S-trans+RNN(aug.) and S-trans&amp;S-rota+RNN, to 50, 100, 200 and 300, respectively. For each model, the number of parameters and the performance increase when we use more neurons. VA-RNN(aug.) always outperforms S-trans+RNN(aug.) and S-trans&amp;S-rota+RNN with similar or fewer parameters for both the CS and CV settings. Note that recognition accuracy increases since more neurons have a higher capability of modeling the evolution of action dynamics for all three schemes, we use 100 neurons for each LSTM layer by default to balance performance, speed and complexity.</p><p>In conclusion, increasing parameters simply by stacking more layers or using more number of neurons is not as efficient as our proposed view adaptation module. With similar amount of parameters, our models outperform the baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Visualization and Analysis from the Learned Views</head><p>The view adaptation subnetworks determine the observation viewpoints (by re-positioning the virtual movable camera) and then transform the input skeleton V t to the rep-resentation V t under the new viewpoint for optimizing the recognition performance. We visualize the representations V t and V t for better understanding our models. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the skeletons from different sequences captured from different viewpoints of (a) the similar posture or (b) the same action. The 2 nd row shows the original skeletons of diverse viewpoints. The 3 rd row shows the transformed skeletons from our VA-RNN model. The transformed skeletons have much more consistent viewpoints even for different subjects and actions. The 4 th row shows the transformed skeletons from our VA-CNN model. Abundant observations on other sequences show that both the VA-RNN and VA-CNN models are capable of transforming skeletons to much more consistent viewpoints. Note that the consistency of viewpoints after our model is the key factor for the success of our scheme. In addition, we have observed the transformed skeletons on many sequences and found the continuity of an action can be maintained by our models.</p><p>The view adaptation model transforms the viewpoint of skeleton sequence based on its contents. One may wonder how many frames the system needs to learn well of the transformation parameters VA-RNN. We have looked into abundant transformed sequences and transformation parameters and have the following observations. First, the network starts to modify the skeleton immediately when receiving the first skeleton frame. However, the learned views of the first several frames are not very good. For the first several frames, the LSTM network has not "seen" enough information to have a good guess of the views. Second, it takes around 5 to 20 frames to transform the Gain(%) <ref type="figure">Fig. 7</ref>: Performance gain in terms of accuracy (%) of VA-RNN model with respect to the S-trans+RNN on the NTU dataset for the CV setting. The index of the horizontal axis denotes the Id of action as provided in <ref type="bibr" target="#b37">[38]</ref>. For example, "23" denotes the action hand waving.</p><p>skeleton to a relative stable view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">Failure Case Discussion</head><p>One may wonder about the failure cases of the VA-RNN or VA-CNN models in comparison with the baseline schemes S-trans+RNN or S-trans+CNN. We have made abundant observations and found even for the misclassified samples, the skeletons are still transformed to consistent views by our proposed models. <ref type="figure">Fig. 7</ref> shows the histogram of the performance gains of our VA-RNN model with respect to the baseline model S-trans+RNN on the NTU dataset for the CV setting. The gain values are obtained by subtracting the accuracy of the S-trans-RNN from that of the VA-RNN on each action classes. We can see that, for most classes, our scheme outperforms the baseline scheme. For the classes on which performance drops, we do not find special reasons and this can be considered as noise. As we know, even for a same network, there is performance fluctuation when different initialization seeds are utilized. Similar phenomenon can be observed for VA-CNN scheme and we do not show to save space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons to Other State-of-the-Art Approaches</head><p>In this section, we show the performance comparisons between our proposed two stream view adaptation scheme VA-fusion(aug.) and other state-of-the-art approaches on these datasets. The performance of VA-RNN(aug.) and VA-CNN(aug.) are also presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">NTU Dataset</head><p>We follow the standard CS and CV protocols proposed by <ref type="bibr" target="#b37">[38]</ref> to evaluate the performance. We compare our method with the deep learning based approaches that also leverage RNN or CNN with skeleton data <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, and some traditional approaches using handcrafted features <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b44">[45]</ref>. The results are shown in <ref type="table" target="#tab_5">Table  5</ref>. This dataset contains as many as 80 viewpoints which will bring difficulty to action recognition. Thanks to the view adaptation models which make the views consistent, VA-RNN(aug.) and VA-CNN(aug.) outperform the baseline  schemes significantly and beat other RNN-based and CNNbased approaches which use human-define normalization methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b37">[38]</ref> or use some advanced techniques <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b41">[42]</ref>, such as attention <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Our proposed method, VA-fusion(aug.), outperforms the best stateof-the-art results by 9.4% and 7.8% on CS and CV settings, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">SYSU Dataset</head><p>We follow the standard protocols proposed by <ref type="bibr" target="#b11">[12]</ref> to evaluate the performance. For setting 1, half of the subjects are used for training and the others for testing. For setting 2, half of the videos of each subject are used for training and the others for testing. For each setting, the averaged results from 30-fold cross validation are shown in <ref type="table" target="#tab_6">Table 6</ref>. With view adaptation models reducing the view variations, our approach achieves the best performance, which is 11.2% and 9.3% higher than that of <ref type="bibr" target="#b11">[12]</ref> for setting-1 and setting-2, respectively, and 10.2% higher than that of <ref type="bibr" target="#b27">[28]</ref> for setting-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">UWA3D Dataset</head><p>We follow the standard protocol proposed by <ref type="bibr" target="#b34">[35]</ref> to evaluate the performance. There are 4 views. The dataset is partitioned in different manners to have 12 kinds of partitions. Each partition has 3 viewpoints, where two are used for training and the other one is used for testing.     <ref type="bibr" target="#b55">[56]</ref> 80.3 Raw skeleton <ref type="bibr" target="#b15">[16]</ref> 79.4 Joint feature <ref type="bibr" target="#b15">[16]</ref> 86.9 HBRNN-L <ref type="bibr" target="#b5">[6]</ref> 80.4 Co-occurrence RNN <ref type="bibr" target="#b61">[62]</ref> 90.4 STA-LSTM <ref type="bibr" target="#b41">[42]</ref> 91.5 ST-LSTM + Trust Gate <ref type="bibr" target="#b26">[27]</ref> 93.3 GCA-LSTM <ref type="bibr" target="#b28">[29]</ref> 94.1 Clips+CNN+MTLN <ref type="bibr" target="#b19">[20]</ref> 93.6 VA-RNN(aug.) 97.5 VA-CNN(aug.) 95.7 VA-fusion(aug.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>98.3</head><p>testing <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b46">[47]</ref>. In <ref type="bibr" target="#b46">[47]</ref>, they only use samples of the first two views as training and the other one as testing. In <ref type="bibr" target="#b4">[5]</ref>, they choose every two views as training with 3 cases in total. <ref type="table" target="#tab_9">Table 8</ref> shows the comparisons of the performance. V1 denotes the partition that samples from views 2 and 3 are taken as the training samples and samples from view 1 as the testing samples. Similarly, V2 denotes samples of view 2 are taken as testing samples. With view adaptation modules, our scheme VA-fusion(aug.) achieves the best performance of 95.3% for the V3 setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">SBU Dataset</head><p>We follow the standard protocol proposed by <ref type="bibr" target="#b55">[56]</ref> which uses cross validation with 5 folders. <ref type="table" target="#tab_10">Table 9</ref> shows the performance comparisons. Our method outperforms other approaches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b61">[62]</ref> significantly, achieving 98.3% in accuracy. Even though there is no large view change in this dataset, our model tends to catch the slight view differences and transforms the views to suitable ones for more efficient action recognition. For this small dataset (only 282 sequences), VA-RNN performs better than VA-CNN. That is because number of parameters of VA-CNN is much larger than that of VA-RNN, and it is easy for ConvNet to be overfitting for small training dataset.</p><p>Optimized with the target of maximizing the recognition performance, the proposed view adaptation model is very effective in choosing the suitable viewpoints. The consistency of viewpoints for various actions/subjects overcomes the challenge caused by the diversity of viewpoints in video capturing, enabling the network to focus on the learning of action-specific features. Unlike many pre-processing strategies, valuable motion information is preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparative Analysis of VA-RNN and VA-CNN</head><p>Thanks to the introduction of the view adaptation modulesxx, both VA-RNN and VA-CNN achieve improvement in comparison with their baselines, as shown in <ref type="table">Table 3</ref>.</p><p>VA-CNN(aug.) is much more powerful than VA-RNN(aug.) in general as shown in <ref type="table">Table 3</ref>. The main reason is that we transform the entire skeleton sequence to an image and deep CNN network (such as ResNet50) is able to explore the spatial and temporal relationship of the joints locally and globally. But RNN has limited memory of the history information.</p><p>The gain of the view adaptation module over deep CNN networks seems smaller than that of RNN networks. We conduct experiments on several backbone CNNs with different model sizes to analyze the effectiveness of view adaptation model. We take CNN-6layers, which consists of five convolutional layers and one fc layer for classification, ResNet10, ResNet18, and ResNet50 as our backbone networks and show the results in <ref type="table" target="#tab_0">Table 10</ref>. We have two conclusions.</p><p>(1) Our model achieves large gains when the CNNs are small. For the CNN-6layers network, our view adaptation model achieves 1.4% and 3.3% gains on the CS and CV settings of the NTU dataset, which are comparable with the gains of RNN-based network as shown in <ref type="table">Table 3</ref>. More results on all datasets are shown in <ref type="table" target="#tab_0">Table 11</ref>. We can find that our view adaptation models achieve significant gains for all datasets compared to CNN baselines when the networks are small. (2) Our view adaptation module achieves sizable gains when the backbone CNNs are large. As the model size or complexity increases, it becomes harder to get the same gain. From <ref type="table" target="#tab_0">Table 10</ref>, we see that the larger the model size of baseline (i.e. the number of the parameters from the model), the smaller gain achieved by increasing the model size by the same amount. However, with negligible increase in model size of the view adaptation module, the performance improves much more compared to increasing the depth of network.</p><p>In addition, we also shows the comparisons of our final VA-RNN(aug.) and VA-CNN(aug.) models in terms of the number of parameters of the models, testing speed when the bach size is set to 1 (number of sequences per second), and the accuracy (%) in <ref type="table" target="#tab_0">Table 12</ref>. Note that the performance of 10: Effectiveness (in accuracy(%)) of the view adaptation design on different backbone CNN networks. Note that S-trans+CNN(aug.) denotes the baseline scheme where sequence level translation pre-processing and data augmentation is performed.   deep CNN is superior to RNN of three LSTM layers. When more layers is utilized in RNN, the performance increase little. Here we assume the sequence length is 300 frames.</p><p>(1) VA-RNN(aug.) has the advantages of small model size (number of parameter), which is only 2% of the model size of VA-CNN(aug.) . (2) For the recognition on the well trimmed sequences, VA-CNN(aug.) has rather high recognition speed, which is 83.3 sequences per second, about 10 times faster than VA-RNN(aug.). For the online detection task, considering the LSTM structure is suitable for framewise processing while VA-CNN(aug.) needs to use a sliding window strategy to process the untrimmed streaming data, VA-RNN(aug.) may be more time efficient, depending on the size of sliding window. For example, if the window slides for each frame, the speed of VA-CNN(aug.) is about 83.3 frames/second while the speed of VA-RNN(aug.) is about 7.9×300 = 2370 frames/second. (3) VA-CNN(aug.) has higher recognition accuracy than that of VA-RNN(aug.) due to its joint spatio-temporal exploration capability, the power of the CNN structure, and larger model size. But for small dataset, VA-RNN(aug.) has superior performance thanks to its small number of parameters. Depending on the requirement of practical applications, users can choose among VA-RNN(aug.), VA-CNN(aug.), and VA-fusion(aug.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present two end-to-end view adaptive neural networks, VA-RNN and VA-CNN, for human action recognition from skeleton data. Rather than following the human predefined criterion to re-position skeletons for action recognition, the proposed networks are capable of adapting the observation viewpoints to the suitable ones by itself, with the optimization target of maximizing the recognition performance. We have designed view adaptation models based on the recurrent neural network and the convolutional neural network respectively. Both models can automatically transform the skeletons to consistent viewpoints which eliminate the influence of the diversity of viewpoints and ease the training. Experimental results demonstrate that the proposed framework consistently improves the recognition performance on five challenging benchmark datasets and achieves state-ofthe-art performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Flowchart of the end-to-end view adaptive neural network. It consists of a main classification network and a view adaptation subnetwork. The view adaptation subnetwork automatically determines the virtual observation viewpoints and transforms the skeleton input to representations under the new viewpoints for classification by the main classification network. The entire network is end-toend trained to optimize the classification performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Architecture of the proposed view adaptive neural networks: a view adaptive RNN with LSTM (VA-RNN), and a view adaptive CNN (VA-CNN). The VA-RNN consists of a view adaptation subnetwork, and a main LSTM network. The view adaptation subnetwork determines the suitable observation viewpoint at each time slot. With the skeleton representations under new observation viewpoints, the main LSTM network determines the action class. The VA-CNN consists of a view adaptation subnetwork, and a main convolutional network (ConvNet). The view adaptation subnetwork determines the suitable observation viewpoints for the sequence. With the skeleton representations under the new observation viewpoint, the main ConvNet determines the action class. The classification scores from the two networks can be fused to provide the fused prediction, denoted as the VA-fusion scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of the change of the observation viewpoint, by assuming there is a movable virtual camera. A skeleton sequence is a record of the skeletons from the first frame f = 1 to the last frame f = T under the global coordinate system O. The action can be re-observed by a movable virtual camera under the observation coordinate systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Performance curve for both baselines and the proposed view adaptation schemes based on RNN on the NTU dataset. The horizontal axis denotes the model size, i.e. number of parameters, while the vertical axis indicates the recognition accuracy (%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Frames of (a) the similar posture captured from different viewpoints for the same subject, and (b) the same action "drinking" captured from different viewpoints for different subjects.2 nd row: original skeletons. 3 rd row: skeleton representations from the observation viewpoints of our VA-RNN model. 4 th row: skeleton representations from the observation viewpoints of our VA-CNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Comparisons of pre-processing methods and our view adaptation model on the NTU dataset.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>CS</cell><cell>CV</cell></row><row><cell>wo/ pre-proc.</cell><cell>Raw + RNN</cell><cell>66.3</cell><cell>73.4</cell></row><row><cell></cell><cell>S-trans + RNN</cell><cell>76.0</cell><cell>82.3</cell></row><row><cell></cell><cell>F-trans + RNN</cell><cell>75.1</cell><cell>80.5</cell></row><row><cell></cell><cell>S-trans&amp;S-rota(w.r.t. shoulder) + RNN</cell><cell>75.8</cell><cell>85.1</cell></row><row><cell>Pre-proc.</cell><cell>S-trans&amp;S-rota + RNN</cell><cell>76.4</cell><cell>85.4</cell></row><row><cell></cell><cell>S-trans&amp;F-rota(w.r.t. shoulder) + RNN</cell><cell>75.8</cell><cell>84.9</cell></row><row><cell></cell><cell>S-trans&amp;F-rota + RNN</cell><cell>75.0</cell><cell>85.1</cell></row><row><cell></cell><cell>F-trans&amp;F-rota + RNN</cell><cell>74.1</cell><cell>83.9</cell></row><row><cell></cell><cell>VA-trans + RNN</cell><cell>77.7</cell><cell>84.9</cell></row><row><cell>View adap.</cell><cell>VA-rota + RNN</cell><cell>79.4</cell><cell>87.1</cell></row><row><cell></cell><cell>VA-RNN</cell><cell>79.4</cell><cell>87.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Effectiveness (in accuracy(%)) of data augmentation on S-trans and VA schemes.</figDesc><table><row><cell></cell><cell>Datasets</cell><cell cols="2">NTU CS CV</cell><cell cols="2">SYSU setting-1 setting-2</cell><cell>UWA3D</cell><cell>N-UCLA</cell><cell>SBU</cell></row><row><cell></cell><cell>S-trans+RNN</cell><cell>76.0</cell><cell>82.3</cell><cell>76.3</cell><cell>75.6</cell><cell>57.4</cell><cell>67.4</cell><cell>93.8</cell></row><row><cell>RNN-based</cell><cell>S-trans+RNN(aug.) VA-RNN</cell><cell>77.0 79.4</cell><cell>85.0 87.6</cell><cell>79.3 77.5</cell><cell>78.5 76.9</cell><cell>69.5 58.3</cell><cell>80.9 70.7</cell><cell>93.9 95.9</cell></row><row><cell></cell><cell>VA-RNN(aug.)</cell><cell>79.8</cell><cell>88.9</cell><cell>80.5</cell><cell>79.8</cell><cell>73.6</cell><cell>84.1</cell><cell>97.5</cell></row><row><cell></cell><cell>S-trans+CNN</cell><cell>87.5</cell><cell>92.2</cell><cell>82.1</cell><cell>80.9</cell><cell>67.2</cell><cell>73.9</cell><cell>86.7</cell></row><row><cell>CNN-based</cell><cell>S-trans+CNN(aug.) VA-CNN</cell><cell>87.9 88.2</cell><cell>93.5 93.8</cell><cell>84.2 83.6</cell><cell>83.4 82.9</cell><cell>77.0 71.2</cell><cell>85.7 81.7</cell><cell>93.0 90.3</cell></row><row><cell></cell><cell>VA-CNN(aug.)</cell><cell>88.7</cell><cell>94.3</cell><cell>85.1</cell><cell>84.8</cell><cell>79.3</cell><cell>86.6</cell><cell>95.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Accuracy (%) comparisons on different num-</cell></row><row><cell cols="5">bers of LSTM layers for the main LSTM network (S-</cell></row><row><cell cols="5">trans+RNN(aug.)), and different numbers of convolutional</cell></row><row><cell cols="5">layers for the main ConvNet (S-trans+CNN(aug.)) on the</cell></row><row><cell>NTU dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Main Network</cell><cell>Structure</cell><cell>#Param.(M)</cell><cell>CS</cell><cell>CV</cell></row><row><cell></cell><cell>1 LSTM layer</cell><cell>0.11</cell><cell>74.5</cell><cell>82.0</cell></row><row><cell></cell><cell>2 LSTM layers</cell><cell>0.19</cell><cell>76.2</cell><cell>84.7</cell></row><row><cell></cell><cell>3 LSTM layers</cell><cell>0.27</cell><cell>77.0</cell><cell>85.0</cell></row><row><cell>S-trans+RNN(aug.)</cell><cell>4 LSTM layers</cell><cell>0.35</cell><cell>76.9</cell><cell>83.9</cell></row><row><cell></cell><cell>5 LSTM layers</cell><cell>0.43</cell><cell>76.2</cell><cell>84.2</cell></row><row><cell></cell><cell>6 LSTM layers</cell><cell>0.51</cell><cell>76.6</cell><cell>84.4</cell></row><row><cell>VA-RNN(aug.)</cell><cell>3+2 LSTM layers</cell><cell>0.47</cell><cell>79.8</cell><cell>88.9</cell></row><row><cell></cell><cell>ResNet18</cell><cell>11.21</cell><cell>86.5</cell><cell>93.1</cell></row><row><cell>S-trans+CNN(aug.)</cell><cell>ResNet50 ResNet101</cell><cell>23.63 42.62</cell><cell>87.9 87.8</cell><cell>93.5 93.5</cell></row><row><cell></cell><cell>ResNet152</cell><cell>58.27</cell><cell>88.2</cell><cell>93.4</cell></row><row><cell>VA-CNN(aug.)</cell><cell>ResNet50+3 layers</cell><cell>24.09</cell><cell>88.7</cell><cell>94.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Accuracy (%) on the NTU dataset.</figDesc><table><row><cell>Method</cell><cell>CS</cell><cell>CV</cell></row><row><cell>Skeleton Quads [7]</cell><cell>38.6</cell><cell>41.4</cell></row><row><cell>Lie Group [45]</cell><cell>50.1</cell><cell>52.8</cell></row><row><cell>Dynamic Skeletons [12]</cell><cell>60.2</cell><cell>65.2</cell></row><row><cell>HBRNN-L [6]</cell><cell>59.1</cell><cell>64.0</cell></row><row><cell>Part-aware LSTM [38]</cell><cell>62.9</cell><cell>70.3</cell></row><row><cell>ST-LSTM + Trust Gate [27]</cell><cell>69.2</cell><cell>77.7</cell></row><row><cell>STA-LSTM [42]</cell><cell>73.4</cell><cell>81.2</cell></row><row><cell>GCA-LSTM [29]</cell><cell>74.4</cell><cell>82.8</cell></row><row><cell>URNN-2L-T [25]</cell><cell>74.6</cell><cell>83.2</cell></row><row><cell>Clips+CNN+MTLN [20]</cell><cell>79.6</cell><cell>84.8</cell></row><row><cell>ESV (Synthesized + Pre-trained) [30]</cell><cell>80.0</cell><cell>87.2</cell></row><row><cell>VA-RNN(aug.)</cell><cell>79.8</cell><cell>88.9</cell></row><row><cell>VA-CNN(aug.)</cell><cell>88.7</cell><cell>94.3</cell></row><row><cell>VA-fusion(aug.)</cell><cell>89.4</cell><cell>95.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Accuracy (%) on the SYSU dataset.</figDesc><table><row><cell>Method</cell><cell>setting-1</cell><cell>setting-2</cell></row><row><cell>LAFF [13]</cell><cell>54.2</cell><cell>-</cell></row><row><cell>Dynamic Skeletons [12]</cell><cell>75.5</cell><cell>76.9</cell></row><row><cell>ST-LSTM + Trust Gate [28]</cell><cell>76.5</cell><cell>-</cell></row><row><cell>VA-RNN(aug.)</cell><cell>80.5</cell><cell>79.8</cell></row><row><cell>VA-CNN(aug.)</cell><cell>85.1</cell><cell>84.8</cell></row><row><cell>VA-fusion(aug.)</cell><cell>86.7</cell><cell>86.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>shows the results on each partition. The four views are rather different which make it hard to recognize actions under unseen views. With the view adaptation model, VA-RNN(aug.) and VA-CNN(aug.) outperform the baseline schemes significantly. Our single model VA-CNN(aug.) outperforms ESV [30] largely by 5.5% even though ESV fuses 10 different models.</figDesc><table /><note>5.3.4 N-UCLA Dataset There are three views in this dataset. Usually, two of the views are used for training and the other one is used for</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>Accuracy (%) on the UWA3D dataset.</figDesc><table><row><cell>Training views Testing views</cell><cell cols="2">V1 &amp; V2 V3 V4</cell><cell cols="2">V1 &amp; V3 V2 V4</cell><cell cols="2">V1 &amp; V4 V2 V3</cell><cell cols="2">V2 &amp; V3 V1 V4</cell><cell cols="2">V2 &amp; V4 V1 V3</cell><cell cols="2">V3 &amp; V4 V1 V2</cell><cell>Average</cell></row><row><cell>HOJ3D [54]</cell><cell>15.3</cell><cell>28.2</cell><cell>17.3</cell><cell>27.0</cell><cell>14.6</cell><cell>13.4</cell><cell>15.0</cell><cell>12.9</cell><cell>22.1</cell><cell>13.5</cell><cell>20.3</cell><cell>12.7</cell><cell>17.7</cell></row><row><cell>AE [46]</cell><cell>45.0</cell><cell>40.4</cell><cell>35.1</cell><cell>36.9</cell><cell>34.7</cell><cell>36.0</cell><cell>49.5</cell><cell>29.3</cell><cell>57.1</cell><cell>35.4</cell><cell>49.0</cell><cell>29.3</cell><cell>39.8</cell></row><row><cell>LARP [45]</cell><cell>49.4</cell><cell>42.8</cell><cell>34.6</cell><cell>39.7</cell><cell>38.1</cell><cell>44.8</cell><cell>53.3</cell><cell>33.5</cell><cell>53.6</cell><cell>41.2</cell><cell>56.7</cell><cell>32.6</cell><cell>43.4</cell></row><row><cell>ESV (Synthesized + Pre-trained) [30]</cell><cell>72.3</cell><cell>76.3</cell><cell>64.7</cell><cell>75.5</cell><cell>63.5</cell><cell>74.0</cell><cell>83.1</cell><cell>75.1</cell><cell>82.4</cell><cell>71.1</cell><cell>83.5</cell><cell>63.5</cell><cell>73.8</cell></row><row><cell>VA-RNN(aug.)</cell><cell>70.9</cell><cell>73.2</cell><cell>68.1</cell><cell>72.0</cell><cell>68.1</cell><cell>71.3</cell><cell>81.3</cell><cell>76.8</cell><cell>79.4</cell><cell>71.7</cell><cell>79.4</cell><cell>71.3</cell><cell>73.6</cell></row><row><cell>VA-CNN(aug.)</cell><cell>77.7</cell><cell>83.1</cell><cell>77.2</cell><cell>83.1</cell><cell>73.2</cell><cell>73.3</cell><cell>86.8</cell><cell>79.9</cell><cell>84.8</cell><cell>73.7</cell><cell>83.3</cell><cell>75.2</cell><cell>79.3</cell></row><row><cell>VA-fusion(aug.)</cell><cell>80.9</cell><cell>84.3</cell><cell>78.7</cell><cell>86.2</cell><cell>75.2</cell><cell>73.3</cell><cell>87.6</cell><cell>84.3</cell><cell>86.0</cell><cell>74.9</cell><cell>86.4</cell><cell>79.5</cell><cell>81.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>Accuracy (%) on the N-UCLA dataset.</figDesc><table><row><cell>Setting (test view)</cell><cell>V3</cell><cell>V2</cell><cell>V1</cell><cell>Average</cell></row><row><cell>HOJ3D [54]</cell><cell>54.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AE [46]</cell><cell>76.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LARP [45]</cell><cell>74.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HBRNN-L [5]</cell><cell>78.5</cell><cell>83.5</cell><cell>79.3</cell><cell>80.5</cell></row><row><cell>ESV(Synthesized+Pre-trained) [30]</cell><cell>92.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VA-RNN(aug.)</cell><cell>90.7</cell><cell>87.5</cell><cell>74.0</cell><cell>84.1</cell></row><row><cell>VA-CNN(aug.)</cell><cell>93.8</cell><cell>86.3</cell><cell>79.7</cell><cell>86.6</cell></row><row><cell>VA-fusion(aug.)</cell><cell>95.3</cell><cell>88.7</cell><cell>80.2</cell><cell>88.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc>Accuracy (%) on the SBU dataset.</figDesc><table><row><cell>Methods</cell><cell>Acc. (%)</cell></row><row><cell>Raw skeleton [56]</cell><cell>49.7</cell></row><row><cell>Joint feature</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 11 :</head><label>11</label><figDesc>Effectiveness (in accuracy(%)) of the view adaptation design with small network CNN-6layers and large network ResNet50 as the backnone CNN Networks. Gain denotes the gap between S-trans+CNN(aug.) and VA-CNN(aug.).</figDesc><table><row><cell>Network</cell><cell>Method</cell><cell cols="2">NTU CS CV</cell><cell cols="2">SYSU setting-1 setting-2</cell><cell>UWA3D</cell><cell>N-UCLA</cell><cell>SBU</cell></row><row><cell></cell><cell>S-trans+CNN(aug.)</cell><cell>87.9</cell><cell>93.5</cell><cell>84.2</cell><cell>83.4</cell><cell>77.0</cell><cell>85.7</cell><cell>93.0</cell></row><row><cell>ResNet50</cell><cell>VA-CNN(aug.) Gain</cell><cell>88.7 0.8</cell><cell>94.3 0.8</cell><cell>85.1 0.9</cell><cell>84.8 1.4</cell><cell>79.3 2.3</cell><cell>86.6 0.9</cell><cell>95.7 2.7</cell></row><row><cell></cell><cell>S-trans+CNN(aug.)</cell><cell>79.6</cell><cell>85.1</cell><cell>74.6</cell><cell>74.4</cell><cell>55.8</cell><cell>72.0</cell><cell>82.7</cell></row><row><cell>CNN-6layer</cell><cell>VA-CNN(aug.) Gain</cell><cell>81.0 1.4</cell><cell>88.4 3.3</cell><cell>76.2 1.6</cell><cell>76.2 1.8</cell><cell>67.3 11.5</cell><cell>79.3 7.3</cell><cell>86.2 3.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 12 :</head><label>12</label><figDesc>Model comparisons of VA-RNN and VA-CNN.</figDesc><table><row><cell>Model</cell><cell>#Param.(M)</cell><cell>Speed (seq./sec.)</cell><cell>Acc.(NTU-CV)(%)</cell></row><row><cell>VA-RNN(aug.)</cell><cell>0.47</cell><cell>7.9</cell><cell>88.7</cell></row><row><cell>VA-CNN(aug.)</cell><cell>24.09</cell><cell>83.3</cell><cell>94.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel Realsense</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/realsense" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human activity recognition from 3d data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">View-invariant motion trajectory-based activity classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Khokhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schonfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="54" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACPR</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning of temporal dynamics for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3010" to="3022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to recognize activities from the wrong view point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tabrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">View-invariant human action recognition via robust locally adaptive multi-view learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="917" to="920" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Space-time representation of people based on 3d skeletal data: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="85" to="105" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for RGB-D activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time RGB-D activity prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">View-invariant action recognition based on artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="412" to="424" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advances in view-invariant human motion analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSMCC</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Interactive body part contrast mining for human interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Informative joints based human action recognition using skeleton contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="29" to="40" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for it is analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception and Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-view action recognition from temporal self-similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
	<note>Multimedia &amp; Expo Workshops</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative virtual views for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive rnn tree for large-scale human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Spatio-temporal LSTM with trust gates for 3D human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Global contextaware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PR</publisher>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Latent multitask learning for viewinvariant action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">3d skeleton-based human action classification: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PR</publisher>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="130" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning action recognition model from depth and skeleton videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Histogram of oriented principal components for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2430" to="2443" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">View-invariance in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">View-invariant action recognition using fundamental ratios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">View-invariant action recognition from point triplets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1898" to="1905" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Action Recognition with Depth Cameras</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="11" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Making action recognition robust to occlusions and viewpoint changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Özuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A survey of vision-based methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">View-invariant action recognition using latent kernelized structural svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cross-view action recognition over heterogeneous feature spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Two-person interaction detection using body-pose features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rgb-dbased action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PR</publisher>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="86" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Crossview action recognition via a continuous virtual path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning view-invariant sparse representations for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning for skeleton based action recognition using regularized deep LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
