<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
							<email>dhuang@buaa.edu.cnhanshumin</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection is a challenging task and usually formulated as an unsupervised learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and efficiency. Given a strong model pre-trained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature alignment enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on three major benchmarks, significantly superior to the state of the art ones. In addition, it makes inferences at a very high speed (with 100 FPS for images of the size at 256×256), at least dozens of times faster than the latest counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection is generally referred to as identifying samples that are atypical with respect to regular patterns in the data set and has shown great potential in various real-world applications such as video surveillance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref>, product quality control <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref> and medical diagno- * Equal contribution. This work is done when Guodong Wang is an intern at Baidu Inc. † Corresponding author.  <ref type="figure" target="#fig_3">Figure 1</ref>. Visual results of our method on three defective images from the MVTec-AD dataset. ResNet-18 is used as backbone and the three bottom blocks (i.e., conv2 x, conv3 x, conv4 x) are selected as feature extractors. Columns from left to right correspond to input images with defects (ground truth regions in red), anomaly maps of the three blocks, and the final anomaly maps respectively. sis <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b39">39]</ref>. Its key challenge lies in the unexpectedness of anomalies which is very difficult to deal with in a supervised way, as labeling all types of anomalous instances seems unrealistic. Previous studies address this challenge by following the unsupervised learning paradigm, i.e., one-class classification <ref type="bibr" target="#b22">[23]</ref>. They approximate the decision boundary for a binary classification problem by searching a feature space where the distribution of normal data is accurately modeled. Deep learning, in particular convolutional neural networks (CNNs) <ref type="bibr" target="#b19">[20]</ref> and residual networks (ResNets) <ref type="bibr" target="#b15">[16]</ref>, provides a powerful alternative to automatically build comprehensive representations at multiple levels. Such deep features prove very effective in capturing the intrinsic characteristics of the normal data manifold <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">44]</ref>. Despite the promising results in their respective fields, all these methods simply predict anomalies at the image-level without spatial localization.</p><p>The pixel-level methods advance anomaly detection by means of pixel-wise comparison of image patches and their reconstructions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref> or per-pixel estimation of probability density on entire images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">36]</ref>, among which Autoencoders, Generative Adversarial Networks (GANs), and their variants are dominating models. However, their performance is prone to serious degradation when images are poorly reconstructed <ref type="bibr" target="#b28">[29]</ref> or likelihoods are inaccurately calibrated <ref type="bibr" target="#b23">[24]</ref>. Some recent attempts leverage the knowledge from other well-studied computer vision tasks. They directly apply the networks pre-trained on image classification and show that they are sufficiently generic to image-level detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. Cohen and Hoshen <ref type="bibr" target="#b10">[11]</ref> investigate this idea in pixel-level detection and calculate anomaly scores by averaging distances over a number of nearest neighbors from a collection of features extracted from a pretrained network on anomaly-free images. This method indeed delivers performance gain; unfortunately, it has the time bottleneck due to per-pixel comparison.</p><p>Bergmann et al. <ref type="bibr" target="#b7">[8]</ref> circumvent the limitations aforementioned by implicitly learning the distribution of normal features with a student-teacher framework and reach decent results. The intuition is that the representation of anomalyfree data suffers from poor generalization outside its own manifold. The difference between the outputs of students and teacher along with the uncertainty among students' predictions serve as the anomaly scoring function. Nevertheless, two major drawbacks still remain: i.e. incompleteness of transferred knowledge and inefficiency of handling scaling. For the former, since knowledge is distilled from a ResNet-18 <ref type="bibr" target="#b15">[16]</ref> into a lightweight teacher network, the big gap between their model capacities <ref type="bibr" target="#b41">[41]</ref> tends to incur loss of important information. For the latter, multiple studentteacher ensemble pairs are required to be separately trained, each for a specific respective field, to achieve scale invariance, leading to a high computational cost. Both the facts leave much room for improvement.</p><p>In this paper, we propose a simple yet powerful approach to anomaly detection, which follows the student-teacher framework for the advantages, but substantially extends it in terms of both accuracy and efficiency. Specifically, given a powerful network pre-trained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture. In this case, the student network learns the distribution of anomaly-free images by matching their features with the counterparts from the pre-trained network, and this one-step transfer preserves the crucial information as much as possible. Furthermore, to enhance the scale robustness, we embed multi-scale feature matching into the network, and this hierarchical feature alignment strategy enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under a stronger supervision and thus allow to detect anomalies of various sizes (see <ref type="figure" target="#fig_3">Figure 1</ref> for visualization). The feature pyramids from the teacher and student networks are compared for prediction, where a larger difference indicates a higher probability of anomaly occurrence.</p><p>Compared to the previous work, especially the preliminary student-teacher model, the benefits of our approach are three-fold. First, feature learning is tailored to the training dataset, making the student network more discriminative for the given task. Second, useful knowledge is well transferred from the pretrained network to the student network within one-step distillation, as they share the same structure. Finally, the scale invariance is conveniently reached by the proposed feature pyramid matching scheme. Due to such strengths, our approach conducts accurate and fast pixellevel anomaly detection. It reports very competitive results on three major benchmarks, outperforming the state of the art ones by a large margin, and makes prediction at a very high speed (∼100 FPS on a GeForce RTX 2080 Ti for images of the size 256×256), 20 and 550 times faster than STAD <ref type="bibr" target="#b7">[8]</ref> and SPADE <ref type="bibr" target="#b10">[11]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image-Level Anomaly Detection</head><p>Image-level techniques manifest anomalies in the form of images of a unseen category. These methods can be coarsely divided into three branches: reconstruction-based, distribution-based and classification-based.</p><p>The first group of approaches reconstruct the training images to capture the normal data manifold. An anomalous image is very likely to possess a high reconstruction error during inference, as it is drawn from a different distribution. The main weakness of these approaches comes from the excellent generalization ability of the deep models they use, including variational autoencoder <ref type="bibr" target="#b2">[3]</ref>, robust autoencoder <ref type="bibr" target="#b44">[44]</ref>, conditional GAN <ref type="bibr" target="#b1">[2]</ref>, and bi-directional GAN <ref type="bibr" target="#b43">[43]</ref>, which probably allows anomalous images to be faithful reconstructed. It is also rather difficult to chose an appropriate loss function for evaluating the quality of reconstruction <ref type="bibr" target="#b28">[29]</ref>.</p><p>Distribution-based approaches model the probabilistic distribution of the normal images. Test images that have low probability density values are designated as anomalous. Recent algorithms such as anomaly detection GAN (ADGAN) <ref type="bibr" target="#b11">[12]</ref> and deep autoencoding Gaussian mixture model (DAGMM) <ref type="bibr" target="#b45">[45]</ref> learn a deep projection that maps high-dimensional images into a low-dimensional latent space. Probability density estimation is typically much easier in the obtained spaces and thus performance has been significantly improved. Nevertheless, these methods have high sample complexity and demand large training data.</p><p>Classification-based approaches have dominated the unsupervised anomaly detection in the last decade. One use-ful paradigm is to feed the deep features extracted by deep generative model <ref type="bibr" target="#b8">[9]</ref> or transferred from pretrained networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref> into a separate shallow classification model like one-class support vector machine (OC-SVM) <ref type="bibr" target="#b35">[35]</ref>. These hybrid techniques are sub-optimal because feature extraction is unaware of the anomaly detection task. Deep support vector data description (Deep-SVDD) <ref type="bibr" target="#b30">[31]</ref>, an adaptation of SVDD <ref type="bibr" target="#b38">[38]</ref> to the deep regime, trains a neural network by minimizing the volume of a hypersphere that encloses the network representations of the data. One-class neural network (OC-NN) <ref type="bibr" target="#b9">[10]</ref> combines an encoder that progressively extracts rich representation of the data with a neural network that searches for a hyperplane to separate all the normal data from the origin. These two algorithms are departures from the hybrid methods in the sense that data representation in them is not task-agnostic.</p><p>Another line of research depends on self-supervised learning. Geom <ref type="bibr" target="#b14">[15]</ref> creates a dataset by applying dozens of geometric transformations to the normal images and trains a multi-class neural network over the self-labeled dataset to discriminate such transformations. At test time, anomalies are expected to be assigned with less confidence in discriminating the transformations. The effectiveness of this method is limited by the visual transformation based selfsupervision (VTSS) hypothesis <ref type="bibr" target="#b27">[28]</ref>, which states that features obtained by self-supervised learning will be less valuable if the transformed instances are already present in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pixel-Level Anomaly Detection</head><p>Pixel-level techniques are particularly designed for visual anomaly detection. All the methods aim to precisely segment the anomalous regions in the test images. This task is much more complicated than a binary classification.</p><p>The expressive power of deep neural networks inspires a series of studies that explore how to transfer the benefits of networks pretrained on image classification tasks to unsupervised anomaly detection. Napoletano et al. <ref type="bibr" target="#b24">[25]</ref> exploit a pretrained ResNet-18 to embed cropped training image patches into a feature space, reduce the dimension of feature vectors by PCA, and model their distribution using Kmeans clustering. This method requires a large number of overlapping patches to obtain a spatial anomaly map at inference time, which results in coarse-grained maps and may become a performance bottleneck.</p><p>To avoid cropping image patches and accelerate feature extraction, Sabokrou et al. <ref type="bibr" target="#b32">[32]</ref> extract descriptors from early future maps of a pretrained fully convolutional neural network (FCN) and adopt a unimodal Gaussian distribution to fit feature vectors of the training anomaly-free images. The resolution of the final anomaly map will decrease substantially on account of the pooling layers, especially when the receptive fields of the networks are very large. The uni-model Gaussian distribution will suffer a failure of characterizing the training feature distribution as long as the problem complexity increases.</p><p>More recently, convolutional adversarial variational autoencoder with guided attention (CAVGA) <ref type="bibr" target="#b40">[40]</ref> incorporates Grad-CAM <ref type="bibr" target="#b37">[37]</ref> into a variational autoencoder that has an attention expansion loss to encourage the deep model itself to focus on all normal regions in the image.</p><p>We are aware of a preprint <ref type="bibr" target="#b10">[11]</ref> that also presents a feature pyramid matching method for unsupervised anomaly detection and achieve promising results. Our study departs from that work on several fronts. First, the proposed SPADE approach directly applies the deep pretrained features to entire image dataset, while the feature learning is tailored to the training anomaly-free dataset in our method. Second, the feature comparison of SPADE is conducted between a given image and its k nearest neighbors in the feature space, which is quite different from ours. Finally, SPADE perform image-level and pixel-level anomaly detections in two successive steps. By contract, our method can simultaneously detect the image-level and pixel-level anomalies. Therefore, our method is much faster than SPADE and can achieve much better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>We make use of the student-teacher learning framework to implicitly model the feature distribution of the normal training images. The teacher is a powerful network pretrained on image classification task (e.g., a ResNet-18 pretrained on ImageNet). To reduce information loss, the student is a network with the same architecture. This is in essence one case of feature-based knowledge distillation <ref type="bibr" target="#b41">[41]</ref>.</p><p>We need to consider a key factor, position of selected distillation feature. Deep neural networks generate a pyramid of features for each input image. Bottom layers result in higher-resolution features encoding low-level information such as textures, edges and colors. By contrast, top layers yield low-resolution features that contain context information. Features created by bottom layers often are generic enough that they can be shared by dissimilar vision tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">42]</ref>. As different layers in deep neural networks correspond to distinct receptive fields, we select the features extracted by a few successive bottom layer groups (e.g., blocks in ResNet-18) of the teacher to guide the student's learning. This hierarchical feature matching allows our method to detect anomalies of various sizes. <ref type="figure">Figure 2</ref> gives a sketch of our method with the MVTec-AD dataset <ref type="bibr" target="#b7">[8]</ref> being an example. The training and test process are formally provided as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Teacher Network</p><formula xml:id="formula_0">F t F s F t F s F t F s Student Network Anomaly Map</formula><p>Training Images Test Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilinear Interpolation Training Process Test Process</head><p>Layer Group <ref type="figure">Figure 2</ref>. Schematic overview of our method. The feature pyramid of a student network is trained to align with the counterpart of a pretrained teacher network. A test image (or pixel) will have high anomaly score if its features from the two models differ significantly. The hierarchical feature alignment enables our method to detect anomalies of varies sizes with a single forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Process</head><p>Given a training data set of anomaly-free images D = {I 1 , I 2 , . . . , I n }, our goal is to capture the normal data manifold by aligning the features extracted by the L bottom layer groups of the teacher with those of the student. For an input image I k ∈ R w×h×c , where h is the height, w is the width and c is the number of color channels, the lth bottom layer group of the teacher and the student outputs a feature map F l t (I k ) ∈ R w l ×h l ×d l and F l s (I k ) ∈ R w l ×h l ×d l respectively, where w l and h l denote the height and width of the feature map. Since there is no prior knowledge regarding the appearance of objects, we simply assume that all regions including background are anomaly-free in the training images if applicable. Note that F l t (I k ) ij ∈ R d l and F l s (I k ) ij ∈ R d l are feature vectors at position (i, j) in the feature maps from the teacher and student respectively. We define the loss at position (i, j) as 2 -distance between the 2 -normalized feature vectors, namely,</p><formula xml:id="formula_1">l (I k ) ij = 1 2 F l t (I k ) ij −F l t (I k ) ij 2 2 ,<label>(1)</label></formula><formula xml:id="formula_2">F l t (I k ) ij = F l t (I k ) ij F l t (I k ) ij 2 ,F l s (I k ) ij = F l t (I k ) ij F l s (I k ) ij 2 .</formula><p>It is worth noting that the 2 distance used in <ref type="formula" target="#formula_1">(1)</ref> is equivalent to the cosine similarity. So the loss l (I k ) ij ∈ (0, 1).</p><p>The loss for the entire image I k is given as an average of the loss at each position,</p><formula xml:id="formula_3">l (I k ) = 1 w l h l h l i=1 w l j=1 l (I k ) ij ,<label>(2)</label></formula><p>and the total loss is the weighted average of the loss at different pyramid scales,</p><formula xml:id="formula_4">(I k ) = L l=1 α l l (I k ), s.t., α l ≥ 0,<label>(3)</label></formula><p>where α l depicts the impact of the lth feature scale on anomaly detection. We simply set the parameter α l = 1, i = 1, . . . , L in all our experiments. Given a minibatch B sampled from the training data set D, we update the student by minimizing the loss B = k∈B (I k ) by a certain stochastic optimization algorithm. The teacher keeps fixed during the entire training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Test Process</head><p>In this phase, we aim to assign an anomaly map Ω of size h × w to a test image J ∈ R h×w×c . The score Ω ij ∈ (0, 1) indicates how much the pixel at position (i, j) deviates from the training data manifold. We forward the test image J into the teacher and the student. Let F l t (J) and F l s (J) denote the feature maps generated by the lth bottom layer group of the teacher and the student. We can compute an anomaly map Ω l (J) of size h l × w l , whose element Ω l ij (J) is the loss (1) at position (i, j). The anomaly map Ω l (J) is unsampled to size h × w by the bilinear interpolation technique. The final anomaly map is defined as element-wise production of these interpolations,</p><formula xml:id="formula_5">Ω(J) = L l=1</formula><p>Upsample(Ω l (J)).</p><p>A test image is designated as anomaly if any regions in the image are anomalous. As a result, we simply choose the maximum value in the anomaly map max(Ω(J)) as the anomaly score for the test image J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the effectiveness of our method, we first conduct experiments on the MVTec Anomaly Detection (MVTec-AD) <ref type="bibr" target="#b6">[7]</ref> dataset. Both image-level and pixel-level anomaly detection are considered. Our method is then applied to the ShanghaiTech Campus (STC) <ref type="bibr" target="#b20">[21]</ref> dataset for unsupervised anomaly pixel-level detection and the CIFAR-10 <ref type="bibr" target="#b18">[19]</ref> dataset for one-class classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head><p>Implement Details. For all experiments, we choose the first three blocks (i.e., conv2 x, conv3 x, conv4 x) of ResNet-18 as the pyramid feature extractors. The parameters of the teacher network take the corresponding values from the ResNet-18 pretrained on ImageNet * while those of student network are initialized randomly. We utilize stochastic gradient descent (SGD) with the momentum of 0.9 as the optimizer. Learning rate is fixed to be 0.4 throughout 100 training epochs. The batch size is 32 and weight decay is 10 −4 . We find that our method is quite robust to the values of the hyper-parameters. To strike a balance between detection accuracy and efficiency, we resize all images in the MVTec-AD and STC datasets to 256×256. Data augmentation is not used, since some standard augmentation techniques may lead to ambiguous determination of anomalies. For instance, flipped metal nut in the MvTec-AD dataset is regraded as anomalous product.</p><p>Evaluation Metrics. The performance of our method is measured by two well-established metrics. The first is the area under the receiver operating characteristic curve (AUC-ROC), which can be computed on different levels when anomalous images and pixels are treated as positive respectively. However, the pixel-level AUC-ROC is in favor of large anomalous regions. In order to weight ground-truth anomaly regions of various sizes equally, we introduce the Per-Region-Overlap (PRO) curve metric proposed in <ref type="bibr" target="#b7">[8]</ref>. Given a threshold for anomaly scores, we obtain a binary mask that indicates whether a pixel is anomalous. The * We adopt the default values provided by PyTorch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MVTec Anomaly Detection Dataset</head><p>Our method is first evaluated on the MVTec-AD dataset, which is specifically created to benchmark algorithms for unsupervised detection of anomalous regions <ref type="bibr" target="#b6">[7]</ref>. It collects more than 5000 high-resolution images of industrial products from 15 different categories. For each category, the training set includes defect-free images merely and the test set comprises normal images as well as images with different types of industrial defects. We begin with the task of finding anomalous images. As defective regions usually occupy a small proportion of the whole image, the test anomalies differ in subtle way from the training images. This makes the MVTec-AD dataset more challenging than those previously used in the literature (e.g., MNIST and CIFAR-10) where anomalous images come from other different categories. <ref type="table" target="#tab_0">Table 1</ref> compares our method against several state-of-the-art approaches, Geom <ref type="bibr" target="#b14">[15]</ref>, GANomaly <ref type="bibr" target="#b1">[2]</ref>, l 2 -AE <ref type="bibr" target="#b4">[5]</ref>, ITAE <ref type="bibr" target="#b16">[17]</ref> and SPADE <ref type="bibr" target="#b10">[11]</ref>. We see clearly that our approach outperforms other methods by a large margin. In particular, the performance is improved up to around 11.7% compared with the second-best SPADE <ref type="bibr" target="#b10">[11]</ref>. This demonstrates that our method can learn category-specific features which are superior to the generic features directly transferred from image classification. We then consider the task of pixel-precise anomaly detection and compare our method with a series of stateof-the-art approaches listed in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b7">[8]</ref>. <ref type="table">Table 2</ref> reports the performance in terms of the AUC-ROC metric. Our method can achieve much better results in most cases, though most of the other approaches use different varieties of autoencoder. This validates our pyramid alignment for feature learning. SPADE is very competitive, but utilizes a cumbersome Wide-ResNet50×2 network to extract features. <ref type="table">Table 3</ref> compares all approaches in terms of PRO metric. This is a pixel-level accuracy measure that gives equal weights to anomalies of different sizes. As shown in <ref type="table">Table 2</ref>. Pixel-precise anomaly detection on the MvTec-AD dataset. The performance is measured by the pixel-level AUC-ROC for each category. The best results are highlighted in boldface. Results for all approaches except ours are quoted from <ref type="bibr" target="#b10">[11]</ref>.  <ref type="table">Table 3</ref>, our method achieves the state-of-the-art against all other methods. Although STAD <ref type="bibr" target="#b7">[8]</ref> leverages the studentteacher learning framework, its performance is always inferior to that of our method. Perhaps this inferiority can be attributed to the information loss in its two-step knowledge transfer process and the low capacity of the student networks. SPADE is very competitive in this experiment. However, it utilizes a cumbersome Wide-ResNet50×2 network to extract features. Besides, our method is much more efficient. For an image at 256×256, STAD <ref type="bibr" target="#b7">[8]</ref> and SPADE <ref type="bibr" target="#b10">[11]</ref> finish detection in 223 ms and 5626 ms on a GeForce RTX 2080 Ti, respectively, while it only takes 10 ms for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Shanghai Tech Campus Dataset</head><p>We further evaluate our method for pixel-level anomaly detection on the STC dataset. This dataset is originally created for anomaly detection in 12 surveillance scenes. Following <ref type="bibr" target="#b40">[40]</ref>, we construct the training and test set by extracting every fifth frame from each scene. The training set only contains normal images while the test set contains both normal and anomalous images. Different from the MvTec-AD dataset, this dataset focuses more on motion anomaly detection such as fighting and car intruding. <ref type="table">Table 4</ref> shows that the performance of our method exceeds that of CAVGA-R u <ref type="bibr" target="#b40">[40]</ref> by a significant margin. Note that CAVGA-R u has reported state-of-the-art result in the literature on unsupervised anomaly detection. Once again, our method outperforms SPADE in spite of a lightweight network used. <ref type="figure">Figure 3</ref> gives the visual results of our method on three anomalous images from the STC dataset. We clearly find that the anomalous regions in the images are very difficult to be precisely localized if we use a specific level of features  merely. Comparison of the features in a hierarchical fashion is certainly a good way to delineate the anomalous regions regardless of their sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CIFAR-10</head><p>Finally, we test whether our method can generalize well on small-sized images. We conduct experiments on CIFAR-10 which is traditionally used for image-level one-class classification. For this dataset, the sizes of feature maps from three blocks are 8×8, 4×4, and 2×2 respectively. The hyper-parameter settings are the same as those used in the MVTec-AD experiment. Following <ref type="bibr" target="#b7">[8]</ref>, we take one class as normal and the remaining nine classes as anomalous in turn, totally resulting in ten runs. We see clearly from table 5 that our performance is much better. For each category, we rank the test images according to their anomaly scores. The top five samples and the bottom five sample for five categories are given in <ref type="figure">Figure 4</ref>. It is clear that our method predicts the test images belonging to the given category as normal. The images from other categories are correctly classified as anomalous. <ref type="table">Table 3</ref>. Pixel-level anomaly detection on the MvTec-AD dataset. The performance is measured by the normalized area under the Per-Region-Overlap (PRO) curve up to an average false positive rate (FPR) of 30% for each category. The best results are highlighted in boldface. Results for all approaches except ours are quoted from <ref type="bibr" target="#b7">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies and Discussions</head><p>We conduct a series of ablation studies on the MVTec-AD dataset to answer the following questions: which block provide most informative features for anomaly detection? Is feature pyramid matching superior to single feature alignment? Is our method independent of the backbone network structure? Is the teacher pretrained on other datasets still useful? Is our method applicable to small training dataset?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Feature Matching</head><p>We first investigate the effectiveness of feature extraction by each block of ResNet-18. Considering that the first block is a simple convolutional layer, we exclude it from comparison. We train the student by matching features extracted by its second, third, fourth and fifth blocks with the counterparts of the teacher respectively. As shown in Table 6, feature alignments conducted at the end of the third and fourth blocks can achieve better performance. This is in good agreement with the previous discovery that the middle-level features play a more important role in knowledge transfer <ref type="bibr" target="#b26">[27]</ref>.</p><p>We then test three different combinations of the successive blocks of ResNet-18. For each combination, we align <ref type="table">Table 5</ref>. Image-level anomaly detection on CIFAR-10. The performance is measured by the average AUC-ROC across 10 categories.The best results are highlighted in boldface. Results for all approaches except ours are quoted from <ref type="bibr" target="#b7">[8]</ref>.  the features extracted by the corresponding blocks from the teacher and the student. <ref type="table" target="#tab_3">Table 6</ref> shows that the mixture of the second, third and fourth blocks outperform other combinations as well as the single components. This implies that Feature pyramid matching is a better way for feature learning. This finding is also validated in <ref type="figure" target="#fig_3">Figure 1</ref>. Anomaly maps generated by low-level features are more suitable for precise anomaly localization, but it is likely to include background noise. By contrast, anomaly maps generated by high-level features is desirable to segment big anomalous regions. The aggregation of anomaly maps at different scales can accurately detect anomalies of various sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Backbone Network Structure</head><p>We further check whether our method is networkagnostic. Our algorithm is applied to backbone networks with different architectures. At the beginning, we conduct experiments using the first successive three bottom blocks of ResNet-34 and ResNet-50 as feature extractors. <ref type="table" target="#tab_4">Table 7</ref> shows that our method is quite robust and it can achieve competitive performance regardless of the depth of the residual network. We then introduce a cumbersome network Wideresnet50x2 that has been used in <ref type="bibr" target="#b10">[11]</ref>. As shown in <ref type="table" target="#tab_4">Table 7</ref>, the performance of this network is almost the same as those of the ResNets. This validates that our method is independent of the backbone network structure. We further make comparison with two lightweight networks, SqueezeNet <ref type="bibr" target="#b13">[14]</ref> and DenseNet <ref type="bibr" target="#b17">[18]</ref>. From Table 7, we see clearly that the performance of these two approaches is slight inferior. This is quite understandable, as they have comparatively smaller sizes and relatively low expressive capacity. Nevertheless, the results are still satisfactory, which are better than those obtained by other methods in <ref type="table" target="#tab_0">Table 1</ref>, 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Pretained Datasets</head><p>To answer the fourth questions, we pretrain the ResNet-18 on a couple of image classification benchmarks, including MNIST, CIFAR-10, CIFAR-100 <ref type="bibr" target="#b18">[19]</ref>, and SVHN <ref type="bibr" target="#b25">[26]</ref>. These pretrained models are exploited to train the student network in our method. The MNIST and SVHN datasets simply contain digital numbers from 0 to 9. We see from <ref type="table">Table 8</ref> that the teacher network pretrained on these two datasets yield worse results. This can be attribute to that the features learned from these two pretrained models generalize poorly on the MVTec-AD dataset. By contrast, the features extracted from the teacher networks pretrained on CIFAR-10 and CIFAR-100 exhibit better generalization, as they include natural images that are quite similar to the images in the MVTec-AD dataset. Note that the performance of these two pretrained teacher is slightly inferior to that of the teacher pretrained on ImageNet. This is because that the ImageNet dataset consists of a huge number of highresolution natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Number of Training Samples</head><p>We investigate the effect of the training set size in this experiment. Only 5% and 10% anomaly-free images are used to train our model. Considering that SPADE is very competitive, we compare it with our approach in these cases. The detection performance is summarized in <ref type="table" target="#tab_5">Table 9</ref>. It can be seen that our model still achieves a satisfactory performance even if there is a few number of training data. By contrast, SPADE suffers a serious performance degradation. This can be explained by the usage of the tailored feature learning. Our model profits from this strategy and require a few training data to capture the feature distribution of anomaly-free images. Furthermore, our method use only 5% training samples to outperform the preliminary studentteacher framework. This validates the effectiveness of the proposed feature pyramid alignment technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a new feature pyramid matching technique and incorporate it into the student-teacher anomaly detection framework. Given a powerful network pretrained on image classification as the teacher, we use its different levels of features to guide a student network with same structure to learn the distribution of anomaly-free images. The anomaly scoring function of a test image can be defined as the difference between feature pyramids generated by the two models. This one-step knowledge transfer largely sim-plifies the detection procedure. On account of the hierarchical feature alignment, our method is capable of detecting anomalies of various sizes with only a single forward pass. Our approach circumvents expensive computation of the upto-date STAD <ref type="bibr" target="#b7">[8]</ref> and SPADE <ref type="bibr" target="#b10">[11]</ref>. Experimental results on three benchmark datasets show that our method achieves superior performance against state-of-the-art approaches by a significant margin. In addition, our method is applicable to the small training sets. Even with a low number of training examples it still achieves competitive results. This interesting fact may shed light on the few-shot anomaly detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Visual results of our method on three anomalous images from the STC dataset. ResNet-18 is used as backbone and the three bottom blocks (i.e., conv2 x, conv3 x, conv4 x) are selected as feature extractors. Columns from left to right correspond to input images, ground truth regions, anomaly maps of the three blocks, and the combined anomaly maps respectively. Results of our method on the CIFAR-10 dataset. ResNet-18 is used as backbone and the three bottom blocks (i.e., conv2 x, conv3 x, conv4 x) are selected as feature extractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>OCGAN 1 -</head><label>1</label><figDesc>NN OC-SVM l 2 -AE VAE STAD Ours 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Image-level anomaly detection on the MvTec-AD dataset. The performance is measured by average AUC-ROC across 15 categories. The best results are highlighted by boldface. Results for all approaches except ours are quoted from<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell cols="5">Geom GANomaly l 2 -AE ITAE SPADE Ours</cell></row><row><cell>0.672</cell><cell>0.762</cell><cell>0.754 0.839</cell><cell>0.855</cell><cell>0.955</cell></row><row><cell cols="5">ground-truth anomalous regions are separated into multiple</cell></row><row><cell cols="5">connected components. The PRO score is defined by the</cell></row><row><cell cols="5">average proportion of the pixels in each component that are</cell></row><row><cell cols="5">detected as anomalous. We scan over the PRO value by in-</cell></row><row><cell cols="5">creasing the threshold until the average false positive rate</cell></row><row><cell cols="5">(FPR) reaches 30%. The normalized integral of the PRO</cell></row><row><cell cols="3">curve is used as a measure.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Category 1 -Table 4 .</head><label>14</label><figDesc>NN K-means l 2 -AE VAE SSIM-AE AnoGAN CNN-Dict STAD SPADE Ours Pixel-level anomaly detection on the STC dataset. The performance is measured by the average AUC-ROC across 12 scenes. The best results are highlighted in boldface. Results for all approaches except ours are quoted from<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell></cell><cell>Carpet</cell><cell></cell><cell>0.512</cell><cell>0.253</cell><cell cols="2">0.456 0.501</cell><cell>0.647</cell><cell>0.204</cell><cell>0.469</cell><cell>0.695</cell><cell>0.947</cell><cell>0.958</cell></row><row><cell>Textures</cell><cell>Grid Leather Tile</cell><cell></cell><cell>0.228 0.446 0.822</cell><cell>0.107 0.308 0.779</cell><cell cols="2">0.582 0.224 0.819 0.635 0.897 0.870</cell><cell>0.849 0.561 0.175</cell><cell>0.226 0.378 0.177</cell><cell>0.183 0.641 0.797</cell><cell>0.819 0.819 0.912</cell><cell>0.867 0.972 0.759</cell><cell>0.966 0.980 0.921</cell></row><row><cell></cell><cell>Wood</cell><cell></cell><cell>0.502</cell><cell>0.411</cell><cell cols="2">0.727 0.628</cell><cell>0.605</cell><cell>0.386</cell><cell>0.621</cell><cell>0.725</cell><cell>0.874</cell><cell>0.936</cell></row><row><cell></cell><cell>Bottle</cell><cell></cell><cell>0.898</cell><cell>0.495</cell><cell cols="2">0.910 0.897</cell><cell>0.834</cell><cell>0.620</cell><cell>0.742</cell><cell>0.918</cell><cell>0.955</cell><cell>0.951</cell></row><row><cell></cell><cell>Cable</cell><cell></cell><cell>0.806</cell><cell>0.513</cell><cell cols="2">0.825 0.654</cell><cell>0.478</cell><cell>0.383</cell><cell>0.558</cell><cell>0.865</cell><cell>0.909</cell><cell>0.877</cell></row><row><cell></cell><cell>Capsule</cell><cell></cell><cell>0.631</cell><cell>0.387</cell><cell cols="2">0.862 0.526</cell><cell>0.860</cell><cell>0.306</cell><cell>0.306</cell><cell>0.916</cell><cell>0.937</cell><cell>0.922</cell></row><row><cell>Objects</cell><cell cols="2">Hazelnut Metal nut Pill</cell><cell>0.861 0.705 0.725</cell><cell>0.698 0.351 0.514</cell><cell cols="2">0.917 0.878 0.830 0.576 0.893 0.769</cell><cell>0.916 0.603 0.830</cell><cell>0.698 0.320 0.776</cell><cell>0.844 0.358 0.460</cell><cell>0.937 0.895 0.935</cell><cell>0.954 0.944 0.946</cell><cell>0.943 0.945 0.965</cell></row><row><cell></cell><cell>Screw</cell><cell></cell><cell>0.604</cell><cell>0.550</cell><cell cols="2">0.754 0.559</cell><cell>0.887</cell><cell>0.466</cell><cell>0.277</cell><cell>0.928</cell><cell>0.960</cell><cell>0.930</cell></row><row><cell></cell><cell cols="3">Toothbrush 0.675</cell><cell>0.337</cell><cell cols="2">0.822 0.693</cell><cell>0.784</cell><cell>0.749</cell><cell>0.151</cell><cell>0.863</cell><cell>0.935</cell><cell>0.922</cell></row><row><cell></cell><cell cols="3">Transistor 0.680</cell><cell>0.399</cell><cell cols="2">0.728 0.626</cell><cell>0.725</cell><cell>0.549</cell><cell>0.628</cell><cell>0.701</cell><cell>0.874</cell><cell>0.695</cell></row><row><cell></cell><cell>Zipper</cell><cell></cell><cell>0.512</cell><cell>0.253</cell><cell cols="2">0.839 0.549</cell><cell>0.665</cell><cell>0.467</cell><cell>0.703</cell><cell>0.933</cell><cell>0.926</cell><cell>0.952</cell></row><row><cell></cell><cell>Mean</cell><cell></cell><cell>0.640</cell><cell>0.423</cell><cell cols="2">0.790 0.639</cell><cell>0.694</cell><cell>0.443</cell><cell>0.515</cell><cell>0.857</cell><cell>0.917</cell><cell>0.921</cell></row><row><cell cols="7">SSIM-AE l 2 -AE CAVGA-R u SPADE Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.76</cell><cell>0.74</cell><cell></cell><cell>0.85</cell><cell>0.899</cell><cell>0.913</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">Alation studies for feature matching on the MVTec-AD</cell></row><row><cell cols="7">dataset. The performance is measured by the average image-level</cell></row><row><cell cols="7">AUC-ROC (ARI ), average pixel-level AUC-ROC (ARP ) and av-</cell></row><row><cell cols="7">erage PRO across 15 categories. The best results are highlighted</cell></row><row><cell cols="2">in boldface.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell cols="2">[2, 3] [2, 3, 4] [2, 3, 4, 5]</cell></row><row><cell>AR I</cell><cell cols="5">0.808 0.917 0.934 0.819 0.849</cell><cell>0.955</cell><cell>0.949</cell></row><row><cell>AR P</cell><cell cols="5">0.915 0.953 0.957 0.860 0.950</cell><cell>0.970</cell><cell>0.969</cell></row><row><cell>PRO</cell><cell cols="5">0.815 0.897 0.835 0.504 0.886</cell><cell>0.921</cell><cell>0.886</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Alation studies for backbone network structure on the MVTec-AD dataset. The performance is measured by the average imagelevel AUC-ROC (ARI ), average pixel-level AUC-ROC (ARP ) and average PRO across 15 categories. The best results are highlighted in boldface.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Metric ResNet-18 ResNet-34 ResNet-50 Wideresnet50x2 SqueezeNet DenseNet</cell></row><row><cell></cell><cell></cell><cell>AR I</cell><cell>0.955</cell><cell>0.939</cell><cell>0.897</cell><cell>0.909</cell><cell>0.874</cell><cell>0.884</cell></row><row><cell></cell><cell></cell><cell>AR P</cell><cell>0.970</cell><cell>0.962</cell><cell>0.966</cell><cell>0.968</cell><cell>0.933</cell><cell>0.951</cell></row><row><cell></cell><cell></cell><cell>PRO</cell><cell>0.921</cell><cell>0.902</cell><cell>0.913</cell><cell>0.919</cell><cell>0.833</cell><cell>0.887</cell></row><row><cell cols="6">Table 8. Alation studies for pretrained datasets on the MVTec-AD</cell><cell></cell></row><row><cell cols="6">dataset. The performance is measured by the average image-level</cell><cell></cell></row><row><cell cols="6">AUC-ROC (ARI ), average pixel-level AUC-ROC (ARP ) and av-</cell><cell></cell></row><row><cell cols="6">erage PRO across 15 categories. The best results are highlighted</cell><cell></cell></row><row><cell>in boldface.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Metric ImageNet MNIST CIFAR-10 CIFAR-100 SVHN</cell><cell></cell></row><row><cell>AR I</cell><cell>0.955</cell><cell>0.619</cell><cell>0.826</cell><cell>0.835</cell><cell>0.796</cell><cell></cell></row><row><cell>AR P</cell><cell>0.970</cell><cell>0.759</cell><cell>0.931</cell><cell>0.937</cell><cell>0.902</cell><cell></cell></row><row><cell>PRO</cell><cell>0.921</cell><cell>0.528</cell><cell>0.863</cell><cell>0.842</cell><cell>0.742</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Ablation studies for number of training samples on the MVTecAD dataset. The performance is measured by the average image-level AUC-ROC (ARI ), average pixel-level AUC-ROC (ARP ) and average PRO across 15 categories.</figDesc><table><row><cell></cell><cell></cell><cell>5%</cell><cell></cell><cell>10%</cell></row><row><cell cols="5">Metric Ours SPADE Ours SPADE</cell></row><row><cell>AR I</cell><cell>0.871</cell><cell>0.782</cell><cell>0.907</cell><cell>0.797</cell></row><row><cell>AR P</cell><cell>0.961</cell><cell>0.932</cell><cell>0.967</cell><cell>0.955</cell></row><row><cell>PRO</cell><cell>0.892</cell><cell>0.842</cell><cell>0.913</cell><cell>0.890</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GANomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Variational autoencoder based anomaly detection using reconstruction probabiliy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>SNU Data Mining Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transfer representation-learning for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Jerone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><forename type="middle">D</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clustering and unsupervised anomaly detection with l2 normalized deep auto-encoder representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Aytekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Cricri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCNN</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep autoencoding models for unsupervised anomaly segmentation in brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mvtec AD -A comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Where&apos;s wally now? deep generative and discriminative embeddings for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Jeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Anomaly detection using one-class neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06360</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Lukas Ruffstephan Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High-dimensional and large-scale anomaly detection using a linear one-class svm with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutharshan</forename><surname>Rajasegarar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanika</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Leckie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡0.5 mb model size</title>
		<editor>Matthew W. Moskewicz Khalid Ashraf William J. Dally Kurt Keutzer Forrest N. Iandola, Song Han</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attribute restoration framework for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection -a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Metric learning for novelty and anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idoia</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Joost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One-class classifier networks for target recognition applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Moya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WCCI</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don&apos;t know? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by CNN-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raimondo</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards a hypothesis on visual transformation based selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreena</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Nallamothu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10594</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael Fauser David Sattlegger Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISIGRAPP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Informed democracy: Voting-based novelty detection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siddiqui</surname></persName>
		</author>
		<imprint>
			<publisher>Alexander Binder</publisher>
			<pubPlace>Emmanuel</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Schmidt-Erfurth. F-Anogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MED IMAGE ANAL</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NEURAL COM-PUT</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Klimscha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><forename type="middle">S</forename><surname>Gerendas René</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schlegl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00686</idno>
		<title level="m">and Georg Langs. Identifying and categorizing anomalies in retinal imaging data</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilona</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Sgarlata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Tomassini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02997</idno>
		<title level="m">Space novelty detection with variational autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08616</idno>
		<title level="m">Rajat Vikram Singh, and Abhijit Mahalanobis. Attention guided anomaly localization in images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk Jin</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05937</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarially learned anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houssam</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manon</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">C</forename><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
