<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeCaFA: Deep Convolutional Cascade for Face Alignment In The Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIP6</orgName>
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<addrLine>4 Place Jussieu</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Datakalab</orgName>
								<address>
									<addrLine>114 Boulevard Malesherbes</addrLine>
									<postCode>75017</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Datakalab</orgName>
								<address>
									<addrLine>114 Boulevard Malesherbes</addrLine>
									<postCode>75017</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ISIR</orgName>
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<addrLine>4 Place Jussieu</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIP6</orgName>
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<addrLine>4 Place Jussieu</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeCaFA: Deep Convolutional Cascade for Face Alignment In The Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face Alignment is an active computer vision domain, that consists in localizing a number of facial landmarks that vary across datasets. State-of-the-art face alignment methods either consist in end-to-end regression, or in refining the shape in a cascaded manner, starting from an initial guess. In this paper, we introduce DeCaFA, an end-to-end deep convolutional cascade architecture for face alignment. DeCaFA uses fully-convolutional stages to keep full spatial resolution throughout the cascade. Between each cascade stage, DeCaFA uses multiple chained transfer layers with spatial softmax to produce landmark-wise attention maps for each of several landmark alignment tasks. Weighted intermediate supervision, as well as efficient feature fusion between the stages allow to learn to progressively refine the attention maps in an end-to-end manner. We show experimentally that DeCaFA significantly outperforms existing approaches on 300W, CelebA and WFLW databases. In addition, we show that DeCaFA can learn fine alignment with reasonable accuracy from very few images using coarsely annotated data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face alignment consists in localizing landmarks (e.g. lips and eyes corners, pupils, nose tip) on an face image. It is an important computer vision field, as it is an essential preprocess for face recognition <ref type="bibr" target="#b15">[16]</ref>, tracking <ref type="bibr" target="#b1">[2]</ref>, expression analysis <ref type="bibr" target="#b23">[24]</ref>, and face synthesis <ref type="bibr" target="#b16">[17]</ref>.</p><p>Most recent face alignment approaches either belongs to cascaded regression methods, or to deep end-to-end regression methods. On the one's hand, cascaded regression consists in learning a sequence of updates, starting from an initial guess, to refine the landmark localization in a coarse-to-fine manner. This allows to robustly learn rigid transformations, such as translation and rotation, in the first cascade stages, while learning non-rigid deformation (e.g. due to facial expression or non-planar rotation) later on. On the other hand, many deep approaches aim at regressing the landmark position from the original image directly. However, because annotating several landmarks on a face image is a tedious task, data is rather scarce and the nature of the annotations usually vary a lot between the databases. Because of the scarcity of the data, end-to-end approaches usually rely on learning an intermediate representation, such as edges detection to drive the alignment process. However, these representations are usually ad hoc and do not guarantee to be optimal to address landmark localization tasks.</p><p>In this paper, we introduce a Deep convolutional Cascade for Face Alignment (DeCaFA). DeCaFA is composed of several stages that each produce landmark-wise attention maps, relatively to heterogeneous annotation markups. <ref type="figure" target="#fig_0">Figure 1</ref> shows attention maps extracted by the subsequent DeCaFA stages (horizontally) and for three different markups (vertically). It illustrates how these attention maps are refined through the successive stages, and how the different prediction tasks can benefit from each other. The contributions of this paper are tree-fold:</p><p>• We introduce a fully-convolutional Deep Cascade for Face Alignment (DeCaFA) that unifies cascaded regression and end-to-end deep approaches, by using landmark-wise attention maps fused to extract local information around a current landmark estimate.</p><p>• We show that intermediate supervision with increasing weights helps DeCaFA to learn coarse attention maps in its early stages, that are refined in the later stages. Through chaining multiple transfer layers, DeCaFA integrates heterogeneous data annotated with different numbers of landmarks and model the intrinsic relationship between these tasks.</p><p>• We show experimentally that DeCaFA significantly outperforms existing approaches on multiple datasets, inluding the recent WFLW database. Additionally, we highlight how coarsely annotated data helps the network to learn fine landmark alignment even with very few annotated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Popular examples of cascaded regression methods include SDM <ref type="bibr" target="#b21">[22]</ref>: in their pioneering work, Xiong et al show that using simple linear regressors upon SIFT features in a cascaded manner already provides satisfying alignment results. LBF [14] is a refinement that employs randomized decision trees to dramatically speed up feature extraction. DAN <ref type="bibr" target="#b7">[8]</ref> uses deep networks to learn each cascade stage. However, one downside of these approaches is that the update regressors are not learned jointly in a end-to-end fashion, thus there is no guarantee that the learned feature point alignment sequences might be optimal. MDM <ref type="bibr" target="#b17">[18]</ref> improves the feature extraction process by sharing the convolutional layer among all steps of the cascade that are performed through a recurrent neural network. This results in memory footprint reduction as well as better representation learning and a more optimized landmark trajectory throughout the cascade. TCDCN <ref type="bibr" target="#b24">[25]</ref> was perhaps the first end-to-end framework that could compete with cascaded regression approaches. It relies on supervised pretraining on a wide database of facial attributes. More recently, PCD-CNN <ref type="bibr" target="#b8">[9]</ref> uses head pose information to drive the training process. CPM+SBR <ref type="bibr" target="#b4">[5]</ref> employs landmark registration to regularize training. SAN <ref type="bibr" target="#b3">[4]</ref> uses adversarial networks to convert images from different styles to an aggregated style, upon which regression is performed. This aggregated style space thus serve as an intermediate representation that is more convenient for training. In <ref type="bibr" target="#b18">[19]</ref> the authors propose to use edge map estimation as an intermediate representation to drive the landmark prediction task, as well as to provide a unified representation when images are annotated in terms of different markups, that correspond to different alignment tasks. Finally, DSRN <ref type="bibr" target="#b12">[13]</ref> relies on Fourier Embedding and low-rank learning to produce such representation. However, the use of such intermediate representation is usually ad hoc and it is hard to know which one would be all-around better for face alignment. Recently, AAN <ref type="bibr" target="#b22">[23]</ref> proposes to use intermediate feature maps as attentional masks to select relevant spatial regions. It also uses intermediate supervision to constrain those maps to correspond to attention maps relatively to landmark localization. However, there is no guarantee that the network will learn to align landmarks in a cascaded, coarse-to-fine manner.</p><p>Furthermore, annotating images in term of several face landmarks is a time-consuming task. As a result, data is rather scarce and annotated in terms of varying number of landmarks. For instance, 300W database <ref type="bibr" target="#b14">[15]</ref> contains approximately 3000 images labelled with 68 landmarks for train, whereas WFLW database <ref type="bibr" target="#b18">[19]</ref> contains 7500 images with 98 landmarks. Thus, one can wonder if we can use all those images within the same framework to learn more robust landmark predictions. In <ref type="bibr" target="#b19">[20]</ref> the authors address this problem by using a classical multi-task formulation. However, this essentially ignores the intrinsic relationship between the structure of different landmark alignment tasks. Likewise, if we can predict the position of 68 landmarks, we can also easily deduce the position of landmarks for a coarser markup, such as eye/mouth corners and nose tip <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DeCaFA overview</head><p>In this Section, we introduce our Deep convolutional Cascade for Face Alignment (DeCaFA) model, as illustrated on <ref type="figure">Figure 2</ref>. DeCaFA consists of S stages, each of which contains a fully-convolutional U-net backbone that preserves the full spatial resolution, as well as an attention map generation sub-network. Section 3.1 shows how we derive landmarkwise attention maps for one landmark prediction task. Section 3.2 explains how several transfer layers can be chained to produce such attention maps, relatively to K landmark prediction tasks. the input of the next stage is obtained by applying a feature fusion algorithm that involves the attention maps, as explained in Section 3.3. In Section 3.4 we describe how DeCaFA is trained in an end-to-end manner with weighted intermediate supervision. Finally, in Section 3.5 we provide implementation details to facilitate reproducibility of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Landmark-wise attention maps</head><p>The U-net at stage i takes an input I i and gives rise to an embedding H i with parameters θ i . In order to produce a suitable embedding from H i for predicting L landmarks, we apply a 1 × 1 convolutional layer with L filters with <ref type="figure">Figure 2</ref>: DeCaFA architecture overview. Several stages with fully-convolutional U-nets are stacked, multiple transfer layers are chained and intermediate supervision with increasing weights is applied to produce landmark estimates for heterogeneous alignment tasks. Landmark-wise attention maps are fused with the input image and the embeddings of the previous stage U-net to enable end-to-end cascaded alignment.</p><formula xml:id="formula_0">U −Net block 1 U −Net block S Landmark-wise Attention maps Landmark-wise Attention maps Face cropẑ 1 L k0 fusion 1 fusion S−1 Cascade stage 1 Cascade stage Ŝ z 1 L kẑ S L k0 z S L k z S L 1 Φ 1 L 1ẑ 1 L 1 T 1 L 1 Φ 1 L k0 T 1 L k0 Φ 1 L k T 1 L k Φ S L 1 T S L 1 Φ S L k0 T S L k0 Φ S L k T S L k H 1 H S</formula><p>parameters θ i . We denote the embeddings outputted by this transfer layer as T L i . In order to highlight its dominant mode we apply a spatial softmax operator. Formally, for a pixel with coordinates (x, y) and a landmark l:</p><formula xml:id="formula_1">Φ L i (x, y, l) = exp(T L i (x, y, l)) X x=1 Y y=1 exp(T L i (x, y, l))<label>(1)</label></formula><p>An estimationẑ L i of the landmark coordinates can be obtained by computing the first order moments of Φ L i :</p><formula xml:id="formula_2">ẑ L i,x (l)=E x,y [xΦ L i (x, y, l)] z L i,y (l)=E x,y [yΦ L i (x, y, l)]<label>(2)</label></formula><p>Whereẑ L i,x andẑ L i,y are two vectors of size L containing the x and y landmark coordinatesẑ L i . The soft-argmax operator is inspired by the work in <ref type="bibr" target="#b10">[11]</ref> in the frame of human pose estimation and provides differentiable landmark coordinates estimate from the attention map Φ L i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Chaining landmark localization tasks</head><p>As it will be explained in Section 4.1, existing datasets for face alignment usually have heterogeneous annotations and varying numbers of annotated landmarks. In order to deal with these heterogeneous annotations, we integrate K tasks that consist in predicting various numbers of landmarks</p><formula xml:id="formula_3">L 1 , ...L K with ∀k 1 , k 2 , k 1 ≤ k 2 =⇒ L k1 &gt; L k2 (i.e.</formula><p>we chain the landmark-wise attention maps in an decreasing order of the number of landmarks to predict). To do so, we apply K transfer layers T L1 i , ...,</p><formula xml:id="formula_4">T L K i with parameters θ (1) i , ..., θ (K) i</formula><p>, at it is depicted on <ref type="figure" target="#fig_1">Figure 3</ref> (a). We have: </p><formula xml:id="formula_5">Φ i L 1 Φ i L k 0 Φ i L k T i L k T i L k0 T i L 1 H i L 1 Fusion Φ i L 1 Φ i L k 0 Φ i L k T i L k T i L k0 T i L 1 H i L 1 Fusion (a) Chained tasks (b) Independant tasks</formula><formula xml:id="formula_6">ẑ L k i,x (l)=E x,y [xΦ L k i (x, y, l)] ∀1 ≤ k ≤ K z L k i,y (l)=E x,y [yΦ L k i (x, y, l)] ∀1 ≤ k ≤ K<label>(3)</label></formula><p>The advantages of stacking the landmarks prediction pipelines in a descending order of the number of landmarks to be localized are two-fold: First, from a semantic perspective, who can do more can do less, meaning that it shall be easier for the network to learn the sequence of transfer layers in that order (i.e. if we can precisely localize a 68-points markup it will be easy to also localize the nose tip, as well as mouth/eyes corners). Second, labelling images with large amounts of landmarks is a tedious task, thus generally the more annotated landmarks in a database, the less images we have at our disposal. Using such architecture ensures that the former (harder) tasks benefit from all the images annotated with the latter (easier) task. This can be seen as weakly supervised learning, where images labelled in terms of coarse markups can help to learn finer alignment tasks. Also note that as these 1 × 1 convolutional layers have very few parameters, thus a lot of gradient can be backpropagated down to the U-net backbone and benefit the K prediction tasks. Finally, as illustrated on <ref type="figure" target="#fig_1">Figure 3</ref>, we use attention maps Φ L k 0 i from markup k 0 to provide richer embeddings for the subsequent stages by applying feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature fusion</head><p>In a standard feedforward deep network with S stacked stages, the i + 1 th stage takes an input I i = F 1 that corresponds to the embeddings H i outputted by the previous stage (with the convention I 0 = I the original image). By contrast, in cascade-based approaches, each stage shall learn an update to bring the feature points closer to the ground truth localizations, by using information sampled around current feature point localizations. Within an end-to-end fully-convolutional deep network, an analogous statement would be that the i + 1 th stage shall use a local embedding F 2 that is calculated using information from the original image I highlighted by landmark-wise attention maps Φ</p><formula xml:id="formula_7">L k 0 i .</formula><p>In our method, we aggregate these maps by summing all the landmark-wise attention maps</p><formula xml:id="formula_8">M i = L l=1 Φ L k 0 i</formula><p>. Thus, we can write the feature fusion model for the basic deep approach as:</p><formula xml:id="formula_9">F 1 (I, H i , M i ) = H i<label>(4)</label></formula><p>and the cascade-like approach as:</p><formula xml:id="formula_10">F 2 (I, H i , M i ) = I M i<label>(5)</label></formula><p>Where denotes the Hadamard product. This fusion scheme between the input image and the mask only preserves local information, for which the values of M i are high. Alternatively, we can reinject the original image I inside each stage so that it can use global information in case where the mask M i is not precise enough or contains localizations errors (as it is the case early in the cascade):</p><formula xml:id="formula_11">F 3 (I, H i , M i ) = I||(I M i )<label>(6)</label></formula><p>With || the channel-wise concatenation operation. Furthermore we can also fuse the relevant parts (as highlighted by mask M i ) of the embedding H i of the previous stage U-net to provide the subsequent stages a richer, more semantically abstract information to estimate the landmarks coordinates:</p><formula xml:id="formula_12">F 4 (I, H i , M i ) = I||(I M i )||(H i M i )<label>(7)</label></formula><p>Finally, we can aso use global information from not only the image I, but also from the embeddings H i :</p><formula xml:id="formula_13">F 5 (I, H i , M i ) = I||(I M i )||H i ||(H i M i ) (8)</formula><p>This fusion model is more efficient and is used in De-CaFA <ref type="figure">(Figure 2</ref>), as it allows using both global and local information around the estimated landmarks so as to learn cascade-like alignment in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning DeCaFA model</head><p>DeCaFA models can be trained end-to-end by optimizing the following loss function w.r.t. parameters of the U-nets θ i and θ</p><formula xml:id="formula_14">(1) i , ..., θ (K) i for the transfer layers T L1 i , ..., T L K i re- spectively, ∀1 ≤ k ≤ K: L(θ 1 , θ (1) 1 , ..., θ (K) 1 , ..., θ S , θ (1) S , ..., θ (K) S ) = K k=1 1 L k |ẑ L k S − z L k * |<label>(9)</label></formula><p>With z L k * the ground truth landmark position for a L klandmarks markup. In practice, the summation in equation <ref type="bibr" target="#b8">(9)</ref> have less terms since usually each example is annotated with only one markup. With this configuration, however, if the whole network is deep enough, few gradient will ever pass through the firsts attention maps. Even worse, there is no guarantee that these feature maps will correspond to landmark-wise attention maps in the early stages, which is key to ensure cascade-like behavior of DeCaFA. To ensure this, we add a differentiable soft-argmax layer after each spatial softmax and a supervised cost at stage i:</p><formula xml:id="formula_15">L(θ 1 , θ (1) 1 , ..., θ (K) 1 , ..., θ S , θ (1) S , ..., θ (K) S ) = S i=1 λ i K k=1 1 L k |ẑ L k i − z L k * |<label>(10)</label></formula><p>In practice, we use a L 1 loss function, as it has been shown to overfit less on very bad examples and lead to more precise results for face alignment. However, we need to make sure that the (relatively) shallow sub-networks does not overfit on these losses, which would result in very narrow heat maps with very localized dominant modes early in the cascade, and thus an overall lower accuracy. This is ensured by applying increasing λ i weights in (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation details</head><p>The DeCaFA models that will be investigated below use 1 to 4 stages that each contains 12 3 × 3 convolutional layers with 64 → 64 → 128 → 128 → 256 → 256 channels for the downsampling portion, and vice-versa for the upsampling portion. The input images are resized to 128 × 128 grayscale images prior to being processed by the network. Each convolution is followed by a batch normalization layer with ReLU activation. In order to generate smooth feature maps we do not use transposed convolution but bilinear image upsampling followed with 3 × 3 convolutional layers. The whole architecture is trained using ADAM optimizer with a 5e −4 learning rate with momentum 0.9 and learning rate annealing with power 0.9. We apply 400000 updates with batch size 8 for each database, with alternating updates between the databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The 300W database, introduced in <ref type="bibr" target="#b14">[15]</ref>, is considered as the benchmark dataset for training and testing face alignment models, with moderate variations in head pose, facial expressions and illuminations. It consists in four databases: LFPW (811 images for train / 224 images for test), HELEN (2000 images for train / 330 images for test), AFW (337 images for train) and IBUG (135 images for test), for a total of 3148 images annotated with 68 landmarks for training the models. For comparison with state-of-the art methods, we refer to LFPW and HELEN test sets as the common subset and I-BUG as the challenging subset of 300W.</p><p>The CelebA database <ref type="bibr" target="#b9">[10]</ref> is a large-scale face attribute database which contains 202599 celebrity images coming from 10177 identities, each annotated with 40 binary attributes and the localization of 5 landmarks (nose, left and right pupils, mouth corners). In our experiments, we train our models using the train partition that contains 162770 images from 8k identities. The test partition contains 19962 instances from 1k identities that are not seen in the train set.</p><p>The Wider Facial Landmarks in the Wild or WFLW database <ref type="bibr" target="#b18">[19]</ref> contains 10000 faces (7500 for training and 2500 for testing) with 98 annotated landmarks. This database also features rich attribute annotations in terms of occlusion, head pose, make-up, illumination, blur and expressions.</p><p>In what follows, we train our models using the train partition of 300W, WFLW and CelebA, and evaluate of the test partition of these datasets. As in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7]</ref> we measure the average point-to-point distance between feature points (ME), normalized by the inter-ocular distance (distance between outer eye corners on ground truth markup). As there is no consensus on how to measure the error we also report AUC and failure rates for a maximum error of 0.1, as well as cumulative error distribution (CED) curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head><p>In this section, we validate the architecture and hyperparameters of our model: the number of stages S, the number of landmark prediction tasks K, the fusion and task ordering scheme as well as the intermediate supervision weights. <ref type="figure" target="#fig_2">Figure 4</ref> shows CED curves for models with S = 1, 2, 3 and 4 cascade stages. The accuracy steadily increases as we add more stages, and saturates after the third on LFPW and HELEN, which is a well-known behavior of cascaded models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">14]</ref>, showing that DeCaFA with weighted intermediate supervision indeed works as a cascade, by first providing coarse estimates and refining in the later stages. On IBUG, this difference is more conspicuous, thus there is for improvement by stacking more cascade stages. <ref type="figure" target="#fig_3">Figure 5</ref> shows the interest of chaining multiple tasks, most notably on LFPW, that contains low-resolution images, and IBUG, which contains strong head pose variations as well as occlusions. Coarsely annotated data (5 landmarks) significantly helps the fine-grained landmark localization, as it is integrated a kind of weakly supervised scheme. This will be discussed more thoroughly in Section 4.4. <ref type="table" target="#tab_0">Table 1</ref> shows a comparison between multiple fusion, task ordering and intermediate supervision weighting schemes. We test our model on 300W (full and challenging), WFLW (All and challenging, i.e. pose subset) as well as CelebA and report the average accuracy on those 5 subsets. First, reinjecting the whole input image (F 3 -Equation (6) vs F 2 -Equation <ref type="formula" target="#formula_10">(5)</ref>) significantly improves the accuracy on challenging data such as 300W-challenging or WFLW-pose, where the first cascade stages may commit errors. F 4 -Equation <ref type="bibr" target="#b6">(7)</ref> and F 3 fusion (cascaded models) using local+global information rivals the basic deep approach F 1 -Equation (4). Furthermore, F 5 -Equation <ref type="formula">(8)</ref> fusion, which uses local and global cues is the best by a significant margin. Furthermore, chaining the transfer layers <ref type="figure" target="#fig_1">(Figure 3-a)</ref> is better than using independant transfer layers <ref type="figure" target="#fig_1">(Figure 3</ref>b): likewise, in such a case, the first transfer layer benefits from the gradients from the subsequents layer at train time. Last but not least, using increasing intermediate supervision weights in Equation (10) (i.e. λ 1 = 1/8, λ 2 = 1/4, λ 3 = 1/2, λ 4 = 1) is better than both using constant weights ( λ 1 = λ 2 = λ 3 = λ 4 = 1) and decreasing weights (λ 1 = 1, λ 2 = 1/2, λ 3 = 1/4, λ 4 = 1/8), as it enables proper cascade-like training of the network, with the first stage outputting coarser attention maps that can be refined in the latter stages of the network.    <ref type="table" target="#tab_2">Table 3</ref> shows a comparison between DeCaFA and recent state-of-the-art approaches on 300W database. Our approach performs better than most existing approaches on the common subset, and performs very close to its best contenders on the challenging subset. Note that DeCaFA trained only on 300W trainset has a ME of 3.69% and is already very competitive with recent approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref>, thanks to its end-to-end cascade architecture. DeCaFA is competitive with the best approaches, LAB <ref type="bibr" target="#b18">[19]</ref> and DAN-MENPO <ref type="bibr" target="#b7">[8]</ref> as well as JMFA-MENPO <ref type="bibr" target="#b2">[3]</ref>, which also use external data. <ref type="table" target="#tab_1">Table 2</ref> shows a comparison between our method and LAB <ref type="bibr" target="#b18">[19]</ref> on WFLW database. As in <ref type="bibr" target="#b18">[19]</ref> we report the aver-age point-to-point error on WFLW test partition, normalized by the outer eye corners. We also report the error on multiple test subsets containing variations in head pose, facial expressions, illumination, make-up as well as partial occlusions and occasional blur. DeCaFA performs better than LAB <ref type="bibr" target="#b18">[19]</ref> and Wing <ref type="bibr" target="#b5">[6]</ref> by a significant margin on every subset. Also, note that DeCaFA trained solely on WFLW already as a ME of 5.01 on the whole test set, which is still better that these two methods. Lastly, there is room for improvement on this benchmark as we do not excplicitely handle any of the factors of variation such as pose or occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with state-of-the-art methods</head><p>Finally, <ref type="table" target="#tab_4">Table 5</ref> shows a comparison of our method and state-of-the-art approaches on CelebA. As in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref> we report the average point-to-point error on the test partition, normalized by the distance between the two eye centers. Our    <ref type="bibr" target="#b25">[26]</ref> 49.87 5.08 Densereg+MDM <ref type="bibr" target="#b0">[1]</ref> 52. <ref type="bibr" target="#b18">19</ref> 3.67 JMFA <ref type="bibr" target="#b2">[3]</ref> 54.9 1.00 JMFA-MENPO <ref type="bibr" target="#b2">[3]</ref> 60.7 0.33 LAB <ref type="bibr" target="#b18">[19]</ref> 58.9 0.83 DeCaFA 0.661 0.15  <ref type="bibr" target="#b21">[22]</ref> 4.35 CFSS <ref type="bibr" target="#b25">[26]</ref> 3,95 DSRN <ref type="bibr" target="#b12">[13]</ref> 3.08 AAN <ref type="bibr" target="#b22">[23]</ref> 2.99 DeCaFA 2.10 approach is the best by a significant margin. Noteworthy, even though we use auxiliary data from 300W and WFLW, we do not use data from the val partition of CelebA, contrary to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, thus there is significant room for improvement.</p><p>Overall, DeCaFA sets a new state-of-the-art on the three databases with several evaluation metrics. Also notice that it embraces few parameters (≈ 10M ) compared to state-of-theart approaches, and is also modular: at test time, DeCaFA allows to find a good trade-of between speed and accuracy (by evaluating only a fraction of the cascade), as well as to predict various numbers of landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Weakly supervised learning</head><p>In this Section, we study the capability of DeCaFA to learn with few examples annotated with 68 and 98-landmarks.</p><p>To do so, we train DeCaFA using only a small, randomly sampled fraction of 300W (100 and 500 images, 3% and 15% of trainset) and WFLW (100 and 500 images, 1% and 6% of trainset) and the whole CelebA trainset, and report results on 300W and WFLW testsets on <ref type="figure">Figure 6</ref>.</p><p>Using coarsely annotated data from CelebA allows to substantially improve the landmark localization on both datasets, most notably when the number of training images is very low. For instance, DeCaFA trained with 3% of 300W trainset and 1% of WFLW trainset already outputs decent fine-grained landmark estimations, as it is better than CFSS <ref type="bibr" target="#b25">[26]</ref> and DVLN ( <ref type="bibr" target="#b19">[20]</ref>, see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative results</head><p>Image 7 shows vizualisations of aligned facial landmarks on WFLW, I-BUG and CelebA, as well as visualizations of attention maps after the cascade stages 1 and 4. Notice how these attention maps are coarse after stage 1 and get refined after stage 4, better highlighting the individual landmarks. Also notice that the predicted landmarks are close to the corresponding ground truth, even in the presence of rotations and occlusions (WFLW) or facial expressions (CelebA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduced DeCaFA for face alignment. DeCaFA unifies cascaded regression approaches and an endto-end trainable deep methods. Its fully-convolutional U-net backbone ensures to keep full spatial resolution throughout the network, and the intermediate supervisions between the cascade stages with increasing weights ensures that the network learns cascaded alignment. Furthermore, by chaining multiple transfer layers to produce attention maps that correspond to multiple alignment tasks, DeCaFA can benefit from heterogeneous data. We empirically show that DeCaFA significantly outperforms state-of-the-art approaches on 300W, CelebA and WFLW databases. In addition, DeCaFA architecture is very modular and is suited for weakly supervised learning using coarsely annotated data with few landmarks. Future work will consist in integrating other sources of data, or possibly other representations and tasks, such as head pose estimation, partial occlusion handling, as well as facial expressions, Action Unit and/or attributes (such as age or gender estimation) recognition within DeCaFA framework. Furthemore, we will study the application of DeCaFA to closely related fields, such as human pose estimation.  <ref type="figure">Figure 7</ref>: From left to right: images, attention maps outputted by stages 1 and 4, alignment results, and ground truth for images from 300W (I-bug, 68 landmarks) and WFLW (98 landmarks). Notice how the summed attention maps are iteratively refined, and how closely the predicted landmarks usually matches the ground truth, even under difficult illumination, non-frontal head poses, make-up, or occlusions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>DeCaFA estimates landmark-wise attention maps at several stages of its architecture (horizontally: stages 1 to 4). By chaining transfer layers, it can integrate heterogeneous data (Vertically: attention maps and predictions for 98, 68 and 5-landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Chained (left) vs independant (right) task order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison is terms of Cumulative error distribution (CED) curves on 300W of models with S = 1,2,3 and 4 stages. As we stack cascade stages, the accuracy increases and saturates after the third/fourth stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>CED curves for models with K = 1,2 and 3 landmark prediction tasks. Models trained with multiple alignment tasks are significantly better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean error (%) comparison for multiple fusion, task ordering and intermediate supervision weighting schemes (lower is better). DeCaFA with F 5 fusion, chained tasks and increasing intermediate supervision weights performs better overall.</figDesc><table><row><cell cols="2">Fusion task order</cell><cell cols="6">weights λ i 300W-Full 300W-Challenging WFLW-All WFLW-Pose CelebA Avg</cell></row><row><cell>F 1</cell><cell>chained</cell><cell>↑</cell><cell>3.36</cell><cell>5.27</cell><cell>4.71</cell><cell>8.3</cell><cell>2.53 4.83</cell></row><row><cell>F 2</cell><cell>chained</cell><cell>↑</cell><cell>3.45</cell><cell>5.45</cell><cell>4.83</cell><cell>8.78</cell><cell>2.70 5.04</cell></row><row><cell>F 3</cell><cell>chained</cell><cell>↑</cell><cell>3.43</cell><cell>5.38</cell><cell>4.76</cell><cell>8.39</cell><cell>2.08 4.81</cell></row><row><cell>F 4</cell><cell>chained</cell><cell>↑</cell><cell>3.40</cell><cell>5.31</cell><cell>4.65</cell><cell>8.25</cell><cell>2.41 4.80</cell></row><row><cell>F 5</cell><cell>chained</cell><cell>↑</cell><cell>3.39</cell><cell>5.26</cell><cell>4.62</cell><cell>8.11</cell><cell>2.10 4.69</cell></row><row><cell>F 5</cell><cell cols="2">independant ↑</cell><cell>3.41</cell><cell>5.31</cell><cell>4.68</cell><cell>8.21</cell><cell>2.16 4.75</cell></row><row><cell>F 5</cell><cell>chained</cell><cell>=</cell><cell>3.41</cell><cell>5.33</cell><cell>4.84</cell><cell>8.77</cell><cell>2.19 4.91</cell></row><row><cell>F 5</cell><cell>chained</cell><cell>↓</cell><cell>3.46</cell><cell>5.45</cell><cell>5.04</cell><cell>9.06</cell><cell>2.23 5.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison in terms of Mean error (lower is better), AUC (higher is better) as well as failure rate (lower is better), on WFLW.</figDesc><table><row><cell>metric</cell><cell>method</cell><cell cols="6">all head pose expression illumination make-up occlusion</cell><cell>blur</cell></row><row><cell>ME (%)</cell><cell>CFSS [26]</cell><cell>9.07</cell><cell>21.36</cell><cell>10.09</cell><cell>8.30</cell><cell>8.74</cell><cell>11.76</cell><cell>9.96</cell></row><row><cell></cell><cell cols="2">DVLN [20] 10.84</cell><cell>46.93</cell><cell>11.15</cell><cell>7.31</cell><cell>11.65</cell><cell cols="2">16.30 13.71</cell></row><row><cell></cell><cell>LAB [19]</cell><cell>5,27</cell><cell>10,24</cell><cell>5,51</cell><cell>5,23</cell><cell>5,15</cell><cell>6,79</cell><cell>6,32</cell></row><row><cell></cell><cell>Wing [6]</cell><cell>5.11</cell><cell>8.75</cell><cell>5.36</cell><cell>4.93</cell><cell>5.41</cell><cell>6.37</cell><cell>5.81</cell></row><row><cell></cell><cell>DeCaFA</cell><cell>4.62</cell><cell>8.11</cell><cell>4.65</cell><cell>4.41</cell><cell>4.63</cell><cell>5.74</cell><cell>5.38</cell></row><row><cell>AUC@0.1</cell><cell>CFSS [26]</cell><cell>0.366</cell><cell>0.063</cell><cell>0.316</cell><cell>0.385</cell><cell>0.369</cell><cell cols="2">0.269 0.303</cell></row><row><cell></cell><cell cols="2">DVLN [20] 0.456</cell><cell>0.147</cell><cell>0.389</cell><cell>0.474</cell><cell>0.449</cell><cell cols="2">0.379 0.397</cell></row><row><cell></cell><cell>LAB [19]</cell><cell>0.532</cell><cell>0.235</cell><cell>0.495</cell><cell>0.543</cell><cell>0.539</cell><cell cols="2">0.449 0.463</cell></row><row><cell></cell><cell>Wing [6]</cell><cell>0.554</cell><cell>0.310</cell><cell>0.496</cell><cell>0.541</cell><cell>0.558</cell><cell cols="2">0.489 0.492</cell></row><row><cell></cell><cell>DeCaFA</cell><cell>0.563</cell><cell>0.292</cell><cell>0.546</cell><cell>0.579</cell><cell>0.575</cell><cell cols="2">0.485 0.494</cell></row><row><cell cols="2">FR@0.1(%) CFSS [26]</cell><cell>20.56</cell><cell>66.26</cell><cell>23.25</cell><cell>17.34</cell><cell>21.84</cell><cell cols="2">32.88 23.67</cell></row><row><cell></cell><cell cols="2">DVLN [20] 10.84</cell><cell>46.93</cell><cell>11.15</cell><cell>7.31</cell><cell>11.65</cell><cell cols="2">16.30 13.71</cell></row><row><cell></cell><cell>LAB [19]</cell><cell>7.56</cell><cell>28.83</cell><cell>6.37</cell><cell>6.73</cell><cell>7.77</cell><cell cols="2">13.72 10,74</cell></row><row><cell></cell><cell>Wing [6]</cell><cell>6.00</cell><cell>22.70</cell><cell>4.78</cell><cell>4.30</cell><cell>7.77</cell><cell>12.50</cell><cell>7.76</cell></row><row><cell></cell><cell>DeCaFA</cell><cell>4.84</cell><cell>21.4</cell><cell>3.73</cell><cell>3.22</cell><cell>6.15</cell><cell>9.26</cell><cell>6.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">: Mean error (ME %) comparison on 300W (lower is</cell></row><row><cell>better).</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Common Chall.</cell><cell>full</cell></row><row><cell>PCD-CNN [9]</cell><cell>3.67</cell><cell cols="2">7.62 4.44</cell></row><row><cell>CPM+SBR [5]</cell><cell>3.28</cell><cell cols="2">7.58 4.10</cell></row><row><cell>SAN [4]</cell><cell>3.34</cell><cell cols="2">6.60 3.98</cell></row><row><cell>DAN [8]</cell><cell>3.19</cell><cell cols="2">5.24 3.59</cell></row><row><cell>LAB [19]</cell><cell>2.98</cell><cell cols="2">5.19 3.49</cell></row><row><cell>DAN-MENPO [8]</cell><cell>3.09</cell><cell cols="2">4.88 3.44</cell></row><row><cell>DeCaFA</cell><cell>2.93</cell><cell cols="2">5.26 3.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: AUC and Failure rates (FR %) for a maximum error of</cell></row><row><cell cols="2">0.1, and comparison with state-of-the-art approaches on 300W.</cell></row><row><cell>Method</cell><cell>AUC@0.1 FR @0.1 (%)</cell></row><row><cell>CFSS</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art approaches on CelebA database (lower is better).</figDesc><table><row><cell>Method</cell><cell>Mean error (%)</cell></row><row><cell>SDM</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 )</head><label>2</label><figDesc>on WFLW. DeCaFA trained with 15% of 300W trainset and 6% of WFLW trainset is on par with SAN on 300W ([4], seeTable 3), and is substantially better than DVLN on WFLW. This indicates that weakly accuracy for the more precise tasks of predicting 68 and 98 landmarks. Thus, due to the chaining of multiple transfer layers, our DeCaFA architecture is well suited for this kind of weakly supervised learning and can be trained at a lower cost with coarsely annotated examples.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Test on 300W</cell><cell></cell><cell></cell><cell></cell><cell>Test on WFLW</cell></row><row><cell>Mean error (%)</cell><cell>3.5 4 4.5 5 5.5 6</cell><cell></cell><cell></cell><cell>0 % CelebA 100 % CelebA Without CelebA With CelebA</cell><cell>5 6 7 8 9 11 10 Mean error (%)</cell><cell></cell><cell></cell><cell>0 % CelebA Without CelebA 100 % CelebA With CelebA</cell></row><row><cell></cell><cell>3</cell><cell>3</cell><cell>15</cell><cell>100</cell><cell>4</cell><cell>1</cell><cell>6</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell cols="2">% data from 300W in train</cell><cell></cell><cell></cell><cell></cell><cell>% data from WFLW in train</cell></row><row><cell cols="9">Figure 6: % mean error comparison when training with small fraction of THE training set and coarsely annotated examples</cell></row><row><cell cols="2">from CelebA.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">supervised learning with examples from CelebA, that are</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">coarsely annotated in terms of 5 landmarks, can significantly</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">improve the prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comprehensive performance evaluation of deformable face tracking &quot;in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Joint multi-view face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06023</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving Landmark Localization with Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A deep regression architecture with two-stage re-initialization for high performance facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Direct shape regression networks for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 FPS via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">300 Faces In-The-Wild Challenge: database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mnemonic Descent Method: A Recurrent Process Applied for End-to-End Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Leveraging intra and inter-dataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Robust Facial Landmark Detection via Recurrent Attentive-Refinement Networks. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attentional alignment network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilateral ordinal relevance multi-instance regression for facial action unit intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
