<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andru</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherif</forename><surname>Shehata</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Mutter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Marescaux</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>De Mathelin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
						</author>
						<title level="a" type="main">EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Laparoscopic videos</term>
					<term>cholecystectomy</term>
					<term>convolu- tional neural network</term>
					<term>tool presence detection</term>
					<term>phase recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the visual features used are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool usage signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Extensive experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I n the community of computer-assisted interventions (CAI), recognition of the surgical workflow is an important topic because it offers solutions to numerous demands of the modern operating room (OR) <ref type="bibr" target="#b0">[1]</ref>. For instance, such recognition is an essential component to develop context-aware systems that can monitor the surgical processes, optimize OR and staff scheduling, and provide automated assistance to the clinical staff. With the ability to segment surgical workflows, it would also be possible to automate the indexing of surgical video databases, which is currently a time-consuming manual process. In the long run, through finer analysis of the video content, such context-aware systems could also be used to alert the clinicians to probable upcoming complications.</p><p>Various types of features have been used in the literature to carry out the phase recognition task. For instance, in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, binary tool usage signals were used to perform phase recognition on cholecystectomy procedures. In more recent Andru P. Twinanda, Sherif Shehata, Michel de Mathelin, and Nicolas Padoy are affiliated with ICube, University of Strasbourg, CNRS, IHU Strasbourg, France (email: twinanda@unistra.fr) Didier Mutter and Jacques Marescaux are affiliated with the University Hospital of Strasbourg, IRCAD and IHU Strasbourg, France. studies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, surgical triplets (consisting of the utilized tool, the anatomical structure, and the surgical action) were used to represent the frame at each time step in a surgery. However, these features are typically obtained through a manual annotation process, which is virtually impossible to perform at test time. Despite existing efforts <ref type="bibr" target="#b5">[6]</ref>, it is still an open question whether such information can be obtained reliably in an automatic manner.</p><p>Another feature type that is typically used to perform the phase recognition task is visual features, such as pixel values and intensity gradients <ref type="bibr" target="#b6">[7]</ref>, spatio-temporal features <ref type="bibr" target="#b7">[8]</ref>, and a combination of features (color, texture, and shape) <ref type="bibr" target="#b8">[9]</ref>. However, these features are handcrafted, i.e., they are empirically designed to capture certain information from the images, leading to the loss of other possibly significant characteristics during the feature extraction process.</p><p>In this paper, we present a novel method for phase recognition that overcomes the afore-mentioned limitations. First, instead of using handcrafted features, we propose to learn inherent visual features from surgical (specifically cholecystectomy) videos to perform phase recognition. We focus on visual features because videos are typically the only source of information that is readily available in the OR. In particular, we propose to learn the features using a convolutional neural network (CNN), because CNNs have dramatically improved the results for various image recognition tasks in recent years, such as image classification <ref type="bibr" target="#b9">[10]</ref> and object detection <ref type="bibr" target="#b10">[11]</ref>. In addition, it is advantageous to automatically learn the features from laparoscopic videos because of the visual challenges inherent in them, which make it difficult to design suitable features. For example, the camera in laparoscopic procedures is not static, resulting in motion blur and high variability of the observed scenes along the surgery. The lens is also often stained by blood which can blur or completely occlude the scene captured by the laparoscopic camera. Second, based on our and others' promising results of using tool usage signals to perform phase recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we hypothesize that tool information can be additionally utilized to generate more discriminative features for the phase recognition task. This has also been shown in <ref type="bibr" target="#b6">[7]</ref>, where the tool usage signals are used to reduce the dimension of the handcrafted visual features through canonical correlation analysis (CCA) in order to obtain more semantically meaningful and discriminative features. To incorporate the tool information, we propose to implement a multi-task framework in the feature learning process. The resulting CNN architecture, that we call EndoNet, is designed to jointly perform the phase recognition and tool presence detection tasks. The latter is the task of automatically determining all types of tools present in an image. In addition to helping EndoNet learn more discriminative features, the tool presence detection task itself is also interesting to perform because it could be exploited for many applications, for instance to automatically index a surgical video database by labeling the tool presence in the videos. Combined with other signals, it could also be used to identify a potential upcoming complication by detecting tools that should not appear in a certain phase. It is important to note that this task differs from the usual tool detection task <ref type="bibr" target="#b12">[13]</ref>, because it does not require tool localization. In addition, the tool presence is solely determined by the visual information from the laparoscopic videos. Thus, it does not result in the same tool information as the one used in <ref type="bibr" target="#b2">[3]</ref>, which cannot always be obtained from the laparoscopic videos alone. For example, the presence of trocars used in <ref type="bibr" target="#b2">[3]</ref> is not always apparent in the laparoscopic videos. Automatic presence detection for such tools would require another source of information, e.g., an external video.</p><p>Training CNN architectures requires a substantial capacity of parallel computing and a large amount of labeled data. In the domain of medicine, labeled data is particularly difficult to obtain due to regulatory restrictions and the cost of manual annotation. Girshick et al. <ref type="bibr" target="#b10">[11]</ref> recently showed that transfer learning can be used to train a network when labeled data is scarce. Inspired by <ref type="bibr" target="#b10">[11]</ref>, we perform transfer learning to train the proposed EndoNet architecture.</p><p>To validate our method, we build a large dataset of cholecystectomy videos containing 80 videos recorded at the University Hospital of Strasbourg. In addition, to demonstrate that our proposed (i.e., EndoNet) features are generalizable, we carry out additional experiments on the EndoVis workflow challenge dataset 1 containing seven cholecystectomy videos recorded at the Hospital Klinikum Rechts der Isar in Munich. Through extensive comparisons, we also show that EndoNet outperforms other state-of-the-art methods. Moreover, we also demonstrate that training the network in a multi-task manner results in a better network than training in a single-task manner.</p><p>In summary, the contributions of this paper are five-fold: (1) for the first time, CNNs are utilized to extract visual features for recognition tasks on laparoscopic videos, (2) we design a CNN architecture that jointly performs the phase recognition and tool presence detection tasks, (3) we present a wide range of comparisons between our method and other approaches, (4) we show state-of-the-art results for both tasks on cholecystectomy videos using solely visual features, and <ref type="bibr" target="#b4">(5)</ref> we demonstrate the feasibility of using EndoNet in addressing several practical CAI applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tool Presence Detection</head><p>The literature addressing the problem of automatic tool presence detection in the CAI community is still limited. The approaches typically focus on other tasks, such as tool detection <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, tool pose estimation <ref type="bibr" target="#b14">[15]</ref>, and tool tracking <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. In addition, most of the methods are only tested on 1 http://grand-challenge.org/site/endovissub-workflow/data/ short sequences, while we carry out the task on the complete procedures.</p><p>In recent studies <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, radio frequency identification (RFID)-tagged surgical tools have been proposed for tool detection and tracking. Such an active tracking system can be used to solve the tool presence detection problem, but this system is complex to integrate into the OR. Thus, it is interesting to investigate other features that are already available in the OR, e.g., visual cues from the videos. For instance, in <ref type="bibr" target="#b19">[20]</ref>, Speidel et al. presented an approach to automatically recognize the types of the tools that appear in laparoscopic images. However, the method consists of many steps, such as tool segmentation and contour processing. In addition, it also requires the 3D models of the tools to perform the tool categorization. In a more recent work <ref type="bibr" target="#b8">[9]</ref>, Lalys et al. proposed to use an approach based on the Viola-Jones object detection framework to automatically detect the tools in cataract surgeries, such as the knife and Intra Ocular Lens instruments. However, the tool presence detection problem on laparoscopic videos poses other challenges that do not appear in cataract surgeries where the camera is static and the tools are not articulated. In this paper, we propose a more direct approach to perform the tool presence detection task by using only visual features without localization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Phase Recognition</head><p>The phase recognition task has been addressed in several types of surgeries, ranging from cataract <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>, neurological <ref type="bibr" target="#b4">[5]</ref>, to laparoscopic surgeries <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Multiple types of features have also been explored to carry out the task, such as tool usage signals <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, surgical action triplets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and visual features <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Since we propose to carry out the task relying solely on the visual features, we focus the literature discussion on methods that use the visual features.</p><p>In <ref type="bibr" target="#b24">[25]</ref>, Padoy et al. proposed an online phase recognition method based on Hidden Markov Model (HMM) that combines the tool usage signals and two visual cues from the laparoscopic images. The first and second cues respectively indicate whether the camera is inside the patient's body and whether clips are in the field of view. However, to recognize the phase, this method requires the tool signals which are not always immediately available in the OR. Instead, Blum et al. <ref type="bibr" target="#b6">[7]</ref> proposed to use the tool usage signals to perform dimensionality reduction on the visual features using CCA. Once the projection function is obtained, the tool information is not required anymore to estimate the surgical phase. At test time, the visual features are mapped to the common space and then later used to determine the phase. The method performed well, resulting in an accuracy of 76%. However, it has only been tested on a dataset of 10 videos. In addition, the method is potentially limited by the choice of handcrafted features that are used: horizontal and vertical gradient magnitudes, histograms and the pixel values of the downsampled image.</p><p>In a more recent work <ref type="bibr" target="#b8">[9]</ref>, Lalys et al. presented a framework to recognize high-level surgical tasks for cataract surgeries using a combination of visual information: shape, color, texture, and mixed information. The features also contain the tool presence information which is automatically extracted from the microscopic videos, as mentioned in Subsection II-A. By using HMM on top of the features, the method yields 91% accuracy. However, the method was evaluated on cataract surgeries, which are substantially different from cholecystectomy surgeries. Cholecystectomy surgeries are generally longer than cataract surgeries. In addition, cholecystectomy videos have visual challenges that are not present in cataract surgeries, such as rapid camera motions, the presence of smoke, and the presence of more articulated tools. In <ref type="bibr" target="#b25">[26]</ref>, Lea et al. used skip-chain conditional random field on top of kinematic and image features to segment and recognize fine-grained surgical activities, such as needle insertion and tying knot. However, the method is tested on a dataset that contains short sequences (around two minutes). Furthermore, the visual features that are utilized in the afore-mentioned methods are handcrafted.</p><p>In <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr">Klank et al.</ref> proposed to learn automatically the visual features from cholecystectomy videos to carry out the phase recognition task. The approach is based on genetic programming that mutates and crosses the features using predefined operators. The method is therefore limited by the set of predefined operators. In addition, the learnt features failed to give better recognition results than the handcrafted features in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convolutional Neural Networks</head><p>In the computer vision community, convolutional neural networks (CNNs) are currently one of the most successful feature learning methods in performing various tasks. For instance, Krizhevsky et al. <ref type="bibr" target="#b9">[10]</ref> addressed the image classification problem on the massive ImageNet database <ref type="bibr" target="#b27">[28]</ref> by proposing to use a CNN architecture, referred to as AlexNet. They showed that the features learnt by the CNN dramatically improve the classification results compared to the state-of-theart handcrafted features, e.g., Fisher Vector on SIFT <ref type="bibr" target="#b28">[29]</ref>. Furthermore, in <ref type="bibr" target="#b29">[30]</ref>, it has been shown that the network trained in <ref type="bibr" target="#b9">[10]</ref> is so powerful that it can be used as a blackbox feature extractor (without any modification) to successfully perform several tasks, including scene classification and domain adaptation.</p><p>CNNs are hard to train because they typically contain a high number of unknowns. For instance, the AlexNet architecture contains over 60M parameters. It is essential to have a high computational power and a huge amount of annotated data to train the networks. Recently, Girshick et al. <ref type="bibr" target="#b10">[11]</ref> showed that a new network can be learnt despite the scarcity of labeled data by performing transfer learning. They proposed to take a pretrained CNN model as initialization and fine-tune the model to obtain a new network. It is shown that the fine-tuned network yielded a state-of-the-art performance for object recognition task, despite being fine-tuned on a network trained for image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The complete pipeline of our proposed approach is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The first step is to train the EndoNet architecture via a fine-tuning process. Once the network is trained, it is used for both the tool presence detection and phase recognition tasks. For the former, the confidence given by the network is directly used to perform the task. For the latter, the network is used to extract the visual features from the images. These features are then passed to the Support Vector Machine (SVM) and Hierarchical HMM to obtain the final estimated phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EndoNet Architecture</head><p>The EndoNet architecture is designed based on two assumptions, which will be confirmed by the experiments presented in Section V:</p><p>• more discriminative features for the phase recognition task can be learnt from the dataset if the network is fine-tuned in a multi-task manner, i.e., if the network is optimized to carry out not only phase recognition, but also tool presence detection; • since the tool signals have been successfully used to carry out phase recognition in previous work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, the inclusion of automatically generated tool detection signals in the final feature can improve the recognition. The proposed EndoNet architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The architecture is an extension of the AlexNet architecture <ref type="bibr" target="#b9">[10]</ref>, which consists of an input layer (in green), five convolutional layers (in red, conv1-conv5), and two fully-connected layers (in orange, fc6-fc7). The output of layer fc7 is connected to a fully-connected layer fc tool, which performs the tool presence detection. Since there are seven tools defined in the dataset used to train the network, the layer fc tool contains 7 nodes, where each node represents the confidence for a tool to be present in the image. This confidence is later concatenated with the output of layer fc7 in layer fc8 to construct the final feature for the phase recognition. Ultimately, the output of layer fc8 is connected to layer fc phase containing 7 nodes, where each node represents the confidence that an image belongs to the corresponding phase. The surgical tool types and the surgical phases are described in Subsection IV-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fine-Tuning</head><p>The network is trained using stochastic gradient descent with two loss functions defined for the tasks. The tool presence detection task is formulated as N t binary classification tasks, where N t = 7 is the number of tools. For each binary classification task, the cross-entropy function is used to compute the loss. Thus for N i images in the batch, the complete loss function of the tool presence detection task for all tools is defined as:</p><formula xml:id="formula_0">L T = −1 N i Nt t=1 Ni i=1 k i t log σ v i t + 1 − k i t log 1 − σ v i t , (1) where i ∈ {1, . . . , N i } and t ∈ {1, . . . , N t } are respec- tively the image and tool indices, k i t ∈ {0, 1}</formula><p>and v i t are respectively the ground truth of tool presence and the output of layer fc tool corresponding to tool t and image i, and σ (·) ∈ (0, 1) is the sigmoid function.</p><p>Phase recognition is regarded as a multi-class classification task. The softmax multinomial logistic function, which is an extension of the cross-entropy function, is utilized to compute the loss. The function is formulated as:</p><formula xml:id="formula_1">L P = −1 N i Ni i=1 Np p=1 l i p log ϕ w i p ,<label>(2)</label></formula><p>where p ∈ {1, . . . , N p } is the phase index and N p = 7 is the number of phases, l i p ∈ {0, 1} and w i p are respectively the ground truth of the phases and the output of layer fc phase corresponding to phase p and image i, and ϕ (·) ∈ [0, 1] is the softmax function.</p><p>The final loss function is the summation of both losses: L = a · L T + b · L P , where a and b are weighting coefficients. In this work, we set a = b = 1 as preliminary experiments have shown no improvement when varying these parameters. One should note that assigning either a = 0 or b = 0 is equivalent to designing a CNN that is optimized to carry out only the phase recognition task or the tool presence detection task, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SVM and Hierarchical HMM</head><p>The output of layer fc8 is taken as the image feature. These features are used to compute confidence values v p ∈ R 7 for phase estimation using a one-vs-all multi-class SVM. Since the confidence v p is obtained without taking into account any temporal information, it is necessary to enforce the temporal constraint of the surgical workflow. Here, we use use an extension of HMM, namely a two-level Hierarchical HMM (HHMM) <ref type="bibr" target="#b30">[31]</ref>. The top-level contains nodes that model the inter-phase dependencies, while the bottom-level nodes model the intra-phase dependencies. We train the HHMM adopting the learning process presented in <ref type="bibr" target="#b30">[31]</ref>. Here, the observations are given by the confidence v p from the SVM. For offline recognition, the Viterbi algorithm <ref type="bibr" target="#b31">[32]</ref> is used to find the most likely path through the HHMM states. As for online recognition, the phase prediction is computed using the forward algorithm.</p><p>One can observe that EndoNet already provides confidence values through the output of layer fc phase, thus it is not essential to pass EndoNet features to the SVM to obtain the confidence values v p . Furthermore, in preliminary experiments, we observed that there was only a slight difference of performance between v p and fc phase in recognizing the phases both before and after applying the HHMM. However, this additional step is necessary in order to provide a fair comparison with other features, which are passed to the SVM to obtain the confidence. In addition, using the output of layer fc phase as the phase estimation confidence is only applicable to datasets that share the same phase definition as the one in the fine-tuning dataset. Thus, this step is also required for the evaluation of the network generalizability to other datasets that might have a different phase definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP A. Dataset</head><p>We have constructed a large dataset, called Cholec80, containing 80 videos of cholecystectomy surgeries performed by 13 surgeons. The videos are captured at 25 fps and downsampled to 1 fps for processing. The whole dataset is labeled with the phase and tool presence annotations. The phases have been defined by a senior surgeon in our partner hospital. Since the tools are sometimes hardly visible in the images and thus difficult to be recognized visually, we define a tool as present in an image if at least half of the tool tip is visible. The tool and the phase lists can be found in <ref type="figure">Fig. 3</ref> and Tab. I-a, respectively.</p><p>The Cholec80 dataset is split into two subsets of equal size (i.e., 40 videos each). The first subset (i.e., the fine-tuning  subset) contains ∼86K annotated images. From this subset, 10 videos have also been fully annotated with the bounding boxes of tools. These are used to train Deformable Part Models (DPM) <ref type="bibr" target="#b32">[33]</ref>. Because the grasper and hook appear more often than other tools, their bounding boxes reach a sufficient number from the annotation of three videos. The second subset (i.e., the evaluation subset) is used to test the methods for both tool presence detection and phase recognition. The statistics of the complete dataset can be found in <ref type="figure">Fig. 4</ref>. The second dataset is a public dataset from the EndoVis workflow challenge at MICCAI 2015, containing seven cholecystectomy videos. Similarly, these videos are captured at 25 fps and processed at 1 fps. We only perform phase detection on this dataset, because the types and the visual appearances of the tools are different from the tools that EndoNet is designed to detect. The list of phases in the EndoVis dataset is shown in Tab. I-b. It can be seen that phase P3 is longer in Endovis than in Cholec80. This is due to the fact that in Cholec80, P3 is typically started when the calot triangle is clearly exposed. Yet, this is not the case in EndoVis. As a result, extra dissection steps are included in P3, leading to a longer P3 in EndoVis.</p><p>The phases in EndoVis have been defined differently from the definition in Cholec80. For instance, a phase placement trocars is defined in the EndoVis dataset, even though it should be noted that this phase is not always visible from the laparoscopic videos. Additional sources of information (e.g., external videos), which are not available in the dataset, are required to label this phase correctly. Another difference is in the definition of the preparation phase. In the EndoVis dataset, the preparation phase includes the calot triangle dissection phase (hence the ID P12 in Tab. I-b). The other phases are defined similarly to the phases in Cholec80. The distribution of the phases in EndoVis is shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fine-Tuning, SVM and HHMM Parameters</head><p>EndoNet is trained by fine-tuning the AlexNet network <ref type="bibr" target="#b9">[10]</ref> which has been pre-trained on the ImageNet dataset <ref type="bibr" target="#b27">[28]</ref>.   The layers that are not defined in AlexNet (i.e., fc tool and fc phase) are initialized randomly. The network is finetuned for 50K iterations with N i = 50 images in a batch. The learning rate is initialized at 10 −3 for all layers, except for fc tool and fc phase, whose learning rate is set higher at 10 −2 because of their random initialization. The learning rates for all layers decrease by a factor of 10 for every 20K iterations. The fine-tuning process is carried out using the Caffe framework <ref type="bibr" target="#b33">[34]</ref>. The evolution of the loss function L during the fine-tuning process is shown in <ref type="figure">Fig. 6</ref>. The graph shows the convergence of the loss, indicating that the network is successfully optimized to learn the optimal features for the phase recognition and tool presence detection tasks. The networks are trained using an NVIDIA GeForce Titan X graphics card. The training process takes ∼80 seconds for 100 iterations, i.e., roughly 11 hours per network. The feature extraction process takes approximately 0.2 second per image. The computational time for SVM training depends on the size of the features, ranging from 0.1 to 90 seconds, while the HHMM training takes approximately 15 seconds using our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATLAB implementation.</head><p>To carry out phase recognition, all features are passed to a one-vs-all linear SVM, except the handcrafted features, which are passed through a histogram intersection kernel beforehand. We tried to use non-linear kernels for other features in our preliminary experiments, but this did not yield any improvements.</p><p>For the HHMM, we set the number of top-level states to seven (equal to N p ), while the number of bottom-level states is data-driven (as in <ref type="bibr" target="#b30">[31]</ref>). To model the output of the SVM, we use a mixture of five Gaussians for every feature, except for the binary tool signal, where one Gaussian is used. The type of covariance is diagonal. In <ref type="figure">Fig. 7</ref>, the graph representation of the HHMM used to recognize the phases in Cholec80 is shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baselines</head><p>For tool presence detection, we compare the results given by EndoNet (i.e., the output of layer fc tool) with two other methods. The first method is DPM <ref type="bibr" target="#b32">[33]</ref>, since it is an ubiquitous method for object detection that is available online.</p><p>In the experiments, we use the default parameters, model each tool using three components and represent the images using HOG features. The second method is a network trained in a single-task manner that solely performs the tool presence detection task (ToolNet). We compare the ToolNet results with the EndoNet results in order to show that performing the finetuning process in a multi-task manner yields a better network than in a single-task manner. The architecture of this network can be seen in <ref type="figure" target="#fig_5">Fig. 8-a</ref>.</p><p>For phase recognition, we run a 4-fold cross-validation on the evaluation subset of Cholec80 and full cross-validation on the EndoVis dataset. Because the recognition pipeline contains methods trained with random initializations, the results might be different in each run. Thus, the displayed results are the average of five experimental runs. Here, we compare the phase recognition results using the following features as input:</p><p>• binary tool information generated from the manual annotation; this is a vector depicting the presence of the tools in an image, i.e. v t ∈ {0, 1} 7 and v t ∈ {0, 1} 10 for the Cholec80 and EndoVis datasets, respectively; • handcrafted visual features: bag-of-word of SIFT, HOG, RGB and HSV histograms; these features are chosen because they have been successful in carrying out classification <ref type="bibr" target="#b34">[35]</ref> on laparoscopic videos; • the afore-mentioned handcrafted visual features + CCA, similar to the approach suggested in <ref type="bibr" target="#b6">[7]</ref>; • the output of layer fc7 of AlexNet trained on the Im-ageNet dataset (i.e., the initialization of the fine-tuning process); • the output of layer fc7 from a network that is fine-tuned to carry out phase recognition in a single-task manner, shown in <ref type="figure" target="#fig_5">Fig. 8-b</ref> (PhaseNet); • our proposed features, i.e., the output of layer fc8 from EndoNet. We also include features called EndoNet-GTbin for phase recognition on the Cholec80 dataset. These features consist of the output of layer fc7 from EndoNet concatenated with binary tool information obtained from the ground-truth annotations. This evaluation allows us to investigate whether the tool information automatically extracted from EndoNet, which is included in our proposed features, is sufficient for the phase recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation</head><p>The performance of the tool presence detection is measured by the average precision (AP) metric. It is obtained by computing the area under the precision-recall curve. For the phase recognition task, several evaluation metrics are used, i.e., precision, recall, and accuracy as defined in <ref type="bibr" target="#b2">[3]</ref>. Recall and  <ref type="figure" target="#fig_1">Fig. 2</ref>). The single-task networks are also trained via transfer learning.</p><p>precision compute the number of correct detections divided by the length of the ground truth and by the length of the complete detections, respectively. Since they are computed for each phase, we show the averages for recall and precision to present summarized results. Accuracy represents the percentage of correct detections in the complete surgery.</p><p>In order to show the improvements that the proposed features yield, we compute the evaluation metrics for phase recognition on the results before and after applying HHMM. To provide a deeper analysis of the results, we also present in Section VI the performance of EndoNet on two practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>A. Cholec80 Dataset 1) Tool Presence Detection: The results of the tool presence detection task are shown in Tab. II. It can be seen that the networks yield significantly better results than DPM. It might be due to the fact that the number of images used for fine-tuning the networks is higher than the number of bounding boxes used for DPM training, but this may only partly explain this large difference. To provide a fairer comparison, we compare the performance of DPM with ToolNet and EndoNet models that are trained only with the 10 videos used to train DPM (see also Subsubsection V-A3 for the influence of the fine-tuning subset size). As expected, the performance of the networks is lower compared to the networks trained on the full fine-tuning subset. However, the mean APs are still better than the one of DPM: 65.9 and 62.0 for ToolNet and EndoNet, respectively. Note that, the networks are only trained using binary annotations (present vs. not-present), while DPM uses bounding boxes containing specific localization information. Furthermore, the networks contain a much higher number of unknowns to optimize than DPM. In spite of these facts, with the same amount of training data, the networks perform the task better than DPM.</p><p>From Tab. II, it can be seen that EndoNet gives the best results for this task. This shows that training the network in a multi-task manner does not compromise the EndoNet's performance in detecting the tool presence. For all methods, there is a decrease in performance for scissors detection. This might be due to the fact that this tool has the smallest amount of training data (see <ref type="figure">Fig. 4-a)</ref>, as it only appears shortly in the surgeries. In addition, it could be confused with  the grasper since they share many visual similarities. Over the seven tools and 40 complete surgeries in the evaluation subset of Cholec80, EndoNet obtains 81% mean AP for tool presence detection. The success of this network suggests that binary annotations are sufficient to train a model for this task. This is particularly interesting, since tagging the images with binary information of tool presence is much easier than providing bounding boxes. It also shows that the networks can successfully detect tool presence without any explicit localization pre-processing steps (such as segmentation and ROI selection).</p><p>2) Phase Recognition: In Tab. III-a, the results of phase recognition on Cholec80 before applying HHMM are shown. These are the results after passing the image features to the SVM. The results show that the CNNs are powerful tools to extract visual features: despite being trained on a completely unrelated dataset, the AlexNet features outperform the handcrafted visual features (without and with CCA) and the binary tool annotation. Furthermore, the fine-tuning step significantly improves the results: the PhaseNet features yield improvements for all metrics compared to the AlexNet features. In addition to yielding the tool presence detection as a by-product, the multi-task framework applied in EndoNet further improves the features for the phase recognition task. It is also interesting to observe that the phase recognition results using the EndoNet-GTbin features are only slightly better than the ones using the EndoNet features, with approximately 0.1% improvement in accuracy. In other words, the tool information generated from the ground-truth does not bring more information than the EndoNet features and the visual features extracted by EndoNet alone are sufficient to carry out the phase recognition task.</p><p>In Tab. IV, the phase recognition results after applying HHMM are shown. Due to the nature of offline phase recognition, where the algorithm can see the complete video, the offline results are better than the online counterparts. However, when we compare the feature performance, the trend is consistent across the offline and online modes. By comparing the results from Tab III-a and Tab IV-a, we can see the improvement that the HHMM brings, which is consistent across all features.</p><p>In <ref type="figure" target="#fig_6">Fig. 9</ref>, we show the top-5 and bottom-5 recognition results based on the accuracy from one (randomly chosen) experimental run in both offline and online modes. In offline mode, it can be seen that the top-5 results are very good, resulting in over 98% accuracies. In addition, the bottom-5 results in offline mode are comparable to the ground truth. The drop of accuracy for the bottom-5 are caused by the jumps that can happen between P5 and P6, which are shown by the alternating blue and red in <ref type="figure" target="#fig_6">Fig. 9</ref>-c. These jumps occur because of the non-linear transitions among these phases (see <ref type="figure">Fig. 7</ref>).</p><p>In online mode, one can observe more frequent jumps in the phase estimations. This is due to the nature of recognition in online mode, where future data is unavailable, so that the model is allowed to correct itself after making an estimation. Despite these jumps, the top-5 online results are still very close to the ground-truth, resulting in accuracies above 92%.</p><p>In order to provide more comprehensive information regarding the performance of EndoNet over the whole dataset, we present the recognition results for all phases in both offline and online modes in Tab. V. It can be seen that the EndoNet features perform very well in recognizing all the phases. A decrease in performance can be observed for the recognition of P5 and P6. This is likely due to the fact that the transitions between these phases are not sequential and that there is not always a clear boundary between them, especially as some images sometimes do not show any activity. This creates some ambiguity in the phase estimation process.</p><p>3) Effects of Fine-Tuning Subset Size : In order to show the importance of the amount of training data for the finetuning process, we fine-tune our networks using fine-tuning subsets with gradually increasing size: 10, 20, 30, and ultimately 40 videos. We perform both tool presence detection and phase recognition tasks on the evaluation subset of Cholec80 using the trained networks. The results are shown in <ref type="figure" target="#fig_0">Fig.  10</ref>. As expected, the performance of the networks increase proportionally to the amount of data in the fine-tuning subset. It can also be seen that EndoNet performs better than the single-task networks (i.e., PhaseNet and ToolNet), except for the tool presence detection task where fewer videos are used to train the networks. This indicates that EndoNet takes more advantage of the big dataset compared to ToolNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EndoVis Dataset</head><p>Similar results for phase recognition are obtained from the EndoVis dataset, as shown in Tab. III and IV-b. It can be observed that the improvements obtained by PhaseNet and EndoNet on EndoVis are not as high as the result improvements on Cholec80, which is expected since these networks are fine-tuned using the videos from Cholec80. In spite of this fact, the results on the EndoVis dataset also show that the EndoNet features improve the phase recognition results significantly. It indicates that the multi-task learning results in a better network than the single-task counterpart. The fact that the features from EndoNet yield the best results for all cases also shows that EndoNet is generalizable to other datasets.</p><p>One should note that we use the output of layer fc8 from EndoNet as the image feature, which includes confidence values for tool presence. Because the tools used in EndoVis dataset are not the same tools as the ones in the Cholec80 dataset (which is used to train EndoNet), these confidence values can simply be regarded as 7 additional scalar features appended to the feature vector. The results show that these values help to construct more discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MEDICAL APPLICATIONS</head><p>Here, we demonstrate the applicability of EndoNet for practical CAI applications. We present the results from the same experimental run that is used to generate <ref type="figure" target="#fig_6">Fig. 9</ref>. First, to show the feasibility of using EndoNet as the basis for automatic surgical video indexing, we show the error of the phase estimation in seconds to indicate how precise the phase Feature  boundary estimations from EndoNet are. Second, we investigate further how accurately EndoNet detects the presence of two tools: clipper and bipolar. These tools are particularly interesting because: (1) the appearance of the clipper typically marks the beginning of the clipping and cutting phase, which is the most delicate phase in the procedure, and (2) the bipolar tool is generally used to stop haemorrhaging, which could lead to possible upcoming complications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Automatic Surgical Video Database Indexing</head><p>For automatic video indexing, the task corresponds to carrying out phase recognition in offline mode. From the results shown in <ref type="figure" target="#fig_6">Fig. 9</ref>-a,c, one can already roughly interpret how accurate the phase recognition results are. To give a more intuitive evaluation, we present the number of phase boundaries that are detected within defined temporal tolerance values in Tab. VI. We can see that EndoNet generally performs very well for all the phases, resulting in 89% of the phase boundaries being detected within 30 seconds. It can also be seen that only 6% of the phase boundaries are detected with an error over 2 minutes. It is also important to note that this error is computed with respect to the strict phase boundaries   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bipolar and Clipper Detection</head><p>In addition to showing the AP for detection of both tools in Tab. II, we present a more intuitive metric to measure the reliability of EndoNet for the bipolar and clipper presence detections. We define a tool block as a set of consecutive frames in which a certain tool is present. Since the tools might not always be visible in an image even though they are currently being used, we merge the blocks (of the same tool) in the ground-truth data that have a gap that is less than 15 seconds. Then, we define a tool block as identified if EndoNet can detect the tool in at least one of the frames inside the block. To show the performance of EndoNet in terms of temporal precision, we also present the time difference between the first frame of the tool block and the first frame of the detection. In this experiment, we determine the tool presence by taking a confidence threshold that gives a high precision for each tool, so that the system can obtain the minimal amount of false positives and retain the sensitivity in correctly detecting the tool blocks. Since the false positive rate is measured using the tool block definition, we also close the gaps between the tool presence detections that are less than 15 seconds.</p><p>We show the block detection results in Tab. VII. It can be seen that all the bipolar blocks are detected very well by En-doNet. Over 90% of the blocks are detected under 5 seconds. EndoNet also yields a very low false positive rate (i.e., 3.8%) for the bipolar. This excellent performance is obtained thanks to the distinctive visual appearance that the bipolar has (e.g., the blue shaft). For the clipper, it can be seen that the false positive rate is higher than for the bipolar. This could be due to the fact that it has the second lowest amount of annotations in the dataset, because, similarly to the scissors, the clipper only appears shortly in the surgeries. However, EndoNet still performs very well for clipper detection, showing that 80% and 97% of the blocks are detected under 5 and 30 seconds, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION AND CONCLUSIONS</head><p>In this paper, we address the problem of phase recognition in laparoscopic surgeries and propose a novel method to learn visual features directly from raw images. This method is based on a convolutional neural network (CNN) architecture, called EndoNet, which is designed to perform two tasks simultaneously: tool presence detection and phase recognition. We show through extensive experiments that this architecture yields visual features that outperform both previously used features and the features obtained from architectures designed for a single task. Interestingly, the EndoNet visual features also perform significantly better in the phase recognition task than binary tool signals indicating which tools can be seen in the image, even though these signals are obtained from ground truth annotations. These results therefore suggest that the images contain additional characteristics useful for recognition in addition to simple tool presence information and that these characteristics are successfully retrieved by EndoNet. Additionally, we have shown that EndoNet also performs well on another smaller dataset, namely EndoVis, and is therefore generalizable.</p><p>To train and evaluate EndoNet, we constructed a large dataset containing 80 videos of cholecystectomy procedures performed by 13 surgeons. Even though the cholecystectomy procedure is a common focus for surgical workflow analysis, to the best of our knowledge, the cholecystectomy datasets used in previous work are limited to less than 20 surgeries. This is therefore the first large-scale study performed for these recognition tasks. This is also the first extensive comparison of the features that can be used to perform phase recognition on laparoscopic surgeries 2 . Furthermore, it is shown by the std of the phase durations in Tab-I-a that the dataset in itself contains a high variability. The state-of-the-art results from EndoNet indicates that our proposed method can cope with such complexity.</p><p>The results of varying sizes of the fine-tuning subset suggest that taking more videos from Cholec80 to fine-tune the networks will lead to better performance. However, it should be noted that the videos in Cholec80 come from one hospital, thus the complexity of the data is limited to the variability of procedure executions by surgeons from the same institution. Training a CNN network with such a dataset can lead to over-fitting and subsequently reduce the generalizability of the network. To obtain more generalizable networks, videos from other medical institutions should be included to ensure a higher variability in the dataset. The success of EndoNet in carrying out the tool presence detection and phase recognition tasks should be considered as a call for action in the community to open their data to accelerate the development of generalizable solutions for these tasks.</p><p>We have shown the applicability of EndoNet for two different applications. These applications focus on video database management, which is one of the demands from our clinical partners. In future work, other related applications should be addressed, such as context-aware assistance during live surgeries. It will also be interesting to explore whether the features generated by EndoNet can be used to perform other tasks in laparoscopic videos, such as the estimation of the completion time of the procedure <ref type="bibr" target="#b2">[3]</ref>, the classification of surgical videos <ref type="bibr" target="#b34">[35]</ref>, and the recognition of the anatomy.</p><p>Despite yielding state-of-the-art results, the presented phase recognition pipeline still has some limitations. For example, the phase recognition still relies on the HHMM, which is required to enforce the temporal constraints in the phase estimation. Thus, the features learnt by EndoNet do not include any temporal information present in the videos. In addition, since the HHMM is trained separately from the EndoNet finetuning process, the EndoNet features are not optimized on the entire phase recognition task. With additional training data, these limitations could be solved by using long short term memory (LSTM) architectures. Such an approach will form part of future efforts to improve phase recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Full pipeline of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>EndoNet architecture (best seen in color). The layers shown in the turquoise rectangle are the same as in the AlexNet architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>45984</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Distribution of annotations in the Cholec80 dataset for (a) tool presence detection and (b) phase recognition tasks. Phase distribution in the EndoVis dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :Fig. 6 :Fig. 7 :</head><label>367</label><figDesc>List of the seven surgical tools used in the Cholec80 dataset. Evolution of the loss function during the fine-tuning process of EndoNet. Graph representation of the two-level HHMM for the surgical phases defined in Cholec80. The top-level states, representing the phases defined in the dataset, are shown in blue. The transitions for top-level states show all possible phase transitions defined in the dataset. The bottom-level states are shown in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Single-task CNN architectures for the (a) tool presence detection and (b) phase recognition tasks. The AlexNet architecture is the same as the one used in EndoNet (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Phase recognition results vs. ground truth on Cholec80 in a color-coded ribbon illustration. The horizontal axis of the ribbon represents the time progression in a surgery. The top ribbon is the estimated phase and the bottom ribbon is the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>Evolution of network performance on Cholec80 with respect to the number of videos in the fine-tuning subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>List of phases in the (a) Cholec80 and (b) EndoVis datasets, including the mean ± std of the duration of each phase in seconds.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Average precision (AP) for all tools, computed on the 40 videos forming the evaluation dataset of Cholec80. The best AP for each tool is written in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Phase recognition results after applying the HHMM (mean ± std) on: (a) Cholec80 and (b) EndoVis. The best result for each evaluation metric is written in bold. The results from our proposed features (EndoNet) are written in italic.</figDesc><table><row><cell>Feature</cell><cell>Metric</cell><cell>P1</cell><cell>P2</cell><cell>P3</cell><cell>P4</cell><cell>P5</cell><cell>P6</cell><cell>P7</cell></row><row><cell>EndoNet -offline</cell><cell cols="8">Prec. 83.5±9.6 97.1±2.0 81.0±7.7 97.3±2.1 73.1±8.0 79.7±10.4 81.9±11.8 Rec. 90.9±5.7 80.8±4.3 88.1±7.4 94.7±1.0 83.7±5.6 79.6±8.8 86.7±11.8</cell></row><row><cell>EndoNet -online</cell><cell cols="8">Prec. 90.0±5.6 96.4±2.0 69.8±10.7 82.8±6.2 55.5±11.9 63.9±10.5 57.5±11.0 Rec. 85.5±3.9 81.1±8.9 71.2±9.7 86.5±4.3 75.5±3.8 68.7±9.1 88.9±7.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Precision and recall of phase recognition for each phase on Cholec80 using the EndoNet features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Number of phases that are correctly identified in offline mode within the defined tolerance values in the 40 evaluation videos of Cholec80. The number of P6 occurrences is not 40 since not all surgeries go through the cleaning and coagulation phase.</figDesc><table><row><cell cols="3">Tolerance (s) Bipolar Clipper</cell></row><row><cell>&lt;5</cell><cell>114</cell><cell>49</cell></row><row><cell>6-29</cell><cell>9</cell><cell>10</cell></row><row><cell>30-59</cell><cell>1</cell><cell>0</cell></row><row><cell>≥60</cell><cell>0</cell><cell>1</cell></row><row><cell>Missed</cell><cell>0</cell><cell>1</cell></row><row><cell cols="2">False positives 3.8%</cell><cell>8.3%</cell></row></table><note>defined in the annotation. In practice, these boundaries are not as harsh or visually obvious. Thus, this error is acceptable in most cases. In other words, it indicates that the results from EndoNet do not require a lot of corrections, which will make surgical video indexing a lot faster and easier.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>Appearance block detection results for bipolar and clipper, including the number of correctly classified blocks and missed blocks, and the false positive rate of the detection.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Since no significant database is currently available to compare the approaches, to encourage open research in this direction, we will make the complete annotated video dataset as well as the trained CNN architectures available to the community upon publication of this work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by French state funds managed by the ANR within the Investissements d'Avenir program under references ANR-11-LABX-0004 (Labex CAMI), ANR-10-IDEX-0002-02 (IdEx Unistra) and ANR-10-IAHU-02 (IHU Strasbourg). The authors would like to thank the IRCAD audio-visual team for their help in generating the dataset. The authors would also like to acknowledge the support of NVIDIA with the donation of the GPU used in this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Or2020 workshop overview: operating room of the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Ki</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CARS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1268</biblScope>
			<biblScope unit="page" from="847" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discovery of high-level tasks in the operating room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loubna</forename><surname>Bouarfa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><forename type="middle">P</forename><surname>Jonker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Dankelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="462" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical modeling and recognition of surgical workflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubertus</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Odile</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="632" to="641" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge-driven formalization of laparoscopic surgeries for rule-based intraoperative context-aware assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darko</forename><surname>Katić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna-Laura</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Gartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rdiger Dillmann, and Stefanie Speidel</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8498</biblScope>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-site study of surgical practice in neurosurgery based on surgical process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Louis</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Meixensberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shafik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wassef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Neumuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Goulet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="822" to="829" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic knowledge-based recognition of low-level tasks in ophthalmological procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling and segmentation of surgical workflow from laparoscopic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubertus</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6363</biblScope>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Surgical gesture classification from video and kinematic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Zappella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamn</forename><surname>Bjar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="732" to="745" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A framework for the recognition of high-level surgical tasks from video images for cataract surgeries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Biomed. Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="966" to="976" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hubertus Feussner, and Nassir Navab. Random forests for phase detection in surgical workflow analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asl</forename><surname>Okur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loc</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kranzfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPCAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8498</biblScope>
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast part-based classification for instrument detection in minimally invasive surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="692" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting surgical tools by modelling local appearance and global shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2603" to="2617" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image based surgical instrument pose estimation with multi-class labelling and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Lin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Hawkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9349</biblScope>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chiara Amat di San Filippo, Vasileios Belagiannis, Abouzar Eslami, and Nassir Navab. Surgical tool tracking and pose estimation in retinal microsurgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Joseph</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Alsheakhali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9349</biblScope>
			<biblScope unit="page" from="266" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature classification for tracking articulated surgical tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peterk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7511</biblScope>
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time instrument detection in minimally invasive surgery using radiofrequency identification technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kranzfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fiolka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Schwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonja</forename><surname>Gillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Schirren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubertus</forename><surname>Feussner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Surgical Research</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="704" to="710" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online recognition of surgical instruments by information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Neumuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Meissner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="304" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic classification of minimally invasive instruments based on endoscopic image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Benzko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Krappe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunther</forename><surname>Sudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedram</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Peter Mller-Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Gutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rdiger</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7261</biblScope>
			<biblScope unit="page" from="72610" to="72610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time task recognition in cataract surgery videos using adaptive spatiotemporal polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwenole</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Cochener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Cazuguel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="877" to="887" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Episode classification for the analysis of tissue/instrument interaction with multiple visual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Benny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ara</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Zhong</forename><surname>Darzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2878</biblScope>
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic phase prediction from low-level surgical activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="833" to="841" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d sensing algorithms towards building an intelligent intensive care unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Facker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><forename type="middle">H</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchi</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Summits on Translational Science</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On-line recognition of surgical activity for monitoring in the operating room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubertus</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Odile</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1718" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An improved model for segmentation and recognition of fine-grained activities with application to surgical training tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1123" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic feature generation in endoscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Klank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="331" to="339" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Hubertus Feussner, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-dimensional signature compression for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Workflow monitoring based on 3D motion features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Odile</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards better laparoscopic video database organization by automatic surgery classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Andru Putra Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPCAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="186" to="194" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
