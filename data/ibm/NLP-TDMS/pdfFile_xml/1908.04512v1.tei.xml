<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpolated Convolutional Networks for 3D Point Cloud Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
							<email>maojiageng@link</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interpolated Convolutional Networks for 3D Point Cloud Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud is an important type of 3D representation. However, directly applying convolutions on point clouds is challenging due to the sparse, irregular and unordered data structure. In this paper, we propose a novel Interpolated Convolution operation, InterpConv, to tackle the point cloud feature learning and understanding problem. The key idea is to utilize a set of discrete kernel weights and interpolate point features to neighboring kernel-weight coordinates by an interpolation function for convolution. A normalization term is introduced to handle neighborhoods of different sparsity levels. Our InterpConv is shown to be permutation and sparsity invariant, and can directly handle irregular inputs. We further design Interpolated Convolutional Neural Networks (InterpCNNs) based on Inter-pConv layers to handle point cloud recognition tasks including shape classification, object part segmentation and indoor scene semantic parsing. Experiments show that the networks can capture both fine-grained local structures and global shape context information effectively. The proposed approach achieves state-of-the-art performance on public benchmarks including ModelNet40, ShapeNet Parts and S3DIS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud is an important data format obtained by 3D sensors and has shown extensive usage in many real-world tasks including autonomous driving <ref type="bibr" target="#b6">[7]</ref>, robotics <ref type="bibr" target="#b31">[32]</ref>, etc. Efficient learning from point cloud data remains a challenge to the research community, given the fact that point clouds are usually irregular, unordered and sparse.</p><p>In view of the great success of convolutional neural networks (CNNs) on 2D images, many endeavors have been made to adapt the convolution operation to 3D point clouds. Currently there are two main approaches to tackle this problem. The first type of attempts <ref type="bibr" target="#b23">[24]</ref> is to directly rasterize irregular point clouds into regular voxel grids, and adopts standard 3D convolutions to learn shape features. However, the transformation of irregular inputs leads to a loss of geometric information, and convolutions on dense voxel grids lead to heavy computational burden. Other ap- proaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b38">39]</ref> build local graphs in the neighborhood of each point in the Euclidean or feature space, then a continuous convolutional kernel is applied on each edge of the graph to learn geometric features. Continuous kernels are commonly modeled by Multi-layer Perceptrons (MLPs). These graph-based methods are able to directly process irregular data structure but have some drawbacks. The construction of local graphs is not sparsity invariant. Namely, different point cloud densities sampled from the same object surface lead to different neighborhood selections, thus it may produce various graph construction results. Besides, compared with discrete convolutions, uti-lizing MLPs to learn an arbitrary continuous function does not work well in practice <ref type="bibr" target="#b48">[49]</ref>.</p><p>In this paper, we propose a novel Interpolated Convolution operation (InterpConv) to address the existing problems in graph and 3D convolutional neural networks. Key to our approach is the use of discrete convolutional kernels and an interpolation function to explicitly measure geometric relations between input point clouds and kernel-weight coordinates. Unlike 3D convolutions which have to transform inputs into regular grids, our InterpConv directly takes irregular point clouds as inputs. Each n × n × c convolutional kernel is split into n 2 kernel weights, each of which has a 1 × c weight vector and its own coordinate p relative to the kernel center. The center of discrete convolutional kernels can be placed at any location in the 3D space and then the kernel-weight absolute coordinates can be determined for each kernel. Input points are interpolated to neighboring kernel-weight coordinates by an interpolation function. To guarantee InterpConv to be sparsity invariant, normalization on points is adopted in the neighborhood of each kernel weight vector. Finally, weighted convolutions can be calculated between kernel weight vectors and point cloud features that are associated to them. With spatiallydiscrete convolutional kernel weights and an explicitly defined interpolation function, our approach performs better than graph-based methods, which use continuous functions as convolutional kernels and implicitly learn geometric relations. See <ref type="figure" target="#fig_0">Figure 1</ref> for illustration.</p><p>We further propose Interpolated Convolutional Neural Networks (InterpCNNs) based on InterpConvs. The classification network is composed of multi-layer and multireceptive-field InterpConv blocks, which can capture both fine-grained geometric structures and context information. The segmentation network explores a deeper architecture to predict semantic labels for all input points. We evaluate our networks on several benchmark datasets, including ModelNet40 <ref type="bibr" target="#b4">[5]</ref>, ShapeNet Parts <ref type="bibr" target="#b49">[50]</ref> and S3DIS <ref type="bibr" target="#b0">[1]</ref>. Experiments show that our approach achieves state-of-the-art performances on those datasets.</p><p>The key contributions of our work are as follows:</p><p>• We propose a novel Interpolated Convolution operation (InterpConv) to effectively deal with point cloud recognition problems. Such an operation is permutation and sparsity invariant, and can directly handle irregular point clouds;</p><p>• We design Interpolated Convolutional Neural Networks based on InterpConvs. The networks perform better than Graph Neural Networks (GNNs) and 3D Convolutional Neural Networks (3D ConvNets) on point cloud recognition and segmentation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our approach is closely related to other deep learning methods on point clouds. We introduce literatures on point cloud feature learning by regular grids and irregular inputs.</p><p>Learning from point clouds by regular grids. When facing irregular point clouds as inputs, an intuitive way is to transform this irregular data structure into regular grids. Some approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref> transform 3D objects or point clouds into 2D regular grids, namely, images by multi-view projection, then 2D CNNs are utilized to learn from these images. Those methods work well owing to the great success of 2D CNNs on images. However, not all the geometric information is kept during projection, and those approaches are usually inefficient and time-consuming when handling sparse point cloud data.</p><p>An alternative approach is to rasterize point clouds into 3D regular grids. VoxNet <ref type="bibr" target="#b23">[24]</ref> transforms original point cloud data into occupancy grids, which store binary values to indicate whether the spaces are occupied. Then a 3D CNN is applied to learn from these voxel grids. The rasterization process loses some fine-grained geometric features, and 3D convolutions are both time and memory consuming. OctNet <ref type="bibr" target="#b29">[30]</ref> exploits the sparsity of voxel grids and uses unbalanced octrees to hierarchically partition the space, which saves much memory. Some other efforts <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29]</ref> have also been made to ease the computational burden but still cannot solve the loss of geometric information during rasterization.</p><p>Compared with the above-mentioned approaches, our approach directly takes irregular point clouds as inputs without rasterization, which is time-saving and accurate.</p><p>Learning from point clouds by irregular inputs. Recently there are many works trying to directly process irregular point cloud data. Pioneering work PointNet <ref type="bibr" target="#b24">[25]</ref> utilizes shared MLPs and a maxpooling layer, which is permutation invariant, to tackle unordered inputs and learn a global representation. PointNet++ <ref type="bibr" target="#b26">[27]</ref> exploits local structures by grouping and sampling point clouds, then a PointNet is applied in each group to aggregate local features. However, how to effectively partition and select point clouds remain a challenge. Many approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> explore new grouping and sampling strategies. In <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>, new modules are added to original PointNet++ in order to gain a better performance.</p><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b32">[33]</ref> have been widely used to deal with irregular data structure. There are also a bunch of works trying to apply GNNs to solve point cloud processing problem. Those approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20]</ref> usually build local graphs in the neighborhood of the Euclidean or feature space, utilize MLPs as continuous convolutional kernel functions, and aggregate local features by weighted sum or pooling from neighborhood to center. DGCNN <ref type="bibr" target="#b44">[45]</ref> proposes an EdgeConv operation which concatenates central and neighboring point features and learns new features by MLP and maxpooling. 3DGNN <ref type="bibr" target="#b27">[28]</ref> applies gated graph neural networks <ref type="bibr" target="#b21">[22]</ref> on semantic segmentation task. SpiderCNN <ref type="bibr" target="#b48">[49]</ref> defines a continuous kernel function as a product of step function and a Taylor polyno-mial. KCNet <ref type="bibr" target="#b33">[34]</ref> proposes a kernel correlation and graph pooling layer to exploit local structures. PointCNN <ref type="bibr" target="#b19">[20]</ref> applies χ-transform operator on local graphs. GNN-based methods still have some problems. First, the graph construction process based on K-nearest neighbors (KNN) is sensitive to point cloud density. Second, using MLPs to directly learn from point coordinates is inefficient, as it ignores some explicitly defined geometric relations. Different from those methods, our approach is not sensitive to point cloud density due to the proposed normalization term, and geometric relations between discrete kernel weights and point clouds are explicitly defined by an interpolation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first revisit convolutions on different types of point sets. We then introduce our proposed Interpolated Convolution operation (InterpConv) and key elements of the InterpConv algorithm. Finally, we show details for our network architectures for 3D object recognition and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolutions on Point Sets</head><p>Standard 2D and 3D convolutions have achieved great success in handling regularly-arranged data such as images and voxel grids. When it comes to sparse and irregular point sets such as 3D point clouds, multiple variants of convolutions have been proposed. In this section, we review those convolutions to motivate the design of our InterpConv operation.</p><p>Considering a standard 3D convolution, let 3D voxel grids or features be denoted by F : Z 3 → R c , and the convolutional kernel weights W be a series of 1 × c weight vectors, where c is the number of channels. Standard convolution at locationp can be formulated as</p><formula xml:id="formula_0">F * W (p) = p ∈Ω F (p + p ) · W (p ),<label>(1)</label></formula><p>where Ω = {p ∈ Z 3 : (−n, −n, −n), · · · , (n, n, n)} is the set of kernel weight vectors' coordinates relative to the kernel center. The kernel size is assumed to be 2n + 1, and · denotes dot production between two vectors. When it comes to irregular inputs, points are no longer regularly-arranged and distances between points become irregular. Some approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b45">46]</ref> adopt a continuous weight function W (p δ ), which takes relative coordinates p δ of neighboring pointsp + p δ to the central pointp as inputs, to predict the convolutional weights. The continuous function W (p δ ) is no longer a 1×c weight vector but a mapping R 3 → R c commonly implemented by MLPs. Then the continuous convolution can be formulated as</p><formula xml:id="formula_1">F * W (p) = p δ F (p + p δ ) · W (p δ ).<label>(2)</label></formula><p>It is worth noting that applying graph neural networks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34]</ref> to handle point clouds essentially shares the same idea with continuous convolutions. Replacing discrete kernel weights W (p ) with continuous functions W (p δ ) remains some problems. Simply learning continuous functions by MLPs cannot always work in practice <ref type="bibr" target="#b48">[49]</ref>. The predicted parameters might be too many, and the learning process is inefficient and sometimes unstable. Knowledge on the great success of discrete kernels in images cannot be transfered to point clouds recognition tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Interpolated Convolution for 3D Point Clouds</head><p>In our approach, we adopt the design of discrete convolutional weights while maintaining the characteristics of continuous distances, by decoupling W (p δ ) into two parts: spatially-discrete kernel weights W (p ) ∈ R c and an interpolation function T (p δ , p ). We note that a spatiallydiscrete kernel weight W (p ) is a 1 × c vector, which can be initialized and updated during training, and p is the relative coordinate of this kernel weight vector to the kernel center. The interpolation function T (p δ , p ) : R 3 × R 3 → R takes the coordinate of a kernel weight vector p and a neighboring input point p δ as inputs, and computes a weight by certain interpolation algorithm. Our approach takes every input point in the neighborhood of a kernel weight vector into account. In order to make convolutions sparsity invariant, a density normalization term N p , which sums the interpolation weights or number of input points in the neighborhood of p , is needed for each kernel weight vector W (p ). Finally, our InterpConv centered at locationp can be formulated as</p><formula xml:id="formula_2">F * W (p) = p 1 N p p δ T (p δ , p )F (p + p δ ) · W (p ). (3)</formula><p>We note that unlike standard convolutions, where kernel weights are regularly-arranged, kernel-weight coordinates p in InterpConvs can be set flexibly or even learned during training.</p><p>There are three key parts of our proposed InterpConv operation: spatially-discrete kernel weights W , an interpolation function T , and a normalization term N . We first discuss those three parts separately, and then introduce the complete algorithm.</p><p>Discrete kernel weights. In 2D convolution <ref type="bibr" target="#b16">[17]</ref>, one kernel can be represented as an n × n × c tensor, where n denotes the kernel size and c denotes the number of channels. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, one kernel is split into n × n weight vectors, each of which is of size 1 × c. By doing so, kernel weights no longer have to be regularly-arranged, but can be flexibly placed on 2D grids.</p><p>In our approach, we further improve this idea by defining a set of kernel weight vectors for each convolutional kernel in the 3D Euclidean space. Each kernel weight vector W (p ) has a 3D coordinate p to store its location relative to the kernel center, and its weights are stored in a 1×c vector, which will be initialized and updated during training. The vector coordinate p can either be fixed or updated during training. To simplify the problem, we fix kernel-weight coordinates in most experiments and organize them as a cube, namely, kernel weight vectors are arranged at 3 × 3 × 3 3D regular grids if the total number of kernel weight vectors is 27. We note that this is an analogy of standard 3 × 3 × 3 discrete convolutions while kernel weight vectors can theoretically be placed at arbitrary locations in the 3D space.</p><p>As we arrange kernel weight vectors as a cube, we define two important hyperparameters: the kernel size n × n × n and kernel length l. The coordinate set of spatially-discrete kernel weight vectors can be formulated as</p><formula xml:id="formula_3">Φ = (x , y , z ) x , y , z = kl, k ∈ − n − 1 2 , · · · , n − 1 2 ,<label>(4)</label></formula><p>where p = (x , y , z ). Similar to the definition of kernel size in standard convolutions, kernel size n × n × n ∈ Z 3 means that there are n kernel weight vectors on each edge of one kernel, and the total number of kernel weight vectors is n 3 . Kernel length l ∈ R is the distance between two adjacent weight vectors. It determines the actual 3D size of a kernel in the Euclidean space and is defined to control the receptive field, from which one convolutional kernel can "see" input point clouds. If l is small, the convolutional kernel is able to capture fine-grained local structures, otherwise it encodes more global shape information.</p><p>Interpolation functions. One problem to apply discrete kernels on irregular point clouds is that kernel weight vectors' spatial locations generally do not align with input points. Naively rasterizing point clouds into regular grids <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref> solves part of the problem, but at the cost of losing local structures. In our approach, we solve this problem while keep all fine-grained structures by adopting an interpolation function. Namely, we first find a set of input points near each kernel weight vector, and then interpolate their features to be assigned to the kernel weight vectors for convolution. We propose two interpolation functions: trilinear interpolation and Gaussian interpolation.</p><p>Trilinear interpolation is a commonly-used method to approximate the value of an intermediate point in a 3D grid by values of adjacent lattice points. The intermediate point's value is calculated by a weighted sum of lattice points' values, and the weights characterize closeness between intermediate and lattice points. In our approach, we adopt the inverse process of trilinear interpolation. Namely, we first compute the weights that lattice points (kernelweight coordinates) contribute to the intermediate point (input point) and then we inversely assign the input point feature to adjacent kernel-weight coordinates with those weights.</p><p>For trilinear interpolation, we find 8 adjacent kernelweight coordinates p for each input point p δ in the kernel, and then we normalize input point and kernel weights into a unit-length cube. Finally we compute the trilinear interpolation weights by <ref type="bibr" target="#b4">(5)</ref> where input point p δ = (x δ , y δ , z δ ) is the relative point coordinate to the kernel center, and kernel-weight coordinate p = (x , y , z ). We further note that Eq. (5) is a simplified format for normalized points. One property of trilinear interpolation is self-normalization, namely, all 8 weights which an input point assigns to can sum up to 1.</p><formula xml:id="formula_4">T tr (p δ , p ) = (1−|x δ −x |)(1−|y δ −y |)(1−|z δ −z |),</formula><p>In Gaussian interpolation, we assign each input point p δ to each kernel weight vector at p with a weight factor calculated by the following Gaussian function,</p><formula xml:id="formula_5">T G (p δ , p ) = e − (x δ −x ) 2 +(y δ −y ) 2 +(z δ −z ) 2 2σ 2 ,<label>(6)</label></formula><p>where the hyperparameter σ controls the decay rate. To save computation, if a 3D point is 3σ away from a weight vector, its assignment coefficient to the vector is directly set to 0 and will not be calculated. It is worth noting that other functions, for example, linear basis functions, can also be adopted as the interpolation function. Normalization terms. Given the fact that we take all neighboring points of a kernel weight vector into calculation, normalization is necessary to keep convolutions invariant to points density. There are two ways of normalization. We can aggregate and normalize the point features by</p><formula xml:id="formula_6">f aggregate = N i=1 t i f i N ,<label>(7)</label></formula><p>where N is the number of neighboring points, f i is the ith point feature, and t i denotes its interpolation weight. Apart from normalizing according to the number of points, we can also normalize the sum of interpolation weights:</p><formula xml:id="formula_7">f aggregate = N i=1 t i f i N i=1 t i .<label>(8)</label></formula><p>We can perform normalization either on each kernel weight vector or on the whole convolutional kernel. We argue that normalization on each kernel weight vector is more accurate, since input points are not uniformly distributed in the whole kernel.</p><p>The InterpConv Algorithm. An InterpConv operation takes point cloud coordinates and their features as input, and outputs new point coordinates and features. We note that output point coordinates can be set as the same as the input points, or downsampled from input point clouds. The center of convolutional kernels is placed at each output point coordinate, and kernel-weight coordinates are further determined by the relative coordinates of the weight vectors following Eq. (4). We calculate interpolation weights between kernel weight vectors and adjacent input points, and then for each neighboring p, feature f p do 5:</p><formula xml:id="formula_8">p δ ← p −p 6: t ← T (p , p δ ) 7: f i ← f i + tf p 8: f i ← N ormalize(f i ) 9:</formula><p>F ← [f 1 , · · · , f n ] 10:</p><p>for each kernel k do 11: </p><formula xml:id="formula_9">W k ← [w k 1 , · · · , w k n ] 12: v k ← F · W k 13:f ← [v 0 , · · · , v c ] 14: returnp,f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architectures</head><p>In this section, we introduce details for two deep architectures based on our InterpConv approaches. We explore embedding mutli-scale context features in the classification network and a deep encoder-decoder architecture in the segmentation network. See <ref type="figure">Figure 2</ref> for details.</p><p>The classification network consists of a series of Inter-pConv blocks which is mainly composed of three Interp-Conv layers. In the InterpConv block, the first and last layer are of kernel size 1 × 1 × 1 and the middle layer has a kernel size 3 × 3 × 3. The first InterpConv layer reduces channel dimensions and the last InterpConv layer increases channel dimensions, leaving the middle InterpConv layer with relatively small input and output channels. One Batch-Norm <ref type="bibr" target="#b12">[13]</ref> and ReLU <ref type="bibr" target="#b47">[48]</ref> layer also follow each Interp-Conv layer in the block.</p><p>Apart from this, we propose the PointInception module to encode multi-scale geometric features. Similar to the Inception module <ref type="bibr" target="#b37">[38]</ref> in 2D CNNs, our PointInception module also concatenates multi-branch features. However, we design each branch as one InterpConv block with a different kernel length l. The hyperparameter l determines the distances between adjacent kernel weight vectors in the Euclidean space and controls the receptive field of the convolution. So the PointInception module is able to capture both fine-grained local structures and shape context information by combining multi-branch outputs. We further explore a deeper model by stacking two PointInception modules.</p><p>In the segmentation network, we share the similar spirit as U-Net <ref type="bibr" target="#b30">[31]</ref> and build a deep encoder-decoder architecture. We stack multiple 3 × 3 × 3 InterpConv layers in the encoder, and in each layer output points are downsampled. In the first 3 × 3 × 3 InterpConv layer, we set the kernel length l as a small value in order to capture fine-grained geometric structures, which is important in semantic segmentation. We then gradually enlarge the kernel length l in the following blocks to capture context information. For the upsampling layers in the decoder, we utilize feature propagation layers following <ref type="bibr" target="#b26">[27]</ref>. Skip connections are added between layers that have the same number of output points. The decoder outputs are then fed into an InterpConv layer with kernel size 1 × 1 × 1 to obtain the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the efficacy of Interpolated Convolutional Neural Networks on multiple tasks, including shape classification, object part segmentation and indoor scene semantic parsing. In all experiments, we implement the models using CUDA and PyTorch on NVIDIA TITAN X GPUs, and we use the Adam optimizer. We first demonstrate the performances of our approach on those tasks. Then we discuss key components of our method in ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Shape Classification</head><p>Dataset. We evaluate the 3D shape classification performance of our network on the benchmark dataset Model-Net40 <ref type="bibr" target="#b4">[5]</ref>. ModelNet40 is composed of 12,311 CAD models which belong to 40 categories with 9,843 for training and 2,468 for testing. We use the point cloud conversion of ModelNet40, where 2,048 points are sampled from each CAD model. We further sample 1,024 points for training and testing following <ref type="bibr" target="#b24">[25]</ref>.</p><p>Implementation details. We adopt the classification network in <ref type="figure">Figure 2(a)</ref>. We use Gaussian interpolation as our interpolation function and fix the Gaussian bandwidth 3σ to 0.1 in all InterpConv blocks. Point clouds are downsampled to half of the input number after each 3×3×3 Inter-pConv layer. The input point clouds are randomly scaled by a factor ranging from 0.8 to 1.2, then jittered by a zero-mean Gaussian noise with 0.02 standard deviation. We trained the network for 480 epoches with initial learning rate 0.001 and decay rate 0.7 every 80 epoches with batch size 16.</p><p>Results. We report the overall accuracy on this dataset. In <ref type="table" target="#tab_2">Table 1</ref>, we compare our InterpCNN with other approaches. We demonstrate that the deep architecture based on InterpConvs performs much better than graph-based and voxel-based counterparts, with 0.8% improvement on the best graph-based network DGCNN <ref type="bibr" target="#b44">[45]</ref>. Our approach performs even better than Point2Seq <ref type="bibr" target="#b22">[23]</ref> and 3DCapsule <ref type="bibr" target="#b7">[8]</ref>   <ref type="figure">Figure 2</ref>. Interpolated Convolutional Neural Networks (InterpCNNs). Gray boxes indicate the size of input and output data, and other boxes are all network layers. In the classification network (a), we extend the idea of Inception module <ref type="bibr" target="#b37">[38]</ref> and stack two multi-branch, multi-receptive-field PointInception modules to capture both local and context geometric information. We note that kernel length l varies at different branches. In the segmentation network (b), we share similar spirit as U-Net <ref type="bibr" target="#b30">[31]</ref> and build an InterpConv-based deep encoderdecoder architecture. Kernel length l begins with a small value and becomes larger as the network goes deeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Acc. Subvolume <ref type="bibr" target="#b25">[26]</ref> voxels 89.2% VRN Single <ref type="bibr" target="#b3">[4]</ref> voxels 91.3% OctNet <ref type="bibr" target="#b29">[30]</ref> hybrid grid octree 86.5% ECC <ref type="bibr" target="#b34">[35]</ref> graphs 87.4% PointwiseCNN <ref type="bibr" target="#b10">[11]</ref> 1024 points 86.1% PointNet <ref type="bibr" target="#b24">[25]</ref> 1024 points 89.2% PointNet++ <ref type="bibr" target="#b26">[27]</ref> 1024 points 90.7% PointNet++ <ref type="bibr" target="#b26">[27]</ref> 5000 points+normal 91.9% Kd-Network <ref type="bibr" target="#b15">[16]</ref> 1024 points 91.8% ShapeContextNet <ref type="bibr" target="#b46">[47]</ref> 1024 points 90.0% KCNet <ref type="bibr" target="#b33">[34]</ref> 1024 points 91.0% PointCNN <ref type="bibr" target="#b19">[20]</ref> 1024 points 92.2% DGCNN <ref type="bibr" target="#b44">[45]</ref> 1024 points 92.2% SO-Net <ref type="bibr" target="#b18">[19]</ref> 2048 points 90.9% SpiderCNN <ref type="bibr" target="#b48">[49]</ref> 1024 points+normal 92.4% Point2Seq <ref type="bibr" target="#b22">[23]</ref> 1024 points 92.6% 3DCapsule <ref type="bibr" target="#b7">[8]</ref> 1024 points 92.7% PointConv <ref type="bibr" target="#b45">[46]</ref> 1024 points+normal 92.5% InterpCNN (ours) 1024 points 93.0% in which many modules and model compacity are added on top of PointNet++ <ref type="bibr" target="#b26">[27]</ref> to gain a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Part Segmentation</head><p>Dataset. We evaluate our segmentation network on the part segmentation dataset ShapeNet Parts <ref type="bibr" target="#b49">[50]</ref>. ShapeNet Parts consists of 16,880 models from 16 shape categories, with 14,006 for training and 2,874 for testing. Each model is annotated with 2 to 6 parts and there are 50 different parts in total. Each point sampled from the models is annotated with a part label.</p><p>Implementation details. We use the segmentation net-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cat.</head><p>Ins. mIOU mIOU PointNet <ref type="bibr" target="#b24">[25]</ref> 80.4% 83.7% PointNet++ <ref type="bibr" target="#b26">[27]</ref> 81.9% 85.1% FCPN <ref type="bibr" target="#b28">[29]</ref> -84.0% SyncSpecCNN <ref type="bibr" target="#b50">[51]</ref> 82.0% 84.7% SSCN <ref type="bibr" target="#b9">[10]</ref> 83.3% 86.0% SPLATNet <ref type="bibr" target="#b35">[36]</ref> 83.7% 85.4% SpiderCNN <ref type="bibr" target="#b48">[49]</ref> 81.7% 85.3% SO-Net <ref type="bibr" target="#b18">[19]</ref> 81.0% 84.9% PCNN <ref type="bibr" target="#b1">[2]</ref> 81.8% 85.1% KCNet <ref type="bibr" target="#b33">[34]</ref> 82.2% 83.7% ShapeContextNet <ref type="bibr" target="#b46">[47]</ref> -84.6% SpecGCN <ref type="bibr" target="#b40">[41]</ref> -85.4% 3DmFV <ref type="bibr" target="#b2">[3]</ref> 81.0% 84.3% RSNet <ref type="bibr" target="#b11">[12]</ref> 81.4% 84.9% PointCNN <ref type="bibr" target="#b19">[20]</ref> 84.6% 86.1% DGCNN <ref type="bibr" target="#b44">[45]</ref> 82.3% 85.1% SGPN <ref type="bibr" target="#b43">[44]</ref> 82.8% 85.8% PointConv <ref type="bibr" target="#b45">[46]</ref> 82.8% 85.7% Point2Seq <ref type="bibr" target="#b22">[23]</ref> -85.2% InterpCNN (ours) 84.0% 86.3% work in <ref type="figure">Figure 2</ref>(b). During training we randomly sample 2,048 points from each object and use the original point clouds for testing. Different from the classification network, we utilize a trilinear interpolation function with a smaller kernel length l, which is shown to perform much better. The kernel length l starts with 0.05 in the first InterpConv layer and doubles in the following layers. We use a minibatch of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall</head><p>Cat. Accuracy mIOU PointNet <ref type="bibr" target="#b24">[25]</ref> 78.5% 47.6% ShapeContextNet <ref type="bibr" target="#b46">[47]</ref> 81.6% 52.7% RSNet <ref type="bibr" target="#b11">[12]</ref> -56.5% PointCNN <ref type="bibr" target="#b19">[20]</ref> 88.1% 65.4% DGCNN <ref type="bibr" target="#b44">[45]</ref> 84.1% 56.1% SGPN <ref type="bibr" target="#b43">[44]</ref> 80.8% 50.4% SPGraph <ref type="bibr" target="#b17">[18]</ref> 85.5% 62.1% InterpCNN (ours) 88.7% 66.7% Results. We report mean IOU over categories and instances in <ref type="table" target="#tab_3">Table 2</ref>. It is worth noting that mean IOU over instances is more realistic. Our approach performs better than compared methods on mean IOU over instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Indoor Scene Segmentation</head><p>Dataset. S3DIS <ref type="bibr" target="#b0">[1]</ref> is an indoor sence semantic parsing dataset which contains 271 rooms in 6 areas. Each room is scanned by Matterport scanners and every point in the scan is annotated with one semantic label from 13 categories. We follow <ref type="bibr" target="#b24">[25]</ref> and split rooms into 1m × 1m blocks for training and testing.</p><p>Implementation details. Similar to the part segmentation task we use the same architecture in <ref type="figure">Figure 2(b)</ref>. The difference is that we take 4,096 points from each 1m × 1m block as inputs during training. We construct a 9D vector (XYZ, RGB, and the normalized location) for each input. Other configurations are the same as that in the object part segmentation task.</p><p>Results. Following <ref type="bibr" target="#b24">[25]</ref>, we adpot 6-fold validations on 6 areas, and we report overall accuracy and mean IOU over categories in <ref type="table" target="#tab_4">Table 3</ref>. Our approach significantly outperforms state-of-the-art methods on both accuracy and mean IOU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We perform ablation studies to investigate components of our InterpCNNs on ModelNet40 and ShapeNet Parts.</p><p>Effectiveness of kernel size n and kernel length l. We explore different settings of hyperparameter n and l for the 1st and 2nd PointInception module in <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref>. We first try the setting of all InterpConvs with kernel size 1×1×1 and only use a maxpooling layer to aggregate global features. We note that this architecture is similar to Point-Net <ref type="bibr" target="#b24">[25]</ref> in which the network cannot capture local structures, and the result is much worse. This indicates the great power of InterpConvs with kernel size more than 1. Simply replacing one 1×1×1 InterpConv with 3×3×3 for each In-terpConv block obtains a performance gain of 3%. We also try InterpConvs with a larger kernel size 5 × 5 × 5 but the performance does not improve. We demonstrate that utilizing 3 × 3 × 3 InterpConvs is sufficiently effective and this also reduces model parameters compared with the 5 × 5 × 5 counterparts. We also explore different kernel lengths and we show that this hyperparameter has a significant effect on the final performance. Either too small or too large the kernel length l will harm the accuracy.</p><p>Effectiveness of interpolation functions. We try both Gaussian and trilinear interpolation functions in all tasks. In <ref type="table">Table 6</ref>, the results show that Gaussian interpolation performs better in classification while trilinear interpolation is better in segmentaion. We argue that trilinear interpolation can capture fine-grained geometric structures better than Gaussian counterpart, which is more important for segmentation. Gaussian interpolation is able to obtain global shape information more effectively.</p><p>Effectiveness of normalization methods. We try normalization according to the number of neightboring points (Eq. <ref type="bibr" target="#b6">(7)</ref>) and the sum of interoplation weights (Eq. (8)). The results in <ref type="table">Table 7</ref> indicate that both two methods are effective and show comparable performances. It is worth noting that in extreme cases where there are only a few close points but many far away points in the neighborhood of a kernelweight coordinate, normalization on the sum of interpolation weights is more appropriate. Normalization method Accuracy interpolation weights 92.8% number of points 93.0% <ref type="table">Table 7</ref>. Results of different normalization methods on Model-Net40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Parameters Accuracy Subvolume <ref type="bibr" target="#b25">[26]</ref> 16.6M 89.2% PointNet <ref type="bibr" target="#b24">[25]</ref> 3.5M 89.2% PointNet++ (MSG) <ref type="bibr" target="#b26">[27]</ref> 12M 90.7% InterpCNN (ours) 12.8M 93.0% <ref type="table">Table 8</ref>. Model parameters and performance comparisons on Mod-elNet40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inference time Accuracy PointNet++ (MSG) <ref type="bibr" target="#b26">[27]</ref> 26.8ms 90.7% DGCNN <ref type="bibr" target="#b44">[45]</ref> 89.7ms 92.2% InterpCNN (ours) 31.4ms 93.0% <ref type="table">Table 9</ref>. Inference time comparisons on ModelNet40.</p><p>Model parameters analysis. We report the number of parameters in our classification network on ModelNet40. Results in <ref type="table">Table 8</ref> show that even with the comparable model parameters, PointNet++ still performs much worse than our approach. InterpCNNs also have fewer parameters than other 3D convolution methods.</p><p>Runtime analysis. We summarize average inference time based on the classification network with batch size 16, 1024 points on an NVIDIA TITAN X GPU, and compare it with pioneering work PointNet++ and DGCNN under the same settings. In <ref type="table">Table 9</ref>, average inference time of our ap- proach is slightly slower than PointNet++, but much faster than graph-based approach DGCNN. Visualization. We visualize activations by different kernels in the first 3 × 3 × 3 InterpConv layer in <ref type="figure">Figure 5</ref> and some failure cases in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel convolution InterpConv and Interpolated Convolutional Neural Networks (InterpCNNs) for 3D classification and segmentation. Experiments on Mod-elNet40, ShapeNet Parts and S3DIS show promising results compared with existing methods. For future work, we plan to explore new deep architectures based on learnable kernelweight coordinates, and apply our approach on other point cloud processing tasks including 3D detection and instance segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) 3D convolutions with rasterization (b) Graph neural networks (c) InterpConv with trilinear interpolation (d) InterpConv with Gaussian interpolation Illustration of different types of convolutions on point clouds. Red points denote point clouds and green points denote spatially-discrete kernel weights. Green lines in (b) denote continuous convolutional kernels. Our InterpConv directly takes irregular point clouds as inputs and interpolates point features to the neighboring kernel weights by an interpolation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>aggregate feature by the weighted sum of all neighboring point features. The aggregated features are further normalized to keep it sparsity invariant. Finally dot production is applied between the normalized features and kernel weight vectors. A convolutional kernel sums all the results and c kernels constitute a 1 × c new feature vector at the output coordinate. See the InterpConv algorithm for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of object part segmentation results on ShapeNet Parts. The first row is ground truth and the second row is our predictions. From left to right are cars, motorbikes, lamps and chairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative evaluation on S3DIS compared with Point-Net++ and DGCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Visualization of feature activations learned by different InterpConv kernels on ModelNet40. Visualization of failure cases on ShapeNet Parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 The InterpConv Algorithm Input: point coordinates p ∈ R 3 , point features f ∈ R c Output: output coordinatesp ∈ R 3 , new featuresf ∈ R c Parameter: c kernels with n weight vectors w ∈ R c and shared weight coordinates p ∈ R 3 in each kernel 1: Samplep from p orp ← p 2: for eachp do</figDesc><table><row><cell>3:</cell><cell>for each p do</cell></row><row><cell>4:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Classification results on ModelNet40. Overall accuracy is reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Segmentation results on ShapeNet Parts. Mean IoU over categories (Cat.) and instances (Ins.) are reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>6-fold validation results on S3DIS. Overall accuracy and mean IOU over categories are reported.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32">in each GPU and 4 GPUs to train a model. We set the initial learning rate to be 0.005. Data augmentation is the same as classification.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by SenseTime Group Limited, in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants CUHK14202217, CUHK14203118, CUHK14205615, CUHK14207814, CUHK14213616, CUHK14208417, CUHK14239816, in part by CUHK Direct Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">and Anath Fischer. 3d point cloud classification and segmentation using 3d modified fisher vector representation for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08241</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3dcapsule: Extending the capsule architecture to classify 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheraghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pointsift: A siftlike network module for 3d point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00652</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5010" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">So-net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fpnn: Field probing neural networks for 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02565</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully-convolutional point networks for large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards 3d point cloud based object maps for household environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dolha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="927" to="941" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dominant set clustering and pooling for multi-view 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="15607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07246</idno>
		<title level="m">Deep convolutional networks on 3d point clouds</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2282" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
