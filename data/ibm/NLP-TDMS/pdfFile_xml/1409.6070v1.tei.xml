<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatially-sparse convolutional neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-09-23">September 23, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
							<email>b.graham@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of Statistics</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatially-sparse convolutional neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-09-23">September 23, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>online character recognition</term>
					<term>convolutional neural network</term>
					<term>sparsity</term>
					<term>computer vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks.</p><p>Motivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82%.</p><p>Although pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28% on CIFAR-10 and 24.30% for CIFAR-100.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks typically consist of an input layer, a number of hidden layers, followed by a softmax classification layer. The input layer, and each of the hidden layers, is represented by a three-dimensional array with size, say, M × N × N . The second and third dimensions are spatial. The first dimension is simply a list of features available in each spatial location. For example, with RGB color images N × N is the image size and M = 3 is the number of color channels.</p><p>The input array is processed using a mixture of convolution and pooling operations. As you move forward through the network, N decreases while M is increased to compensate. When the input array is spatially sparse, it makes sense to take advantage of the sparsity to speed up the computation. More importantly, knowing you can efficiently process sparse images gives you greater freedom when it comes to preparing the input data.</p><p>Consider the problem of online isolated character recognition; online means that the character is captured as a path using a touchscreen or electronic stylus, rather than being stored as a picture. Recognition of isolated characters can be used as a building block for reading cursive handwriting, and is a challenging problem in its own right for languages with large character sets.</p><p>Each handwritten character is represented as a sequence of strokes; each stroke is stored as a list of x-and y-coordinates. We can draw the characters as N × N binary images: zero for background, one for the pen color. The number of pixels is N 2 , while the typical number of non-zero pixels is only O(N ), so the first hidden layer can be calculated much more quickly by taking advantage of sparsity.</p><p>Another advantage of sparsity is related to the issue of spatial padding for convolutional networks. Convolutional networks conventionally apply their convolutional filters in valid mode-they are only applied where they fit completely inside the input layer. This is generally suboptimal as makes it much harder to detect interesting features on the boundary of the input image. There are a number of ways of dealing with this.</p><p>• Padding the input image <ref type="bibr" target="#b0">[1]</ref> with zero pixels. This has a second advantage:</p><p>training data augmentation can be carried out in the form of adding translations, rotations, or elastic distortions to the input images. • Adding small amounts of padding to each of the convolutional layers of the network; depending on the amount of padding added this may be equivalent to applying the convolutions in full mode. This has a similar effect to adding lots of padding to the input image, but it allows less flexibility when it comes to augmenting the training data. • Applying the convolutional network to a number of overlapping subsets of the image <ref type="bibr" target="#b1">[2]</ref>; this is useful if the input images are not square. This can be done relatively computationally efficiently as there is redundancy in the calculation of the lower level convolutional filters. However, the (often large) fully connected classification layers of the network must be evaluated several times.</p><p>Sparsity has the potential to combine the best features of the above. The whole object can be evaluated in one go, with a substantial amount of padding added at no extra cost. In Section 2.1-2.2 we describe a family of convolutional networks with many layers of max-pooling. In Section 2.3-2.6 we describe how sparsity applies to character recognition and image recognition. In Section 3 we give our results. In Section 4 we discuss other possible uses of sparse CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep convolutional networks and spatial sparsity</head><p>Early convolutional networks tended to make use of pooling in a relatively restrained way, with two layers of pooling being quite typical. As computing power has become cheaper, the advantages of combining many layers of pooling with small convolutional filters to build much deeper networks have become apparent <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Applying pooling slowly, using many layers of 2 × 2 pooling rather than a smaller number of 3 × 3 or 4 × 4 layers, may be particularly important for handwriting recognition. The speed with which pooling is applied affects how a CNN will generalize from training data to test data. Slow max-pooling retains more spatial information; this is better for handwriting recognition, as handwriting is highly structured. Consider three small pictures:</p><formula xml:id="formula_0">o o o</formula><p>Using fast max-pooling, all three pictures look the same: they all contain a circle and a line. Using slower max-pooling, more spatial information is retained, so the first two pictures will be different from the third. For general input, slow pooling is relatively computationally expensive as the spatial size of the hidden layers reduces more slowly, see <ref type="figure">Figure 1</ref>. For sparse input, this is offset by the fact that sparsity is preserved in the early hidden layers, see <ref type="figure" target="#fig_1">Figure 2</ref>. This is particularly important for handwriting recognition, which needs to operate on relatively low-power tablet computers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DeepCNet( , k)</head><p>Inspired by <ref type="bibr" target="#b0">[1]</ref> we first consider a simple family of CNNs with alternating convolutional and max-pooling layers. We use two parameters and k to characterize the structure: there are + 1 layers of convolutional filters, separated by layers of 2 × 2 max-pooling. The number of convolutional filters in the n-th convolutional layer is taken to be nk. The spatial size of the filters is 3 × 3 in the first layer, and then 2 × 2 in the subsequent layers. Finally, at the top of the network is an output layer.</p><p>If the input layer has spatial size N × N with N = 3 × 2 , then the final convolution produces a fully connected hidden layer. We call this network DeepCNet( , k). For example, DeepCNet(4, 100) is the architecture from <ref type="bibr" target="#b0">[1]</ref> with input layer size N = 48 = 3 × 2 4 and four layers of max-pooling:</p><formula xml:id="formula_1">input-100C3-MP2-200C2-MP2-300C2-MP2-400C2-MP2-500C2-output</formula><p>For activation functions, we used the positive part function f (x) = max{0, x} for the hidden layers (rectified linear units) and softmax for the output layer.</p><p>The size of the hidden layers decays exponentially, so for general input the cost of evaluating the network is essentially just the cost of evaluating the first few layers: roughly speaking it is O(N 2 k 2 ). Conversely, the number of trainable parameters of the model is</p><formula xml:id="formula_2">3 2 M k + 2 2 k(k + 1) + 2 2 (2k)(3k) + · · · + 2 2 (lk)((l + 1)k) = O(l 3 k 2 );</formula><p>most of the parameters are located in the top few layers.  <ref type="figure">Figure 1</ref>: The hidden layer dimensions for l = 5 DeepCNets, and LeNet-7. In both cases, the spatial sizes decrease from 96 down to 1. The DeepCNet applies max-pooling much more slowly than LeNet-7.</p><p>When using dropout <ref type="bibr" target="#b3">[4]</ref> with a DeepCNets(l, k), we need l + 2 numbers to describe the amount of dropout applied to the input of the l + 1 convolutional layers and the classification layer. The lower levels of a DeepCNet seem to be fairly immune to overfitting, but applying dropout in the upper levels improved test performance for larger values of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DeepCNiN</head><p>Inspired by <ref type="bibr" target="#b2">[3]</ref>, we have tried modifying our DeepCNets by adding network-innetwork layers. A NiN layer is a convolutional layer where the filters have spatial size just 1 × 1. They can be thought of as single layer networks that increase the learning power of a convolutional layer without changing the spatial structure. We placed NiN layers after each max-pooling layer and the final convolutional layer. With k and as before, we call the resulting network DeepCNiN( , k). For example, DeepCNiN(4,100) is</p><formula xml:id="formula_3">input−100C3 − MP2 − 100C1 − . . . · · · −200C2 − MP2 − 200C1 − . . . · · · −300C2 − MP2 − 300C1 − . . . · · · −400C2 − MP2 − 400C1 − . . . · · · −500C2 − 500C1 − output</formula><p>As the spatial structure is unchanged, the cost of evaluating these networks is not much greater than for the corresponding DeepCNet. We took two steps to help the backpropagation algorithm operate effectively through such deep networks. First, we only applied dropout to the convolutional layers, not the NiN layers. Second, we used a form of leaky rectified linear units <ref type="bibr" target="#b4">[5]</ref>, taking the activation function to be</p><formula xml:id="formula_4">f (x) = x, x 0, x/3, x &lt; 0.</formula><p>Compared to <ref type="bibr" target="#b4">[5]</ref>, we have used x/3 rather than x/100 for the x &lt; 0 case. This seems to speed up learning without harming the representational power of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spatial sparsity for convolutional networks</head><p>Imagine putting an all-zero array into the input layer of a CNN. As you evaluate the network in the forward direction, the translational invariance of the input is propagated to each of the hidden layers in turn. We can therefore think of each hidden variable as having a ground state corresponding to receiving no meaningful input; the ground state is generally non-zero because of bias terms. When the input array is sparse, you only have to calculate the values of the hidden variables where they differ from their ground state. <ref type="figure" target="#fig_1">Figure 2</ref> shows how the active spatial locations change through the layers.</p><p>Essentially, we want to memoize the convolutional and pooling operations. Memoizing can be done using a hash table, but that would be inefficient here as for each operation there is only one input, corresponding to regions in the ground state, that we expect to see repeatedly.</p><p>Instead, to forward propagate the network we calculate two matrices for each layer of the network:</p><p>• A feature matrix which is a list of row vectors, one for the ground state, and one for each active spatial location in the layer; the width of the matrix is the number of features per spatial location. • A pointer matrix with size equal to the spatial size of the convolutional layer.</p><p>For each spatial location in the convolutional layer, we store the number of the corresponding row in the feature matrix.</p><p>This representation is very loosely biologically inspired. The human visual cortex separates into two streams of information: the dorsal (where) and ventral (what) streams. Similar data structures can be used in reverse order for backpropagation.</p><p>For a regular convolutional network, the convolutional and pooling operations within a layer can be performed in parallel on a GPU (or even spread over multiple GPUs). Exactly the same holds here; there is simply less work to do as we know the inactive output spatial locations are all the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Online Character Recognition</head><p>Online character recognition is the task of reading characters represented as a collection of pen-stroke paths. Two rather different techniques work particularly well for online Chinese character recognition.</p><p>• Render the pen strokes at a relatively high resolution, say n × n with n = 40, and then use a CNN as a classifier <ref type="bibr" target="#b0">[1]</ref>. • Draw the character in a much lower resolution grid, say n × n with n = 8.</p><p>In each square of the grid calculate an 8-dimensional histogram measuring the amount of movement in each of the 8 compass directions. The resulting 8n 2 = 512 dimensional vectors are suitable input for general purpose statistical classifiers <ref type="bibr" target="#b5">[6]</ref>.</p><p>The first representation records more accurately where the pen went, while the second is better at recording the direction the pen was taking. Using sparsity, we can try to get the best of both worlds. Combining the two representations gives an array of size (1+8)×n×n. Setting n = 64 gives a sparse representation of the character suitable for feeding into a CNN. This preserves sparsity as the histogram is all zero at sites the pen does not touch. Increasing the number of input features per spatial location only increases the cost of evaluating the first hidden layer, so for sparse input it tends to have a negligible impact on performance.</p><p>The idea of supplementing pictures of online pen strokes with extra features has been used before, for example in the context of cursive English handwriting recognition <ref type="bibr" target="#b6">[7]</ref>. The key difference to previous work is the use of sparsity to allow a substantial increase in the spatial resolution, allowing us to obtain good results for challenging datasets such as CASIA-OLHWDB1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Offline/Image recognition</head><p>In contrast to online character recognition, for offline character recognition you simply have a picture of each character-for example, the MNIST digit dataset <ref type="bibr" target="#b7">[8]</ref>. The 28 × 28 pictures have on average 150 non-zero pixels.</p><p>The input layer for a DeepCNet(5, ·) network is 96 × 96. Placing the MNIST digits in the middle of the input layer produces a sparse dataset as 150 is much smaller than 96 2 . The background of each MNIST picture is zero, so extending the images simply increases the size of the background.</p><p>To a lesser extent, we can also think of the 32 × 32 images in the CIFAR-10 and CIFAR-100 datasets as being sparse, again by placing them into a 96 × 96 grid. For the CIFAR datasets, we scale the RGB features to the interval [−1, 1]. Padding the pictures with zeros corresponds to framing the images with gray pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Object scale versus input scale</head><p>Let n denote the size of the sparse objects to be recognized. For offline/image recognition, this is the width of the images. For online character recognition, we are free to choose a scale n on which to draw the characters.</p><p>Given n, we must choose the -parameter such that the characters fit comfortably into the N × N input layer, where N = 3 × 2 . DeepCNets work well with n ≈ N/3. There are a number of ways to account for this:</p><p>• To process the n × n sized input down to a zero-dimensional quantity, the number = log 2 (N/3) of levels of 2 × 2 max-pooling should be approximately log 2 n. • Counting the number of paths through the CNN from the input to output layers reveals a plateau; see <ref type="figure" target="#fig_2">Figure 3</ref>. Each corner of the input layer has only one route to the output layer; in contrast, the central (N/3) × (N/3) points in the input layer each have 3 2 × 2 2( −1) such paths. • If the input-layer is substantially larger than the object to be recognized, then you can apply various transformations (translations, rotations, etc) to the training images without having to worry about truncating the input. • When the input is sparse, translating the input by 2 k corresponds to translating the output of the k-th level of 2 × 2 max-pooling by exactly one. Adding moderately large random shifts to the training data forces the network to learn how the alignment of input with the max-pooling layers affects the flow of information through the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The results are all test errors for single networks. We consider two ways of augmenting the training data:</p><p>• By translation, adding random shifts in the x and y directions.</p><p>• By affine transformations, using a randomized mix of translations, rotations, stretching and shearing operations.</p><p>In Section 3.1.1 we see that it is crucial to move the training data around inside the much larger input field. We guess that this is an unavoidable consequence of the slow max-pooling. It is therefore difficult to compare meaningfully DeepCNet test results with the results of other papers, as they often deliberately limit the use of data augmentation to avoid the temptation of learning from the test data. As a compromise, we restrict ourselves to only extending the MNIST dataset by translations. For other datasets, we use affine transformations, but we do not use elastic deformations or cropped versions of the training images.</p><p>Some language specific handwriting recognition systems preprocess the characters to truncate outlying strokes, or apply local scalings to make the characters more uniform. The aim is to remove variation between writing styles, and to make the most of an input layer that is limited in size. However, there is a danger that useful information is being discarded. Our method seems able to handle a range of datasets without preprocessing. This seems to be a benefit of the large input grid size and the use of slow max-pooling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Small online datasets: 10, 26 and 183 characters</head><p>We will first look at three relatively small datasets <ref type="bibr" target="#b9">[10]</ref> to study the effect of varying the network parameters. See <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>• The Pendigits dataset contains handwritten digits 0-9. It contain about 750 training characters for each of the ten classes. • The UJIpenchars database includes the 26 lowercase letters. The training set contains 80 characters for each of the 26 classes. • The Online Handwritten Assamese Characters Dataset contains 45 samples of 183 Indo-Aryan characters. We used the first 36 handwriting samples as the training set, and the remaining 9 samples for a test set.</p><p>As described in Section 2.6, when using a DeepCNet( , k) we draw the characters with size 2 × 2 in the center of the input layer of size (3 × 2 ) × (3 × 2 ). As described in Section 2.4, the number of features M per spatial location is either 1 (simple pictures) or 9 (pictures and 8-directional histogram features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Varying</head><p>We first explore the impact of increasing the number of layers of max-pooling; equivalently we are increasing the scale 2 at which the characters are drawn. For simplicity we only look here at the Assamese dataset. In terms of computational cost, increasing by one approximately doubles the number of active input spatial locations. When l = 6, characters are drawn in a 64 × 64 square, and about 6% of the spatial sites are active. With only 36 handwriting samples for training, we should expect the dataset to be quite challenging, given the large alphabet size, and the relatively intricate characters. We see in <ref type="table">Table 1</ref> that adding translations to the training data is clearly a crucial part of training DeepCNets. Adding more general affine transforms also helps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Adding 8-direction histogram features</head><p>Here we look at the effect of increasing the feature set size. To make the comparisons interesting, we deliberately restrict the DeepCNet k and parameters. Increasing k or is computationally expensive. In contrast, increasing M only increases the cost of evaluating the first layer of the CNN; in general for sparse CNNs the first layer represents only a small fraction of the total cost of evaluating the network. Thus increasing M is cheap, and we will see that it tends to improve test performance. We trained DeepCNet(4, 20) networks, extending the training set by either just translations or affine transforms. In <ref type="table" target="#tab_1">Table 2</ref> we see that adding extra features can improve generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">CASIA: 3755 character classes</head><p>The CASIA-OLHWDB1.1 <ref type="bibr" target="#b10">[11]</ref>   dataset with affine transformations. In <ref type="table" target="#tab_3">Table 3</ref> we see that sparsity allows us to get pretty good performance at relatively low computational cost, and good performance with a larger network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">ICDAR2013 Chinese Handwriting Recognition Competition Task 3</head><p>We entered a version of our program into the ICDAR2013 competition for task 3, trained on the CASIA OLHWDB1.0-1.2 datasets. Our entry won with a test error of 2.61%, compared to 3.13% for the second best entry. Human performance on the test dataset was reported to be 4.81%. Our ten-best-guesses included the correct character 99.88% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MNIST digits</head><p>Let us move now from online to offline handwriting recognition. We extended the MNIST training set by translations only, with shifts of up to ±2 pixels. We first trained a very small-but-deep network, DeepCNet <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10)</ref>. This gave a test error of 0.58%. Using a NVIDIA GeForce GTX 680 graphics card, we can classify 3000 characters per second. Training a DeepCNet(5, 60) with dropout per level of 0, 0, 0, 0.5, 0.5, 0.5, 0.5 resulted in a test error of 0.31%. Better results on the MNIST dataset have been obtained, but only by training committees of neural networks, and by using elastic distortions to augment the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CIFAR-10 and CIFAR-100 small pictures</head><p>These two dataset each contain 50,000 32 × 32 color training images, split between 10 and 100 categories, respectively. For each of the datasets, we extended the training set by affine transformations, and trained a DeepCNet(5,300) network with dropout per convolutional level of 0, 0, 0.1, 0.2, 0.3, 0.4, 0.5. The resulting test errors were 8.37% and 29.81%. For comparison <ref type="bibr" target="#b2">[3]</ref> reports errors of 8.81% (with data augmentation) and 35.68% (without data augmentation).</p><p>Adding Network in Network layers to give a DeepCNiN(5,300) network produced substantially lower test errors: 6.28% and 24.30%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have implemented a spatially-sparse convolutional neural network, and shown that it can be applied to the problems of handwriting and image recognition. Sparsity has allowed us to use CNN architectures that we would otherwise have rejected as being too slow.</p><p>For online character recognition, we have extended the dense 8-directional histogram representation to produce a sparse representation. There are other ways of creating sparse representation, for example by using path curvature as a feature. Trying other sparse representation could help improve performance.</p><p>Very recently, convolutional networks somewhat similar to our DeepCNiN, but trained with multiple GPUs or networks of computers, have been shown to perform well on the LSVRC2012 image dataset <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. For example, a network from <ref type="bibr" target="#b11">[12]</ref> uses layers of type C3-C3-MP2 as opposed to our C2-MP2-C1 DeepCNiN layers. Running larger, sparse CNNs could potentially improve accuracy or efficiency by making it practical to train on uncropped images, and with greater flexibility for training set augmentation.</p><p>Sparsity could also be used in combination with a segmentation algorithm. Segmentation algorithms divide an image into irregularly shaped foreground and background elements. A sparse CNN could be applied to classify efficiently the different segments. Also, by mixing and matching neighboring segments, a sparse CNN could be used to check if two segments are really both parts of a larger object.</p><p>Sparsity could also be used to implement higher dimensional convolutional networks, where the convolutional filters range over three or more dimensions <ref type="bibr" target="#b13">[14]</ref>. Higher dimensional convolutions can be used to analyze static objects in 3D, or objects moving in 2+1 or 3+1 dimensional space-time. Just as lines form sparse sets when embedded in two or higher dimensional spaces, surfaces form sparse sets when embedded in three or higher dimensional spaces. Possible applications include analyzing 3D representations of human faces (2D surfaces in 3D space) or analyzing the paths of airplanes (1D lines in 4D space-time).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The active spatial locations (black) when a circle with diameter 32 is placed in the center of a DeepCNet(5, ·) 96 × 96 input layer and fed forward. Sparsity is most important in the early stages where most of the spatial locations are in their ground state (gray).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Top: A comparison of the number of possible paths from the input layer (96 × 96) to the fully connected layer for = 5 DeepCNets and LeNet-7<ref type="bibr" target="#b8">[9]</ref>. The DeepCNet's larger number of hidden layers translates into a larger maximum number of paths, but with a narrower plateau.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Samples from the Pendigits, UJIpenchars and Assamese datasets (resolution 40 × 40) and from the CASIA dataset (80 × 80).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test errors with and without adding 8-directional histogram features to the input representation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test errors for CASIA-OLHWDB1.1.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgement</head><p>Many thanks to Fei Yin, Qiu-Feng Wang and Cheng-Lin Liu for their work organizing the ICDAR2013 competition.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study on the use of 8-directional features for online handwritten Chinese character recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">A</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="262" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">word-level training of a handwritten word recognizer based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Pattern Recognition</title>
		<editor>In IAPR</editor>
		<meeting>of the International Conference on Pattern Recognition<address><addrLine>Jerusalem</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994-10" />
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="88" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR&apos;04</title>
		<meeting>CVPR&apos;04</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CASIA online and offline Chinese handwriting databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>11th International Conference on Document Analysis and Recognition (ICDAR)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arxiv.org/abs/1409.1556</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.4842" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
