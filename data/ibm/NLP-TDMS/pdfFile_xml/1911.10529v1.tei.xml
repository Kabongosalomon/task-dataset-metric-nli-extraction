<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Su</surname></persName>
							<email>wensu@zstu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mechanical Engineering and Automation</orgName>
								<orgName type="institution">Zhejiang Sci-Tech University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfu</forename><surname>Wang</surname></persName>
							<email>zfwang@ustc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Intelligent Machines</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We rethink a well-known bottom-up approach for multiperson pose estimation and propose an improved one. The improved approach surpasses the baseline significantly thanks to (1) an intuitional yet more sensible representation, which we refer to as body parts to encode the connection information between keypoints, (2) an improved stacked hourglass network with attention mechanisms, (3) a novel focal L2 loss which is dedicated to "hard keypoint and keypoint association (body part) mining, and (4) a robust greedy keypoint assignment algorithm for grouping the detected keypoints into individual poses. Our approach not only works straightforwardly but also outperforms the baseline by about 15% in average precision and is comparable to the state of the art on the MS-COCO test-dev dataset. The code and pre-trained models are publicly available online 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The problem of multi-person pose estimation aims at recognizing and localizing the anatomical keypoints (or body joints) of all persons in a given image. Considerable progress has been made in this field, benefiting from the development of more powerful convolutional neural networks (CNNs), such as ResNet <ref type="bibr" target="#b6">(He et al. 2016)</ref> and DenseNet , and more representative benchmarks, such as MS-COCO <ref type="bibr" target="#b14">(Lin et al. 2014)</ref>.</p><p>Existing approaches tackling this problem can be divided into two categories: top-down and bottom-up. The top-down approaches, e.g. <ref type="bibr" target="#b19">Papandreou et al. 2017;</ref><ref type="bibr" target="#b23">Sun et al. 2019)</ref>, usually employ a state-ofthe-art (SOTA) detector, such as SSD <ref type="bibr" target="#b16">(Liu et al. 2016)</ref>, to capture all the persons from the image first. Then, the cropped persons are resized and fed into the SOTA pose estimator designed for a single person, e.g. <ref type="bibr" target="#b27">(Wei et al. 2016;</ref><ref type="bibr" target="#b18">Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b3">Chen et al. 2018)</ref>. In contrast, the bottom-up approaches, e.g. <ref type="bibr" target="#b19">(Papandreou et al. 2018;</ref><ref type="bibr" target="#b11">Kreiss, Bertoni, and Alahi 2019)</ref>, directly infer the keypoints and the connection information between keypoints of all persons in the image without a human detector. Afterwards, the keypoints are grouped to form multiple human poses based on the inferred connection information. 1 https://github.com/jialee93/Improved-Body-Parts Top-down approaches, e.g. <ref type="bibr" target="#b19">(Papandreou et al. 2017;</ref><ref type="bibr" target="#b23">Sun et al. 2019;</ref><ref type="bibr" target="#b13">Li et al. 2019)</ref>, usually have complicated structures and low performance-cost ratios. Compared with them, the bottom-up approaches, e.g. <ref type="bibr" target="#b17">(Newell, Huang, and Deng 2017;</ref><ref type="bibr" target="#b11">Kreiss, Bertoni, and Alahi 2019)</ref>, can be more efficient in inference and independent of human detectors. However, they have to group the keypoints correctly. And the keypoint grouping (paring or association in other words) can be a big challenge, resulting in another bottleneck for real-time usage <ref type="bibr" target="#b20">(Pishchulin et al. 2016;</ref><ref type="bibr" target="#b9">Iqbal and Gall 2016)</ref>. CMU-Pose <ref type="bibr" target="#b1">(Cao et al. 2017)</ref>, PersonLab <ref type="bibr" target="#b19">(Papandreou et al. 2018)</ref> and PifPaf <ref type="bibr" target="#b11">(Kreiss, Bertoni, and Alahi 2019)</ref> use the greedy parsing algorithm to group detected keypoints into individual poses and break through the bottleneck to some extent.</p><p>It is worth mentioning that the approach proposed by <ref type="bibr" target="#b1">(Cao et al. 2017</ref>) (referred to as CMU-Pose here for convenience) is the first bottom-up approach to perform the task of multiperson pose estimation in the wild with high accuracy, and almost in real time. Our approach is mainly inspired by this work but is more intuitive yet more powerful. Hence, CMU-Pose is selected to be the baseline approach in this work.</p><p>The main contributions of this paper are summarized as follows: (1) we rethink the encoding of joint association, which is named as Part Affinity Fields (PAFs) <ref type="bibr" target="#b1">(Cao et al. 2017)</ref>, and propose a simplified yet more reasonable one, which we call body parts, (2) we present an improved stacked hourglass network with attention mechanisms to generate high-res and high-quality heatmaps, (3) we design a novel loss to help the network learn "hard samples, and (4) we develop the greedy keypoint assignment algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work and Rethinking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Person Pose Estimation</head><p>Classical approaches tackling the problem of person pose estimation are mainly based on the pictorial structures <ref type="bibr" target="#b5">(Fischler and Elschlager 1973;</ref><ref type="bibr" target="#b0">Andriluka, Roth, and Schiele 2009)</ref> or the graphical models <ref type="bibr" target="#b2">(Chen and Yuille 2014)</ref>. They usually formulate this problem as a tree-structured or graphical model problem and detect the keypoints based on hand-crafted features . Recently, the SOTA approaches leverage advanced CNNs and more abundant datasets, making enormous progress in pose estimation. <ref type="figure">Figure 1</ref>: Qualitative comparison between our approach and CMU-Pose <ref type="bibr" target="#b1">(Cao et al. 2017)</ref>. Left: results produced by CMU-Pose. Right: our results. CMU-Pose works even when many people appear in the scene, but it suffers precision loss in keypoint localization and it can not detect or group the "hard" keypoints (such as the occluded keypoints) well. By comparison, our approach is more accurate in keypoint localization and more robust to complex poses and moderate overlaps.</p><p>Here, we mainly discuss the CNN based approaches.</p><p>DeepPose <ref type="bibr" target="#b25">(Toshev and Szegedy 2014)</ref> employs CNNs to solve this problem for the first time, by regressing the Cartesian coordinates of the joints (or keypoints) directly. By contrast, the work <ref type="bibr" target="#b24">(Tompson et al. 2014</ref>) presents CNNs to firstly predict the Gaussian response heatmaps of keypoints, and subsequently it obtains the keypoint positions via finding the local maximums in the heatmaps. Some up to date work, e.g. <ref type="bibr" target="#b19">(Papandreou et al. 2018;</ref><ref type="bibr" target="#b11">Kreiss, Bertoni, and Alahi 2019)</ref>, decomposes the problem of keypoint localization into two subproblems at each pixel location: (1) binary classification (0 or 1), and (2) regression of offset vector to the nearest keypoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Person Pose Estimation</head><p>Top-down approaches. Most of the SOTA results have been achieved by the top-down approaches, such as CPN  and HRNet . Benefiting from the existing well-trained person detectors, the SOTA top-down approaches bypass the difficult subproblem of human body detection and turn the detection challenges into their advantages. However, they depend on the human detector heavily and they perform the task in two separate steps. The inference time will significantly increase if many people appear together.</p><p>Bottom-up approaches. The bottom-up approaches, e.g. <ref type="bibr" target="#b1">(Cao et al. 2017;</ref><ref type="bibr" target="#b17">Newell, Huang, and Deng 2017;</ref><ref type="bibr" target="#b11">Kreiss, Bertoni, and Alahi 2019)</ref>, are more efficient in keypoint inference and do not rely on the human detector. However, they tend to be less accurate. One main reason is that too large and too small persons in the image are difficult to detect at the same time (pose variation and feature map downsampling make things even worse). Another main reason lies in the fact that the offset of only a few pixels away from the annotated keypoint location can lead to a big drop in the evaluation metrics ) on the MS-COCO benchmark <ref type="bibr" target="#b14">(Lin et al. 2014</ref>). On the contrary, the top-down approaches are immune to these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some Rethinking</head><p>The topic of scale invariance in pose estimation is of great importance. Image pyramid (or multi-scale search in other words) technique is usually employed during testing to cover the human poses of different scales as much as possible <ref type="bibr" target="#b1">(Cao et al. 2017;</ref><ref type="bibr" target="#b17">Newell, Huang, and Deng 2017;</ref><ref type="bibr" target="#b19">Papandreou et al. 2018)</ref>, while the network is supervised at relatively a smaller scale range during training. Besides, some related work, e.g. <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b4">Chu et al. 2017;</ref><ref type="bibr" target="#b10">Ke et al. 2018)</ref>, has designed special model structures to enhance the model invariance across scales.</p><p>High-res and high-quality feature maps (include the output heatmaps) are critical for accurate keypoint localization. Offset regression in PersonLab <ref type="bibr" target="#b19">(Papandreou et al. 2018)</ref> and CornerNet <ref type="bibr" target="#b12">(Law and Deng 2018)</ref>, integral pose regression , and retaining  or even magnifying ) the resolution of feature maps through the network are all good tries to relieve the precision loss (can not be avoided) caused by image or feature map resizing and small input or feature map size.</p><p>The encoding of connection (or association) information between keypoints is paid a lot of attention in some prior work, e.g. <ref type="bibr" target="#b1">(Cao et al. 2017;</ref><ref type="bibr" target="#b17">Newell, Huang, and Deng 2017;</ref><ref type="bibr" target="#b11">Kreiss, Bertoni, and Alahi 2019;</ref><ref type="bibr" target="#b19">Papandreou et al. 2018)</ref>. New representations bring about new ways of addressing problems. In this work, we only review the encoding of joint association named Part Affinity Fields <ref type="bibr" target="#b1">(Cao et al. 2017</ref>) due to the limited space.</p><p>Another topic worthy of mention is the problem of imbalanced data: "positive samples vs "negative samples (between classes) and "easy samples vs "hard samples (within classes). A (Gaussian response) heatmap has most of its area equal to zero (background) and only a small portion of it corresponds to the Gaussian distribution (foreground). Thus, the spread of the Gaussian peaks should be controlled properly to balance the foreground and background. On the other hand, too many easy samples (such as Gaussian peaks of facial keypoints and easy background pixels) can prevent the network from learning the "hard" samples (such as Gaussian peaks corresponding to occluded keypoints or body parts) well. These two types of data imbalance problems are critical and should be addressed properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Approach</head><p>We perform the task in three steps: (1) we predict the keypoint heatmaps and the body part heatmaps of all persons in a given image, (2) we get the candidate keypoints and body parts by performing Non-Maximum Suppression (NMS) on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition of Heatmaps</head><p>Considering the vague concept of keypoints and body parts, and human annotation variances (jitters), our network is supervised to regress the Gaussian responses (0∼1 values) around the keypoint or body part area before obtaining the final localization, introducing the smooth mapping regularization and forcing the network to learn more features nearby. The heatmaps here include keypoint heatmaps and body part heatmaps. Each pixel value in the keypoint heatmaps encodes the confidence that a nearest keypoint of a particular type occurs. We generate the ground truth keypoint heatmaps by putting unnormalized Gaussian distributions with a standard deviation σ k at all annotated keypoint positions. For example, the generated Gaussian peak of the right shoulder (RS) keypoint depicted in the Left of <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The body part here refers to the body area which lies between the two adjacent keypoints (for instance, the forearm area in the Left of <ref type="figure" target="#fig_0">Figure 2</ref>). A set of body parts are used to encode the connection information between keypoints and extract the visual patterns of human skeleton (see the Middle of <ref type="figure" target="#fig_0">Figure 2</ref>). Since body part segmentations are not available and person masks only cover visible human body areas, we use an elliptical area to approximately represent the body part. We generate the ground truth body part heatmaps by putting unnormalized elliptical Gaussian distributions with a standard deviation σ p in all body part areas.</p><p>By the way, we map the pixel p at the location (x, y) in the j-th ground truth heatmap to its original floating point</p><formula xml:id="formula_0">locationp(x,ỹ) =p(x · R + R/2 − 0.5, y · R + R/2 − 0.5)</formula><p>in the input image, in which R is the output stride, before generating the precise ground truth Gaussian peaks.</p><p>The PAFs proposed by <ref type="bibr" target="#b1">(Cao et al. 2017</ref>) is a 2D vector field for each pixel in the limb area, which encodes the location and direction information of a limb. All the pixels within the approximate limb area (may include outliers of the limb) have the same ground truth value, which brings about vagueness or even conflicts to the information representation. The body part representation is more sensible and composite. Pixels near to the major axes of the body parts have higher confidence and vice versa. And we only need the half dimensions of PAFs to encode the keypoint connection information, reducing the demand for model capacity.</p><p>During training, a single loss supervises the network to infer two kinds of Gaussian peaks, which have similar visual patterns and share the same formulation.</p><p>The standard deviations, σ k and σ p , control the spread of the Gaussian peaks and they should be set properly to balance the foreground pixels and background pixels. The hyper-parameters r 0 and d 0 (see their meanings in <ref type="figure" target="#fig_0">Figure  2</ref>) determine the boundaries of the ground truth Gaussian peaks, truncating the unnormalized Gaussian distribution at a fixed value thre. It plays a role in our loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Structure</head><p>Large receptive fields in CNN are critical for learning long range spatial relationships and can bring about accuracy improvement <ref type="bibr" target="#b27">(Wei et al. 2016</ref>). On the other hand, the detailed information (in smaller receptive fields) is needed for finegrained localization. To consolidate the global and local features, hourglass networks <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b17">Newell, Huang, and Deng 2017)</ref> have been designed to capture the different spatial extent information of each keypoint and association between keypoints by repeated bottom-up and top-down inference. In this work, we select the hourglass network, designed for multi-person pose estimation in Associative Embedding (AE) <ref type="bibr" target="#b17">(Newell, Huang, and Deng 2017)</ref>, as the base model and present an improved one. The improved variant, which we call Identity Mapping Hourglass Network (IMHN), significantly outperforms the hourglass network in AE <ref type="bibr" target="#b17">(Newell, Huang, and Deng 2017)</ref>.</p><p>The proposed IMHN, whose structure is depicted in <ref type="figure">Figure 3</ref>, is fully convolutional. It takes an image of any shape as the input and outputs multi-scale keypoint and body part heatmaps of all persons (if any) in the scene simultaneously. Before fed into the stacked hourglass modules, the original input is down-sampled twofold by some convolutional layers and max-pooling layers.</p><p>A first order hourglass module is designed as shown in <ref type="figure">Figure 4</ref>. The down-sampling path reduces the spatial extent of the input feature map by half once and increases the number of the feature map channels C by N (C = 256 and N = 128 in all experiments unless mentioned otherwise). After replacing the dashed box in <ref type="figure">Figure 4</ref> with another first order module, we get a second order module. A fourth order module can be made by repeating this operation and it is the default hourglass module to build the IMHN. Therefore, the feature map in the deepest path  <ref type="figure">Figure 3</ref>: Identity Mapping Hourglass Network with spatial attention and channel attention mechanisms. The feature maps at 5 different scales (see <ref type="figure">Figure 4</ref>, the feature maps here refer to those surrounded by the blue dashed box in all down-sampling paths) are extracted from each (stage) hourglass module and they are used to produce heatmaps of different scales. Only the heatmap regression at the 1/4 scale is illustrated due to space limitation. The regressed feature maps and heatmaps from the previous stage are transformed and reused in the next stage by element-wise addition (i.e., identity mappings).</p><p>of our IMHN has 768 channels. Please note that we just follow the related work <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b17">Newell, Huang, and Deng 2017)</ref> to use the fourth order hourglass module and the same C and N , ensuring we can make fair comparisons.</p><p>During training, multi-scale supervision <ref type="bibr" target="#b10">(Ke et al. 2018</ref>) marked with blue arrows in <ref type="figure">Figure 3</ref> is applied to supervise the fourth order stacked IMHN to infer heatmaps at 5 scales from coarse to fine, which explicitly introduces the "spatial attention mechanism. Supervising the network at smaller scales can force the network to capture multi-scale structure information of each keypoint and body part. The low-res heatmaps can provide the guidance of location refinement in the subsequent high-res layers, contributing to the generation of high-quality and high-res heatmaps. Incidentally, the ground truth heatmaps at fractional scales are down-sampled from the full-size ones using adaptive average-pooling.</p><p>The feature map at a certain scale, which is used to regress the heatmaps at the same scale, is highly self-correlated, for it encodes pose structure information. An SE (squeeze and excitation) block <ref type="bibr" target="#b7">(Hu, Shen, and Sun 2018)</ref> is inserted into each feature map at each scale to learn the channel relationships, which automatically introduces the "channel attention" mechanism. Here, we just employ existing techniques to quickly validate our thoughts.</p><p>Another important innovation in IMHN is that we add identity mappings between the same spatial extent feature maps and heatmaps across different stages (please refer to <ref type="figure">Figure 3</ref>). They can ease the network training experimentally: stabilizing different stages losses and helping the total loss converge faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions</head><p>The L2 loss is frequently used to measure the distance between the predicted heatmaps and the target heatmaps, e.g. <ref type="bibr" target="#b27">(Wei et al. 2016;</ref><ref type="bibr" target="#b1">Cao et al. 2017;</ref><ref type="bibr" target="#b10">Ke et al. 2018)</ref>. To handle the "hard" keypoints, the work  proposes the L2 loss with online "hard" keypoint mining. Here, we present a novel loss, which we refer to as focal L2 loss, under the unified definition of keypoint and body part heatmaps, to deal with the two types of sample imbalance problems as introduced in Section Some Rethinking.</p><p>At each stage of the stacked IMHN, K keypoint heatmaps and P body part heatmaps are inferred at 5 different scales. A pixel value in the inferred heatmaps represents the confidence of being a certain category of keypoint or body part. Assuming the predicted score maps (or heatmaps 2 ) of</p><formula xml:id="formula_1">size w i × h i at stage t are S t = S t 1 , S t 2 , · · · , S t K+P , t ∈ {1, 2, · · · , T },</formula><p>where T is the total number of stacked hourglass modules. Supposing the ground truth heatmaps of the same size are S * = S * 1 , S * 2 , · · · , S * K+P and the Gaussian peak generation function is G. Let S * j (p) denote the ground truth score at the pixel location p(x, y) ∈ R wi×hi in the j-th heatmap, we compute S * j (p) as:</p><formula xml:id="formula_2">S * j (p) = G(x, y|R, σ k , r 0 ), 1 ≤ j ≤ K G(x, y|R, σ p , d 0 ), K &lt; j ≤ K + P. (1)</formula><p>We define Sd t j (p):</p><formula xml:id="formula_3">Sd t j (p) = S t j (p) − α, S * j (p) &gt; thre 1 − S t j (p) − β, else,<label>(2)</label></formula><p>where thre (mentioned in Section Definition of Heatmaps) is the threshold to distinguish between the foreground heatmap pixels and the background heatmap pixels, and α, β are compensation factors to reduce the punishment of easy samples (both easy foreground pixels and easy background pixels) so that we can make full use of the training data. The focal L2 loss (FL) between the predicted heatmaps and target heatmaps of size w i × h i at stage t is computed as follow:  <ref type="figure">Figure 4</ref>: First order hourglass module. The two branches in this module extract different spatial features and merge them later by element-wise addition.</p><formula xml:id="formula_4">FL t i = K+P j=1 p∈R w i ×h i [η · I (j ≤ K) + 1] · W (p) · S t j (p) − S * j (p) 2 2 · 1 − Sd t j (p) 2 2 ,<label>(3)</label></formula><p>here, W is a binary mask with W (p) = 0 when the annotation is missing at the location p, I is the indicator function, and η is the hyper-parameter to balance the keypoint heatmap loss and body part heatmap loss. The presented scaling factor term 1 − Sd t j (p) 2 2 implies two prior information: the inferred responses (scores) of easy foregrounds tend to be high (close to 1, e.g.,0.9); the inferred responses of easy backgrounds tend to be low (usually less than 0.01 in practice). Thus, it can automatically down-weight the contribution of easy samples during training, which is inspired by Focal Loss <ref type="bibr" target="#b15">(Lin et al. 2017)</ref>.</p><p>In this work, we set σ k = 9, σ p = 7, thre = 0.01 for the gradient balance between the foreground and background pixels, and we set η = 2 accordingly. In addition, we set α = 0.1 and β = 0.02 roughly (α and β should be set close to 0 and α &gt; β &gt; 0). For better understandings of the important hyper-parameters, we provide more descriptions.</p><p>As for the standard deviations of keypoint and body part Gaussian peaks, i.e., σ k and σ p , if we set them too small, the accurate localization information is preserved but the inferred responses at these peaks tend to be low, resulting in more false negatives. On the other hand, if we set them too big, the Gaussian peaks spread so flat that the localization information tends to become vague at inference time, harming localization precision (using offset regression may relieve this problem). As to the hyper-parameter thre, it is set to 0.01 to significantly compress the loss of a mass of easy background pixels. After the network becomes able to distinguish the background well, then, the loss of the foreground starts to play a major role in the network learning.</p><p>The total loss of the stacked IMHN across 5 different scales can be written as:</p><formula xml:id="formula_5">L = T t=1 5 i=1 λ t i ·FL t i / 5 i=1 λ t i ,<label>(4)</label></formula><p>in which λ t 1 = 1, λ t 2 = 2, λ t 3 = 4, λ t 4 = 16 and λ t 5 = 64 are presented for the balance between losses at different scales. Now, "hard" keypoints and "hard" keypoint association (as the form of body parts ) can be learned better with the help of the proposed loss under our heatmap definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keypoint Assignment Algorithm</head><p>The candidate keypoints are assigned provided that the candidate body parts are assembled into corresponding human skeletons. We perform NMS (3 × 3 window) on the predicted heatmaps to find the candidate keypoints. Then, we obtain the candidate body parts that lie between the candidate adjacent keypoints, and calculate their scores that represent the confidence of being body parts, by sampling a set of Gaussian responses within the body part areas. After that, the candidate body parts of the same type are sorted in descending order according to the weighted scores of body parts and connected keypoints. Consequently, K sets of keypoints J = {Js 1 , Js 2 , · · · , Js K } and P sets of body parts L = {Ls 1 , Ls 2 , · · · , Ls P } are obtained.</p><p>Each element l i,j ∈ Ls i is a body part instance with type ID i, connected keypoints and weighted score S li,j . Here, one candidate keypoint can not be shared by two or more body parts of the same type, i.e., ∀ j, k and m, {l i,j , l i,k } ⊆ Ls i and j = k, we ensure that l i,j ∩ l i,k ∩ Js m = φ, where Js m ∈ J. This rule works in analogy to the NMS for candidate bounding boxes in object detection task.</p><p>Supposing the set of assembled human poses is H = {h 1 , h 2 , h 3 , · · ·}, in which h n ∈ H represents a single person pose. Then, h n has 1 ∼ P assigned body parts, corresponding type IDs and total score S hn . Our goal is to select the proper candidate body parts and find the best grouping strategy between the body parts in Ls 1 , Ls 2 , · · · , Ls P , such that hn H S hn reaches to its global maximum. Instead of solving the problem of global graph matching globally, CMU-Pose <ref type="bibr" target="#b1">(Cao et al. 2017</ref>) proposes a greedy algorithm upon a minimum spanning tree (MST) of human skeleton to match the adjacent tree nodes independently at only a fraction of the original computational cost.</p><p>We follow the greedy strategy in CMU-Pose and assemble the human skeletons (see the Middle of <ref type="figure" target="#fig_0">Figure 2</ref>) by matching adjacent body parts independently. Our keypoint assignment algorithm is based on several simple connection rules. As redundant body parts are introduced in our human skeleton, the assembled body parts having lower scores are removed by the body parts having higher scores, which share the same connected keypoint(s).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Implementation Details</head><p>Dataset and evaluation metrics. Our models are trained and evaluated on the MS-COCO dataset <ref type="bibr" target="#b14">(Lin et al. 2014)</ref>, which consists of the training set (includes around 60K images), the test-dev set (includes around 20K images) and the validation set (includes 5K images). The MS-COCO evaluation metrics, OKS-based 3 average precision (AP) and average recall (AR), are used to evaluate the results.</p><p>Training details. The training images with random transformations are cropped and resized to the fixed spatial extent of 384 × 384. And the generated ground truth heatmaps have the size of 96 × 96. We implement both the 3-stage IMHN and 4-stage IMHN using Pytorch. To train the networks, we use the SGD optimizer with the learning rate of 1e-4 (multiplied by 0.2 for every 15 epochs), the momentum of 0.9, the batch size of 32 and the weight decay of 5e-4. We train the IMHNs with L2 loss first and then continue to train them with the focal L2 loss (FL) until the performance refuses to improve. The last but not least, our networks are in mixed precision to reduce the memory consumption and speed up the experiments. The implemented 4-stage IMHN 3 OKS (Object Keypoint Similarity) defines the similarity between different human poses. Only the top 20 scoring poses are considered during evaluation.</p><p>(see Ours-3 in <ref type="table" target="#tab_3">Table 2</ref>) can be trained at the speed of 33 FPS with 4 RTX 2080TI GPUs, and we train it for about 3 days.</p><p>Testing details. We first resize and pad the input image so that it fits the network. Then, the input image is inferred at multiple scales (for example, ×0.5, ×1, ×1.5, ×2) with flip augmentation. Next, the inferred heatmaps are averaged across scales (i.e., multi-scale search). After collecting the poses from the heatmaps via the keypoint assignment algorithm, we sort them in descending order according to their scores in heatmaps. To compare equally, we have run the released models of CMU-Pose <ref type="bibr" target="#b1">(Cao et al. 2017)</ref>, AE <ref type="bibr" target="#b17">(Newell, Huang, and Deng 2017)</ref> and PifPaf <ref type="bibr" target="#b11">(Kreiss, Bertoni, and Alahi 2019)</ref>, obtaining their results without the refinement for detected person poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analyses</head><p>Results on the MS-COCO dataset. We have trained all our networks from scratch only with the MS-COCO data. The hyper-parameters of our approach are tuned according to the performance on the validation set. We compare our approach with the state of the art in Tables 1 and 2. The backbone of 4-stage IMHN has nearly the same number of convolutional layers as ResNet-101 <ref type="bibr" target="#b6">(He et al. 2016</ref>). Thus, we specially compare our networks with those based on ResNet-101 backbone in PersonLab <ref type="bibr" target="#b19">(Papandreou et al. 2018)</ref> and PifPaf <ref type="bibr" target="#b11">(Kreiss, Bertoni, and Alahi 2019)</ref> impar-tially. The entries marked with "*" in <ref type="table" target="#tab_2">Tables 1 and 2</ref> are results reported in their original papers.</p><p>The notations in our tables are explained as follows: "Input" denotes the long edge of the test image, "Stride" is the ratio of the input image size to the output feature map size, "MST" is short for minimum spanning tree of human skeleton without redundant body parts, "CMU-Net" represents the cascaded CNN used in CMU-Pose <ref type="bibr" target="#b1">(Cao et al. 2017)</ref>, "Hourglass" denotes the hourglass network, "Refine" indicates whether or not the result is additionally refined by a single person pose estimator. The entry named "Top-down" in <ref type="table" target="#tab_2">Table 1</ref> is a top-down approach cited from CPN . It employs a SOTA human detector and an 8-stage hourglass network for single person pose estimation.</p><p>Inference speed. The speed of our system is tested on the MS-COCO test-dev set <ref type="bibr" target="#b14">(Lin et al. 2014</ref>) .</p><p>• Inference speed of our 4-stage IMHN with 512 × 512 input on one 2080TI GPU: 38.5 FPS (100% GPU-Util). • Processing speed of the keypoint assignment algorithm that is implemented in pure Python and a single process on CPU: 5.2 FPS (has not been well accelerated).</p><p>Ablation studies. Some examples of the qualitative comparison between our approach and CMU-Pose is illustrated in <ref type="figure">Figure 1</ref>. The detailed ablation experiments of our approach, numbered for clarity, are shown in <ref type="table" target="#tab_2">Table 1</ref>. Experiments 1 and 4 use different encodings of keypoint association (PAFs and body parts respectively). They reveal that the encoding of body parts is better. According to experiments 2 and 6, our approach equipped with the same network and L2 loss is comparable to AE which utilizes validation data. The focal L2 loss can bring about a 3∼4% AP improvement over the L2 loss (see experiment 5 vs 4, experiment 8 vs 7 and experiment 14 vs 13) under the same definition of heatmaps. Experiments 8 and 11 demonstrate doing "hard" keypoint and body part (keypoint association) mining meanwhile is better. The 3-stage IMHN can lead to around a 5% AP improvement compared with the 3-stage CMU-Net (see experiment 7 vs 4 and experiment 8 vs 5), and the 4-stage IMHN outperforms the 4-stage hourglass network (see experiment 13 vs 6) by a big margin, indicating IMNHs' advantages. The introduced "spatial attention" and "channel attention" mechanisms contribute 1.2% and 0.4% AP increase respectively to the 3-stage IMHN (see experiments 8, 9 and 10). The redundant connections in human skeleton bring about a 0.7% AP improvement according to the comparative experiments of 8 and 12. Bigger input size or feature map size and more stacks can improve the accuracy consistently according to the results in <ref type="table" target="#tab_2">Table 1</ref>. Thus, we continue to train the model in experiment 14 with 512 × 512 input and more data augmentation, and obtain the model named "4-stage IMHN plus" in experiment 16. Further, the 4-stage IMHN is able to fit the MS-COCO train-val data at 74.1% AP (see experiment 17), indicating the big promotion space of our system.</p><p>Comparisons with the state of the art. All the models compared in the tables are evaluated without model ensemble. Some SOTA top-down approaches may outperform all the SOTA bottom-up approaches including ours, but they all depend on advanced human detectors and use very powerful networks for single person pose estimation. According to the results, it is safe to conclude that our approach outperforms the baseline by a big margin (experiment 5 vs 1 in <ref type="table" target="#tab_2">Table 1</ref> and Ours-1 vs the baseline in <ref type="table" target="#tab_3">Table 2</ref>) and even surpasses the TOP-DOWN approaches with equal level backbones (please refer to "Top-down" in <ref type="table" target="#tab_2">Table 1</ref> and "G-RMI" in <ref type="table" target="#tab_3">Table 2)</ref>.</p><p>Our approach has achieved comparable or even better results on both single scales (see the experiments with IDs 18∼ 22 in <ref type="table" target="#tab_2">Table 1</ref>) and multiple scales (see the results in <ref type="table" target="#tab_3">Table 2</ref>), compared with the latest SOTA bottom-up approaches, PifPaf and PersonLab, under fair conditions. However, PersonLab benefits greatly from the big input size (see experiment 22 vs 21 in <ref type="table" target="#tab_2">Table 1</ref>). It can be seen that PifPaf and Personlab are superior to our approach in keypoint average recall (AR). But our approach is superior to them when it comes to the robustness to person scales. Both PersonLab and PifPaf drop over 10% from AP L metric to AP M metric (see the entries in <ref type="table" target="#tab_3">Table 2</ref>), while our approach (Ours-4) performs well regardless of the person scales, be they middle scales (M) and large scales (L). What is more, most of other work benefits greatly from the networks pre-trained for the ImageNet classification task <ref type="bibr" target="#b21">(Russakovsky et al. 2015)</ref>, while we train networks from scratch (see experiment 17).</p><p>Referring to the results on the MS-COCO test-dev set in <ref type="table" target="#tab_3">Table 2</ref>, the proposed techniques except for the designed IMHN bring about a 6.4% AP improvement (Ours-1 vs baseline). The 3-stage IMHN can lead to a 5.9% AP increase compared with the 3-stage CMU-Net (Ours-2 vs Ours-1). The 4-stage IMHN can further contribute a 1% AP improvement over the 3-stage IMHN. And our final model, "4-stage IMHN plus" with bigger input (means bigger output feature maps), brings about a 15.1% AP improvement in total over the baseline. Incidentally, our approach significantly outperforms CMU-Pose and AE, though they have additionally refined the results using a single person pose estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we rethink and develop a bottom-up approach for multi-person pose estimation. We provide some insights into valuable design choices: (1) doing hard sample mining of keypoint and keypoint association meanwhile, (2) using a powerful network to generate high-res and high-quality heatmaps, and (3) introducing the scale invariance across person scales, which are more critical to improve the performance. The experimental results have demonstrated the significant improvement achieved by our approach over the baseline (+15.1% AP on the MS-COCO test-dev dataset). To the best of our knowledge, our approach, which is straightforward and easy to follow, is the first bottom-up approach to provide both the source code and pre-trained models with over 67% AP on the MS-COCO test-dev dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Definition of heatmaps. Left: examples of keypoint Gaussian peak and body part Gaussian peak. Middle: the human skeleton with redundant connections (which we refer to as redundant body parts). Right: examples of the inferred heatmaps by our network in practice. the inferred heatmaps, and (3) we perform the keypoint assignment algorithm and collect all individual poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>SE Block SE Block Convs Sum Convs + Poolings ··· ··· Convs 1×1 Conv 1×1 Conv</head><label></label><figDesc></figDesc><table><row><cell>384×384×3</cell><cell cols="2">96×96×256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input Image</cell><cell></cell><cell>··· 1 16 ··· 1 8</cell><cell>1 4</cell><cell>··· 1 2</cell><cell>··· 1</cell><cell>··· 16 1</cell><cell>8 1</cell><cell>··· 2 4 1 1</cell><cell>··· 1</cell><cell>Output Heatmaps</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>···</cell></row><row><cell>Hourglass Module</cell><cell>Feature Map</cell><cell>Heatmaps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on the MS-COCO 2017 validation set.</figDesc><table><row><cell>ID</cell><cell>Method</cell><cell cols="4">Input Stride FL AP ID</cell><cell>Method</cell><cell cols="4">Input Stride FL AP</cell></row><row><cell>1</cell><cell>CMU-Pose (6-stage CMU-Net)</cell><cell>368</cell><cell>8</cell><cell>N</cell><cell>56.0 12</cell><cell>3-stage IMHN, w/ MST</cell><cell>384</cell><cell>4</cell><cell>Y</cell><cell>65.1</cell></row><row><cell>2</cell><cell>AE (4-stage Hourglass, + val data)</cell><cell>512</cell><cell>4</cell><cell>N</cell><cell>59.7 13</cell><cell>4-stage IMHN</cell><cell>384</cell><cell>4</cell><cell>N</cell><cell>64.5</cell></row><row><cell>3</cell><cell>Top-down  *  (8-stage Hourglass)</cell><cell>256</cell><cell>4</cell><cell>N</cell><cell>66.9 14</cell><cell>4-stage IMHN</cell><cell>384</cell><cell>4</cell><cell>Y</cell><cell>67.3</cell></row><row><cell>4</cell><cell>Ours (3-stage CMU-Net)</cell><cell>368</cell><cell>8</cell><cell>N</cell><cell>56.5 15</cell><cell>4-stage IMHN, + val data</cell><cell>384</cell><cell>4</cell><cell>Y</cell><cell>72.3</cell></row><row><cell>5</cell><cell>Ours (3-stage CMU-Net)</cell><cell>368</cell><cell>8</cell><cell>Y</cell><cell>60.7 16</cell><cell>4-stage IMHN plus</cell><cell>512</cell><cell>4</cell><cell>Y</cell><cell>69.1</cell></row><row><cell>6</cell><cell>Ours (4-stage Hourglass)</cell><cell>512</cell><cell>4</cell><cell>N</cell><cell cols="2">60.0 17 4-stage IMHN plus, + val data</cell><cell>512</cell><cell>4</cell><cell>Y</cell><cell>74.1</cell></row><row><cell>7</cell><cell>3-stage IMHN</cell><cell>384</cell><cell>4</cell><cell>N</cell><cell>61.5 18</cell><cell>4-stage IMHN, one scale</cell><cell>768</cell><cell>4</cell><cell>Y</cell><cell>63.4</cell></row><row><cell>8</cell><cell>3-stage IMHN</cell><cell>384</cell><cell>4</cell><cell>Y</cell><cell cols="2">65.8 19 4-stage IMHN plus, one scale</cell><cell>768</cell><cell>4</cell><cell>Y</cell><cell>65.9</cell></row><row><cell>9</cell><cell>3-stage IMHN, w/o spatial attention</cell><cell>384</cell><cell>4</cell><cell>Y</cell><cell cols="2">64.6 20 PifPaf (ResNet-101), one scale</cell><cell>801</cell><cell>8</cell><cell>-</cell><cell>65.7</cell></row><row><cell cols="2">10 3-stage IMHN, w/o channel attention</cell><cell>384</cell><cell>4</cell><cell>Y</cell><cell>65.4 21</cell><cell>PersonLab  *  , one scale</cell><cell>801</cell><cell>8</cell><cell>-</cell><cell>61.2</cell></row><row><cell cols="2">11 3-stage IMHN, FL only for keypoint</cell><cell>384</cell><cell>4</cell><cell>Y</cell><cell>64.2 22</cell><cell>PersonLab  *  , one scale</cell><cell>1401</cell><cell>8</cell><cell>-</cell><cell>66.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the MS-COCO 2017 test-dev set. Backbone Pretrain Train Input Test Input Refine AP AP M AP L AR AR 50</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Bottom-up: multi-person keypoint detection and grouping</cell><cell></cell><cell></cell></row><row><cell>CMU-Pose (baseline) (Cao et al. 2017)</cell><cell>6-stage CMU-Net</cell><cell>N</cell><cell>368×368</cell><cell>∼ 368 2</cell><cell>N</cell><cell>52.9 50.9 57.2 57.0 79.2</cell></row><row><cell>CMU-Pose  *  (Cao et al. 2017)</cell><cell>6-stage CMU-Net</cell><cell>N</cell><cell>368×368</cell><cell>∼ 368 2</cell><cell>Y</cell><cell>61.8 57.1 68.2 66.5 87.2</cell></row><row><cell>AE  *  (Newell, Huang, and Deng 2017)</cell><cell>4-stage Hourglass</cell><cell>N</cell><cell>512×512</cell><cell>∼ 512 2</cell><cell>Y</cell><cell>65.5 60.6 72.6 70.2 89.5</cell></row><row><cell>PersonLab  *  (Papandreou et al. 2018)</cell><cell>ResNet-101</cell><cell>Y</cell><cell>801×801</cell><cell>∼ 1401 2</cell><cell>N</cell><cell>67.8 63.0 74.8 74.5 92.2</cell></row><row><cell>PifPaf (Kreiss, Bertoni, and Alahi 2019)</cell><cell>ResNet-101</cell><cell>Y</cell><cell>401×401</cell><cell>∼ 641 2</cell><cell>N</cell><cell>64.9 60.6 71.2 70.3 90.2</cell></row><row><cell>PifPaf  *  (Kreiss, Bertoni, and Alahi 2019)</cell><cell>ResNet-152</cell><cell>Y</cell><cell>401×401</cell><cell>∼ 641 2</cell><cell>N</cell><cell>66.7 62.4 72.9 72.2 90.9</cell></row><row><cell>Ours-1, w/ FL</cell><cell>3-stage CMU-Net</cell><cell>N</cell><cell>368×368</cell><cell>∼ 368 2</cell><cell>N</cell><cell>59.3 56.2 63.8 63.5 84.6</cell></row><row><cell>Ours-2, w/ FL</cell><cell>3-stage IMHN</cell><cell>N</cell><cell>384×384</cell><cell>∼ 384 2</cell><cell>N</cell><cell>65.2 63.7 68.5 69.8 87.7</cell></row><row><cell>Ours-3, w/ FL</cell><cell>4-stage IMHN</cell><cell>N</cell><cell>384×384</cell><cell>∼ 384 2</cell><cell>N</cell><cell>66.2 66.4 66.6 71.2 88.6</cell></row><row><cell>Ours-4 (final), w/ FL</cell><cell>4-stage IMHN plus</cell><cell>N</cell><cell>512×512</cell><cell>∼ 512 2</cell><cell>N</cell><cell>68.1 66.8 70.5 72.1 88.2</cell></row><row><cell>Ours-5, w/ FL</cell><cell>4-stage IMHN plus</cell><cell>N</cell><cell>512×512</cell><cell>∼ 384 2</cell><cell>N</cell><cell>67.6 64.5 72.6 71.3 87.6</cell></row><row><cell cols="5">Top-down: human detection and single-person keypoint detection</cell><cell></cell><cell></cell></row><row><cell>G-RMI  *  (Papandreou et al. 2017)</cell><cell>RseNet-101</cell><cell>Y</cell><cell>353×257</cell><cell>353×257</cell><cell>-</cell><cell>64.9 62.3 70.0 69.7 88.7</cell></row><row><cell>CPN  *  (Chen et al. 2018)</cell><cell>ResNet-Inception</cell><cell>Y</cell><cell>384×288</cell><cell>384×288</cell><cell>-</cell><cell>72.1 68.7 77.2 78.5 95.1</cell></row><row><cell>HRNet-W48  *  (Sun et al. 2019)</cell><cell>HRNet-W48</cell><cell>Y</cell><cell>384×288</cell><cell>384×288</cell><cell>-</cell><cell>75.5 71.9 81.5 80.5 95.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use "heatmap" and "score map" interchangeably throughout our paper for clarity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>Special thanks to Assoc. Prof. Qiongru Zheng who helped check and correct language mistakes.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Computers</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1973" />
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="67" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="713" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<title level="m">Rethinking on multi-stage networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3711" to="3719" />
		</imprint>
	</monogr>
	<note>The European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Magnify-net for multi-person 2d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
