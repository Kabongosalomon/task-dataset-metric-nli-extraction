<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MultiGrain: a unified image embedding for classes and instances</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esat-Psi</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MultiGrain: a unified image embedding for classes and instances</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MultiGrain is a network architecture producing compact vector representations that are suited both for image classification and particular object retrieval. It builds on a standard classification trunk. The top of the network produces an embedding containing coarse and fine-grained information, so that images can be recognized based on the object class, particular object, or if they are distorted copies. Our joint training is simple: we minimize a cross-entropy loss for classification and a ranking loss that determines if two images are identical up to data augmentation, with no need for additional labels. A key component of MultiGrain is a pooling layer that takes advantage of high-resolution images with a network trained at a lower resolution.</p><p>When fed to a linear classifier, the learned embeddings provide state-of-the-art classification accuracy. For instance, we obtain 79.4% top-1 accuracy with a ResNet-50 learned on Imagenet, which is a +1.8% absolute improvement over the AutoAugment method. When compared with the cosine similarity, the same embeddings perform on par with the state-of-the-art for image retrieval at moderate resolutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image recognition is central to computer vision, with dozens of new approaches being proposed every year, each optimized for particular aspects of the problem. From coarse to fine, we may distinguish the recognition of (a) classes, where one looks for a certain type of object regardless of intra-class variations, (b) instances, where one looks for a particular object despite changes in the viewing conditions, and (c) copies, where one looks for a copy of a specific image despite edits. While these problems are in many ways similar, the standard practice is to use specialized, and thus incompatible, image representations for each case.</p><p>Specialized representations may be accurate, but constitute a significant bottleneck in some applications. Consider for example image retrieval, where the goal is to match a query image to a large database of other images. Very often * Did this work during an internship at Facebook AI Research. <ref type="figure" target="#fig_4">Figure 1</ref>: Our goal is to extract an image descriptor incorporating different levels of granularity, so that we can solve, in particular, classification and particular object recognition tasks: The descriptor is either fed to a linear classifier, or directly compared with cosine similarity. one would like to search the same database with multiple granularities, by matching the query by class, instance, or copy. The performance of an image retrieval system depends primarily on the image embeddings it uses. These strike a trade-off between database size, matching and indexing speed, and retrieval accuracy. Adopting multiple embeddings, narrowly optimized for each type of query, means multiplying the resource usage.</p><p>In this paper we present a new representation, Multi-Grain, that can achieve the three tasks together, regardless of differences in their semantic granularity, see <ref type="figure" target="#fig_4">fig. 1</ref>. We learn MultiGrain by jointly training an image embedding for multiple tasks. The resulting representation is compact and outperforms narrowly-trained embeddings.</p><p>Instance retrieval has a wide range of industrial applications, including detection of copyrighted images and exemplar-based recognition of unseen objects. In settings where billion of images have to be treated, it is of interest to obtain image embeddings suitable for more than one recognition task. For instance, an image storage platform is likely to perform some classification of the input images, aside from detecting copies or instances of the same object. An embedding relevant to all these tasks advantageously reduces both the computing time per image and storage space.</p><p>In this perspective, convolutional neural networks (CNNs) trained only for classification already go a long way towards universal features extractors. The fact that we can learn image embeddings that are simultaneously good for classifi-cation and instance retrieval is surprising but not contradictory. In fact, there is a logical dependence between the tasks: images that contain the same instance also contain, by definition, the same class; and copied images contain the same instance. This is in contrast to multi-task settings where tasks are in competition and are thus difficult to combine. Instead, both class, instance, and copy congruency lead to embeddings that should be close in feature space. Still, the degree of similarity is different in the different cases, with classification requiring more invariance to appearance variations and copy detection sensitivity to small image details.</p><p>In order to learn an image representation that satisfies the different trade-offs, we start from an existing image classification network. We use a generalized mean layer that converts a spatial activation map to a fixed-size vector. Most importantly, we show that it is an effective way to learn an architecture that can adapt to different resolutions at test time, and offer higher accuracies. This circumvents the massive engineering and computational effort needed to learn networks for larger input resolutions <ref type="bibr" target="#b17">[18]</ref> The joint training of classification and instance recognition objectives is based on cross-entropy and contrastive losses, respectively. Remarkably, instance recognition is learned for free, without using labels specific to instance recognition or image retrieval: we simply use the identity of the images as labels, and data augmentation as a way to generate different versions of each image.</p><p>In summary, our main contributions are as follows:</p><p>• We introduce the MultiGrain architecture, which outputs an image embedding incorporating different levels of granularity. Our dual classification+instance objective improves the classification accuracy on its own. • We show that part of this gain is due to the batching strategy, where each batch contains repeated instances of its images with different data augmentations for the purpose of the retrieval loss; • We incorporate a pooling layer inspired by image retrieval. It provides a significant boost in classification accuracy when provided with high-resolution images.</p><p>Overall, our architecture offers competing performance both for classification and image retrieval. Noticeably, we report a significant boost in accuracy on Imagenet with a ResNet-50 network over the state of the art. The paper is organized as follows. Section 2 introduces related works. Section 3 introduces our architecture, the training procedure and explains how we adapt the resolution at test time. Section 4 reports the main experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image classification. Most computer vision architectures designed for a wide range of tasks leverage a trunk architecture initially designed for classification, such as Residual networks <ref type="bibr" target="#b16">[17]</ref>. An improvement on the trunk architecture eventually translates to better accuracies in other tasks <ref type="bibr" target="#b15">[16]</ref>, as shown on the detection task of the LSVRC'15 challenge. While recent architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46]</ref> have exhibited some additional gains, other lines of research have been investigated successfully. For instance, a recent trend <ref type="bibr" target="#b29">[30]</ref> is to train high capacity networks by leveraging much larger training sets of of weakly annotated data. To our knowledge, the state of the art on Imagenet ILSVRC 2012 benchmark for a model learned from scratch on Imagenet train data only is currently hold by the gigantic AmoebaNet-B architecture <ref type="bibr" target="#b22">[23]</ref> (557M parameters), which takes 480x480 images as input.</p><p>In our paper, we choose ResNet-50 <ref type="bibr" target="#b16">[17]</ref> (25.6M parameters), as this architecture is adopted in the literature in many works both on image classification and instance retrieval.</p><p>Image search: from local features to CNN. "Image search" is a generic retrieval task that is usually associated with and evaluated for more specific problems such as landmark recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33]</ref>, particular object recognition <ref type="bibr" target="#b30">[31]</ref> or copy detection <ref type="bibr" target="#b7">[8]</ref>, for which the objective is to find the images most similar to the query in a large image collection. In this paper "image retrieval" will refer to instance-level retrieval, where object instances are as broad as possible, i.e. not restricted to buildings, as in the Oxford/Paris benchmark. Effective systems for image retrieval rely on accurate image descriptors. Typically, a query image is described by an embedding vector, and the task amounts to searching the nearest neighbors of this vector in the embedding space. Possible improvement include refinement steps such as geometric verification <ref type="bibr" target="#b32">[33]</ref>, query expansion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref>, or database-side pre-processing or augmentation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>Local image descriptors are traditionally aggregated to global image descriptors suited for matching in an inverted database, as in the seminal bag-of-words model <ref type="bibr" target="#b38">[39]</ref>. After the emergence of convolutional neural networks (CNNs) for large-scale classification on ImageNet <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref>, it has become apparent that CNNs trained on classification datasets are very competitive image feature extractors for various vision tasks, including instance retrieval <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Specific architectures for particular object retrieval are built upon a regular classification trunk, and modified so the pooling stage gives more spatial locality in order to cope with small objects and clutter. For instance, a competitive baseline for instance retrieval on various datasets is the R-MAC image descriptor <ref type="bibr" target="#b42">[43]</ref>. It aggregates regionally pooled features extracted from a CNN. The authors show that this specialized pooling combined with PCA whitening <ref type="bibr" target="#b23">[24]</ref> leads to efficient many-to-many comparisons between image regions, highly beneficial to image retrieval. Gordo et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> show that fine-tuning these regionally-aggregated representations endto-end on an external image retrieval dataset using a ranking loss yields significant improvements for instance retrieval.</p><p>Radenović et al. <ref type="bibr" target="#b33">[34]</ref> show that R-MAC pooling is advantageously replaced by a generalized mean pooling (see section 3.1), which is a spatial pooling of the features exponentiated to an exponent p over the whole image. The exponentiation localizes the features on the point of interests in the image, replacing regional aggregation in R-MAC.</p><p>Multi-task training is an active area of research <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>, motivated by the observation that deep neural networks are transferable to a wide range of vision tasks <ref type="bibr" target="#b34">[35]</ref>. Moreover trained deep neural networks exhibit a high level of compressibility <ref type="bibr" target="#b14">[15]</ref>. In some cases, sharing the capacity of neural networks between different tasks through shared parameters helps the learning by allowing complementary training among datasets and low-level features. Despite some successes with multi-task networks for vision such as UberNet <ref type="bibr" target="#b26">[27]</ref>, the design and training of multi-task networks still involve numerous heuristics. Ongoing lines of work include finding the right architecture for an efficient sharing of parameters <ref type="bibr" target="#b35">[36]</ref>, and finding the right optimization parameters for such networks in order to depart from the traditional setting of single-task single-dataset end-to-end gradient descent, and efficiently weight the gradients in order to obtain a well-performing network in all tasks <ref type="bibr" target="#b12">[13]</ref>.</p><p>Data augmentation is a cornerstone of the training in largescale vision applications <ref type="bibr" target="#b27">[28]</ref>, which improves generalization and reduces over-fitting. In a stochastic gradient descent (SGD) optimization setting, we show that including multiple data-augmented instances of the same image in one optimization batch, rather than having only distinct images in the batch, significantly enhances the effect of data-augmentations and improve the generalization of the network. A related batch augmented (BA) sampling strategy was concurrently introduced by Hoffer et al. <ref type="bibr" target="#b18">[19]</ref>. When augmenting the size of the batches in a large-scale distributed optimization of a neural network, they show that filling these bigger batches with data-augmented copies of the image in the batch yields better generalization performance, and uses computing resources more efficiently through reduced data processing time. As discussed in section 3.3 and highlighted in our classification results (section 4.4), we show that a gain in performance under this sampling scheme is obtained using the same batch size, i.e., with a lower number of distinct images per batch. We consider this scheme of repeated augmentations (RA) within the batch as a way to boost the effect of data augmentation over the course of the optimization. Our results indicate that RA is a technique of general interest, beyond large-scale distributed training applications, for improving the generalization of neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture design</head><p>Our goal is to develop a convolutional neural network that is suitable for both image classification and instance retrieval. In the current best practices, the architectures and training procedures used for class and instance recognition differ in a significant manner. This section describes such technical differences, summarized in table 1, together with our solutions to bridge them. This leads us to a unified architecture, shown in <ref type="figure" target="#fig_0">fig. 2</ref>, that we jointly train for both tasks in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial pooling operators</head><p>This section considers the final, global spatial pooling layer. Local pooling operators, usually max pooling, are found throughout the layers of most convolutional networks to achieve local invariance to small translations. By contrast, global spatial pooling converts a 3D tensor of activations produced by a convolutional trunk to a vector.</p><p>Classification. In early models such as LeNet-5 <ref type="bibr" target="#b28">[29]</ref> or AlexNet <ref type="bibr" target="#b27">[28]</ref>, the final spatial pooling is just a linearization of the activation map. It is therefore sensitive to the absolute location. Recent architectures such as ResNet and DenseNet employ average pooling, which is permutation invariant and hence offers a more global translation invariance.</p><p>Image retrieval requires more localized geometric information: particular objects or landmarks are visually more similar, but the task suffers more from clutter, and a given query image has no specific training data devoted to it. This is why the pooling operator tries to favor more locality. Next we discuss the generalized mean pooling operator. Let x ∈ R C×W ×H be the feature tensor computed by a convolutional neural network for a given image, where C is the number of feature channels and H and W are the height and width of the map, respectively. We denote by u ∈ Ω = {1, . . . , H} × {1, . . . , W } a "pixel" in the map, by c the channel, and by x cu the corresponding tensor element: x = [x cu ] c=1..C,u∈Ω . The generalized mean pooling (GeM) layer computes the generalized mean of each channel in a tensor. Formally, the GeM embedding is given by where p &gt; 0 is a parameter. Setting this exponent as p &gt; 1 increases the contrast of the pooled feature map and focuses on the salient features of the image <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref>. GeM is a generalization of the average pooling commonly used in classification networks (p = 1) and of spatial max-pooling layer (p = ∞). It is employed in the original R-MAC as an approximation of max pooling <ref type="bibr" target="#b6">[7]</ref>, yet only recently <ref type="bibr" target="#b33">[34]</ref> it was shown to be competitive on its own with R-MAC for image retrieval.</p><formula xml:id="formula_0">e = 1 |Ω| u∈Ω x p cu 1 p c=1..C<label>(1)</label></formula><p>To the best of our knowledge, this paper is the first to apply and evaluate GeM pooling in an image classification setting. More importantly, we show later in this paper that adjusting the exponent is an effective way to change the input image resolution between train and test time for all tasks, which explains why image retrieval has benefited from it considering that this task employs higher-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training objective</head><p>In order to combine the classification and retrieval tasks, we use a joint objective function composed of a classification loss and an instance retrieval loss. The two-branch architecture is illustrated in <ref type="figure" target="#fig_0">fig. 2</ref> and detailed next.</p><p>Classification loss. For classification, we adopt the standard cross-entropy loss. Formally, let e i ∈ R d be the embedding computed by the deep network for image i, w c ∈ R d the parameters of a linear classifier for class c ∈ {1, . . . , C}, and y i be the ground-truth class for that image. Then</p><formula xml:id="formula_1">class (e i , W , y i ) = − w yi , e i + log C c=1 exp w c , e i ,<label>(2)</label></formula><p>where W = [w c ] c=1..C . We omit it for simplicity, but by adding a constant channel to the feature vector, the bias of the classification layer is incorporated in its weight matrix.</p><p>Retrieval loss. For image retrieval, the embeddings of two matching images (a positive pair) should have distances smaller than embeddings of non-matching images (a negative pair). This can be enforced in two ways. The contrastive loss <ref type="bibr" target="#b13">[14]</ref> requires distances between positive pairs to be smaller than a threshold, and distances between negative pairs to be greater. The triplet loss instead requires an image to be closer to a positive sibling than to a negative sibling <ref type="bibr" target="#b37">[38]</ref>, which is relative property of image triplets. These losses requires adjusting multiple parameters, including how pairs and triplets are sampled. These parameters are sometimes hard to tune, especially for the triplet loss.</p><p>Wu et al. <ref type="bibr" target="#b44">[45]</ref> proposed an effective method that addresses these difficulties. Given a batch of images, they re-normalize their embeddings to the unit sphere, sample negative pairs as a function of the embedding similarity, and use those pairs in a margin loss, a variant of contrastive loss that shares some of the benefits of the triplet loss.</p><p>In more detail, given images i, j ∈ B in a batch with embeddings e i , e j ∈ R d , the margin loss is expressed as</p><formula xml:id="formula_2">retr (e i , e j , β, y ij ) = max{0, α + y ij (D(e i , e j ) − β)} (3)</formula><p>where D(e i , e j ) = ei / ei − ej / ej is the Euclidean distance between the normalized embeddings, the label y ij is equal to 1 if the two images match and −1 otherwise, α &gt; 0 the margin (a constant hyper-parameter), and β &gt; 0 is a parameter (learned during training together with the model parameters), controlling the volume of the embedding space occupied embedding vectors. Due to the normalization, D(e i , e j ) is equivalent to a cosine similarity, which, up to whitening (section 3.4), is also used in retrieval.</p><p>Loss <ref type="formula">(3)</ref> is computed on a subset of positive and negative pairs (i, j) ∈ B 2 selected with the sampling <ref type="bibr" target="#b44">[45]</ref> </p><formula xml:id="formula_3">P + (B) = {(i, j) ∈ B 2 : y ij = 1}, P − (B) = {(i, j * ) : (i, j) ∈ P + (B), j * ∼ p(·|i)}, P(B) = P + (B) ∪ P − (B),<label>(4)</label></formula><p>where the conditional probability of choosing a negative j</p><formula xml:id="formula_4">for image i is p(j|i) ∝ min{τ, q −1 (D(e i , e j ))}·1 {yij =−1} , where τ &gt; 0 is a parameter and q(z) ∝ z d−2 (1 − z 2 /4) d−3 2</formula><p>is a PDF that depends on the embedding dimension d. The use of distance weighted-sampling with margin loss is very suited to our joint training setting: this framework tolerates relatively small batch sizes (|B| ∼ 80 to 120 instances) while requiring only a small amount of positives images (3 to 5) of each instance in the batch, without the need for elaborate parameter tuning or offline sampling.</p><p>Joint loss and architecture. The joint loss is a combination of classification and retrieval loss weighted by a factor λ ∈ [0, 1]. For a batch B of images, the joint loss writes as</p><formula xml:id="formula_5">λ |B| · i∈B class (e i , w, y i )+ 1 − λ |P(B)| · (i,j)∈P(B)</formula><p>retr (e i , e j , β, y ij ), (5) i.e., losses are normalized by the number of items in the corresponding summations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Batching with repeated augmentation (RA)</head><p>Here, we propose to use only a training dataset for image classification, and train instance recognition via data augmentation. The rationale is that data augmentation produces another image that contains the same object instance. This approach does not require more annotation beyond the standard classification set.</p><p>We introduce a new sampling scheme for training with SGD and data augmentation, which we refer to as repeated augmentations. In RA we form an image batch B by sampling |B|/m different images from the dataset, and transform them up to m times by a set of data augmentations to fill the batch. Thus, the instance level ground-truth y ij = +1 iff images i and j are two augmented versions of the same training image. The key difference with the standard sampling scheme in SGD is that samples are not independent, as augmented versions of the same image are highly correlated. While this strategy reduces the performance if the batch size is small, for larger batch sizes RA outperforms the standard i.i.d. scheme -while using the same batch size and learning rate for both schemes. This is different from the observation of <ref type="bibr" target="#b18">[19]</ref>, who also consider repeated samples in a batch, but simultaneously increase the size of the latter.</p><p>We conjecture that the benefit of correlated RA samples is to facilitate learning features that are invariant to the only difference between the repeated images -the augmentations. By comparison, with standard SGD sampling, two versions of the same image are seen only in different epochs. A study of an idealized problem illustrates this phenomenon in the supplementary material A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">PCA whitening</head><p>In order to transfer features learned via data augmentation to standard retrieval datasets, we apply a step of PCA whitening, in accordance with previous works in image retrieval <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. The Euclidean distance between transformed features is equivalent to the Mahalanobis distance between the input descriptors. This is done after training the network, using an external dataset of unlabelled images.</p><p>The effect of PCA whitening can be undone in the parameters of the classification layer, so that the whitened embeddings can be used for both classification and instance retrieval. In detail, let e be an image embedding vector and w c the weight vector for class c, such that w c , e are the outputs of the classifier as in eq. (2). The whitening operation Φ can be written as <ref type="bibr" target="#b10">[11]</ref> Φ(e) = S e e − µ given the whitening matrix S and centering vector µ; hence</p><formula xml:id="formula_6">w c , e = w c , Φ −1 (Φ(e)) = e ( w c , Φ(e) + b c )</formula><p>where w c = S −1 w c and b c = w c , µ are the modified weight and bias for class c. We observed that inducing decorrelation via a loss <ref type="bibr" target="#b4">[5]</ref> is insufficient to ensure that features generalize well, which concurs with prior works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Input sizes</head><p>The standard practice in image classification is to resize and center-crop input images to a relatively low resolution, e.g. 224×224 pixels <ref type="bibr" target="#b27">[28]</ref>. The benefits are a smaller memory footprint, faster inference, and the possibility of batching the inputs if they are cropped to a common size. On the other hand, image retrieval is typically dependent on finer details in the images, as an instance can be seen under a variety of scales, and cover only a small amount of pixels. The currently best-performing feature extractors for image retrieval therefore commonly use input sizes of 800 <ref type="bibr" target="#b10">[11]</ref> or 1024 <ref type="bibr" target="#b33">[34]</ref> pixels for the largest side, without cropping the image to a square. This is impractical for end-to-end training of a joint classification and retrieval network.</p><p>Instead, we train our architecture at the standard 224×224 resolution, and use larger input resolutions at test time only. This is possible due to a key advantage of our architecture: a network trained with a pooling exponent p and resolution s can be evaluated at a larger resolution s * &gt; s using a larger pooling exponent p * &gt; p, see our validation in section 4.4.</p><p>Proxy task for cross-validation of p * . In order to select the exponent p * , suitable for all tasks, we create a synthetic retrieval task IN-aug in between classification and retrieval. We sample 2,000 images from the training set of ImageNet, 2 per class, and create 5 augmented copies of each of them, using the "full" data augmentation described before.</p><p>We evaluate the retrieval accuracy on IN-aug in a fashion similar to UKBench <ref type="bibr" target="#b30">[31]</ref>, with an accuracy ranging from 0 to 5 depending measuring how many of the first 5 augmentations are ranked in top 5 positions. We pick the bestperforming p * ∈ {1, 2, . . . , 10} on IN-aug, which provides the following choices as a function of λ and s * : The optimal p * obtained on IN-aug provides a trade-off between retrieval and classification. Experimentally, we observed that other choices are suitable for setting this parameter: fine-tuning the parameter p * alone using training Input image resolution s * = 224 p * = 1 p * = 3 full resolution p * = 1 p * = 3 <ref type="figure" target="#fig_6">Figure 3</ref>: An off-the-shelf ResNet-50 reacts strongly on channel 909 of the last activation map for class "racing car". The image on the left is a hard example for the class. We show channel 909 for that image, at several resolutions and with GeM parameters p * = 1 and p * = 3. In the low resolution version, the cars are too small to be visible individually on the activation map. In the full resolution version, the location of the cars is more clear. In addition, p * = 3 reduces the noisy detections relative to the true locations.</p><p>inputs at a given resolution by back-propagation of the crossentropy loss provides similar results and values of p * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>After presenting the datasets, we provide a parametric study and our results in image classification and retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental settings</head><p>Base architecture and training settings. The convolutional trunk is ResNet-50 <ref type="bibr" target="#b16">[17]</ref>. SGD starts with a learning rate of 0.2 which is reduced tenfold at epochs 30, 60, 90 for a total of 120 epochs (a standard setting <ref type="bibr" target="#b31">[32]</ref>). The batch size |B| is set to 512 and an epoch is defined as a fixed number of T = 5005 iterations. With uniform batch sampling, one epoch corresponds to two passes over the training set; with RA and m = 3, one epoch corresponds to ∼ 2/3 of the images of the training set. All classification baselines are trained using this longer schedule for a fair comparison.</p><p>Data augmentation. We use standard flips, random resized crops <ref type="bibr" target="#b19">[20]</ref>, random lighting noise and a color jittering of brightness, contrast and saturation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20]</ref>. We refer to this set of augmentations as "full", see details in supplemental C. As indicated in table 2 our network reaches 76.2% top-1 validation error under our chosen schedule and data augmentation when trained with cross-entropy alone and uniform batch sampling. This figure is on the high end of accuracies reported for the ResNet-50 network <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> without specially-crafted regularization terms <ref type="bibr" target="#b47">[48]</ref> or data augmentations <ref type="bibr" target="#b5">[6]</ref>.</p><p>Pooling exponent. During the end-to-end training of our network, we consider two settings for the pooling exponent in the GeM layer of section 3.1: we set either p = 1 or p = 3. p = 1 corresponds to average pooling, as used in classification architectures. The relevant literature <ref type="bibr" target="#b33">[34]</ref> and our preliminary experiments on off-the-shelf classification networks suggest that the value p = 3 improves the retrieval performance on standard benchmarks. <ref type="figure" target="#fig_6">Figure 3</ref> illustrates this choice. By setting p = 3, the car is detected with high confidence and without spurious detections. Boureau et al. <ref type="bibr" target="#b2">[3]</ref> analyse average-and max-pooling of sparse features. They find that when the number of pooled features increases, it is beneficial to make them more sparse, which is consistent with the observation we make here.</p><p>Input size and cropping. As described in section 3.5, we train our network on crops of size 224 × 224 pixels. For testing, we experiment with computing MultiGrain embeddings at resolutions s * = 224, 500, 800. For resolution s * = 224, we follow the classical image classification protocol "resolution 224": the smallest side of an image is resized to 256 and then a 224 × 224 central crop is extracted. For resolution s * &gt; 224, we instead follow the protocol common in image retrieval and resize the largest side of the image to the desired number of pixels and evaluate the network on the rectangular image, without cropping.</p><p>Margin loss and batch sampling. We use m = 3 dataaugmented repetitions per batch. We use the default margin loss hyperparameters of <ref type="bibr" target="#b44">[45]</ref> (details in supplementary B). As in <ref type="bibr" target="#b44">[45]</ref> the distance-weighted sampling is performed independently on each of the 4 GPUs used for training.</p><p>Datasets. We train our networks on the ImageNet-2012 training set of 1.2 million images labelled into 1,000 object categories <ref type="bibr" target="#b36">[37]</ref>. Classification accuracies are reported on the 50,000 validation images of this dataset. For image retrieval, we report the mean average precision on the Holidays dataset <ref type="bibr" target="#b24">[25]</ref>, with images rotated manually when necessary, as in prior evaluations on this dataset <ref type="bibr" target="#b9">[10]</ref>. We also report the accuracy on the UKB object recognition benchmark <ref type="bibr" target="#b30">[31]</ref>, which contains 2,550 instances of objects under 4 varying viewpoints each; each image is used as a query to find its 4 closest neighbors in embedding space; the number of correct neighbors is averaged across all images, yielding a maximum score of 4. We also report the performance of our network in a copy detection setting, indicating the mean average precision on the "strong" subset of the INRIA Copydays dataset <ref type="bibr" target="#b7">[8]</ref>. We add 10K distractor images randomly sampled from the YFCC100M large-scale collection of unlabelled images <ref type="bibr" target="#b39">[40]</ref>. We call the combination C10k.</p><p>The PCA whitening transformations are computed from the features of 20K images from YFCC100M, distinct from the C10k distractors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Expanding resolution with pooling exponent</head><p>As our reference scheme, we train the network at resolution 224x224 with RA sampling and pooling exponent p = 3. When testing on images with the same 224x224 resolution, this gives a 76.9% top-1 validation accuracy on Imagenet, 0.7% points above the non-RA baseline, see <ref type="table" target="#tab_1">table 2</ref>.</p><p>We now feed larger images at test time, i.e., we consider resolutions s * &gt; 224 and vary the exponents p * = p = 3 at test time. <ref type="figure" target="#fig_2">Figures 4a and 4b</ref> show the classification accuracy on ImageNet validation and the retrieval accuracy on Holidays at different resolutions, for different values of the test pooling exponent p * . As expected, at s * = 224, the pooling exponent yielding best accuracy in classification is the exponent with which the network has been trained, p * = 3.</p><p>Observe that testing at larger scale requires an exponent p * &gt; p, both for classification and for retrieval.</p><p>In the following, we adopt the values obtained by our cross-validation on IN-aug, see section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of the tradeoff parameter</head><p>We now analyze the impact of the tradeoff parameter λ. Note, this parameter does not directly reflect the relative importance of the two loss terms during training, since these are not homogeneous: λ = 0.5 does not mean that they have equal importance. <ref type="figure">Figure 5</ref> analyzes the actual relative importance of the classification and margin loss terms, by measuring the average norm of the gradient back-propagated through the network at epochs 0 and 120. One can see that λ = 0.5 means that the classification has slightly more weight at the beginning of the training. The classification term becomes dominant at the end of the training, meaning that the network has already learned to cancel data augmentation.</p><p>In terms of performance, λ = 0.1 leads to a poor classification accuracy. Interestingly, the classification performance is higher for the intermediate λ = 0.5 (77.4% at s * = 224) than for λ = 1, see <ref type="table" target="#tab_1">Table 2</ref>. Thus, the margin loss leads to a performance gain for the classification task.</p><p>We set λ = 0.5 in our following experiments, as it gives the best classification accuracy at the practical resolutions s * = 224 and 500 pixels. As a reference, we also report a Fraction of the classification and retrieval terms, measured as g class /( g class + g retr ), where the g class vector is the gradient from the λ class component. Note, how the retrieval loss' influence is decreasing over epochs.</p><p>few results with λ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Classification results</head><p>From now on, our MultiGrain nets are trained at resolution s = 224 with exponent p = 1 (standard average pooling) or p = 3 in the GeM pooling. For each evaluation resolutions s * = 224, 500, 800, the same exponent p * is selected according to section 3.5, yielding a single embedding for classification and for the retrieval. <ref type="table" target="#tab_1">Table 2</ref> presents the classification results. There is a large improvement in classification performance from our baseline Resnet-50 with p = 1, s = 224, "full" data augmentation (76.2% top-1 accuracy), to a MultiGrain model at p = 3, λ = 0.5, s = 500 (78.6% top-1). We identify four sources for this improvement:</p><p>1. Repeated augmentations: adding RA batch sampling (section 3.3) yields an improvement of +0.6% (p = 1).</p><p>2. Margin loss: the retrieval loss helps the generalizing effect of data augmentation: +0.2% (p = 1).</p><p>3. p = 3 pooling: GeM at training (section 3.1) allows the margin loss to have a much stronger effect thanks to increased localization of the features: +0.4%.</p><p>4. Expanding resolution: evaluating at resolution 500 adds +1.2% to the p = 3 MultiGrain network, reaching the 78.6 top-1 accuracy. This is made possible by the p = 3 training -which yields sparser features, more generalizable over different resolutions, and by the p * pooling adaptation -without it the performance at this resolution is only 78.0%.</p><p>The p * selection for evaluation at higher resolutions has its limits: at 800 pixels, due to the large discrepancy between the training and testing scale for the feature extractor, the accuracy drops to 77.2% (76.2% without the p * adaptation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AutoAugment</head><p>[6] (AA) is a method to learn dataaugmentation using reinforcement learning techniques to improve the accuracy of classification networks on ImageNet. We directly integrate the data-augmentations found by the algorithm <ref type="bibr" target="#b5">[6]</ref> trained on their Resnet-50 model using a long schedule of 270 passes over the dataset, with batch size 4096. We have observed that this longer training gives more impact to the AA-generated augmentations. We therefore use a longer schedule of 7508 iterations per epoch, keeping the batch size to |B| = 512. Our method benefits from this data-augmentation: Multi-Grain reaches 78.2% top-1 accuracy at resolution 224 with p = 3, λ = 0.5. To the best of our knowledge, this is the best top-1 accuracy reported for Resnet-50 when training and evaluating at this resolution, significantly higher than the 77.6% reported with AutoAugment alone <ref type="bibr" target="#b5">[6]</ref> or 76.7% for mixup <ref type="bibr" target="#b47">[48]</ref>. Using a higher resolution at test time improves the accuracy further: we obtain 79.4% top-1 accuracy at resolution 500. Our strategy of adapting the pooling exponent to a larger resolution is still effective, and significantly outperforms the state of the art performance for a ResNet-50 learned on ImageNet at training resolution 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Retrieval results</head><p>We present our retrieval results in table 3, with an ablation study and copy-detection results in the supplemental material (D). Our MultiGrain nets improve accuracies on all datasets with respect to the Resnet-50 baseline for comparable resolutions. Repeated augmentations (RA) is again a key ingredient in this context.</p><p>We compare with baselines where no annotated retrieval dataset is used. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> give off-the-shelf network accuracies with R-MAC pooling. MultiGrain compares favorably with their results at a comparable resolution (s * =800). They reach accuracies above 93% mAP on Holidays but this requires a resolution s ≥ 1000 pixels.</p><p>It is also worth noting that we reach reasonable retrieval <ref type="table" target="#tab_3">Table 3</ref>: Instance search results and baselines, on Holidays (% mAP) and UKB (/4). We set p = 3 pooling at training time for our MultiGrain models, and p * set as given in section 3.5. †GeM is fine-tuned at resolution 362x362 on additional images tailored to the retrieval task. Their best result is obtained with multi-scale input and implies additional processing. performance at resolution s * = 500, which is a interesting operating point with respect to the traditional inference resolutions s = 800-1000 for retrieval. Indeed, a forward pass of Resnet-50 on 16 processor cores takes 3.80s at resolution 500, against 18.9s at resolution 1024 (5× slower). Because of this quadratic increase in timing, and the single embedding computed by MultiGrain, our solution is particularly adapted to large-scale or low-resource vision applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>For comparison, we also report some older related results on the UKB and C10k datasets, that are not competitive with MultiGrain. Neural codes <ref type="bibr" target="#b0">[1]</ref> is one of the first works on retrieval with deep features. The Fisher vector <ref type="bibr" target="#b25">[26]</ref> is a pooling method that uses local SIFT descriptors.</p><p>At resolutions 500 we see that the results with the margin loss (λ = 0.5) are slightly lower than without (λ =1). This is partly due to the limited transfer from the IN-aug task to the variations observed in retrieval datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we have introduced MultiGrain, a unified embedding for image classification and instance retrieval. MultiGrain relies on a classical convolutional neural network trunk, with a GeM layer topped with two heads at training time. We have discovered that by adjusting this pooling layer we are able to increase the resolution of images used a inference time, while maintaining a small resolution at training time. We have shown that MultiGrain embeddings can perform well on classification and retrieval. Interestingly, MultiGrain also sets a new state of the art on pure classification compared to all results obtained with the same convolutional trunk. Overall, our results show that retrieval and classification tasks can benefit from each other.</p><p>An implementation of our method is open-sourced at https://github.com/facebookresearch/multigrain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We report a few additional experiments and results that did not fit in the main paper. Section A shows the effect of data-augmented batches when training a simple toy model. Sections B and C list the values of a few hyper-parameters used in our method. Section D gives a some more ablation results in the retrieval setting. Finally, Section E shows how to use the ingredients of MultiGrain to improve the accuracy of an off-the-shelf pre-trained ConvNet at almost no additional training cost. It obtains what appear to be the best reported classification results on imagenet-2012 for a convnet with publicly available weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data-augmented batches: toy model</head><p>We have observed in Sections 3.3 and 4.3 that training our architecture (ResNet-50 trunk) with data-augmented batches yields improvements with respect to the vanilla uniform sampling scheme, despite the decrease in image diversity.</p><p>This observation holds even in the absence of ranking triplet loss, all things being equal otherwise: same number of iterations per epoch, number of epochs, learning rate schedule, and batch size. As an example, <ref type="figure" target="#fig_4">fig. A.1</ref> shows the evolution of the validation accuracy of our network trained under cross-entropy with our training schedule and a p = 1 pooling, batches of size 512, with the data augmentation introduced in section 4.1, with uniform batches vs. with batch sampling. While initial epochs suffer from the reduced diversity of the batches compared to the uniformly-sampled variant, the reinforced effect on data augmentation compensates for this in the long run, and makes the batch-augmented variant reach a higher final accuracy.</p><p>Since we observe this better performance even for a pure image classification task, an interesting question is whether this benefit is specific to our architecture and training method (batch-norm, etc), or if it is more generally applicable? Hereafter we analyse a linear model and synthetic classification task that seems to align with the second hypothesis.</p><p>We consider an idealized model of the effect of including different data-augmented instances of the same image in one batch using standard stochastic gradient descent. We create a synthetic training set D of points pictured in <ref type="figure">fig</ref>  (p i x , p i y ) by sampling from two 2D Gaussian distributions:</p><formula xml:id="formula_7">p i x ∼ N (µ = 0, σ = 1) p i y ∼ N (µ = y * i , σ = 1) (A.1)</formula><p>with y * i = ±1 being the ground truth label. We sample a test dataset in the same manner.</p><p>We consider the SGD training of an SVM  as a label-preserving data-augmentation suited to our synthetic dataset. We train the SVM (A.2) using one pass through the data-augmented datasetD of size 4N , using batches of size 2.</p><p>The only difference between the two optimization schedules is the order in which the samples are batched and presented to the optimizer. We consider two batch sampling strategies:</p><p>• Uniform sampling: we sample the elements of the batch randomly fromD, without replacement; • Paired sampling: we generate a batch by pairing a random element fromD and its data-augmentation, removing these two elements fromD.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Margin loss hyper-parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data augmentation hyper-parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional results and ablation study for</head><p>Multigrain in retrieval As already reported in the main paper, for some datasets the choice of not using the triplet loss (λ = 1) is as good or better than our generic choice (λ = 0.5). Of course, then the embedding is not multi-purpose anymore. Overall, the different elements employed in our architecture (RA and the layers specific to Multigrain) still give a significant improvement over simply using the activations, and is competitive with the state of the art for the same resolution/complexity. Note, the AutoAugment data augmentation does not transfer well to the retrieval tasks. This can be explained by their specificity to Imagenet classification. This shows the limitation of a particular choice of data-augmentation if a single embedding for classification and retrieval datasets is desired. Learning AutoAugment specifically for the retrieval task would certainly help, but would probably also result in less general embeddings. Hence, data-augmentation is a limiting factor for multi-purpose embeddings: improving for one task like classification hurts the performance for other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation of off-the-shelf classifiers at higher resolutions</head><p>In this section, we present some additional classification results using off-the-shelf pretrained classification networks trained with standard average pooling (p = 1).</p><p>As outlined in sections 3.5 and 4.2, one of our contributions is a strategy for evaluating classifier networks trained with GeM pooling at scale s and exponent p at a higher resolution s * and adapted exponent p * . It can be used on II  <ref type="bibr" target="#b0">[1]</ref>. Resnet-50 corresponds to features extracted from a classification baseline with p = 1 or p = 3 GeM pooling, trained with cross-entropy with our training schedule, data augmentation, and uniform batch sampling.  pretrained networks as well.</p><p>For an evaluation scale s * , we use the alternative strategy described in section 3.5 to choose p * : we finetune the parameter p * by stochastic gradient descent, backpropagating the cross-entropy loss on training images from imagenet, rescaled to the desired input resolution. Compared to a full finetuning at this input resolution, this strategy has a limited memory footprint, given that the backpropagation only has to be done on the ultimate classification layer before reaching the pooling layer, allowing for an efficient computation of the gradient of p * . Experimentally we also found that this process converges on a few thousands of training samples, while a finetuning of the classification layer would require several data-augmented epochs on the full training set.</p><p>The finetuning is done using SGD with batches of |B| = 4 (non-cropped) images, with momentum 0.9 and initial learning rate lr (0) = 0.005, decayed under a polynomial learning rate decay lr (i) = lr (0) 1 − i i max 0.9 (E.1) with i max the total number of iterations.</p><p>We select 50, 000 images from the training set (50 per category) for the fine-tuning and do one pass on this reduced dataset. We use off-the-shelf pretrained convnets from the Cadene/pretrained-models.pytorch GitHub repository 1 . Table E.1 outlines the resulting validation accuracies. We see that for each network there is a scale and choice of p * that performs better than the standard evaluation.</p><p>These networks have not been trained using GeM pooling with p &gt; 1; as exhibited in our classification results <ref type="table" target="#tab_1">(table 2)</ref> we found this to be another key ingredient in ensuring a higher scale insensitivity and better performance at larger resolution. As in our main experiments with the MultiGrain architecture with a ResNet-50 backbone, it is likely that these networks would reach higher values when training from scratch with a p &gt; 1 pooling, and adding repeated augmentations and margin loss. However, running training experiments on these large networks is significantly more expensive. Therefore, we leave this for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental references</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our Multigrain architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Retrieval and classification accuracies as a function of pooling exponent p * and the image resolution. At training time, the pooling was p = 3. Note the clear interaction between the resolution s * and the pooling exponent p * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5: Fraction of the classification and retrieval terms, measured as g class /( g class + g retr ), where the g class vector is the gradient from the λ class component. Note, how the retrieval loss' influence is decreasing over epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A. 1 :</head><label>1</label><figDesc>. A.2 of N = 100 positive and N = 100 negative training points p i = Evolution of the validation accuracy on ImageNet-val with and without data-augmented batches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A. 2 :</head><label>2</label><figDesc>Training set for the toy model in appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>fFigure A. 3 :</head><label>3</label><figDesc>w (p i ) = w p i (A.2) using the Hinge losshinge = max (1 − y * i f w (p i ), 0). (A.3)We consider the symmetry across the x-axis φ((p i x , p i y )) = φ((p i x , −p i y )) Evolution of the test accuracy of the SVM trained on the synthetic data, averaged accross 100 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A. 3</head><label>3</label><figDesc>shows the evaluation of the accuracy with the iterations in both of these cases, averaged across 100 runs. It is clear that pairing the data-augmented pairs in one batch accelerates the convergence of this model. This idealized experiment demonstrates that there are cases in which the repeated augmentation scheme provides an optimization and generalization boost, and reinforces the effect of data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>[</head><label></label><figDesc>49] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, and K. Murphy. Progressive neural architecture search. In Proc. ECCV, pages 19-34, 2018. [50] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning transferable architectures for scalable image recognition. In Proc. CVPR, pages 8697-8710, 2018.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Differences between classification and image retrieval: Retrieval architectures incorporate a final pooling layer that is regionalized (RMAC) or magnifies activations (GeM). The triplet loss requires a batching strategy with pairs of matching images.</figDesc><table><row><cell></cell><cell>classification</cell><cell>retrieval</cell></row><row><cell>spatial pooling</cell><cell cols="2">avg. pooling RMAC [42] or GeM [34]</cell></row><row><cell>loss</cell><cell>cross-entropy</cell><cell>triplet [10]</cell></row><row><cell>batch sampling</cell><cell>diverse</cell><cell>similar images in batch</cell></row><row><cell>whitening</cell><cell>no</cell><cell>yes</cell></row><row><cell>resolution</cell><cell cols="2">low (224 2 -300 2 ) high (800-1k×scaled)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ImageNet 2012 validation performance at top-1 / top-5 accuracies (%). Resnet-50 is a classification baseline trained with cross-entropy with our training schedule, data augmentation, and uniform batch sampling. MultiGrain uses the same Resnet-50 trunk. At resolutions s * &gt; 224 we evaluate with exponent p * as described in section 3.5.</figDesc><table><row><cell>Architecture</cell><cell>λ</cell><cell cols="2">data resol.</cell><cell cols="2">train-time pooling</cell></row><row><cell></cell><cell></cell><cell>aug.</cell><cell>s  *</cell><cell>p = 1</cell><cell>p = 3</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>full</cell><cell>224</cell><cell cols="2">76.2 / 92.9 76.2 / 93.1</cell></row><row><cell>MultiGrain</cell><cell>1</cell><cell>full</cell><cell>224</cell><cell cols="2">76.8 / 93.2 76.9 / 93.5</cell></row><row><cell>MultiGrain</cell><cell cols="2">0.5 full</cell><cell>224</cell><cell cols="2">77.0 / 93.6 77.4 / 93.6</cell></row><row><cell>MultiGrain</cell><cell cols="2">0.5 AA</cell><cell>224</cell><cell cols="2">77.4 / 93.6 78.2 / 93.9</cell></row><row><cell>MultiGrain</cell><cell cols="2">0.5 full</cell><cell>500</cell><cell cols="2">76.5 / 93.5 78.6 / 94.4</cell></row><row><cell>MultiGrain</cell><cell cols="2">0.5 AA</cell><cell>500</cell><cell cols="2">77.7 / 94.0 79.4 / 94.8</cell></row><row><cell>MultiGrain</cell><cell cols="2">0.5 full</cell><cell>800</cell><cell cols="2">73.5 / 93.5 77.2 / 93.5</cell></row><row><cell>MultiGrain</cell><cell cols="2">0.5 AA</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table B</head><label>B</label><figDesc></figDesc><table><row><cell cols="2">.1: Margin loss hyper-parameters</cell></row><row><cell>parameter</cell><cell>value</cell></row><row><cell>margin α</cell><cell>0.2</cell></row><row><cell>initial β 0</cell><cell>1.2</cell></row><row><cell cols="2">β learning rate 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table B .</head><label>B</label><figDesc>1 gives the value of the hyper-parameters for the margin loss used during the training of our models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table C .</head><label>C</label><figDesc>1: full data-augmentation transforms and parameters</figDesc><table><row><cell>transformation</cell><cell>parameter range</cell></row><row><cell>horizontal flip</cell><cell></cell></row><row><cell>random resized crop</cell><cell>scale ∈ [0.08, 1.0] ratio ∈ [3/4, 4/3]</cell></row><row><cell></cell><cell>brightness 0.3</cell></row><row><cell>color jitter</cell><cell>contrast 0.3</cell></row><row><cell></cell><cell>saturation 0.3</cell></row><row><cell>lighting transform</cell><cell>intensity 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table C .</head><label>C</label><figDesc>1 gives the transformations in the full data augmentation used in our experiments (section 4.1), along with their parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table D.1 reports additional results of the MultiGrain architecture, with an ablation study analyzing the effect of each component.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table D .</head><label>D</label><figDesc>1: Full results including Copydays + 10k distractors (CD10k, % mAP), and ablation study for the MultiGrain models. The Pytorch model simply extract the last activation layer as a descriptor</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table E.1: Additional top-1/top-5 validation classification accuracies obtained by finetuning p * for higher evaluation scales on off-the-shelf networks. The first column indicates the training resolution s and the accuracy we measured at this resolution, with standard evaluation (resize of the largest scale to s · 256/224 + center crop). The subsequent columns show the accuracy measured at higher resolutions s * = 350, 400, 450, 500 without cropping, together with the p * found by finetuning for these resolutions (appendix E). 75.1/92.5 2.1 74.2/92.1 2.4 71.8/90.9 2.6 68.4/89.0 SENet154 [21] 224 81.3/95.5 1.6 82.6/96.2 1.6 83.0/96.5 1.6 83.1/96.5 1.7 82.7/96.3 PNASNet-5-Large [49] 331 82.7/96.0 1.0 81.3/85.4 1.4 82.6/96.1 1.5 83.2/96.4 1.7 83.6/96.7</figDesc><table><row><cell></cell><cell cols="2">original evaluation</cell><cell></cell><cell>s  *  = 350</cell><cell>s  *  = 400</cell><cell>s  *  = 450</cell><cell></cell><cell>s  *  = 500</cell></row><row><cell>Architecture</cell><cell>s</cell><cell>acc. (%)</cell><cell>p  *</cell><cell>acc. (%)</cell><cell>p  *  acc. (%) p  *</cell><cell>acc. (%)</cell><cell>p  *</cell><cell>acc. (%)</cell></row><row><cell cols="2">NASNet-A-Mobile [50] 224</cell><cell>74.1/91.7</cell><cell>1.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Url: https://github.com/Cadene/pretrained-models.pytorch III</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Kaiming He for useful feedback and references. Maxim Berman is supported by Research Foundation -Flanders (FWO) through project number G0A2716N. PSI-ESAT acknowledges a GPU server donation from FAIR Partnership Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient match kernel between sets of features for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reducing overfitting in deep networks by decorrelating representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno>abs/1511.06068</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of gist descriptors for web-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sandhawalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIVR</title>
		<meeting>CIVR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end learning of deep visual representations for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic task prioritization for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="282" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09335</idno>
		<title level="m">Augment your batch: better training with larger batches. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5402</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>III</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Negative evidences and cooccurences in image retrieval: The benefit of pca and whitening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5454" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fine-tuning CNN image retrieval with no human annotation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop</title>
		<meeting>CVPR Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8119" to="8127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Yfcc100m: the new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image search with selective match kernels: aggregation across single and multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="261" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual query expansion with or without geometry: Refining local descriptors by feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno>abs/1511.05879</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Better matching with fewer features: The selection of useful features in large database recognition problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turcot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Taskonomy: Disentangling task transfer learning. Proc. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
