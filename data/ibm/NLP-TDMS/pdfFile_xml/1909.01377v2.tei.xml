<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Equilibrium Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>Bosch Center for AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Equilibrium Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective "depth" of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these stateof-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments. The code is available at https://github. com/locuslab/deq.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most modern feedforward deep networks are built on the core concept of layers. In the forward pass, each network consists of a stack of some L transformations, where L is the depth of the network. To update these networks, the backward passes rely on backpropagating through the same L layers via the chain rule, which typically necessitates that we store the intermediate values of these layers. The value for L is usually a hyperparameter and is picked by model designers (e.g., ResNet-101 <ref type="bibr" target="#b24">[25]</ref>). Among the many applications of deep networks, sequence modeling has witnessed continuous advances in model architectures. Specifically, while recurrent networks have long been the dominant model for sequences <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref>, deep feedforward architectures based on temporal convolutions <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7]</ref> and self-attention <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13]</ref> have (re-)emerged to claim superior performance on a variety of sequence prediction tasks.</p><p>In very general terms, a deep feedforward sequence model can be written as the following iteration: </p><p>where i is the layer index; z</p><p>[i] 1:T is the hidden sequence of length T at layer i; x 1:T is the input sequence (i.e., we are choosing to explicitly model skip connections, for reasons we explain later); and f <ref type="bibr">[i]</ref> θ is some nonlinear transformation which typically enforces causality (i.e., future time points cannot influence past ones). Our paper derives its motivation from surprising recent works that employ the same transformation in each layer (known as weight tying, with f [i] θ = f θ , ∀i) and still achieve results competitive with the state-of-the-art <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref>. This raises an interesting question: If the same transformation is applied at each layer of a deep network, what is the limit of this process, and how do we model it?</p><p>In this paper, we propose a new approach to "deep" modeling that addresses this question. Specifically, we introduce the deep equilibirum model (DEQ), a method that directly computes the fixed point z 1:T of a nonlinear transformation, i.e., the solution to the nonlinear system z 1:T = f θ (z 1:T ; x 1:T ).</p><p>(2)</p><p>This solution corresponds to the eventual hidden layer values of an infinite depth network. But instead of finding this value by iterating the model, we propose to directly (and in practice, more quickly) solve for the equilibrium via any black-box root-finding method. Importantly, we show that DEQ can directly differentiate through the fixed point equations via implicit differentation, which does not require storing any intermediate activation values. In other words, we can backpropagate through the infinite-depth network while using only constant memory, equivalent to a single layer's activations.</p><p>After developing the generic DEQ approach, we study in detail the instantiation of DEQ via two feedforward sequence models: trellis networks (weight-tied temporal convolutions) <ref type="bibr" target="#b7">[8]</ref> and memoryaugmented universal transformers (weight-tied multi-head self-attention) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref>, both of which have obtained state-of-the-art performance (SOTA) on various sequence tasks. We show how both the forward and backward passes can be implemented efficiently via quasi-Newton methods. Finally, we demonstrate via experiments on large-scale high-dimensional sequence modeling benchmarks (e.g., WikiText-103 language modeling) that, despite only using constant memory, DEQ can attain modeling accuracy on par with (or even slightly better than) corresponding layer-based networks. We believe that DEQ offers a novel perspective on the analysis of sequential data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Deep sequence models. Given an input sequence x 1:T = [x 1 , . . . , x T ] ∈ R T ×p , where x i ∈ R p (e.g., a word embedding) and T ∈ N is the sequence length, we define a sequence model as any function G that produces output G(x 1:T ) = y 1:T =∈ R T ×q that satisfies the causality constraint: y t depends only on x 1:t and not on any element of x t+1:T . Recent progress on autoregressive sequence tasks has been based on deep learning, where three major families of sequence models stand out. Recurrent networks (RNNs) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b50">51]</ref> as well as their variants such as LSTM <ref type="bibr" target="#b25">[26]</ref> are universally applied and optimized in a variety of time-series tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref>. Alternatively, prior work has shown that deeply stacked temporal convolutions <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref> can achieve competitive results, especially on long sequences. Finally, the self-attention transformer architecture <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b15">16]</ref> has also achieved SOTA on several NLP benchmarks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>. Efforts have also been devoted to drawing deeper connections among the three model families. Bai et al. <ref type="bibr" target="#b7">[8]</ref> study the underlying relationship between RNNs and ConvNets, unifying these in the Trellis Network, which combines the benefits of both families. Dehghani et al. <ref type="bibr" target="#b17">[18]</ref> introduce a recurrently-stacked universal transformer and demonstrate its effectiveness on text understanding and generation.</p><p>Memory-efficient deep networks. An important factor that limits the development of highcapacity networks is limited memory on hardware devices used for training. To address this issue, <ref type="bibr" target="#b11">[12]</ref> proposes gradient checkpointing that reduces an L-layer network's memory requirement to O( √ L) at the cost of extra forward passes (i.e., extra computations). Alternatively, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref> develop reversible networks, where each layer's activations can be reconstructed from the next layer during backpropagation to reduce memory requirements. DEQs reduce memory consumption to a constant (i.e., independent of network "depth") by directly differentiating through the equilibrium point and thus circumventing the construction and maintenance of "layers".</p><p>Continuous view of deep networks. Some prior works have studied continuous views of deep networks. <ref type="bibr" target="#b40">[41]</ref> proposes a biologically inspired equilibrium propagation framework for an energybased model whose prediction is the fixed-point of the energy dynamics at its local minimum. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref> model deep ResNets by black-box ODE solvers in forward and backward passes (as if the network has smaller "layer steps") given the start-and end-points of a dynamical system. For deep sequence models, <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b35">36]</ref> consider the RNN as a dynamical system to investigate its stability properties.</p><p>Our work takes a further step in the direction of the aforementioned areas. While some of the prior work has primarily focused on the analysis of residual architectures or small symmetric-weight energy-based models, our work is not predicated on any specific type of interlayer transformation. We show that DEQs can be easily instantiated via two very different sequence learning architectures. More fundamentally, unlike ODE-based methods, which use the adjoint system to backpropagate through the entire latent trajectory, the DEQ model solves directly for sequence-level equilibria via a quasi-Newton method and backpropagates directly through this fixed point, without regard for the solution path that brought it there. Moreover, while ODE-based models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref> were verified on numerical experiments and MNIST classification, computation and numerical stability issues challenge their application to large-scale problems. In comparison, we demonstrate the applicability of DEQs on realistic high-dimensional sequence tasks with competitive performance, while enjoying similar constant-memory benefits as <ref type="bibr" target="#b10">[11]</ref>.</p><p>Implicit layers in deep learning. The DEQ model can be viewed as an infinitely deep network, but interestingly can also be viewed as a single-layer network, with the caveat that the layer is defined implicitly: the output z 1:T is defined as the value which solves some non-linear equation. There has been a growing interest in implicit layers in recent years <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>, but the precise formulation of the DEQ is quite different, and our current models represent the largest-scale practical application of implicit layers in deep learning of which we are aware. Concurrent work <ref type="bibr" target="#b19">[20]</ref> also looks at such implicit layers in a broad sense and focuses on training small models via Lagrangian methods; a combination of these approaches with the DEQ model is a promising avenue for future work.</p><p>Another thread of work on implicit layers traces back to some of the original papers on recurrent networks trained via recurrent backpropagation (RBP) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>. Recent work <ref type="bibr" target="#b27">[28]</ref> has re-examined RBP and established an implicit, constant-memory variant based on conjugate gradient and Neumann series. A number of related papers also enforce fixed point conditions within RNN architectures <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b26">27]</ref>. Whereas the DEQ model shares similarities with the RBP approach, some major differences involve: 1) the explicit use of equilibrium as a replacement for depth in general networks, along with our proof of the universality of these models to replace depth; 2) the use of the approach in methods outside of fixed-input RNNs (i.e., same input vector x t for all t), especially the compatibility with SOTA architectures; and 3) the scalability of the DEQ model to practical tasks where it achieves results on par with the current SOTA, whereas RBP has typically been applied in small-scale settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Deep Equilibrium Sequence Model</head><p>We broadly consider the class of weight-tied deep sequence models (with passthrough connections from the input to each layer), which consist of the update </p><p>We note that this model encapsulates classes such as the trellis network <ref type="bibr" target="#b7">[8]</ref> and the universal transformer <ref type="bibr" target="#b17">[18]</ref> (which is typically not written with passthrough connections, but this is a trivial modification). Such weight-tying is generally considered to come with four major benefits: 1) it acts as a form of regularization that stabilizes training and supports generalization; 2) it significantly reduces the model size; 3) it is trivial to show that any deep network can be represented by a weight-tied deep network of equal depth and only a linear increase in width (see Appendix C); and 4) the network can be unrolled to any depth, typically with improved feature abstractions as depth increases <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. However, in practice almost all such models (and deep nets in general) are stacked, trained and evaluated by unrolling a pre-determined, fixed number of layers. One reason is the limited memory on training hardware: the models need to store intermediate hidden units for backpropagation and thus cannot be trained beyond a certain depth that depends on the available memory.</p><p>In principle, the network could have infinite depth. This is attained in the limit of unrolling a weighttied model for an ever higher number of layers. What is the limit of this process? In practice, for certain classes of f θ (discussed later), we hypothesize and observe that such weight-tied models tend to converge to a fixed point as depth increases towards infinity (see Appendix D for empirical evidence). In other words, as each layer refines the previous one by combining temporal features across the sequence, increasing depth towards infinity brings "diminishing returns": each additional layer has a smaller and smaller contribution until the network reaches an equilibrium:</p><formula xml:id="formula_2">lim i→∞ z [i] 1:T = lim i→∞ f θ z [i] 1:T ; x 1:T ≡ f θ z 1:T ; x 1:T ) = z 1:T<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Deep Equilibrium Approach</head><p>We introduce the deep equilibrium model (DEQ) which, instead of iteratively stacking f θ , directly solves for and differentiates through the equilibrium state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Forward Pass</head><p>Unlike a conventional network where the output is the activations from the L th layer, the output of a DEQ is the equilibrium point itself. Therefore, the forward evaluation could be any procedure that solves for this equilibrium point. Conventional deep sequence networks, if they converge to an equilibrium, can be considered a form of fixed-point iterations:</p><formula xml:id="formula_3">z [i+1] 1:T = f θ z [i] 1:T ; x 1:T for i = 0, 1, 2, . . .<label>(5)</label></formula><p>One can alternatively use other methods that provide faster convergence guarantees. For notational convenience, we define g θ and rewrite Eq. (4) as g θ (z 1:T ;</p><formula xml:id="formula_4">x 1:T ) = f θ z 1:T ; x 1:T − z 1:T → 0.</formula><p>The equilibrium state z 1:T is thus the root of g θ , which we can find more easily with Newton's method or quasi-Newton methods (e.g., Broyden's method <ref type="bibr" target="#b9">[10]</ref>):</p><formula xml:id="formula_5">z [i+1] 1:T = z [i] 1:T − αBg θ (z [i] 1:T ; x 1:T ) for i = 0, 1, 2, . . .<label>(6)</label></formula><p>where B is the Jacobian inverse (or its low-rank approximation) at z</p><p>[i] 1:T , and α is the step size. But generally, one can exploit any black-box root-finding algorithm to solve for the equilibrium point in the forward pass, given an initial estimate z </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Backward Pass</head><p>A major problem with using a black-box RootFind is that we are no longer able to rely on explicit backpropagation through the exact operations in the forward pass. While one can certainly fix an algorithm (say Newton's method) to obtain the equilibrium, and then store and backpropagate through all the Newton iterations, we provide below an alternative procedure that is much simpler, requires constant memory, and assumes no knowledge of the black-box RootFind. </p><p>Then the loss gradient w.r.t. (·) (for instance, θ or</p><formula xml:id="formula_7">x 1:T ) is ∂ ∂(·) = − ∂ ∂z 1:T J −1 g θ z 1:T ∂f θ (z 1:T ; x 1:T ) ∂(·) = − ∂ ∂h ∂h ∂z 1:T J −1 g θ z 1:T ∂f θ (z 1:T ; x 1:T ) ∂(·) ,<label>(8)</label></formula><p>where J −1 g θ x is the inverse Jacobian of g θ evaluated at x.</p><p>The proof is provided in Appendix A. The insight provided by Theorem 1 is at the core of our method and its various benefits. Importantly, the backward gradient through the "infinite" stacking can be represented as one step of matrix multiplication that involves the Jacobian at equlibrium. For instance, an SGD update step on model parameters θ would be</p><formula xml:id="formula_8">θ + = θ − α · ∂ ∂θ = θ + α ∂ ∂z 1:T J −1 g θ z 1:T ∂f θ (z 1:T ; x 1:T ) ∂θ .<label>(9)</label></formula><p>Note that this result is independent of the root-finding algorithm we choose or the internal structure of the transformation f θ , and thus does not require any storage of the intermediate hidden states, which is necessary for backpropagation in conventional deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Accelerating DEQ by Approximating the Inverse Jacobian</head><p>One challenge of enforcing the forward and backward passes described in Sections 3.1.1 and 3.1.2 is the cost of computing the exact inverse Jacobian J −1 g θ at every intermediate Newton iteration. We propose to address this using Broyden's method <ref type="bibr" target="#b9">[10]</ref>, a quasi-Newton approach that makes low-rank updates to approximate J −1 g θ via the Sherman-Morrison formula <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_9">J −1 g θ z [i+1] 1:T ≈ B [i+1] g θ = B [i] g θ + ∆z [i+1] − B [i] g θ ∆g [i+1] θ ∆z [i+1] B [i] g θ ∆g [i+1] θ ∆z [i+1] B [i] g θ ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_10">∆z [i+1] = z [i+1] 1:T − z [i]</formula><p>1:T and ∆g</p><formula xml:id="formula_11">[i+1] θ = g θ (z [i+1] 1:T ; x 1:T ) − g θ (z [i]</formula><p>1:T ; x 1:T ). Initially, we set B</p><p>[0] g θ = −I and the Broyden iterations are stopped when either the norm of g <ref type="bibr">[i]</ref> θ falls below a tolerance ε or when the maximum number of iterations is reached. This lets us avoid the cubic cost induced by the inverse operation.</p><p>A similar idea can be used for the backward pass as well. Specifically, to compute − ∂ ∂z 1:T</p><formula xml:id="formula_12">J −1 g θ z 1:T</formula><p>in Theorem 1, we can alternatively solve the linear system</p><formula xml:id="formula_13">J g θ z 1:T x + ∂ ∂z 1:T = 0,<label>(11)</label></formula><p>where the first term (a vector-Jacobian product) can be efficiently computed via autograd packages (e.g., PyTorch <ref type="bibr" target="#b44">[45]</ref>) for any x, without explicitly writing out the Jacobian matrix. Such linear systems can generally be solved by any indirect methods that leverage fast matrix-vector products; we thus propose to also rely on Broyden's method (other indirect methods would also suffice) to solve for Eq. (11) and directly backpropagate through the equilibrium by Theorem 1 in the backward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Properties of Deep Equilibrium Models</head><p>Section 3.1 develops a sequence model that, while still based on the deep learning philosophy, is quite different from other approaches in the field, as its output is agnostic to the choice of the RootFind algorithm in the forward pass. We now discuss some implications of the DEQ approach.</p><p>Memory cost of DEQ. An important benefit of DEQ is its extreme memory efficiency. As outlined in Section 3.1.3, since we are able to use any root-finding algorithm for both the forward and backward passes (e.g., Broyden's method <ref type="bibr" target="#b9">[10]</ref>), a DEQ only needs to store z 1:T (the equilibrium sequence), x 1:T (input-related, layer-independent variables), and f θ for the backward pass. Note that as we only need the vector-Jacobian product (with dimension N × T d, where N is the minibatch size) in Eq. <ref type="formula" target="#formula_0">(11)</ref>, we never need to explicitly construct the Jacobian J g θ z 1:T , which could be prohibitively large on long and high-dimensional sequences (with dimension N × (T d) <ref type="bibr" target="#b1">2</ref> ). Compared to other deep networks, DEQs therefore offer a constant-memory alternative that enables models that previously required multiple GPUs and other implementation-based techniques (e.g., half-precision or gradient checkpointing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>) to fit easily into a single GPU.</p><p>The choice of f θ . Our analysis in Sections 3.1.1, 3.1.2, and 3.1.3 is independent of the choice of f θ , and the same kind of memory benefit is present regardless of the type of f θ . However, to find the equilibrium in a reliable and efficient manner, generally f θ needs to be stable and constrained.  <ref type="bibr" target="#b0">[1]</ref> , θ <ref type="bibr" target="#b1">[2]</ref> the sets of parameters for stable transformations f θ <ref type="bibr" target="#b0">[1]</ref> : <ref type="bibr" target="#b1">[2]</ref> ; RootFind g v θ <ref type="bibr" target="#b0">[1]</ref> ;</p><formula xml:id="formula_14">R r × R p → R r and v θ [2] : R d × R r → R d , respectively. Then there exists Γ Θ : R d+r × R p → R d+r , where Θ = θ [1] ∪ θ [2] , s.t. z 1:T = RootFind g f θ</formula><formula xml:id="formula_15">x 1:T = RootFind g Γ Θ ; x 1:T [:,−d:] ,<label>(12)</label></formula><p>where [·] [:,−d:] denotes the last d feature dimensions of [·].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Instantiations of DEQ</head><p>While the forward and backward analyses of DEQ do not depend on the internal structure of f θ , in this section we briefly highlight two examples of f θ as specific instantiations of DEQ. Both models (TrellisNet <ref type="bibr" target="#b7">[8]</ref> and self-attention <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b17">18]</ref>) achieve state-of-the-art results on various sequence modeling benchmarks. Importantly, through these two very different models and their properties, we illustrate the compatibility of the DEQ approach with all three major families of existing deep sequence networks: transformers, RNNs, and temporal convolutional networks (TCNs). <ref type="bibr" target="#b0">[1]</ref> z <ref type="bibr" target="#b1">[2]</ref> z ?  </p><formula xml:id="formula_16">Input z [i] Output z [i+1] y x y = x Weight-tied Deep Networks z [0] ! z [1] ! . . . f ✓ (x) g ✓ (x) = f ✓ (x) x z [0] z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Equilibrium Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equilibrium Solver for</head><formula xml:id="formula_17">z ⇤ = f✓(z ⇤ ; x) z [i+1] = f ✓ (z [i] ; x)</formula><p>Deep Equilibrium Model  Trellis networks. We briefly introduce the trellis network (TrellisNet) here and refer interested readers to <ref type="bibr" target="#b7">[8]</ref> for a detailed description. Generally, TrellisNet is a TCN with two modifications. First, a linear transformation of the original input sequence x 1:T is added to the convolutional outputs at all layers. Second, the convolutional kernel weights are tied across the depth of the network (i.e., TrellisNet is a weight-tied TCN). Thus we can write TrellisNet with convolutional kernel size k, dilation s, and nonlinearity ψ in DEQ form as</p><formula xml:id="formula_18">x 1:T = Input injection (i.e., linearly transformed inputs by Conv1D(x 1:T ; W x )) f θ (z 1:T ; x 1:T ) = ψ(Conv1D([u −(k−1)s: , z 1:T ]; W z ) +x 1:T )</formula><p>where u −(k−1)s: is typically: 1) the last (k − 1)s elements of the previous sequence's output (if using history padding <ref type="bibr" target="#b7">[8]</ref>); or 2) simply zero-padding. [·, ·] means concatenation along the temporal dimension. Following <ref type="bibr" target="#b7">[8]</ref>, we use the LSTM gated activation for ψ.</p><p>Weight-tied transformers. At a high level, multi-head self-attention transformers <ref type="bibr" target="#b47">[48]</ref> are very different from most deep networks. Instead of convolutions or recurrence, a self-attention layer maps the input into Q (query), K (key), and V (value) and computes the attention score between time-steps t i and t j as [QK ] i,j . This attention score is then normalized via softmax and multiplied with the V sequence to produce the output. Since the transformer is order-invariant, prior work proposed to add positional embeddings (PE) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b15">16]</ref> to the self-attention operation. Following this design, <ref type="bibr" target="#b17">[18]</ref> further proposed the universal transformer, which "recurrently stacks" the transformer's self-attention and transition function block φ through a number of layers. Referring readers to <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> for more details, we write a weight-tied transformer in the DEQ form as where W QKV ∈ R d×3d produces the Q, K, V for the multi-head self-attention, and LN stands for layer normalization <ref type="bibr" target="#b4">[5]</ref>. Note that we add input injectionx 1:T to Q, K, V in addition to the positional embedding and initialize with z 1:T = 0. Following prior work <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>, we use a 2-layer positionwise feedforward residual block for φ. In our implementation, we use the memory-augmented transformer proposed by <ref type="bibr" target="#b15">[16]</ref>, where we feed [z −T : , z 1:T ] (i.e., with history padding of length T ) and relative positional embedding PE −T :T to the self-attention operation. <ref type="figure" target="#fig_3">Figure 1</ref> provides a generic comparison between these conventional weight-tied deep networks and the DEQ approach, highlighting the constant memory requirements of the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate DEQ on both synthetic stress tests and realistic large-scale language modeling (where complex long-term temporal dependencies are involved). We use the two aforementioned instantiations of f θ in DEQ. On both WikiText-103 <ref type="bibr" target="#b34">[35]</ref> (which contains &gt;100M words and a vocabulary size of &gt;260K) and the smaller Penn Treebank corpus (where stronger regularizations are needed for  conventional deep nets) for word-level language modeling, we show that DEQ achieves competitive (or better) performance even when compared to SOTA methods (of the same model size, both weighttied and not) while using significantly less memory. We provide a more detailed introduction of the tasks and datasets in Appendix F.</p><p>Setting. Both instantiations of DEQ use Broyden's method <ref type="bibr" target="#b9">[10]</ref> to avoid direct computation of the inverse Jacobian, as described in Section 3.1.3. We note that the use of DEQ implicitly introduces a new "hyperparameter" -the stopping criterion for Broyden iterations. During training, we set this tolerance ε of forward and backward passes to ε = √ T · 10 −5 and √ T · 10 −8 , respectively. At inference, we relax the tolerance to ε = √ T · 10 −2 (or we can use a smaller maximum iteration limit for Broyden's method; see discussions later). For the DEQ-TrellisNet instantiation, we roughly follow the settings of <ref type="bibr" target="#b7">[8]</ref>. For DEQ-Transformers, we employ the relative positional embedding <ref type="bibr" target="#b15">[16]</ref>, with sequences of length 150 at both training and inference on the WikiText-103 dataset. Implementations and pretrained models can be found at https://github.com/locuslab/deq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Copy Memory Task</head><p>The goal of the copy memory task is simple: to explicitly test a sequence model's ability to exactly memorize elements across a long period of time (see Appendix F). As shown in <ref type="table" target="#tab_1">Table 1</ref>, DEQ demonstrates good memory retention over relatively long sequences (T = 400), with substantially better results than recurrent architectures such as LSTM/GRU (consistent with the findings in <ref type="bibr" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Large-Scale Language Modeling</head><p>One issue encountered in prior works that take a continuous view of deep networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> is the challenge of scaling these approaches to real, high-dimensional, large-scale datasets. In this subsection, we evaluate the DEQ approach on some large-scale language datasets and investigate its effectiveness as a practical "implicit-depth" sequence model.</p><p>Performance on Penn Treebank. Following the set of hyperparameters used by <ref type="bibr" target="#b7">[8]</ref> for TrellisNet, we evaluate the DEQ-TrellisNet instantiation on word-level language modeling with the PTB corpus. Note that without an explicit notion of "layer", we do not add auxiliary losses, as was done in <ref type="bibr" target="#b7">[8]</ref>. As shown in <ref type="table" target="#tab_2">Table 2</ref>, when trained from scratch, the DEQ-TrellisNet achieves a test perplexity on par with the original deeply supervised TrellisNet.</p><p>Performance on WikiText-103. On the much larger scale WT103 corpus (about 100x larger than PTB), the DEQ-TrellisNet achieves better test perplexity than the original deep TrellisNet. For the Transformer instantiation, we follow the design of the Transformer-XL model <ref type="bibr" target="#b15">[16]</ref>. We specifically compare to a "medium" Transformer-XL model (the largest released model that can fit on GPUs)  and a "small" Transformer-XL model, while noting that the largest Transformer-XL network has massive memory requirements (due in part to very wide hidden features, batch sizes, and training-time sequence lengths, which would not be decreased by a DEQ) and can only be trained on TPUs <ref type="bibr" target="#b15">[16]</ref>.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we show that the DEQs yield competitive performance, outperforming prior SOTA approaches such as <ref type="bibr" target="#b15">[16]</ref> on similar model sizes while consuming much less memory during training.  <ref type="table" target="#tab_2">of Tables 2 and 3</ref>, with controlled sequence lengths and batch sizes for fairness. On both instantiations, the DEQ approach leads to an over 80% (up to 88%) reduction in memory consumption by the model (excluding word embeddings, which are orthogonal to the comparison here). Moreover, we empirically verify (using a 70-layer TrellisNet) that DEQ consumes even less memory than gradient checkpointing <ref type="bibr" target="#b11">[12]</ref>, a popular technique that reduces the memory required to train a layer-based model to O( √ L). Note that the DEQ's memory footprint remains competitive even when compared with baselines that are not weight-tied (a reduction of over 60%), with similar or better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization of DEQ.</head><p>To train DEQ models, it is critical to ensure that the model is stable, such that the equilibrium state can be reliably approximated via quasi-Newton methods. While we found that the most commonly used initialization schemes with small values (around 0) suffice, it is generally important to make sure that DEQ starts with a small operator norm in the weight matrices. For both DEQ-TrellisNet and DEQ-Transformer, we observe that they are not sensitive to any specific initialization scheme since non-linearities such as σ/tanh and LayerNorm also help make f θ contractive (and stable). We initialize the parameters of f θ by sampling from N (0, 0.05).  <ref type="figure" target="#fig_11">Figure 3</ref>: DEQ can be accelerated by leveraging higher tolerance ε (left) or a lower Broyden iteration limit (right). In general, poor estimates of the equilibrium can hurt DEQ performances.</p><p>Convergence to equilibrium. The deep equilibrium model does not have "layers". One factor that affects computation time in DEQs is the number of Broyden iterations in forward/backward passes, where each forward Broyden step evaluates f θ once, and a backward step computes a vector-Jacobian product. We find that in general the number of Broyden iterations gradually increases with training epochs <ref type="figure" target="#fig_10">(Figure 2</ref>, left, where the y-axis is computed by Total Broyden Iterations Sequence Length ), an observation similar to the one reported for training Neural ODEs <ref type="bibr" target="#b10">[11]</ref>. One factor contributing to this phenomenon could be that the training pushes the operator norm of J f θ to larger values, making the fixed point harder to solve. Meanwhile, the backward pass requires much fewer iterations than the forward, primarily due to the simplicity of the linear system in Eq. <ref type="bibr" target="#b10">(11)</ref>. We also find that DEQs can almost always converge to the sequence-level fixed point, much more efficiently than original weight-tied transformers <ref type="figure" target="#fig_10">(Figure  2</ref>, right). Note that after 12 epochs, deeply stacked self-attention tends to oscillate around the fixed point, while DEQs exhibit stable convergence with the quasi-Newton method.</p><p>Broyden iterations and the runtime of DEQ. Unlike conventional deep networks that come with a fixed number L of layers, the runtime of DEQ depends strongly on the number of Broyden steps to reach the equilibrium. Therefore, it's challenging to fairly compare the runtimes of implicit-depth models like DEQ with those of corresponding weight-tied deep networks (e.g., using higher depth necessarily takes longer to run). Ideally, the values of ε should be as small as possible so as to ensure that the analytical gradients from Theorem 1 are accurate. However, we empirically observe that using a higher ε or a lower iteration limit allows the DEQ to be trained and evaluated much faster with only a small degradation in performance. For instance, generally we find ε &lt; 0.1 or an iteration limit of 30 (on sequence length 75) to be sufficient for competitive performance. <ref type="figure" target="#fig_11">Figure 3</ref> visualizes this tradeoff on a medium DEQ-Transformer (without adaptive embedding). Note that accuracy quickly diverges when tolerance ε is too large <ref type="figure" target="#fig_11">(Figure 3, left</ref>), suggesting that a poor estimate of the equilibrium can hurt DEQ performances. <ref type="table" target="#tab_5">Table 4</ref> provides approximate runtimes for competitive-accuracy DEQs on WikiText-103. DEQs are typically slower than layer-based deep networks.</p><p>Additional empirical remarks as well as training tips are provided in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Deep networks have predominantly taken the form of stacks of layers. We propose the deep equilibrium approach (DEQ), which models temporal data by directly solving for the sequence-level fixed point and optimizing this equilibrium for better representations. DEQ needs only O(1) memory at training time, is agnostic to the choice of the root solver in the forward pass, and is sufficiently versatile to subsume drastically different architectural choices. Our experiments have shown that DEQs have good temporal memory retention, are able to scale to realistic, large-scale sequence tasks, and perform competitively with, or slightly outperform, SOTA methods. Overall, we believe that the DEQ approach provides an interesting and practical new perspective on designing and optimizing sequence models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Universality of Weight-tied, Input-injected Networks</head><p>Although the DEQ model corresponds to an infinite-depth network, as mentioned above it applies only to the specific case of weight-tied, input-injected infinite-depth models. This seems at first glance a substantial restriction over traditional deep networks, which have no requirement that the weights at each layer be identical. However, as we show below, this is not an actual restriction on the representational capacity from a mathematical point of view. Specifically, any deep network can be represented as a deep weight-tied network with no increase in depth and only a linear increase in the size of the hidden layer. This argument is equivalent to that presented in the TrellisNet work [8, Theorem 1], but we include it here in a slightly simpler and more general form. We emphasize that in practice we do not use the sparse structure below to construct the weight-tied layers for DEQ, but instead just use dense matrices W z and W x . However, the theorem below is important in establishing that there is no notable representational loss. </p><formula xml:id="formula_19">z [i+1] = σ [i] (W [i] z [i] + b [i] ), i = 0, . . . , L − 1, z [0] = x<label>(17)</label></formula><p>where z [i] denotes the hidden features at depth i, W <ref type="bibr">[i]</ref> , b <ref type="bibr">[i]</ref> are parameters of the network, σ [i] is the non-linearity at depth i, and x is the original input. Then the same network can be represented by a weight-tied, input-injected network of equivalent depth</p><formula xml:id="formula_20">z [i+1] = σ(W zz [i] + W x x +b), i = 0, . . . , L − 1.<label>(18)</label></formula><p>where σ, W z , W x andb are constant over all layers.</p><p>Proof of Theorem 3. The proof is constructive: we build the weight-tied network equivalent to the original network by contructing the relevant matrices using a simple "shift" operation. In particular, we define the network parameters as</p><formula xml:id="formula_21">W z =        0 0 . . . 0 0 W [1] 0 . . . 0 0 0 W [2] . . . 0 0 . . . . . . . . . . . . . . . 0 0 . . . W [L−1] 0        , W x =     W [0] 0 . . . 0     ,b =      b [0] b [1] . . . b [L−1]      , σ =      σ [0]</formula><p>σ <ref type="bibr" target="#b0">[1]</ref> . . .</p><formula xml:id="formula_22">σ [L−1]      .<label>(19)</label></formula><p>It is clear from inspection that after L applications of the layer, i.e.,</p><formula xml:id="formula_23">z [i+1] = σ(W zz [i] + W x x +b)<label>(20)</label></formula><p>using these parameters the hidden vectorz will take on the valuẽ</p><formula xml:id="formula_24">z [L] =     </formula><p>z <ref type="bibr" target="#b0">[1]</ref> z <ref type="bibr" target="#b1">[2]</ref> . . .</p><formula xml:id="formula_25">z [L]      .<label>(21)</label></formula><p>Thus the weight-tied network computes all the same terms as the original network, using the same depth as the original network, and with a hidden unit size that is just the sum of the individual hidden unit sizes in the original network. This establises the claim of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Empirical Convergence of Weight-tied Deep Nets</head><p>As mentioned in Section 3, one motivation for optimizing the sequence-level equilibrium comes from our empirical observations that, starting at some point of the deep stacking, weight-tied deep networks begin to converge to a fixed point. We show in <ref type="figure" target="#fig_12">Figure 4</ref> the convergence of trained layer-based TrellisNet (weight-tied temporal convolutions) and universal transformer (weight-tied self-attention) on randomly selected test samples of different lengths T =100, 200, 400 and 800. In both cases, we see a tendency of the activations to converge. Notably, for transformers, we find stacked iterations  usually lead to a oscillatory behavior on the plots <ref type="figure" target="#fig_12">(Figure 4)</ref>, with values fluctuating around the actual fixed point (which we empirically verify can be found much more easily with Newton or quasi-Newton methods).</p><p>In practice, due to limited computation, we usually set the number of layers to a predetermined number (e.g., 60 layers) and rarely reach the inference depths analyzed in <ref type="figure" target="#fig_12">Figure 4</ref>. Moreover, in the specific case of transformers, <ref type="bibr" target="#b0">[1]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Remarks on DEQ</head><p>Applicability of other deep techniques. While the DEQ approach does not preclude specific architectural choices of f θ (which means techniques such as layer normalization <ref type="bibr" target="#b4">[5]</ref> or weight normalization <ref type="bibr" target="#b38">[39]</ref> can work as is), it is not clear how certain regularizations such as auxiliary losses <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b7">8]</ref> could be applied on DEQ, since there are no more "layers". For dropout <ref type="bibr" target="#b43">[44]</ref>, we follow the practice of <ref type="bibr" target="#b7">[8]</ref>, which adapts the RNN variational dropout <ref type="bibr" target="#b21">[22]</ref> scheme to feedforward networks by applying the same mask at all levels. We empirically find that adding dropout makes the quasi-Newton methods slower to converge (i.e., inference-time equilibria are easier to find without the presence of noisy zeros). Since the weights of f θ (and thus its operator norm) are directly related to the stability of root-finding, we observe that weight normalization <ref type="bibr" target="#b38">[39]</ref> typically finds more stable parameters and slows down the growth of forward/backward Broyden's iterations (as described in <ref type="figure" target="#fig_10">Figure 2</ref>).</p><p>Imbalances within minibatches. Not all sequences in a minibatch converge to the equilibrium with the same number of iterations. However, with standard batched CUDA operations, the sequences that converge faster essentially need to "wait" for the slower ones. Though we empirically find such imbalance to be relatively small in scale, it could mean an inefficient GPU utilization at times.</p><p>Warmup of DEQ models with shallow nets. Instead of training the DEQ from scratch, empirically we find that one can accelerate the DEQ training by pretraining a shallow weight-tied stack of f θ (e.g., 2 layers), and using the resulting parameters to initialize the DEQ. In general, a shallow model plateaus at much lower accuracy than corresponding DEQs or deeper weight-tied networks. However, given the very small number of layers, a shallow model offers a memory-and computation-efficient starting point for DEQ training.</p><p>Training DEQs with subsequences. On extremely long sequences (e.g., T &gt; 1000), the forwardpass fixed points can be challenging to solve accurately (especially at the start of the training) even with the help of the root-finding methods. Therefore, in practice, we suggest breaking these long sequences into a few subsequences when needed (recall that the forward pass can be any black-box root-finder). Moreover, with the help of Theorem 1, such subsequence technique can be used in the backward pass as well (where we solve for Eq. <ref type="formula" target="#formula_0">(11)</ref>). For instance, on a sequence  <ref type="bibr" target="#b21">(22)</ref> where terms (A) and (B) require one evaluation of f θ (z (T /2):T ; [x (T /2):T , z 1:(T /2) ]) and term (C) requires one evaluation of f θ (z 1:(T /2) ; x 1:(T /2) ). Hence, the memory cost is equivalent to that of applying f θ once on the entire z 1:T (but with the subsequences' equilibrium likely easier to optimize).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Task Descriptions</head><p>We briefly introduce the three sequence prediction tasks/datasets that we employ to evaluate the DEQ approach in Section 5.</p><p>Copy memory task. The copy memory task is a small but challenging synthetic stress test that has been frequently used in prior work to test a sequence model's memory retention ability <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref>. In this task, each sequence x 1:(T +20) is 1-dimensional and has length T + 20, with x 1:10 randomly selected from integers 1, 2, . . . , 8 (with repetition). The rest of the input elements are all filled with zeros, except for the x T +10 = 9. The goal of this task is to produce y 1:(T +20) such that y 1:T +10 = 0 and y T +11:T +20 = x 1:10 . In other words, a sequence model trained on this task is expected to "recall" the first 10 elements of the sequence once it sees the delimiter x T +10 = 9, and copy them to the end of the sequence. We generate 20K training samples and 2K testing samples. In prior works, <ref type="bibr" target="#b6">[7]</ref> have shown that RNNs generally struggle with the task, especially when T &gt; 100, whereas feedforward models tend to have better memory.</p><p>Penn Treebank. The Penn Treebank (PTB) corpus <ref type="bibr" target="#b30">[31]</ref> is a commonly used dataset for characterand word-level language modeling. When used for word-level language modeling, PTB contains about 888K words at training, with a vocabulary size of 10,000. As this is a comparatively small language corpus (with punctuations and capitalization removed), prior work has shown that well-designed regularizations are required for best results <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>WikiText-103. The training corpus of WikiText-103 (WT103) <ref type="bibr" target="#b34">[35]</ref> is about 110 times larger than PTB, with a vocabulary size over 260K. In general, this dataset is considered much more realistic than many others because it contains many rare words and retains punctuation, numbers, and capitalization from the original Wikipedia articles. WT103 is thus used to evaluate how well a sequence model scales to long sequences from a large vocabulary. This dataset has been frequently used in recent work with high-capacity sequence models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>; x 1:T for i = 0, 1, 2, . . . , L − 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>; x 1:T ), i = 0, . . . , L − 1, z [0] 1:T = 0, G(x 1:T ) ≡ z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(which we set to 0): z 1:T = RootFind(g θ ; x 1:T )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 1 .</head><label>1</label><figDesc>(Gradient of the Equilibrium Model) Let z 1:T ∈ R T ×d be an equilibrium hidden sequence with length T and dimensionality d, and y 1:T ∈ R T ×q the ground-truth (target) sequence. Let h : R d → R q be any differentiable function and let L : R q × R q → R be a loss function (where h, L are applied in a vectorized manner) that computes = L(h(z 1:T ), y 1:T ) = L(h(RootFind(g θ ; x 1:T )), y 1:T ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) A simple illustration of solving for an equilibrium point in 2D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>f✓(x; . . . ) = x Fixed Memory storage needed at training time History (or zero) padding (i.e., previous equilibrium)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) A deep equilibrium model operates with significantly less memory than conventional deep nets due to an analytical backward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of the DEQ with conventional weight-tied deep networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>x 1 :</head><label>1</label><figDesc>T = Input injection (i.e., linearly transformed inputs by x 1:T W x ) f θ (z 1:T ; x 1:T ) = LN(φ(LN(SelfAttention(z 1:T W QKV +x 1:T ; PE 1:T ))))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 :</head><label>2</label><figDesc>Left: number of Broyden iterations in forward and backward passes gradually grows with epochs. Right: DEQ-Transformer finds the equilibrium in a stable and efficient manner (whereas the deep transformer could oscillate around the fixed point, even when one exists).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Theorem 3 .</head><label>3</label><figDesc>(Universality of Weight-tied Deep Networks) Consider a traditional L-layer deep network defined by the relation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 :</head><label>4</label><figDesc>The convergence of intermediate activations in TrellisNet (with kernel size 2) and weighttied transformers on different sequence lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>z 1 :</head><label>1</label><figDesc>T = z 1:(T /2) z (T /2):T :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The two instantiations we provide in Section 4 are examples of stable transformations. (The gated activation in TrellisNet and layer normalization in the transformer constrain the output ranges.) Stacking the DEQ? A natural question arises: if one DEQ is good, can we get additional benefits by "stacking" DEQs (with potentially different classes of transformations)? The answer, somewhat surprisingly, is no, as evidenced by the following theorem, which is proved in Appendix B. The theorem essentially shows that stacking multiple DEQs does not create extra representational power over a single DEQ. Theorem 2. (Universality of "single-layer" DEQs.) Let x 1:T ∈ R T ×p be the input sequence, and θ</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>DEQ achieves strong performance on the long-range copy-memory task.</figDesc><table><row><cell></cell><cell></cell><cell>Models (Size)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">DEQ-Transformer (ours) (14K) TCN [7] (16K) LSTM [26] (14K) GRU [14] (14K)</cell></row><row><cell>Copy Memory T =400 Loss</cell><cell>3.5e-6</cell><cell>2.7e-5</cell><cell>0.0501</cell><cell>0.0491</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>DEQ achieves competitive performance on word-level Penn Treebank language modeling (on par with SOTA results, without fine-tuning steps<ref type="bibr" target="#b33">[34]</ref>). † The memory footprints are benchmarked (for fairness) on input sequence length 150 and batch size 15, which does not reflect the actual hyperparameters used; the values also do not include the memory for word embeddings.</figDesc><table><row><cell cols="3">Word-level Language Modeling w/ Penn Treebank (PTB)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell># Params</cell><cell cols="3">Non-embedding Test perplexity Memory  † model size</cell></row><row><cell>Variational LSTM [22]</cell><cell>66M</cell><cell>-</cell><cell>73.4</cell><cell>-</cell></row><row><cell>NAS Cell [55]</cell><cell>54M</cell><cell>-</cell><cell>62.4</cell><cell>-</cell></row><row><cell>NAS (w/ black-box hyperparameter tuner) [32]</cell><cell>24M</cell><cell>20M</cell><cell>59.7</cell><cell>-</cell></row><row><cell>AWD-LSTM [34]</cell><cell>24M</cell><cell>20M</cell><cell>58.8</cell><cell>-</cell></row><row><cell>DARTS architecture search (second order) [29]</cell><cell>23M</cell><cell>20M</cell><cell>55.7</cell><cell>-</cell></row><row><cell>60-layer TrellisNet (w/ auxiliary loss, w/o MoS) [8]</cell><cell>24M</cell><cell>20M</cell><cell>57.0</cell><cell>8.5GB</cell></row><row><cell>DEQ-TrellisNet (ours)</cell><cell>24M</cell><cell>20M</cell><cell>57.1</cell><cell>1.2GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>DEQ-based models are competitive with SOTA deep networks of the same model size on the WikiText-103 corpus, with significantly less memory. † SeeTable 2for more details on the memory benchmarking. Transformer-XL models are not weight-tied, unless specified otherwise.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">Word-level Language Modeling w/ WikiText-103 (WT103)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell cols="3"># Params</cell><cell cols="2">Non-Embedding Test perplexity Memory  † Model Size</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Generic TCN [7]</cell><cell></cell><cell>150M</cell><cell></cell><cell>34M</cell><cell>45.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gated Linear ConvNet [17]</cell><cell></cell><cell>230M</cell><cell></cell><cell>-</cell><cell>37.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AWD-QRNN [33]</cell><cell></cell><cell>159M</cell><cell></cell><cell>51M</cell><cell>33.0</cell><cell>7.1GB</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Relational Memory Core [40]</cell><cell></cell><cell>195M</cell><cell></cell><cell>60M</cell><cell>31.6</cell><cell>-</cell></row><row><cell cols="4">Transformer-XL (X-large, adaptive embed., on TPU) [16]</cell><cell></cell><cell>257M</cell><cell></cell><cell>224M</cell><cell>18.7</cell><cell>12.0GB</cell></row><row><cell></cell><cell cols="3">70-layer TrellisNet (+ auxiliary loss, etc.) [8]</cell><cell></cell><cell>180M</cell><cell></cell><cell>45M</cell><cell>29.2</cell><cell>24.7GB</cell></row><row><cell cols="4">70-layer TrellisNet with gradient checkpointing</cell><cell></cell><cell>180M</cell><cell></cell><cell>45M</cell><cell>29.2</cell><cell>5.2GB</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DEQ-TrellisNet (ours)</cell><cell></cell><cell>180M</cell><cell></cell><cell>45M</cell><cell>29.0</cell><cell>3.3GB</cell></row><row><cell></cell><cell></cell><cell cols="2">Transformer-XL (medium, 16 layers)</cell><cell></cell><cell>165M</cell><cell></cell><cell>44M</cell><cell>24.3</cell><cell>8.5GB</cell></row><row><cell></cell><cell></cell><cell cols="2">DEQ-Transformer (medium, ours).</cell><cell></cell><cell>172M</cell><cell></cell><cell>43M</cell><cell>24.2</cell><cell>2.7GB</cell></row><row><cell cols="4">Transformer-XL (medium, 18 layers, adaptive embed.)</cell><cell></cell><cell>110M</cell><cell></cell><cell>72M</cell><cell>23.6</cell><cell>9.0GB</cell></row><row><cell cols="4">DEQ-Transformer (medium, adaptive embed., ours)</cell><cell></cell><cell>110M</cell><cell></cell><cell>70M</cell><cell>23.2</cell><cell>3.7GB</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Transformer-XL (small, 4 layers)</cell><cell></cell><cell>139M</cell><cell></cell><cell>4.9M</cell><cell>35.8</cell><cell>4.8GB</cell></row><row><cell></cell><cell cols="3">Transformer-XL (small, weight-tied 16 layers)</cell><cell></cell><cell>138M</cell><cell></cell><cell>4.5M</cell><cell>34.9</cell><cell>6.8GB</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DEQ-Transformer (small, ours).</cell><cell></cell><cell>138M</cell><cell></cell><cell>4.5M</cell><cell>32.4</cell><cell>1.1GB</cell></row><row><cell># of Broyden Iter. per Time Step</cell><cell>0.4 0.5 0.6 0.7 0.8 0.9</cell><cell>0</cell><cell>2 DEQ-Transformer on WT103 (Seq. Length=150) 4 6 8 10 12 Training Epoch Forward (eps=1e-6) Backward (eps=1e-8)</cell><cell>Difference Norm ||f(x)-x||</cell><cell>10 4 10 3 10 2 10 1 10 0 10 1 10 2 10 3</cell><cell>0</cell><cell cols="2">50 DEQ-Transformer on WT103 (Seq. Length=150) Number of Function Evaluations 100 150 200 250 300 350</cell></row></table><note>Weight-tied Trans. (Ep. 1) Weight-tied Trans. (Ep. 12) DEQ-Trans. (Ep. 1) DEQ-Trans. (Ep. 12)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Memory footprint of DEQ. For conventional deep networks with L layers, the training memory complexity is O(L) since all intermediate activations are stored for backpropagation. In comparison, DEQs have an O(1) (i.e., constant) memory footprint due to the root-finding formulation. We benchmark the reduced memory consumption in the last column</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Runtime ratios between DEQs and corresponding deep networks at training and inference (&gt; 1× implies DEQ is slower). The ratios are benchmarked on WikiText-103.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">DEQ / 18-layer Transformer</cell><cell cols="2">DEQ / 70-layer TrellisNet</cell></row><row><cell></cell><cell></cell><cell>Training</cell><cell cols="2">Inference</cell><cell>Training</cell><cell>Inference</cell></row><row><cell></cell><cell></cell><cell>2.82×</cell><cell cols="2">1.76×</cell><cell>2.40×</cell><cell>1.64×</cell></row><row><cell>Validation Perplexity</cell><cell>25 50 75 100 125 150 175 200</cell><cell cols="2">DEQ-Transformer on WT103 DEQ-Transformer</cell><cell>Validation Perplexity</cell><cell>24 25 26 27 28 29</cell><cell>DEQ-Transformer on WT103</cell><cell>DEQ-Transformer</cell></row><row><cell></cell><cell>10 6</cell><cell cols="2">10 5 Forward Threshold Epsilon (Step Avg.) 10 4 10 3 10 2 10 1</cell><cell>10 0</cell><cell>20</cell><cell cols="2">40 Forward Broyden Iteration Limit 60 80</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>stablizes the training of deep transformers (64-layer) on characterlevel language modeling with the help of various strong auxiliary losses at intermediate layers. In a certain sense, the addition of auxiliary losses have a similar effect as solving for equilibrium: we want intermediate-level hidden units to be both close to the target and as stable as possible (without drastic interlayer differences).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Backward Pass of the Deep Equilibrium Model</head><p>One of the core benefits of the DEQ approach comes from its analytical backward gradient at equilibrium. In this section, we provide a proof to Theorem 1 (which we restate here). Theorem 1. (Gradient of the Equilibrium Model) Let z 1:T ∈ R T ×d be an equilibrium hidden sequence with length T and dimensionality d, and y 1:T ∈ R T ×q the ground-truth (target) sequence. Let h : R d → R q be any differentiable function and L : R q × R q → R be a loss function (where h, L are applied in vectorized manner) that computes = L(h(z 1:T ), y 1:T ) = L(h(RootFind(g θ ; x 1:T )), y 1:T ).</p><p>Then the loss gradient w.r.t. (·) (for instance, θ or</p><p>where J −1 g θ x is the inverse Jacobian of g θ evaluated at x.</p><p>Proof of Theorem 1. We first write out the equilibrium sequence condition: f θ (z 1:T ; x 1:T ) = z 1:T . By implicitly differentiating two sides of this condition with respect to (·):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Sufficiency of a Single DEQ "Layer"</head><p>A hypothetical extension to the DEQ idea follows from the "deep" philosophy: if one DEQ works so well, why don't we stack multiple DEQ modules with different parameters f θ [i] (i = 1, 2, . . . )? We (re-)state and prove the following theorem, which demonstrates the universality of the DEQ model (i.e., sufficiency of exactly one DEQ "layer"). Theorem 2. (Universality of "Single-layer" DEQs) Let x 1:T ∈ R T ×p be the input sequence, and θ <ref type="bibr" target="#b0">[1]</ref> , θ <ref type="bibr" target="#b1">[2]</ref> the sets of parameters for stable transformations f θ <ref type="bibr" target="#b0">[1]</ref> : R r ×R p → R r and v θ <ref type="bibr" target="#b1">[2]</ref> :</p><p>z 1:T = RootFind g v θ <ref type="bibr" target="#b1">[2]</ref> ; RootFind g f θ <ref type="bibr" target="#b0">[1]</ref> ;</p><p>where [·] [:,−d:] denotes the last d feature dimensions of [·].</p><p>Proof of Theorem 2. Assume z <ref type="bibr" target="#b0">[1]</ref> 1:T = RootFind g f θ <ref type="bibr" target="#b0">[1]</ref> ; x 1:T ∈ R r is the equilibrium of the first DEQ module under transformation f θ <ref type="bibr" target="#b0">[1]</ref> . Define Θ = θ <ref type="bibr" target="#b0">[1]</ref> ∪θ <ref type="bibr" target="#b1">[2]</ref> , and Γ Θ (w 1:T ; </p><p>1:T , x 1:T ) v θ <ref type="bibr" target="#b1">[2]</ref> (w </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04444</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A learning rule for asynchronous perceptrons with feedback in a combinatorial environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A class of methods for solving nonlinear simultaneous equations. Mathematics of Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles G Broyden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent stacking of layers for compact neural machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<title level="m">Universal transformers. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangda</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Travacca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Askari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Implicit deep learning</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger B</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<title level="m">Stable architectures for deep neural networks. Inverse Problems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implicitly-defined neural networks for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaeel</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reviving and improving recurrent back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kijung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xaq</forename><surname>Pitkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reversible recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An analysis of neural language modeling at multiple scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08240</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">When recurrent models don&apos;t need to be recurrent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10369</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SparseMAP: Differentiable sparse structured inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalization of back propagation to recurrent and higher order neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relational recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. Frontiers in Computational Neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Scellier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adjustment of an inverse matrix corresponding to a change in one element of a given matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winifred</forename><forename type="middle">J</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fixed point analysis for recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patrice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">B</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana H</forename><surname>Ottaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning longer-term dependencies in RNNs with auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Donti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Backpropagation through time: What it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Architectural complexity measures of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Equilibrated recurrent neural network: Neuronal time-delayed self-feedback improves accuracy and stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00755</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
