<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MODEL-FREE EPISODIC CONTROL WITH ONLINE STATE AGGREGATION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-25">August 25, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">C</forename><surname>Pinto</surname></persName>
							<email>rafael.pinto@canoas.ifrs.edu.br</email>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Federal de Educação</orgName>
								<address>
									<country>Ciência</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tecnologia do Rio Grande do Sul Canoas</orgName>
								<address>
									<region>RS</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MODEL-FREE EPISODIC CONTROL WITH ONLINE STATE AGGREGATION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-25">August 25, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Reinforcement Learning · Episodic Control · State Aggregation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Episodic control provides a highly sample-efficient method for reinforcement learning while enforcing high memory and computational requirements. This work proposes a simple heuristic for reducing these requirements, and an application to Model-Free Episodic Control (MFEC) is presented. Experiments on Atari games show that this heuristic successfully reduces MFEC computational demands while producing no significant loss of performance when conservative choices of hyperparameters are used. Consequently, episodic control becomes a more feasible option when dealing with reinforcement learning tasks.</p><p>This work is structured as follows: section 2 presents related works and concepts in episodic control and state aggregation for reinforcement learning, while section 3 presents our proposed algorithm, which introduces state aggregation into episodic control. Section 4 shows experimental results, and section 5 concludes with discussions and future works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning is at the center of many recent accomplishments in artificial intelligence, such as playing Atari games <ref type="bibr" target="#b0">[1]</ref> and playing go at the grandmaster level <ref type="bibr" target="#b1">[2]</ref>. Such an approach is very appealing due to its low reliance on supervision, needing only sparse reward signals to acquire useful behaviors. However, most common algorithms suffer from low sample efficiency, which means that a high number of training episodes are necessary for the agent to acquire the desired level of competence on diverse tasks. The Deep Q-Learning Network (DQN) <ref type="bibr" target="#b2">[3]</ref> and its variants <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> as well as A3C <ref type="bibr" target="#b5">[6]</ref> and other model-free actor-critic or policy gradient methods <ref type="bibr" target="#b6">[7]</ref> may require tens of millions of agent-environment interactions due to very inefficient learning. This kind of inefficiency is not acceptable for some classes of tasks, such as robotics, where failure and damage must be minimized. A robot cannot afford to fall from stairs a thousand times before learning to avoid them. Faster learning is also beneficial for more conventional problems (including games and simulated environments) by allowing for more evaluations and faster iterations. Improvements in sample efficiency are crucial for the advancement of the field and for enabling more practical applications.</p><p>It has been hypothesized that gradient-based reinforcement learning methods, such as the above-mentioned ones, suffer from slow learning <ref type="bibr" target="#b7">[8]</ref>. The need for low learning rates prevents the rapid acquisition of new behaviors and immediate incorporation of new information. To offer faster alternatives, non-parametric and instance-based methods have been proposed as replacements <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. The main drawback of such approaches is their large memory requirements, as all learning experiences must be stored for later recall. This not only prevents life-long learning but also slows down computations, as large high-dimensional searches are necessary at each step. We propose a way of reducing those requirements by aggregating similar experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The present work is built upon the concepts of episodic control and state aggregation in reinforcement learning, which shall be reviewed in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Episodic Control</head><p>In <ref type="bibr" target="#b8">[9]</ref>, the case for episodic control is made. Its differences to other kinds of control as well as its advantages and disadvantages are described and theoretically analyzed. It is shown that episodic control bests model-based (noisy) learning in various circumstances in the low data regime.</p><p>Model-Free Episodic Control (MFEC) <ref type="bibr" target="#b9">[10]</ref> was proposed as a highly sample-efficient alternative to deep reinforcement learning <ref type="bibr" target="#b2">[3]</ref> by storing all unique observations encountered by the agent, each one associated with its predicted Q-value and stored in their respective action buffers (there is one buffer per possible action). Action selection is performed by finding the k Nearest Neighbors (kNN) from the current observation in each action buffer. By averaging the Q-values of the k observations in each buffer, it is possible to select the action with the largest predicted value.</p><p>Neural Episodic Control (NEC) <ref type="bibr" target="#b11">[12]</ref> replaces vanilla kNN with distance weighted kNN, resulting in a differentiable module that can be integrated into neural networks. As a consequence, convolutional layers can be used to extract better state embeddings to be passed as inputs to episodic control (while MFEC uses random projections <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> or Variational Autoencoders <ref type="bibr" target="#b14">[15]</ref> for this purpose). Our methods could be applied to NEC as well, but we focus on MFEC in this work.</p><p>Finally, <ref type="bibr" target="#b10">[11]</ref> provides an extensive review of episodic control and its comparison to model-free and model-based learning, including parallels in neuroscience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">State Aggregation</head><p>State aggregation was proposed as a way to reduce computational demands in reinforcement learning problems with continuous state spaces <ref type="bibr" target="#b15">[16]</ref>. The essential idea consists in grouping states which should be treated in the same way by the algorithm, i.e., similar states with similar Q-values / policy outputs. As an example, the Growing Neural Gas (GNG) algorithm <ref type="bibr" target="#b16">[17]</ref> has been applied as a state quantizer in reinforcement learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, aggregating and splitting states online as necessary. It is important to note that the authors acknowledge the necessity of considering both input (state) and output (Q-values) spaces when merging states, something that is also part of our proposal. Very similar states which require different actions should stay separated and updated differently, otherwise, a phenomenon known as perceptual aliasing <ref type="bibr" target="#b19">[20]</ref> occurs (in this case, in the aggregated state space).</p><p>In <ref type="bibr" target="#b20">[21]</ref>, the authors propose a memory-efficient variant of MFEC where the least recently used (LRU) policy used for replacing observations when a buffer is full is replaced by online clustering (which is a form of state aggregation). Our proposal differs from this one in the sense that aggregation occurs early during learning, as soon as new observations arrive, and not only when a buffer is full.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MFEC with State Aggregation</head><p>Here we propose a simple heuristic for merging states as soon as they are observed. Given two new non-negative real-valued hyper-parameters ε in and ε out , a new observed state-action entry is added to its respective buffer if, and only if, the Euclidean distance from it to its nearest stored neighbor in that buffer is larger than the threshold ε in , or the absolute difference between its Q-value estimate and the one stored in its nearest neighbor entry is larger than ε out . By setting ε in to 0, we get vanilla MFEC (all unique states are added). Smaller values of ε in and ε out result in higher memory requirements (and thus higher computational demand, due to kNN lookup), while larger values produce more aggressive state aggregation, resulting in smaller buffers and faster lookups. A detailed description of this procedure is shown in algorithm 1.</p><p>We also note that to preserve the original behavior of kNN, aggregated states should count multiple times when selecting the nearest neighbors. For instance, three aggregated states with counts (number of individual states aggregated into a single entry) 2, 2 and 3, respectively, contain a total of 7 individual states and thus should be enough when searching for any k ≤ 7 neighbors, while the first two are enough for k ≤ 4. Besides minimizing neighborhood distortions in relation to vanilla kNN, correct weights are attributed to each aggregated state, minimizing differences in the final average. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the problem and the proposed solution.</p><p>Algorithm 1 Model-free episodic control with state aggregation.</p><p>Input: ε (exploration rate), k (number of neighbors), ε in (input threshold), ε out (output threshold) 1: for each episode do 2:</p><p>for t = 1, 2, 3, ..., T do Run episode <ref type="bibr">3:</ref> Receive observation o t from environment. <ref type="bibr" target="#b3">4</ref>:</p><formula xml:id="formula_0">Let s t = φ(o t ).</formula><p>Random projection or other kind of embedding <ref type="bibr">5:</ref> if X ∼ U(0, 1) &lt; ε then ε-greedy policy <ref type="bibr">6:</ref> Select random action a t uniformly from the set of all available actions <ref type="bibr">7:</ref> else Select greedy action <ref type="bibr">8:</ref> for each action a do <ref type="bibr">9:</ref> if (s t , a) ∈ Q EC then If an exact match is found <ref type="bibr" target="#b9">10</ref>:</p><formula xml:id="formula_1">Q EC (s t , a) ← Q EC (s t , a)</formula><p>Use single entry estimate <ref type="bibr">11:</ref> else Otherwise, compute kNN estimate <ref type="bibr" target="#b11">12</ref>:</p><formula xml:id="formula_2">Q EC (s t , a) ← 0, sum ← 0 13:</formula><p>for i = 1, 2, ..., k states s (i) nearest to s t do <ref type="bibr" target="#b13">14</ref>:</p><formula xml:id="formula_3">Q EC (s t , a) ← Q EC (s t , a) + Q EC (s (i) , a) × C EC (s (i)</formula><p>, a) Aggregated kNN weighting <ref type="bibr">15:</ref> sum ← sum + C EC (s, a) <ref type="bibr">16:</ref> if sum &gt;= k then Aggregated kNN radius truncation <ref type="bibr">17:</ref> break <ref type="bibr" target="#b17">18</ref>:</p><formula xml:id="formula_4">Q EC (s t , a) ← Q EC (st,a) sum</formula><p>Corrected kNN estimate <ref type="bibr" target="#b18">19</ref>:</p><formula xml:id="formula_5">Let a t = argmax a Q EC (s t , a)</formula><p>Greedy policy (could be ε-greedy) <ref type="bibr">20:</ref> Take action a t , receive reward r t+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>for t = T, T − 1, ..., 1 do For each stored experience, backwards (at end of episode) <ref type="bibr">22:</ref> s nearest ← argmin s s − s t , ∀s ∈ buffer for action a t Find nearest <ref type="bibr">23:</ref> if s t = s nearest then If exact match <ref type="bibr" target="#b23">24</ref>:</p><formula xml:id="formula_6">Q EC (s nearest , a t ) ← max{Q EC (s nearest , a t ), R t } Replace Q 25: else if s t − s nearest &lt; ε in and R t − Q EC (s nearest , a t ) &lt; ε out then If close enough 26: C EC (s nearest , a t ) ← C EC (s nearest , a t ) + 1 Increment counter 27: η ← 1 C EC (snearest,at)</formula><p>Compute learning rate 28: </p><formula xml:id="formula_7">Q EC (s nearest , a t ) ← Q EC (s nearest , a t ) + η R t − Q EC (s nearest ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe our experimental setup and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>To investigate the pros and cons of performing state aggregation in episodic control, we applied our algorithm to 6 Atari games with 4 different ε in values, including 0, which equates to vanilla MFEC. ε out was set to 100, k was set to 11 and ε (exploration rate) was set to 0.005 for all experiments (no parameter search was performed). A random projection was performed from a downscaled (84x84, bilinear) grayscale transformation of the game screen into a 128-dimensional input vector. Average and peak performance, as well as total buffer size (the sum of all action buffers), were assessed every 100000 frames or so (episodes ran until finished even if the limit was reached). Each game ran for 100 assessments (10 million frames) and this was repeated for 5 sequential seeds from the set 0, 1, 2, 3, 4. We are aware of works such as <ref type="bibr" target="#b21">[22]</ref> which show that more evaluations are needed to obtain more meaningful results and to avoid effects of handpicking seeds, but due to hardware and time restrictions, we argue that using 5 sequential seeds such as those is enough to avoid the handpicking issue. The "v0" version of each game was used, with no forms of non-determinism (as is the default in previous works on episodic control) and with frameskip set to 4 (no frame stacking  Aggregated states are marked with an inner black dot. Note that, in this case, the effective radius considered is increased and, as each point still has the same weight, the center of mass is also distorted, producing very discrepant results when computing the kNN average. c) Our proposed solution: aggregated states count as multiple points. Since the total count of the three nearest states (i.e., 7) already surpasses k = 5, no other points are considered. A weighted average is computed by weighting each point according to its count. The effective radius and center of mass are kept similar to vanilla kNN.</p><p>was used, thus 75% of all frames are lost). Our code was implemented in PyTorch <ref type="bibr" target="#b22">[23]</ref> and will soon be available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Figures 2 and 3 show mean curves and standard error intervals for average and peak performance in the games, respectively. <ref type="figure" target="#fig_7">Figure 4</ref> shows mean curves and standard error intervals for total buffer size, i.e., the sum of all action buffers for each game. It can be observed that, albeit not always giving great improvements in terms of size, the ε in = 0.01 setting is a quite safe option regarding average performance, even giving better results in Frostbite. In the case of Ms. Pacman, Space Invaders, and River Raid, no setting incurred a significant loss of average performance. The same behavior can not be observed in the peak performance case for Q*bert and Space Invaders, as any level of state aggregation incurs in performance loss. It seems that fine-grained state information is necessary to achieve very high scores in these games, even if only occasionally (projectile size in Space Invaders being evidence for this, as well as small brightness differences in Q*bert blocks when discarding color information, as is the case). Even vanilla MFEC produces peak performance much above its average, suggesting that it struggles to return to past high rewarding states after discovering them (this problem is approached by <ref type="bibr" target="#b23">[24]</ref>, which could signal promising venues of improvement for episodic control as well). River Raid shows its peculiar behavior in all metrics: there is no significant loss of average or even peak performance for all settings, but the size reduction is not dramatic in any case. We hypothesize that this is due to the continuous vertical scrolling in the game, which results in every state being completely different in pixel space (note that this game produces the largest buffers among the experiments). In any case, it is clear that the hyper-parameters are problem-dependent and should be tuned for each task. A possible solution, not explored in this work, would be to automatically find good values for ε in and ε out in a data-driven manner, by observing the distribution of states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In previous sections, a simple heuristic for reducing the number of stored states in MFEC was presented. While MFEC stores all unique states encountered during learning, we proposed to apply thresholds for storing states according to a maximal dissimilarity criterion. These dissimilarity thresholds should apply to both input (state) and output (Q-value) spaces to avoid aliasing (multiple optimal actions for single stored states). States too similar, with close enough Q-values are aggregated, reducing memory and computational requirements. Besides requiring less memory, it allows for faster k-nearest neighbor lookups, as the number of candidates is reduced.</p><p>Experiments were performed on six Atari games, showing that conservative choices of thresholds (aggregating only very similar states) successfully reduce computational demands, while resulting in no significant loss of performance. It could be observed that different threshold values have different effects on each game. This is related to how each state space has its own distribution characteristics. A method for automatic tuning could make the heuristic more robust and general. We suggest that maintaining variance estimates for each aggregated state could be a simple way to define a  per-state similarity threshold that would adapt to each task. Another possibility would be to avoid state aggregations that result in policy changes (a change in argmax a Q).</p><p>Some interesting directions for future research can be conjectured from this study. A possible simple modification to the algorithm would be to introduce distance-weighted approximations and remove the hard k limit of neighbors (our preliminary experiments in this direction were not promising), or even combine both criteria. This, in turn, would allow the contributions of this work to be applied to NEC as well. Introducing visit counters into each state should be trivial for episodic learning in general and would allow for efficient count-based exploration <ref type="bibr" target="#b24">[25]</ref>. Also regarding efficient exploration, we observed the k hyper-parameter to have a high impact on the quality of exploration, being responsible for the low reliance on the ε exploration rate hyper-parameter (which can be set to very low values compared to other  reinforcement learning methods). This suggests that the role of k could be beyond a mere approximation of Q-values. In fact, newly explored regions contain a low number of states, forcing kNN to include distant states into its estimate, producing a form of implicit exploration in novel states and exploitation in well-explored regions. This could explain why distance weighting produced no satisfactory results in some of our tentative experiments, but more in-depth studies are required since other works suggest distance weighting to be beneficial <ref type="bibr" target="#b20">[21]</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Vanilla kNN. (b) kNN with state aggregation. (c) k correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>kNN behaves differently with state aggregation for the same k. a) kNN (k = 5) without state aggregation. Red dots represent the k selected points closest to the query point (in black). b) kNN (k = 5) with state aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Average score on each game, considering 100 thousand frames per epoch and a total of 10 million frames. Ms. Pacman, SpaceInvaders and River Raid have no significant impact on performance with different hyper-parameter settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Peak score on each game, considering 100 thousand frames per epoch and a total of 10 million frames. Only River Raid is insensitive to the different hyper-parameter settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Total buffer size (i.e., the sum of all action buffers) on each game, considering 100 thousand frames per epoch and a total of 10 million frames. All settings have an impact on buffer sizes. Dramatic reductions in buffer size generally imply loss of performance, as can be observed in Q*bert, Frostbite and Hero, according to figures 2 and 3.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Under user https://github.com/rafaelcp/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reinforcement learning, fast and slow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeb</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="408" to="422" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hippocampal contributions to control: the third way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Máté</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avraham</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04460</idno>
		<title level="m">Daan Wierstra, and Demis Hassabis. Model-free episodic control</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement learning and episodic memory in humans and animals: an integrative framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">D</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="101" to="128" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Puigdomenech</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01988</idno>
		<title level="m">Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joram</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random projection in dimensionality reduction: applications to image and text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heikki</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement learning with soft state aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A growing neural gas network learns topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Fritzke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">State aggregation by growing neural gas for reinforcement learning in continuous state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hans Kleine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 10th International Conference on Machine Learning and Applications and Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="430" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved state aggregation with growing neural gas in multidimensional state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Klerx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans Kleine</forename><surname>Büning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ERLARS</title>
		<meeting>of ERLARS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement learning with perceptual aliasing: The perceptual distinctions approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lonnie</forename><surname>Chrisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">1992</biblScope>
			<biblScope unit="page" from="183" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Memoryefficient episodic control reinforcement learning with dynamic online k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Sarrico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil Anthony</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09560</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning that matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12919</idno>
		<title level="m">First return then explore</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
