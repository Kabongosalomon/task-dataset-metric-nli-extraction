<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GENDIS: GENetic DIscovery of Shapelets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-07">7 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Vandewiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<postCode>9052</postCode>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Femke</forename><surname>Ongenae</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<postCode>9052</postCode>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>De Turck</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<postCode>9052</postCode>
									<settlement>Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GENDIS: GENetic DIscovery of Shapelets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-07">7 Jan 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>genetic algorithms</term>
					<term>time series classification</term>
					<term>time series analysis</term>
					<term>interpretable machine learning</term>
					<term>data mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the time series classification domain, shapelets are small time series that are discriminative for a certain class. It has been shown that classifiers are able to achieve state-of-the-art results on a plethora of datasets by taking as input distances from the input time series to different discriminative shapelets. Additionally, these shapelets can easily be visualized and thus possess an interpretable characteristic, making them very appealing in critical domains, such as the health care domain, where longitudinal data is ubiquitous. In this study, a new paradigm for shapelet discovery is proposed, which is based upon evolutionary computation. The advantages of the proposed approach are that (i) it is gradient-free, which could allow to escape from local optima more easily and to find suited candidates more easily and supports non-differentiable objectives, (ii) no brute-force search is required, which drastically reduces the computational complexity by several orders of magnitude, (iii) the total amount of shapelets and length of each of these shapelets are evolved jointly with the shapelets themselves, alleviating the need to specify this beforehand, (iv) entire sets are evaluated at once as opposed to single shapelets, which results in smaller final sets with less similar shapelets that result in similar predictive performances, and (v) discovered shapelets do not need to be a subsequence of the input time series. We present the results of experiments which validate the enumerated advantages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>Due to the uprise of the Internet-of-Things (IoT), mass adoption of sensors in all domains, including critical domains such as health care, can be noticed. These sensors produce data of a longitudinal form, i.e. time series. Time series differ from classical tabular data, since a temporal dependency is present where each value in the time series correlates with its neighboring values. One important task that emerges from this type of data is the classification of time series in their entirety. A model able to solve such a task can be applied for a wide variety of applications, such as distinguishing between normal brain activity and epileptic activity <ref type="bibr" target="#b5">(Chaovalitwongse et al., 2006)</ref>, determining different types of physical activity <ref type="bibr" target="#b17">(Liu et al., 2015)</ref>, or profiling electronic appliance usage in smart homes <ref type="bibr" target="#b14">(Li et al., 2016)</ref>. Often, the largest discriminative power can be found in smaller subsequences of these time series, called shapelets. Shapelets semantically represent intelligence on how to discriminate between the different targets of a time series dataset <ref type="bibr" target="#b7">(Grabocka et al., 2014)</ref>. We can use a set of shapelets and the corresponding distances from each of these shapelets to each of the input time series as features for a classifier. It has been shown that such an approach outperforms a nearest neighbor search based on dynamic time warping distance on almost every dataset, which was deemed to be the state-of-the-art for a long time <ref type="bibr" target="#b0">(Abanda et al., 2019)</ref>. Moreover, shapelets possess an interpretable characteristic since they can easily be visualized and be retraced back to the input signal, making them very interesting for decision support applications in critical domains, such as the medical domain. In these critical domains, it is of vital importance that a corresponding explanation can be provided alongside a prediction, since a wrong decision can have a significant negative impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>Shapelet discovery was initially proposed by <ref type="bibr" target="#b23">Ye and Keogh (2009)</ref>. Unfortunately, the initial algorithm quickly becomes intractable, even for smaller datasets, because of its large computational complexity (O(N 2 M 4 ), with N the number of time series and M the length of the smallest time series in the dataset). This complexity was improved two years later, when <ref type="bibr" target="#b19">Mueen et al. (2011)</ref> proposed an extension to this algorithm, that makes use of caching for faster distance computation and a better upper bound for candidate pruning. These improvements reduce the complexity to O(N 2 M 3 ), but have a larger memory footprint. <ref type="bibr" target="#b20">Rakthanmanon and Keogh (2013)</ref> proposed an approximative algorithm, called Fast Shapelets (F S), that finds a suboptimal shapelet in O(N M 2 ) by first transforming the time series in the original set to Symbolic Aggregate approXimation (SAX) representations <ref type="bibr" target="#b15">(Lin et al., 2003)</ref>. Although no guarantee can be made that the discovered shapelet is the one that maximizes a pre-defined metric, they show they are able to achieve very similar classification performances, empirically on 32 datasets.</p><p>All the aforementioned techniques search for a single shapelet that optimizes a certain metric, such as information gain. Often, one shapelet is not enough to achieve good predictive performances, especially for multi-class classification problems. Therefore, the shapelet discovery is applied in a recursive fashion in order to construct a decision tree. <ref type="bibr" target="#b16">Lines et al. (2012)</ref> proposed Shapelet Transform (S T), which performs only a single pass through the time series dataset and maintains an ordered list of shapelet candidates, ranked by a metric, and then finally takes the top-k from this list in order to construct features. While the algorithm only performs a single pass, the computational complexity still remains to be O(N 2 M 4 ), which makes the technique intractable for larger datasets. Extensions to this technique have been proposed in the subsequent years which drastically improved the performance of the technique <ref type="bibr" target="#b9">(Hills et al., 2014;</ref><ref type="bibr" target="#b4">Bostrom and Bagnall, 2017)</ref>. <ref type="bibr" target="#b16">Lines et al. (2012)</ref> compared their technique to 36 other algorithms for time series classification on 85 datasets , which showed that their technique is one of the top-performing algorithms for time series classification and the best-performing shapelet extraction technique in terms of predictive performance. <ref type="bibr" target="#b7">Grabocka et al. (2014)</ref> proposed a technique where shapelets are learned through gradient descent, in which the linear separability of the classes after transformation to the distance space is optimized, called Learning Time Series Shapelets (L T S). The technique is competitive to S T, while not requiring a brute-force search, making it tractable for larger datasets. Unfortunately, L T S requires the user to specify the number of shapelets and the length of each of these shapelets, which can result in a rather time-intensive hyper-parameter tuning process in order to achieve a good predictive performance. Three extensions of L T S which improve the computational runtime of the algorithm, have been proposed in the subsequent years. Unfortunately, in order to achieve these speedups, predictive performance had to be sacrificed. A first extension is called Scalable Discovery (S D) . It is the fastest of the three extensions, improving the runtime two to three orders of magnitude, but at a cost of having a worse predictive performance than L T S on almost every tested dataset. Second, in 2015, Ultra-Fast Shapelets (U F S)  was proposed. It is a better compromise of runtime and predictive performance, as it is an order of magnitude slower than S D, but sacrifices less of its predictive performance. A final and most recent extension is called Fused LAsso Generalized eigenvector method (F L A G) <ref type="bibr" target="#b11">(Hou et al., 2016)</ref>. It is the most notable of the three extensions as it has runtimes competitive to S D while being only slightly worse than L T S in terms of predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Our Contribution</head><p>In this paper, we introduce an evolutionary algorithm, G E N D I S, that discovers a set of shapelets from a collection of labeled time series. The aim of the proposed algorithm is to achieve state-of-the-art predictive performances similar to the best-performing algorithm, S T, with a smaller number of shapelets, while having a low computational complexity similar to L T S.</p><p>The goal of G E N D I S is to retain some of the positive properties from L T S such as its scalable computational complexity, the fact that entire sets of shapelets are discovered as opposed to single shapelets and that it can discover shapelets outside the original dataset. We demonstrate the added value of these two final properties through intuitive experiments in Subsections 3.2 and 3.3 respectively. Moreover, G E N D I S has some benefits over L T S. First, genetic algorithms are gradient-free, allowing for any objective function and an easier escape from local optima. Second, the total amount of shapelets and the length of each of these shapelets do not need to be defined prior to the discovery, alleviating the need to tune this, which could be computationally expensive and may require domain knowledge. Finally, we show by a thorough comparison, in Subsection 3.5, that G E N D I S empirically outperforms L T S in terms of predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In the following section we will first explain some general concepts from the time series analysis and shapelet discovery domain, on which we will then build further to elaborate our proposed algorithm, G E N D I S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Time series Matrix &amp; Label Vector</head><p>The input to a shapelet discovery algorithm is a collection of N time series. For ease of notation, we will assume that the time series are synchronized and have a fixed length of M , resulting in an input matrix T ∈ R N ×M . It is important to note that G E N D I S could perfectly work with variable length time series as well. In that case, M would be equal to the minimal time series length in the collection. Since shapelet discovery is a supervised approach, we also require a label vector y of length N , with each element y i ∈ {1, . . . , C} and C the number of classes and y i corresponding to the label of the i-th time series in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shapelets and Shapelet Sets</head><p>Shapelets are small time series which semantically represent intelligence on how to discriminate between the different targets of a time series dataset. In other words, they are very similar to subsequences from time series of certain (groups of) classes, while being dissimilar to subsequences of time series of other classes. The output of a shapelet discovery algorithm is a collection of K shapelets, S = {s 1 , . . . , s K }, called a shapelet set. In G E N D I S, K and the length of each shapelet does not need to be set beforehand and each shapelet can have a variable length, smaller than M . These K shapelets can then be used to extract features for the time series, as we will explain subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Distance Matrix Calculation</head><p>Given an input matrix T and a shapelet set S, we can construct a pairwise distance matrix D ∈ R N ×K :</p><formula xml:id="formula_0">dist(S, T ) = D</formula><p>The distance matrix, D, is constructed by calculating the distance between each (t, s)-pair, where t ∈ T is an input time series and s ∈ S a shapelet from the candidate shapelet set. This matrix can then be fed to a machine learning classifier. Often, K &lt;&lt; M such that we effectively reduce the dimension of our data. In order to calculate the distance from a shapelet s in S to a time series t from T , we convolute the shapelet across the time series and take the minimum distance:</p><formula xml:id="formula_1">dist(s, t) = min 1 ≤ i ≤ |t|−|s| d(s, t[i : i + |s| − 1])</formula><p>with d(.) a distance metric, such as the Euclidean distance, and t[i : i + |s| − 1] a slice from t starting at index i and having the same length as s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Shapelet Set Discovery Objective</head><p>Conceptually, the goal of a shapelet set extraction technique is to find a set of shapelets, S, that produces a distance matrix, D, that minimizes the loss function, L, of the machine learning technique to which it is fed, h(.), given the ground truth, y.</p><formula xml:id="formula_2">min S L(h(dist(T , S)), y)</formula><p>It should be noted that the shapelet extraction and classification phases are completely decoupled, as depicted in <ref type="figure">Figure 1</ref>. A set of shapelets (S) is first independently mined, which is then used to transform the time series into features that correspond to distances from each of the time series to the shapelets in the set. These features are then fed to a machine learning classifier.  <ref type="figure">Fig. 1</ref>: A schematic overview of shapelet discovery. First, shapelets are independently extracted using the training set. These shapelets are then used to transform the train and test set in features. A classifier can afterwards be fit on the training features and evaluated on the test features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">G E N D I S: GENetic Discovery of Interpretable Shapelets</head><p>In this paper, we propose a genetic algorithm that evolves a set of variablelength shapelets, S, in O(N M 2 ) which produces a distance matrix D, based on a collection of time series T , that results in optimal predictive performance when provided to a machine learning classifier. The intuition behind the approach is similar to L T S, which we mentioned in Section 1.2, but the advantage is that both the size of S, K, and the length of each shapelet s ∈ S are evolved jointly, alleviating the need to specify the number of shapelets and the length of each shapelet prior to the extraction. Moreover, the technique is gradient-free, which allows for non-differentiable objectives and to escape local optima more easily.</p><p>The building blocks of a genetic algorithm consist of at least a crossover, mutation and selection operator <ref type="bibr" target="#b18">(Mitchell, 1998)</ref>. Additionally, we seed, or initialize, the algorithm with specific candidates instead of completely random candidates <ref type="bibr" target="#b12">(Julstrom, 1994)</ref> and apply elitism <ref type="bibr" target="#b21">(Sheblé, 1995)</ref> to make sure the fittest candidate set is never discarded from the population or never experiences mutations that detriment its fitness. Each of these operations are elaborated upon in the following subsections.</p><p>Initialization In order to seed the algorithm with initial candidate sets, we generate P candidate sets S ′ containing K shapelets, with K a random integer picked uniformly from [2, W ], W a hyper-parameter of the algorithm, and P the population size. K is randomly chosen for each individual and the default value of W is set to be √ M . These two boundaries are chosen to be low in order to start with smaller candidate sets and grow them incrementally. This is beneficial for both the size of the final shapelet set as well as the runtime of each generation. For each candidate set we initialize, we randomly pick one of following two strategies with equal probability: Initialization 1: apply K-means on a set of random subseries of a fixed random length sampled from T . The K resulting centroids form a candidate set. Initialization 2: generate K candidates of random lengths (∈ {4, . . . , max len}) by sampling them from T .</p><p>The max len is a hyper-parameter that limits the length of the discovered shapelets, in order to combat overfitting. While Initialization 1 results in strong initial individuals, Initialization 2 is included in order to increase population diversity and to decrease the time required to initialize the entire population.</p><p>Fitness One of the most important components of a genetic algorithm, is its fitness function. In order to determine the fitness of a candidate set S ′ we first construct D ′ , which is the distance matrix obtained by calculating the distances between S ′ and T . The goal of our genetic algorithm is find an S ′ which produces a D ′ that results in the most optimal predictive performance when provided to a classifier. We measure the predictive performance directly by means of an error function defined on the predictions of a logistic regression model and the provided label vector y. When two candidate shapelet sets produce the same error, the set with the lowest complexity is deemed to be the fittest. The complexity of a shapelet set is expressed as the sum of shapelet lengths ( s∈S |s|).</p><p>The fitness calculation is the bottleneck of the algorithm. Calculating the distance of a shapelet with length L to a time series of length M requires (M − L + 1) × L pointwise comparisons. Thus, in the worst case, O(M 2 ) operations need to be performed per time series, resulting in a computational complexity of O(N M 2 ). We apply these distance calculations to each individual representing a collection of shapelets from our population, in each generation. Therefore, the complexity of the entire algorithm is equal to O(GP KN M 2 ), with G the total number of generations, P the population size, and K the (maximum) number of shapelets in the bag each individual of the population represents.</p><p>Crossover We define three different crossover operations, which take two candidate shapelet sets, S ′ and S ′′ , as input and produce two new sets, S * and S * * :</p><p>Crossover 1: apply one-or two-point crossover on two shapelet sets (each with a probability of 50%). In other words, we create two new shapelet sets that are composed of shapelets from both S ′ and S ′′ . An example of this operation is provided in <ref type="figure">Figure 2</ref>. Crossover 2: iterate over each shapelet s in S ′ and apply one-or two-point crossover (again with a probability of 50%) with another randomly chosen shapelet from S ′′ to create S * . Apply the same, vice versa, to obtain S * * . This differs from the first crossover operation as the one-or two-point crossover are performed on individual shapelets as opposed to entire sets. An example of this operation can be seen in <ref type="figure">Figure 3</ref>. Crossover 3: iterate over each shapelet s in S ′ and merge it with another randomly chosen shapelet from S ′′ . The merging of two shapelets can be done by calculating the mean (or barycenter) of the two time series. When two shapelets being merged have varying length, we merge the shorter shapelet with a random part of the longer shapelet. A schematic overview of this strategy, on shapelets having the same length, is depicted in <ref type="figure">Figure 4</ref>.</p><p>It is possible that all or no techniques are applied on a pair of individuals. Each technique has a probability equal to the configured crossover probability (p crossover ) of being applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutations</head><p>The mutation operators are a vital part of the genetic algorithm, as they ensure population diversity and allow to escape from local optima in the search space. They take a candidate set S ′ as input and produce a new, modified S * . In our approach, we define three simple mutation operators: <ref type="figure">Fig. 2</ref>: An example of a one-point crossover operation on two shapelet sets. Each original set is partitioned in two, and we take a partition from each set in order to construct a new set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S'</head><p>S* Mutation 1: take a random s ∈ S ′ and randomly remove a variable amount of data points from the beginning or ending of the time series. Mutation 2: remove a random s ∈ S ′ .</p><p>Mutation 3: create a new candidate using Initialization 2 and add it to S ′ .</p><p>Again, all techniques can be applied on a single individual, each having a probability equal to the configured mutation probability (p mutation ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection, Elitism &amp; Early Stopping</head><p>After each generation, a fixed number of candidate sets are chosen based on their fitness for the next generation. Many different techniques exist to select these candidate sets. We chose to apply tournament selection with small tournament sizes. In this strategy, a number of candidate sets are sampled uniformly from the entire population to form a tournament. Afterwards, one candidate set is sampled from the tournament, where the probability of being sampled is determined by its fitness. Smaller tournament sizes ensure better population diversity as the probability of the fittest individual being included in the tournament decreases. Using this strategy, it is however possible that the fittest candidate set from the population is never chosen to compete in a tournament. Therefore, we apply elitism and guarantee that the fittest candidate set is always transferred to the next generation's population. Finally, since it can be hard to determine the ideal number of generations that a genetic algorithm should run, we implemented early stopping where the algorithm preemptively stops as soon as no candidate set with a better fitness has been found for a certain number of iterations (patience). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of all hyper-parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In the following subsections, we will present the setup of different experiments and the corresponding results in order to highlight the advantages of G E N D I S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Efficiency of genetic operators</head><p>In this section, we assess the efficiency of the introduced genetic operators by evaluating the fitness in function of the number of generations using different sets of operators. It should be noted that our implementation easily allows to configure the number and type of operators used for each of the different steps in the genetic algorithm, allowing the user to tune these according to the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We pick six datasets, with varying characteristics, to evaluate the fitness of different configurations on. The chosen datasets, and their corresponding properties are summarized in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization operators</head><p>We first compare the fitness of GENDIS using three different sets of initialization operators:</p><p>-Initializing the individuals with K-Means (Initialization 1) -Randomly initializing the shapelet sets (Initialization 2)  Each configuration was tested using a small population (25 individuals), in order to reduce the required computational time, for 75 generations, as the impact of the initialization is highest in the earlier generations. All mutation and crossover operators were used. We show the average fitness of all individuals in the population in <ref type="figure" target="#fig_2">Figure 5</ref>. From these results, we can conclude that the two initialization operators are competitive to each other, as one operator will outperform the other on several datasets and vice versa on the others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crossover operators</head><p>We now compare the average fitness of all individuals in the population, in function of the number of generations, when configuring GENDIS to use four different sets of crossover operators:</p><p>-Using solely point crossovers on the shapelet sets (Crossover 1) -Using solely point crossovers on individual shapelets (Crossover 2) -Using solely merge crossovers (Crossover 3) -Using all three crossover operations Each run had a population of 25 individuals and ran for 200 generations. All mutation and initialization operators were used. As the average fitness is rather similar in the earlier generations, we truncate the first 50 measurements to better highlight the differences. The results are presented in <ref type="figure" target="#fig_3">Figure 6</ref>. As can be seen, it is again difficult to single out an operation that significantly outperforms the others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutation operators</head><p>The same experiment was performed to assess the efficiency of the mutation operators. Four different configurations were used:</p><p>-Masking a random part of a shapelet (Mutation 1) -Removing a random shapelet from the set (Mutation 2) -Adding a shapelet, randomly sampled from the data, to the set (Mutation 3) -Using all three mutation operations</p><p>The average fitness of the individuals, in function of the number of generations is depicted in <ref type="figure">Figure 7</ref>. It is clear that the addition of shapelets (Mutation 3) is the most significant operator. Without it, the fitness quickly converges to a suboptimal value. The removal and masking of shapelets does not seem to increase the average fitness often, but are important operators in order to keep the the number of shapelets and the length of the shapelets small. <ref type="figure">Fig. 7</ref>: The fitness in function of the number of generations, for six datasets, using four different configurations of mutation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluating sets of candidates versus single candidates</head><p>A key factor of G E N D I S, is that it evaluates entire sets of shapelets (a dependency between the shapelets is introduced), as opposed to evaluating single candidates independently and taking a top-k. The disadvantage of the latter approach is that similar shapelets will achieve similar values given a certain metric. When entire sets are evaluated, we can both optimize a quality metric for candidate sets, as the size of each of these sets. This results in smaller sets with less similar shapelets. Moreover, interactions between shapelets can be explicitly taken into account. To demonstrate these advantages, we compare G E N D I S to S T, which evaluates candidate shapelets individually, on an artificial three-class dataset, depicted in <ref type="figure">Figure 8</ref>. The constructed dataset contains a large number of very similar time series of class 0, while having a smaller number of more dissimilar time series of class 1 and 2. The distribution of time series across the three classes in both the train and test dataset is thus skewed, with the number of samples in class 0, 1, 2 being equal to 25, 5, 5 respectively. This imbalance causes the independent approach to focus solely on extracting shapelets that can discriminate class 0 from the two others, since the information gain will be highest for these individual shapelets. Clearly, this is not ideal as subsequences taken from time series of class 0 possess little to no discriminative power for the two other classes, as the distances to time series from these two classes will be nearly equal.</p><p>We extract two shapelets with both techniques, which allows us to visualize the different test samples in a two-dimensional transformed distance space, as shown in <ref type="figure">Figure 9</ref>. Each axis of this space represents the distances to a certain shapelet. For the independent approach, we can clearly see that the distances of the samples for all three classes to both shapelets are clustered near the origin of the space, making it very hard for a classifier to draw a separation boundary. On the other hand, a very clear separation can be seen for the samples of the three classes when using the shapelets discovered by G E N D I S, a dependent approach. The low discriminative power of the indepent approach is confirmed by fitting a Logistic Regression model with tuned regularization type and strength on the obtained distances. The classifier fitted on the distances extracted by the independent approach is only able to achieve an accuracy of 0.8286 ( 29 35 ) on the rather straight-forward dataset. The accuracy score of G E N D I S, a dependent approach, equals 1.0. <ref type="figure">Fig. 8</ref>: The generated train and test set for the artificial classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discovering shapelets outside the data</head><p>Another advantage of G E N D I S, is that the discovery of shapelets is not limited to be a subseries from T . Due to the nature of the evolutionary process, the discovered shapelets can have a distance greater than 0 to all time series in the dataset. More formally: ∃s ∈ S. ∀t ∈ T . dist(s, t) &gt; 0. While this can be somewhat detrimental concerning interpretability, it can be necessary to get an excellent predictive performance. We demonstrate this through a very simple, artificial example. Assume we have a two-class classification problem and are provided two time series per class, as illustrated in <ref type="figure">Figure 10a</ref>. The extracted shapelet, and the distances to each time series, by a brute force approach and a slightly modified version of G E N D I S can be found in <ref type="figure">Figure 10b</ref>. The modification we made to G E N D I S is that we specifically search for only one shapelet instead of an entire set of shapelets. We can see that the exhaustive search approach is not able to find a subseries in any of these four time series that separates both classes while the shapelet extracted by G E N D I S ensures perfect separation.</p><p>It is important to note here that discovering shapelets outside the data sacrifices interpretability for an increase in predictive performance of the shapelets. As <ref type="figure">Fig. 9</ref>: The samples of the test set are represented by markers (circles for G E N D I S) and crosses for S T) while the axes correspond to the distance to a shapelet.</p><p>the operators that are used during the genetic algorithm are completely configurable for G E N D I S, one can use only the first crossover operation (one-or two-point crossover on shapelet sets) to ensure all shapelets come from within the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Stability</head><p>In order to evaluate the stability of our algorithm, we compare the extracted shapelets of two different runs on the ItalyPowerDemand dataset. We set the algorithm to evolve a large population (100 individuals) for a large number of generations (500) in order to ensure convergence. Moreover, we limit the maximum number of extracted shapelets to 10, in order to keep the visualization clear. We then calculated the similarity of the discovered shapelets between the two runs, using Dynamic Time Warping <ref type="bibr" target="#b3">(Berndt and Clifford, 1994)</ref>. A heatmap of the distances is depicted in <ref type="figure" target="#fig_5">Figure 11</ref>. While the discovered shapelets are not exactly the same, we can often find pairs that contain the same semantic intelligence, such as a saw pattern or straight lines.  <ref type="figure">Fig. 10</ref>: A two-class problem with two time series per class and the extracted shapelets with corresponding distances on an ordered line by a brute-force approach versus G E N D I S. The crosses on the ordered line correspond to distances of the shapelet to the time series from Class 1 while the circles on the ordered line correspond to distances to Class 0 (the more to the right, the higher the distance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparing G E N D I S to F S, S T and L T S</head><p>In this section, we compare our algorithm G E N D I S to the results from , which are hosted online 1 . In that study, 31 different algorithms, including three shapelet discovery techniques, have been compared on 85 datasets. The 85 datasets stem from different data sources and different domains, including electrocardiogram data from the medical domain and sensor data from the IoT domain. The three included shapelet techniques are Shapelet Transform (S T) <ref type="bibr" target="#b16">(Lines et al., 2012)</ref>, Learning Time Series Shapelets (L T S) <ref type="bibr" target="#b7">(Grabocka et al., 2014)</ref>, and Fast Shapelets (F S) <ref type="bibr" target="#b20">(Rakthanmanon and Keogh, 2013)</ref>. A discussion of all three techniques can be found in Section 1.2.</p><p>For 84 of the 85 datasets, we conducted twelve measurements by concatenating the provided training and testing data and re-partitioning in a stratified manner, as done in the original study. Only the 'Phoneme' dataset could not be included due to problems downloading the data while executing this experiment. On every dataset, we used the same hyper-parameter configuration for G E N D I S: a population size of 100, a maximum of 100 iterations, early stopping after 10 iterations and crossover &amp; mutation probabilities of 0.4 and 0.1 respectively. The only parameter that was tuned for every dataset separately was a maximum length for each shapelet, to combat overfitting. To tune this, we picked the length l ∈ [ M 4 , M 2 , 3M 4 , M ] that resulted in the best logarithmic (or entropy) loss using 3-fold cross validation on the training set. The distance matrix obtained through the extracted shapelets of G E N D I S was then fed to a heterogeneous ensemble consisting of a rotation forest, random forest, support vector machine with linear kernel, support vector with quadratic kernel and a k-nearest neighbor classifier ). This ensemble matches the one used by the best-performing algorithm, S T, closely. This is in contrast with F S which produces a decision tree and L T S which learns a separating hyperplane (similar to logistic regression) jointly with the shapelets. This setup is also depicted schematically in <ref type="figure" target="#fig_6">Figure 12</ref>. Trivially, the ensemble will empirically outperform each of the individual classifiers (?), but it does take a longer time to fit and somewhat takes the focus away from the quality of the extracted shapelets. Nevertheless, it is necessary to use an ensemble in order to allow for a fair comparison with S T, as that was used by <ref type="bibr" target="#b1">Bagnall et al. (2017)</ref> to generate their results. To give more insights into the quality of the extracted shapelets, we also report the accuracies using a Logistic Regression classifier. We tuned the type of regularization (Ridge vs Lasso) and the regularization strength (C ∈ {0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0}) using the training set. We recommend future research to compare their results to those obtained with  Train and test data are first concatenated and then re-distributed to form new train and test sets with similar distributions to the ones prior to the concatenation. The newly created train set is then used to tune the optimal maximal length of the shapelets in cross-validation and used to extract shapelets and to train the ensemble of classifiers. Finally, the test set is used to evaluate the extracted shapelets and fitted ensemble.</p><p>The mean accuracy over the twelve measurements of G E N D I S in comparison to the mean of the hundred original measurements of the three other algorithms, retrieved from the online repository, can be found in <ref type="table">Table 2</ref> and 3. While a smaller number of measurements is conducted within this study, it should be noted that the measurements from <ref type="bibr" target="#b1">Bagnall et al. (2017)</ref> took over 6 months to generate. Moreover, accuracy is often not the most ideal metric to measure the predictive performance with. Although it is one of the most intuitive metrics, it has several disadvantages such as skewness when data is imbalanced. Nevertheless, the accuracy metric is the only one allowing for comparison to related work, as that metric was used in those studies. Moreover, the used datasets are merely benchmark datasets and the goal is solely to compare the quality of the shapelets extracted by G E N D I S to those of S T. We recommend to use different performance metrics, which should be tailored to the specific use case. An example is using the area under the receiver operating characteristic curve (AUC) in combination with precision and recall for medical datasets.</p><p>For each dataset, we also perform an unpaired student t-test with a cutoff value of 0.05 to detect statistically significant differences. When the performance of an algorithm for a certain dataset is statistically better than all others, it is indicated in bold. From these results, we can conclude that F S is inferior to the three other techniques while S T most often achieves the best performance, but at a very high computational complexity.</p><p>The average number of shapelets extracted by G E N D I S is reported in the final column. The number of shapelets extracted by S T in the original study equals 10 * N . Thus, the total number of shapelets used to transform the original time series to distances is at least an order of magnitude less when using G E N D I S.</p><p>In order to compare the algorithms across all datasets, a Friedman ranking test <ref type="bibr" target="#b6">(Friedman (1937)</ref>), was applied with a Holm post-hoc correction <ref type="bibr" target="#b10">(Holm (1979)</ref>; <ref type="bibr" target="#b2">Benavoli et al. (2016)</ref>). We present the average rank of each algorithm using a critical difference diagram, with cliques formed using the results of the Friedman test with a Holm post-hoc correction at a significance cutoff level of 0.1 in <ref type="figure" target="#fig_7">Figure 13</ref>. The higher the cutoff level, the less probable it is to form cliques. For G E N D I S, both the results obtained with the ensemble and with the logistic regression classifier are used. From this, we can conclude that there is no statistical difference between S T and G E N D I S while both are statistically better than F S and L T S. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, an innovative technique, called G E N D I S, was proposed to extract a collection of smaller subsequences, i.e. shapelets, from a time series dataset that are very informative in classifying each of the time series into categories. G E N D I S searches for this set of shapelets through evolutionary computation, a paradigm mostly unexplored within the domain of time series classification, which offers several benefits:</p><p>evolutionary algorithms are gradient-free, allowing for an easy configuration of the optimization objective, which does not need to be differentiable -only the maximum length of all shapelets has to be tuned, as opposed to the number of shapelets and a length of each shapelet, due to the fact that G E N D I S evaluates entire sets of shapelets easy control over the runtime of the algorithm the possibility of discovering shapelets that not need to be a subsequence of the input time series</p><p>Moreover, the proposed technique has a computational complexity that is multiple orders of magnitude smaller (O(GP KN M 2 ) vs O(N 2 M 4 )) than current state-of-the-art, S T, while outperforming it in terms of predictive performance, with much smaller shapelet sets.</p><p>We demonstrate these benefits through intuitive experiments where it was shown that techniques that evaluate single candidates can perform subpar on imbalanced datasets and how sometimes the necessity arises to extract shapelets that are no subsequences of input time series to achieve good separation. In addition, we compare the efficiency of the different genetic operators on six different datasets and assess the algorithm's stability by comparing the output of two different runs on the same dataset. Moreover, we conducted an extensive comparison on a large amount of datasets to show that G E N D I S is competitive to the current state-of-the-art while having a much lower computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Reproducibility &amp; code availability</head><p>An implementation of G E N D I S in Python 3 is available on GitHub 2 . Moreover, code in order to perform the experiments to reproduce the results is included.  <ref type="table">Table 2</ref>: A comparison between G E N D I S and three other shapelet techniques on 85 datasets. For each dataset, we report the total number of classes, the length of the time series, the number of time series in the train and test set, the accuracy score on the test set achieved by G E N D I S (using both an ensemble (Ens) and logistic regression (LR)), S T, L T S &amp; F S, and finally the average number of shapelets extracted by G E N D I S. When a technique is statistically significant better than the others, according to a student t-test with a cutoff of 0.05, it is marked as bold.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>An example of one-and two-point crossover applied on individual shapelets. An example of the shapelet merging crossover operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>-</head><label></label><figDesc>We now present an overview of all hyper-parameters included in G E N D I S, along with their corresponding explanation and default values. Maximum shapelets per candidate (W ): the maximum number of shapelets in a newly generated individual during initialization (default: √ M ). -Population size (P ): the total number of candidates that are evaluated and evolved in every iteration (default: 100). -Maximum number of generations (G): the maximum number of iterations the algorithm runs (default: 100). -Early stopping patience (patience): the algorithm preemptively stops evolving when no better individual has been found for patience iterations (default: 10). -Mutation probability (p mutation ): the probability that a mutation operator gets applied to an individual in each iteration (default: 0.1). -Crossover probability (p crossover ): the probability that a crossover operator is applied on a pair of individuals in each iteration (default: 0.4). -Maximum shapelet length (max len): the maximum length of the shapelets in each shapelet set (individual). (default: M ). -The operations used during the initialization, crossover and mutation phases are configurable as well. (default: all mentioned operations).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>The fitness in function of the number of generations, for six datasets, using three different configurations of initialization operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>The fitness in function of the number of generations, for six datasets, using four different configurations of crossover operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 :</head><label>11</label><figDesc>A pairwise distance matrix, constructed using Dynamic Time Warping, between discovered shapelet sets of two different runs on the ItalyPowerDemand dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 :</head><label>12</label><figDesc>The evaluation setup used to compare G E N D I S to other shapelet techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 :</head><label>13</label><figDesc>A critical difference diagram of the four evaluated shapelet discovery algorithms. For each technique, the average rank is calculated. Then cliques (bold black lines) are formed if the p-value of the Holm post-hoc test is lower than 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The chosen datasets, having varying characteristics, for the evaluation of the genetic operators' efficiency. #Cls = number of classes, TS len = length of time series, #Train = number of training time series, #Train = number of testing time series</figDesc><table /><note>-Using all two initialization operations</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Logistic Regression classifier.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>K-Nearest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Neighbors</cell></row><row><cell>train</cell><cell></cell><cell>train'</cell><cell>Tune length</cell><cell>Linear SVM</cell></row><row><cell>+</cell><cell>data</cell><cell>Stratified split</cell><cell>GENDIS</cell><cell>Quadratic SVM</cell></row><row><cell>test</cell><cell></cell><cell>test'</cell><cell></cell><cell>Rotation Forest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Random Forest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Weighted ensemble</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>6 Acknowledgements</head><label></label><figDesc>Vandewiele is funded by a PhD SB fellow scholarship of Fonds Wetenschappelijk Onderzoek (FWO) (1S31417N). F. Ongenae is funded by a Bijzonder On-derzoeksFonds (BOF) grant from Ghent University.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Cls TS len #Train #Test</cell><cell>GENDIS ST LTS FS #Shaps Ens LR</cell></row><row><cell>Adiac</cell><cell cols="2">37 176</cell><cell>390</cell><cell cols="2">391 66.2 69.8 76.8 42.9 55.5 39</cell></row><row><cell>ArrowHead</cell><cell>3</cell><cell>251</cell><cell>36</cell><cell cols="2">175 79.4 82.0 85.1 84.1 67.5 39</cell></row><row><cell>Beef</cell><cell>5</cell><cell>470</cell><cell>30</cell><cell>30</cell><cell>51.5 58.8 73.6 69.8 50.2 41</cell></row><row><cell>BeetleFly</cell><cell>2</cell><cell>512</cell><cell>20</cell><cell>20</cell><cell>90.6 87.5 87.5 86.2 79.6 42</cell></row><row><cell>BirdChicken</cell><cell>2</cell><cell>512</cell><cell>20</cell><cell>20</cell><cell>90.0 90.5 92.7 86.4 86.2 45</cell></row><row><cell>CBF</cell><cell>3</cell><cell>128</cell><cell>30</cell><cell cols="2">900 99.1 97.6 98.6 97.7 92.4 43</cell></row><row><cell>Car</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>G.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.timeseriesclassification.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/IBCNServices/GENDIS</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 3</ref><p>: A comparison between G E N D I S and three other shapelet techniques on 85 datasets. For each dataset, we report the total number of classes, the length of the time series, the number of time series in the train and test set, the accuracy score on the test set achieved by G E N D I S (using both an ensemble (Ens) and logistic regression (LR)), S T, L T S &amp; F S, and finally the average number of shapelets extracted by G E N D I S. When a technique is statistically significant better than the others, according to a student t-test with a cutoff of 0.05, it is marked as bold.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review on distance based time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="378" to="412" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="606" to="660" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Should we really use post-hoc tests based on mean-ranks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mangili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using dynamic time warping to find patterns in time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Berndt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD workshop</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Binary shapelet transform for multiclass time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Large-Scale Data-and Knowledge-Centered Systems XXXII</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="24" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Electroencephalogram (eeg) time series classification: Applications in epilepsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Chaovalitwongse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Prokopyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Pardalos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="227" to="250" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The use of ranks to avoid the assumption of normality implicit in the analysis of variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="675" to="701" />
			<date type="published" when="0200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning timeseries shapelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03238</idno>
		<title level="m">Scalable discovery of timeseries shapelets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification of time series by shapelet transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baranauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="851" to="881" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple sequentially rejective multiple test procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian journal of statistics</title>
		<imprint>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient learning of timeseries shapelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Seeding the population: improved performance in a genetic algorithm for the rectilinear steiner problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Julstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 ACM symposium on Applied computing</title>
		<meeting>the 1994 ACM symposium on Applied computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="222" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The heterogeneous ensembles of standard classification algorithms (hesca): the whole is greater than the sum of its parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Profiling household appliance electricity usage with n-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Bissyandé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kubler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Traon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Industrial Technology (ICIT), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="604" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A symbolic representation of time series, with implications for streaming algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM SIG-MOD workshop on Research issues in data mining and knowledge discovery</title>
		<meeting>the 8th ACM SIG-MOD workshop on Research issues in data mining and knowledge discovery</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A shapelet transform for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sensor-based human activity recognition system with a multilayered model using time series shapelets. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="138" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An introduction to genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Logical-shapelets: an expressive primitive for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1154" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast shapelets: A scalable algorithm for discovering time series shapelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 2013 SIAM International Conference on Data Mining</title>
		<meeting>the 2013 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="668" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Refined genetic algorithm-economic dispatch example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald B Gand</forename><surname>Sheblé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brittig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="124" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ultra-fast shapelets for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.05018</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Time series shapelets: a new primitive for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
