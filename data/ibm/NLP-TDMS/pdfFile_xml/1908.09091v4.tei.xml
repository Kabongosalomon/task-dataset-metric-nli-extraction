<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERT for Coreference Resolution: Baselines and Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-22">22 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
							<email>mandar90@cs.washington.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
							<email>omerlevy@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
							<email>weld@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute of Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">§</forename><forename type="middle">†</forename><surname>Allen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Seattle</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BERT for Coreference Resolution: Baselines and Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-22">22 Dec 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We apply BERT to coreference resolution, achieving strong improvements on the OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent BERT-based models have reported dramatic gains on multiple semantic benchmarks including question-answering, natural language inference, and named entity recognition <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>. Apart from better bidirectional reasoning, one of BERT's major improvements over previous methods <ref type="bibr" target="#b18">(Peters et al., 2018;</ref><ref type="bibr" target="#b16">McCann et al., 2017)</ref> is passage-level training, 2 which allows it to better model longer sequences.</p><p>We fine-tune BERT to coreference resolution, achieving strong improvements on the GAP <ref type="bibr" target="#b20">(Webster et al., 2018)</ref> and <ref type="bibr">OntoNotes (Pradhan et al., 2012)</ref> benchmarks. We present two ways of extending the c2f-coref model in . The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT. The overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens. BERT-large improves over ELMo-based c2f-coref 3.9% on OntoNotes and 11.5% on GAP (both absolute). 1 https://github.com/mandarjoshi90/coref 2 Each BERT training example consists of around 512 word pieces, while ELMo is trained on single sentences.</p><p>A qualitative analysis of BERT and ELMobased models <ref type="table" target="#tab_2">(Table 3)</ref> suggests that BERT-large (unlike BERT-base) is remarkably better at distinguishing between related yet distinct entities or concepts (e.g., Repulse Bay and Victoria Harbor). However, both models often struggle to resolve coreferences for cases that require world knowledge (e.g., the developing story and the scandal). Likewise, modeling pronouns remains difficult, especially in conversations.</p><p>We also find that BERT-large benefits from using longer context windows (384 word pieces) while BERT-base performs better with shorter contexts (128 word pieces). Yet, both variants perform much worse with longer context windows (512 tokens) in spite of being trained on 512-size contexts. Moreover, the overlap variant, which artificially extends the context window beyond 512 tokens provides no improvement. This indicates that using larger context windows for pretraining might not translate into effective long-range features for a downstream task. Larger models also exacerbate the memory-intensive nature of span representations <ref type="bibr" target="#b11">(Lee et al., 2017)</ref>, which have driven recent improvements in coreference resolution. Together, these observations suggest that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>For our experiments, we use the higher-order coreference model in  which is the current state of the art for the English OntoNotes dataset <ref type="bibr" target="#b19">(Pradhan et al., 2012)</ref>. We refer to this as c2f-coref in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of c2f-coref</head><p>For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y :</p><formula xml:id="formula_0">P (y) = e s(x,y) y ′ ∈Y e s(x,y ′ ) (1)</formula><p>The scoring function s(x, y) between spans x and y uses fixed-length span representations, g x and g y to represent its inputs. These consist of a concatenation of three vectors: the two LSTM states of the span endpoints and an attention vector computed over the span tokens. It computes the score s(x, y) by the mention score of x (i.e. how likely is the span x to be a mention), the mention score of y, and the joint compatibility score of x and y (i.e. assuming they are both mentions, how likely are x and y to refer to the same entity). The components are computed as follows:</p><formula xml:id="formula_1">s(x, y) = s m (x) + s m (y) + s c (x, y) (2) s m (x) = FFNN m (g x ) (3) s c (x, y) = FFNN c (g x , g y , φ(x, y))<label>(4)</label></formula><p>where FFNN(·) represents a feedforward neural network and φ(x, y) represents speaker and metadata features. These span representations are later refined using antecedent distribution from a spanranking architecture as an attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Applying BERT</head><p>We replace the entire LSTM-based encoder (with ELMo and GloVe embeddings as input) in c2fcoref with the BERT transformer. We treat the first and last word-pieces (concatenated with the attended version of all word pieces in the span) as span representations. Documents are split into segments of max segment len, which we treat as a hyperparameter. We experiment with two variants of splitting:</p><p>Independent The independent variant uses nonoverlapping segments each of which acts as an independent instance for BERT. The representation for each token is limited to the set of words that lie in its segment. As BERT is trained on sequences of at most 512 word pieces, this variant has limited encoding capacity especially for tokens that lie at the start or end of their segments.</p><p>Overlap The overlap variant splits the document into overlapping segments by creating a Tsized segment after every T /2 tokens. These segments are then passed on to the BERT encoder independently, and the final token representation is derived by element-wise interpolation of representations from both overlapping segments.</p><p>Let r 1 ∈ R d and r 2 ∈ R d be the token representations from the overlapping BERT segments. The final representation r ∈ R d is given by:</p><formula xml:id="formula_2">f = σ(w T [r 1 ; r 2 ]) (5) r = f · r 1 + (1 − f ) · r 2 (6)</formula><p>where w ∈ R 2d×d is a trained parameter and [; ] represents concatenation. This variant allows the model to artificially increase the context window beyond the max segment len hyperparameter.</p><p>All layers in both model variants are then finetuned following <ref type="bibr" target="#b5">Devlin et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate our BERT-based models on two benchmarks: the paragraph-level GAP dataset <ref type="bibr" target="#b20">(Webster et al., 2018)</ref>, and the documentlevel English OntoNotes 5.0 dataset <ref type="bibr" target="#b19">(Pradhan et al., 2012)</ref>. OntoNotes examples are considerably longer and typically require multiple segments to read the entire document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation and Hyperparameters</head><p>We extend the original Tensorflow implementations of c2f-coref 3 and BERT. <ref type="bibr">4</ref> We fine tune all models on the OntoNotes English data for 20 epochs using a dropout of 0.3, and learning rates of 1 × 10 −5 and 2 × 10 −4 with linear decay for the BERT parameters and the task parameters respectively. We found that this made a sizable impact of 2-3% over using the same learning rate for all parameters.</p><p>We trained separate models with max segment len of 128, 256, 384, and 512; the models trained on 128 and 384 word pieces performed the best for BERT-base and BERT-large respectively. As span representations are memory intensive, we truncate documents randomly to eleven segments for BERT-base and 3 for BERT-large during training. Likewise, we use a batch size of 1 document following . While training the large model requires 32GB GPUs, all models can be tested on 16GB GPUs. We use the cased English variants in all our experiments.</p><p>Baselines We compare the c2f-coref + BERT system with two main baselines: (1) the original ELMo-based c2f-coref system , and <ref type="formula">(2)</ref>   2017), which does not use contextualized representations. In addition to being more computationally efficient than e2e-coref, c2f-coref iteratively refines span representations using attention for higher-order reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Paragraph Level: GAP</head><p>GAP <ref type="bibr" target="#b20">(Webster et al., 2018</ref>) is a human-labeled corpus of ambiguous pronoun-name pairs derived from Wikipedia snippets. Examples in the GAP dataset fit within a single BERT segment, thus eliminating the need for cross-segment inference. Following <ref type="bibr" target="#b20">Webster et al. (2018)</ref>, we trained our BERT-based c2f-coref model on OntoNotes. 5 The predicted clusters were scored against GAP examples according to the official evaluation script. <ref type="table" target="#tab_1">Table 2</ref> shows that BERT improves c2f-coref by 9% and 11.5% for the base and large models respectively. These results are in line with large gains reported for a variety of semantic tasks by BERTbased models <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Level: OntoNotes</head><p>OntoNotes (English) is a document-level dataset from the CoNLL-2012 shared task on coreference resolution. It consists of about one million words of newswire, magazine articles, broadcast news, broadcast conversations, web data and conversational speech data, and the New Testament. The main evaluation is the average F1 of three metrics -MUC, B 3 , and CEAF φ 4 on the test set according to the official CoNLL-2012 evaluation scripts. <ref type="table" target="#tab_0">Table 1</ref> shows that BERT-base offers an improvement of 0.9% over the ELMo-based c2fcoref model. Given how gains on coreference resolution have been hard to come by as evidenced by the table, this is still a considerable improvement. However, the magnitude of gains is relatively modest considering BERT's arguably better architecture and many more trainable parameters. This is in sharp contrast to how even the base variant of BERT has very substantially improved the state of the art in other tasks. BERT-large, however, improves c2f-coref by the much larger margin of 3.9%. We also observe that the overlap variant offers no improvement over independent.</p><p>Concurrent with our work, <ref type="bibr" target="#b10">Kantor and Globerson (2019)</ref>, who use higher-order entity-level representations over "frozen" BERT features, also report large gains over c2f-coref. While their feature-based approach is more memory efficient, the fine-tuned model seems to yield better results. Also concurrent, SpanBERT <ref type="bibr" target="#b9">(Joshi et al., 2019)</ref>, another self-supervised method, pretrains span representations achieving state of the art results (Avg. F1 79.6) with the independent variant. Category Snippet #base #large</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Entities</head><p>Watch spectacular performances by dolphins and sea lions at the Ocean Theater... 12 7 It seems the North Pole and the Marine Life Center will also be renovated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical</head><p>Over the past 28 years , the Ocean Park has basically.. The entire park has been ... 15 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pronouns</head><p>In the meantime , our children need an education. That's all we're asking. 17 13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention Paraphrasing</head><p>And in case you missed it the Royals are here. 14 12 Today Britain's Prince Charles and his wife Camilla...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversation</head><p>(Priscilla:) My mother was Thelma Wahl . She was ninety years old ... 18 16 (Keith:) Priscilla Scott is mourning . Her mother Thelma Wahl was a resident ..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Misc.</head><p>He is my, She is my Goddess , ah 17 17</p><p>Total 93 74   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>We performed a qualitative comparison of ELMo and BERT models <ref type="table" target="#tab_2">(Table 3)</ref> on the OntoNotes English development set by manually assigning error categories (e.g., pronouns, mention paraphrasing) to incorrect predicted clusters. <ref type="bibr">6</ref> Overall, we found 93 errors for BERT-base and 74 for BERT-large from the same 15 documents. <ref type="bibr">6</ref> Each incorrect cluster can belong to multiple categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strengths</head><p>We did not find salient qualitative differences between ELMo and BERT-base models, which is consistent with the quantitative results <ref type="table" target="#tab_0">(Table 1)</ref>. BERT-large improves over BERT-base in a variety of ways including pronoun resolution and lexical matching (e.g., race track and track).</p><p>In particular, the BERT-large variant is better at distinguishing related, but distinct, entities. <ref type="table" target="#tab_2">Table  3</ref> shows several examples where the BERT-base variant merges distinct entities (like Ocean Theater and Marine Life Center) into a single cluster. BERT-large seems to be able to avoid such merging on a more regular basis.</p><p>Weaknesses An analysis of errors on the OntoNotes English development set suggests that better modeling of document-level context, conversations, and entity paraphrasing might further improve the state of the art. Longer documents in OntoNotes generally contain larger and more spread-out clusters. We focus on three observations -(a) <ref type="table" target="#tab_3">Table 4</ref> shows how models perform distinctly worse on longer documents, (b) both models are unable to use larger segments more effectively <ref type="table" target="#tab_4">(Table 5</ref>) and perform worse when the max segment len of 450 and 512 are used, and, (c) using overlapping segments to provide additional context does not improve results <ref type="table" target="#tab_0">(Table 1)</ref>. Recent work <ref type="bibr" target="#b9">(Joshi et al., 2019)</ref> suggests that BERT's inability to use longer sequences effectively is likely a by-product pretraining on short sequences for a vast majority of updates.</p><p>Comparing preferred segment lengths for base and large variants of BERT indicates that larger models might better encode longer contexts. However, larger models also exacerbate the memoryintensive nature of span representations, 7 which have driven recent improvements in coreference resolution. These observations suggest that future research in pretraining methods should look at more effectively encoding document-level context using sparse representations <ref type="bibr" target="#b1">(Child et al., 2019)</ref>.</p><p>Modeling pronouns especially in the context of conversations <ref type="table" target="#tab_2">(Table 3)</ref>, continues to be difficult for all models, perhaps partly because c2f-coref does very little to model dialog structure of the document. Lastly, a considerable number of errors suggest that models are still unable to resolve cases requiring mention paraphrasing. For example, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from  belongs to this family of models <ref type="bibr" target="#b17">(Ng and Cardie, 2002;</ref><ref type="bibr" target="#b0">Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b4">Denis and Baldridge, 2008;</ref><ref type="bibr" target="#b8">Fernandes et al., 2012;</ref><ref type="bibr" target="#b6">Durrett and Klein, 2013;</ref><ref type="bibr" target="#b21">Wiseman et al., 2015;</ref><ref type="bibr" target="#b3">Clark and Manning, 2016;</ref><ref type="bibr" target="#b11">Lee et al., 2017)</ref>.</p><p>More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations <ref type="bibr" target="#b18">(Peters et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2019;</ref><ref type="bibr" target="#b16">McCann et al., 2017;</ref><ref type="bibr" target="#b9">Joshi et al., 2019;</ref><ref type="bibr" target="#b14">Liu et al., 2019b)</ref>. Of these, BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies. SpanBERT <ref type="bibr" target="#b9">(Joshi et al., 2019)</ref> focuses on pretraining span representations achieving current state of the art results on OntoNotes with the independent variant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>its predecessor, e2e-coref(Lee et al.,  OntoNotes:  BERT improves the c2f-coref model on English by 0.9% and 3.9% respectively for base and large variants. The main evaluation is the average F1 of three metrics -MUC, B 3 , and CEAF φ4 on the test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MUC</cell><cell></cell><cell></cell><cell>B 3</cell><cell></cell><cell></cell><cell>CEAF φ 4</cell></row><row><cell></cell><cell></cell><cell>P</cell><cell></cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Avg. F1</cell></row><row><cell>Martschat and Strube (2015)</cell><cell></cell><cell cols="4">76.7 68.1 72.2</cell><cell cols="3">66.1 54.2 59.6</cell><cell cols="2">59.5 52.3 55.7</cell><cell>62.5</cell></row><row><cell>(Clark and Manning, 2015)</cell><cell></cell><cell cols="4">76.1 69.4 72.6</cell><cell cols="3">65.6 56.0 60.4</cell><cell cols="2">59.4 53.0 56.0</cell><cell>63.0</cell></row><row><cell>(Wiseman et al., 2015)</cell><cell></cell><cell cols="4">76.2 69.3 72.6</cell><cell cols="3">66.2 55.8 60.5</cell><cell cols="2">59.4 54.9 57.1</cell><cell>63.4</cell></row><row><cell>Wiseman et al. (2016)</cell><cell></cell><cell cols="4">77.5 69.8 73.4</cell><cell cols="3">66.8 57.0 61.5</cell><cell cols="2">62.1 53.9 57.7</cell><cell>64.2</cell></row><row><cell>Clark and Manning (2016)</cell><cell></cell><cell cols="4">79.2 70.4 74.6</cell><cell cols="3">69.9 58.0 63.4</cell><cell cols="2">63.5 55.5 59.2</cell><cell>65.7</cell></row><row><cell>e2e-coref (Lee et al., 2017)</cell><cell></cell><cell cols="4">78.4 73.4 75.8</cell><cell cols="3">68.6 61.8 65.0</cell><cell cols="2">62.7 59.0 60.8</cell><cell>67.2</cell></row><row><cell>c2f-coref (Lee et al., 2018)</cell><cell></cell><cell cols="4">81.4 79.5 80.4</cell><cell cols="3">72.2 69.5 70.8</cell><cell cols="2">68.2 67.1 67.6</cell><cell>73.0</cell></row><row><cell>Fei et al. (2019)</cell><cell></cell><cell cols="4">85.4 77.9 81.4</cell><cell cols="3">77.9 66.4 71.7</cell><cell cols="2">70.6 66.3 68.4</cell><cell>73.8</cell></row><row><cell cols="2">EE (Kantor and Globerson, 2019)</cell><cell cols="4">82.6 84.1 83.4</cell><cell cols="3">73.3 76.2 74.7</cell><cell cols="2">72.4 71.1 71.8</cell><cell>76.6</cell></row><row><cell cols="6">BERT-base + c2f-coref (independent) 80.2 82.4 81.3</cell><cell cols="3">69.6 73.8 71.6</cell><cell cols="2">69.0 68.6 68.8</cell><cell>73.9</cell></row><row><cell cols="2">BERT-base + c2f-coref (overlap)</cell><cell cols="4">80.4 82.3 81.4</cell><cell cols="3">69.6 73.8 71.7</cell><cell cols="2">69.0 68.5 68.8</cell><cell>73.9</cell></row><row><cell cols="6">BERT-large + c2f-coref (independent) 84.7 82.4 83.5</cell><cell cols="3">76.5 74.0 75.3</cell><cell cols="2">74.1 69.8 71.9</cell><cell>76.9</cell></row><row><cell cols="2">BERT-large + c2f-coref (overlap)</cell><cell cols="4">85.1 80.5 82.8</cell><cell cols="3">77.5 70.9 74.1</cell><cell cols="2">73.8 69.3 71.5</cell><cell>76.1</cell></row><row><cell>Model</cell><cell>M</cell><cell>F</cell><cell>B</cell><cell>O</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>e2e-coref</cell><cell cols="4">67.7 60.0 0.89 64.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>c2f-coref</cell><cell cols="4">75.8 71.1 0.94 73.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">BERT + RR Liu et al. (2019a) 80.3 77.4 0.96 78.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base + c2f-coref</cell><cell cols="4">84.4 81.2 0.96 82.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-large + c2f-coref</cell><cell cols="4">86.9 83.0 0.95 85.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>GAP: BERT improves the c2f-coref model by 11.5%. The metrics are F1 score on Masculine and Feminine examples, Overall, and a Bias factor (F / M).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Qualitative Analysis: #base and #large refers to the number of cluster-level errors on a subset of the OntoNotes English development set. Underlined and bold-faced mentions respectively indicate incorrect and missing assignments to italicized mentions/clusters. The miscellaneous category refers to other errors including (reasonable) predictions that are either missing from the gold data or violate annotation guidelines.</figDesc><table><row><cell cols="5">Doc length #Docs Spread F1 (base) F1 (large)</cell></row><row><cell>0 -128</cell><cell>48</cell><cell>37.3</cell><cell>80.6</cell><cell>84.5</cell></row><row><cell>128 -256</cell><cell>54</cell><cell>71.7</cell><cell>80.0</cell><cell>83.0</cell></row><row><cell>256 -512</cell><cell>74</cell><cell>109.9</cell><cell>78.2</cell><cell>80.0</cell></row><row><cell>512 -768</cell><cell>64</cell><cell>155.3</cell><cell>76.8</cell><cell>80.2</cell></row><row><cell>768 -1152</cell><cell>61</cell><cell>197.6</cell><cell>71.1</cell><cell>76.2</cell></row><row><cell>1152+</cell><cell>42</cell><cell>255.9</cell><cell>69.9</cell><cell>72.8</cell></row><row><cell cols="2">All 343</cell><cell>179.1</cell><cell>74.3</cell><cell>77.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance on the English OntoNotes dev set generally drops as the document length increases. Spread is measured as the average number of tokens between the first and last mentions in a cluster.</figDesc><table><row><cell cols="3">Segment Length F1 (BERT-base) F1 (BERT-large)</cell></row><row><cell>128</cell><cell>74.4</cell><cell>76.6</cell></row><row><cell>256</cell><cell>73.9</cell><cell>76.9</cell></row><row><cell>384</cell><cell>73.4</cell><cell>77.3</cell></row><row><cell>450</cell><cell>72.2</cell><cell>75.3</cell></row><row><cell>512</cell><cell>70.7</cell><cell>73.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance on the English OntoNotes dev set with varying values for max segment len.</figDesc><table><row><cell>Neither model is able to effectively exploit larger</cell></row><row><cell>segments; they perform especially badly when</cell></row><row><cell>maximum segment len of 512 is used.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://github.com/kentonl/e2e-coref/ 4 https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">This is motivated by the fact that GAP, with only 4,000 name-pronoun pairs in its dev set, is not intended for fullscale training.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08, 7 We required a 32GB GPU to finetune BERT-large</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08, 7 We required a 32GB GPU to finetune BERT-large<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1405" to="1415" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1245</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2256" to="2262" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Specialized models and ranking for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end deep reinforcement learning based coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="660" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eraldo</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cícero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy</forename><surname>Milidiú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coreference resolution with entity equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="673" to="677" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarseto-fine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The referential reader: A recurrent entity network for anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5918" to="5925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>RoBERTa: A robustly optimized BERT pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.3115/1072228.1072367</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>COLING &apos;02. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mind the GAP: A balanced corpus of gendered ambiguous pronouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="605" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
