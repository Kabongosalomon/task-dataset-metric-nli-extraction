<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeper Task-Specificity Improves Joint Entity and Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Crone</surname></persName>
							<email>pcrone@ancestry.com</email>
						</author>
						<title level="a" type="main">Deeper Task-Specificity Improves Joint Entity and Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-task learning (MTL) is an effective method for learning related tasks, but designing MTL models necessitates deciding which and how many parameters should be task-specific, as opposed to shared between tasks. We investigate this issue for the problem of jointly learning named entity recognition (NER) and relation extraction (RE) and propose a novel neural architecture that allows for deeper task-specificity than does prior work. In particular, we introduce additional task-specific bidirectional RNN layers for both the NER and RE tasks and tune the number of shared and taskspecific layers separately for different datasets. We achieve state-of-the-art (SOTA) results for both tasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA results on the NER task and competitive results on the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggests that previous solutions to joint NER and RE undervalue task-specificity and demonstrates the importance of correctly balancing the number of shared and task-specific parameters for MTL approaches in general.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-task learning (MTL) refers to machine learning approaches in which information and representations are shared to solve multiple, related tasks. Relative to single-task learning approaches, MTL often shows improved performance on some or all sub-tasks and can be more computationally efficient <ref type="bibr" target="#b1">[Caruana, 1997;</ref><ref type="bibr" target="#b2">Cipolla et al., 2018;</ref><ref type="bibr" target="#b9">Vandenhende et al., 2019;</ref><ref type="bibr" target="#b6">Li et al., 2019a]</ref>. We focus here on a form of MTL known as hard parameter sharing. Hard parameter sharing refers to the use of deep learning models in which inputs to models first pass through a number of shared layers. The hidden representations produced by these shared layers are then fed as inputs to a number of task-specific layers.</p><p>Within the domain of natural language processing (NLP), MTL approaches have been applied to a wide range of prob-In 1809, author Edgar Allan Poe PEOP was born in Boston LOC <ref type="figure">Figure 1</ref>: Example of a sentence containing named entities and relations from the CoNLL04 dataset. This sentence expresses a Lives-In relation between Edgar Allan Poe and Boston.</p><p>lems <ref type="bibr" target="#b6">[Li et al., 2019a]</ref>. In recent years, one particularly fruitful application of MTL to NLP has been joint solving of named entity recognition (NER) and relation extraction (RE), two important information extraction tasks with applications in search, question answering, and knowledge base construction <ref type="bibr" target="#b5">[Jiang, 2012]</ref>. NER consists in the identification of spans of text as corresponding to named entities and the classification of each span's entity type. RE consists in the identification of all triples (e i , e j , r), where e i and e j are named entities and r is a relation that holds between e i and e j according to the text. For example, in <ref type="figure">Figure 1</ref>, Edgar Allan Poe and Boston are named entities of the types People and Location, respectively. In addition, the text indicates that the Lives-In relation obtains between Edgar Allan Poe and Boston.</p><p>One option for solving these two problems is a pipeline approach using two independent models, each designed to solve a single task, with the output of the NER model serving as an input to the RE model. However, MTL approaches offer a number of advantages over the pipeline approach. First, the pipeline approach is more susceptible to error prorogation wherein prediction errors from the NER model enter the RE model as inputs that the latter model cannot correct. Second, the pipeline approach only allows solutions to the NER task to inform the RE task, but not vice versa. In contrast, the joint approach allows for solutions to either task to inform the other. For example, learning that there is a Lives-In relation between Edgar Allan Poe and Boston can be useful for determining the types of these entities. Finally, the joint approach can be computationally more efficient than the pipeline approach. As mentioned above, MTL approaches are generally more efficient than single-task learning alternatives. This is due to the fact that solutions to related tasks often rely on similar information, which in an MTL setting only needs to be represented in one model in order to solve all tasks. For example, the fact that Edgar Allan Poe is followed by was born can help a model determine both that Edgar Allan Poe is an instance of a People entity and that the sentence expresses a Lives-In relation.</p><p>While the choice as to which and how many layers to share between tasks is known to be an important factor relevant to the performance of MTL models <ref type="bibr" target="#b9">[Zhao et al., 2018;</ref><ref type="bibr" target="#b9">Vandenhende et al., 2019]</ref>, this issue has received relatively little attention within the context of joint NER and RE. As we show below in Section 2, prior proposals for jointly solving NER and RE have typically made use of very few taskspecific parameters or have mostly used task-specific parameters only for the RE task. We seek to correct for this oversight by proposing a novel neural architecture for joint NER and RE. In particular, we make the following contributions:</p><p>1. We allow for deeper task-specificity than does previous work via the use of additional task-specific bidirectional recurrent neural networks (BiRNNs) for both tasks.</p><p>2. Because the relatedness between the NER and RE tasks is not constant across all textual domains, we take the number of shared and task-specific layers to be an explicit hyperparameter of the model that can be tuned separately for different datasets.</p><p>We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset <ref type="bibr" target="#b4">[Gurulingappa et al., 2012]</ref> and the CoNLL04 dataset <ref type="bibr" target="#b9">[Roth and Yih, 2004]</ref>. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks. Moreover, we achieve these results using an order of magnitude fewer trainable parameters than the current SOTA architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We focus in this section on previous deep learning approaches to solving the tasks of NER and RE, as this work is most directly comparable to our proposal. Most work on joint NER and RE has adopted a BIO or BILOU scheme for the NER task, where each token is labeled to indicate whether it is the (B)eginning of an entity, (I)nside an entity, or (O)utside an entity. The BILOU scheme extends these labels to indicate if a token is the (L)ast token of an entity or is a (U)nit, i.e. the only token within an entity span.</p><p>Several approaches treat the NER and RE tasks as if they were a single task. For example, Gupta et al. <ref type="bibr">[2016]</ref>, following <ref type="bibr" target="#b8">Miwa and Sasaki [2014]</ref>, treat the two tasks as a tablefilling problem where each cell in the table corresponds to a pair of tokens (t i , t j ) in the input text. For the diagonal of the table, the cell label is the BILOU tag for t i . All other cells are labeled with the relation r, if it exists, such that (e i , e j , r), where e i is the entity whose span's final token is t i , is in the set of true relations. A BiRNN is trained to fill the cells of the table. Zheng et al. <ref type="bibr">[2017]</ref> introduce a BILOU tagging scheme that incorporates relation information into the tags, allowing them to treat both tasks as if they were a single NER task. A series of two bidirectional LSTM (BiLSTM) layers and a final softmax layer are used to produce output tags. <ref type="bibr" target="#b6">Li et al. [2019b]</ref> solve both tasks as a form of multi-turn question answering in which the input text is queried with question templates first to detect entities and then, given the detected entities, to detect any relations between these entities. <ref type="bibr">Li et al. use BERT [Devlin et al., 2019]</ref> as the backbone of their question-answering model and produce answers by tagging the input text with BILOU tags to identify the span corresponding to the answer(s).</p><p>The above approaches allow for very little task-specificity, since both the NER task and the RE task are coerced into a single task. Other approaches incorporate greater taskspecificity in one of two ways. First, several models share the majority of model parameters between the NER and RE tasks, but also have separate scoring and/or output layers used to produce separate outputs for each task. For example, Katiyar and Cardie <ref type="bibr">[2017]</ref> and <ref type="bibr" target="#b0">Bekoulis et al. [2018]</ref> propose models in which token representations first pass through one or more shared BiLSTM layers. Katiyar and Cardie use a softmax layer to tag tokens with BILOU tags to solve the NER task and use an attention layer to detect relations between each pair of entities. Bekoulis et al., following Lample et al. <ref type="bibr">[2016]</ref>, use a conditional random field (CRF) layer to produce BIO tags for the NER task. The output from the shared BiLSTM layer for every pair of tokens is passed through relation scoring and sigmoid layers to predict relations.</p><p>A second method of incorporating greater task-specificity into these models is via deeper layers for solving the RE task. <ref type="bibr" target="#b7">Miwa and Bansal [2016]</ref> and  pass token representations through a BiLSTM layer and then use a softmax layer to label each token with the appropriate BILOU label. Both proposals then use a type of tree-structured bidirectional LSTM layer stacked on top of the shared BiLSTM to solve the RE task. Nguyen and Verspoor <ref type="bibr">[2019]</ref> use BiLSTM and CRF layers to perform the NER task. Label embeddings are created from predicted NER labels, concatenated with token representations, and then passed through a RE-specific BiL-STM. A biaffine attention layer <ref type="bibr" target="#b4">[Dozat and Manning, 2016]</ref> operates on the output of this BiLSTM to predict relations.</p><p>An alternative to the BIO/BILOU scheme is the span-based approach, wherein spans of the input text are directly labeled as to whether they correspond to any entity and, if so, their entity types. Luan et al.</p><p>[2018] adopt a span-based approach in which token representations are first passed through a BiL-STM layer. The output from the BiLSTM is used to construct representations of candidate entity spans, which are then scored for both the NER and RE tasks via feed forward layers. Luan et al.</p><p>[2019] follow a similar approach, but construct coreference and relation graphs between entities to propagate information between entities connected in these graphs. The resulting entity representations are then classified for NER and RE via feed forward layers. To the best of our knowledge, the current SOTA model for joint NER and RE is the span-based proposal of Eberts and Ulges <ref type="bibr">[2019]</ref>. In this architecture, token representations are obtained using a pre-trained BERT model that is fine-tuned during training. Representations for candidate entity spans are obtained by max pooling over all tokens in each span. Span representations are passed through an entity classification layer to solve the NER task. Representations of all pairs of spans that are predicted to be entities and representations of the contexts between these pairs are then passed through a final layer with sigmoid activation to predict relations between entities. With respect to their degrees of task-specificity, these span-based approaches resemble the BIO/BILOU approaches in which the majority of model parameters are shared, but each task possesses independent scoring and/or output layers. Overall, previous approaches to joint NER and RE have experimented little with deep task-specificity, with the exception of those models that include additional layers for the RE task. To our knowledge, no work has considered including additional NER-specific layers beyond scoring and/or output layers. This may reflect a residual influence of the pipeline approach in which the NER task must be solved first before additional layers are used to solve the RE task. However, there is no a priori reason to think that the RE task would benefit more from additional task-specific layers than the NER task. We also note that while previous work has tackled joint NER and RE in variety of textual domains, in all cases the number of shared and task-specific parameters is held constant across these domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>The architecture proposed here is inspired by several previous proposals <ref type="bibr" target="#b5">[Katiyar and Cardie, 2017;</ref><ref type="bibr" target="#b0">Bekoulis et al., 2018;</ref><ref type="bibr">Nguyen and Verspoor, 2019]</ref>. We treat the NER task as a sequence labeling problem using BIO labels. Token representations are first passed through a series of shared, BiRNN layers. Stacked on top of these shared BiRNN layers is a sequence of task-specific BiRNN layers for both the NER and RE tasks. We take the number of shared and task-specific layers to be a hyperparameter of the model. Both sets of taskspecific BiRNN layers are followed by task-specific scoring and output layers. <ref type="figure">Figure 2</ref> illustrates this architecture. Below, we use superscript e for NER-specific variables and layers and superscript r for RE-specific variables and layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shared Layers</head><p>We obtain contextual token embeddings using the pre-trained </p><formula xml:id="formula_0">v i = t elmo i • t glove i • t char i • t casing i<label>(1)</label></formula><p>For an input text with n tokens, v 1:n are fed as input to a sequence of one or more shared BiRNN layers, with the output sequence from the ith shared BiRNN layer serving as the input sequence to the i + 1st shared BiRNN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NER-Specific Layers</head><p>The final shared BiRNN layer is followed by a sequence of zero or more NER-specific BiRNN layers; the output of the final shared BiRNN layer serves as input to the first NERspecific BiRNN layer, if such a layer exists, and the output from from the ith NER-specific BiRNN layer serves as input to the i + 1st NER-specific BiRNN layer. For every token t i , let h e i denote an NER-specific hidden representation for t i corresponding to the ith element of the output sequence from the final NER-specific BiRNN layer or the final shared BiRNN layer if there are zero NER-specific BiRNN layers.</p><p>An NER score for token t i , s e i , is obtained by passing h e i through a series of two feed forward layers:</p><formula xml:id="formula_1">s e i = FFNN (e2) (FFNN (e1) (h e i ))<label>(2)</label></formula><p>The activation function of FFNN (e1) and its output size are treated as hyperparameters. FFNN (e2) uses linear activation and its output size is |E|, where E is the set of possible entity types. The sequence of NER scores for all tokens, s e 1:n , is then passed as input to a linear-chain CRF layer to produce the final BIO tag predictions,ŷ e 1:n . During inference, Viterbi decoding is used to determine the most likely sequenceŷ e 1:n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RE-Specific Layers</head><p>Similar to the NER-specific layers, the output sequence from the final shared BiRNN layer is fed through zero or more REspecific BiRNN layers. Let h r i denote the ith output from the final RE-specific BiRNN layer or the final shared BiRNN layer if there are no RE-specific BiRNN layers.</p><p>Following previous work <ref type="bibr" target="#b4">[Gupta et al., 2016;</ref><ref type="bibr" target="#b0">Bekoulis et al., 2018;</ref><ref type="bibr">Nguyen and Verspoor, 2019]</ref>, we predict relations between entities e i and e j using learned representations from the final tokens of the spans corresponding to e i and e j . To this end, we filter the sequence h r 1:n to include only elements h r i such that token t i is the final token in an entity span. During training, ground truth entity spans are used for filtering. During inference, predicted entity spans derived fromŷ e 1:n are used. Each h r i is concatenated to a learned NER label embedding for t i , l e i :</p><formula xml:id="formula_2">g r i = h r i • l e i<label>(3)</label></formula><p>Ground truth NER labels are used to obtain l e 1:n during training, and predicted NER labels are used during inference. <ref type="bibr">2</ref> Next, RE scores are computed for every pair (g r i , g r j ). If R is the set of possible relations, we calculate the DISTMULT score <ref type="bibr" target="#b9">[Yang et al., 2014]</ref> for every relation r k ∈ R and every pair (g r i , g r j ) as follows:</p><formula xml:id="formula_3">DISTMULT r k (g r i , g r j ) = (g r i ) T M r k g r j<label>(4)</label></formula><p>M r k is a diagonal matrix such that M r k ∈ R p×p , where p is the dimensionality of g r i . We also pass each RE-specific hidden representation g r i through a single feed forward layer:</p><formula xml:id="formula_4">f r i = FFNN (r1) (g r i )<label>(5)</label></formula><p>As in the case of FFNN (e1) , the activation function of FFNN (r1) and its output size are treated as hyperparameters. Let DISTMULT r i,j denote the concatenation of DISTMULT r k (g r i , g r j ) for all r k ∈ R and let cos i,j denote the cosine distance between vectors f r i and f r j . We obtain RE scores for (t i , t j ) via a feed forward layer:</p><formula xml:id="formula_5">s r i,j = FFNN (r2) f r i • f r j • cos i,j •DISTMULT r i,j<label>(6)</label></formula><p>2 Because ground truth NER labels are used to generate label embeddings during training, the output from the BiRNN layer(s) described in Section 3.2 is opaque to the RE-specific portion of the model during training. If predicted NER labels were used during training instead, as in <ref type="bibr">[Nguyen and Verspoor, 2019]</ref>, these layers would be shared rather than NER-specific. FFNN (r2) uses linear activation, and its output size is |R|.</p><p>Final relation predictions for a pair of tokens (t i , t j ),ŷ r i,j , are obtained by passing s r i,j through an elementwise sigmoid layer. A relation is predicted for all outputs from this sigmoid layer exceeding θ r , which we treat as a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>During training, character embeddings, label embeddings, and weights for the weighted average layer, all BiRNN weights, all feed forward networks, and M r k for all r k ∈ R are trained in a supervised manner. As mentioned above, BIO tags for all tokens are used as labels for the NER task. For the the RE task, binary outputs are used. For every relation r k ∈ R and for every pair of tokens (t i , t j ) such that t i is the final token of entity e i and t j is the final token of entity e j , the RE label y r k i,j = 1 if (e i , e j , r k ) is a true relation. Otherwise, we have y r k i,j = 0. For both output layers, we compute the cross-entropy loss. If L N ER and L RE denote the cross-entropy loss for the NER and RE outputs, respectively, then the total model loss is given by L = L N ER + λ r L RE . The weight λ r is treated as a hyperparameter and allows for tuning the relative importance of the NER and RE tasks during training. Final training for both datasets used a value of 5 for λ r .</p><p>For the ADE dataset, we trained using the Adam optimizer with a mini-batch size of 16. For the CoNLL04 dataset, we used the Nesterov Adam optimizer with and a mini-batch size of 2. For both datasets, we used a learning rate of 5 × 10 −4 , During training, dropout was applied before each BiRNN layer, other than the character BiRNN layer, and before the RE scoring layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the architecture described above using the following two publicly available datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE</head><p>The Adverse Drug Events (ADE) dataset <ref type="bibr" target="#b4">[Gurulingappa et al., 2012]</ref> consists of 4,272 sentences describing adverse effects from the use of particular drugs. The text is annotated using two entity types (Adverse-Effect and Drug) and a single relation type (Adverse-Effect). Of the entity instances in the dataset, 120 overlap with other entities. Similar to prior work using BIO/BILOU tagging, we remove overlapping entities. We preserve the entity with the longer span and remove any relations involving a removed entity. There are no official training, dev, and test splits for the ADE dataset, leading previous researchers to use some form of cross-validation when evaluating their models on this dataset. We split out 10% of the data to use as a held-out dev set. Final results are obtained via 10-fold cross-validation using the remaining 90% of the data and the hyperparameters obtained from tuning on the dev set. Following previous work, we report macro-averaged performance metrics averaged across each of the 10 folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoNLL04</head><p>The CoNLL04 dataset <ref type="bibr" target="#b9">[Roth and Yih, 2004]</ref> consists of 1,441 sentences from news articles annotated with four entity types (Location, Organization, People, and Other) and five relation types (Works-For, Kill, Organization-Based-In, Lives-In, and Located-In). This dataset contains no overlapping entities.</p><p>We use the three-way split of <ref type="bibr" target="#b4">[Gupta et al., 2016]</ref>, which contains 910 training, 243 dev, and 288 test sentences. All hyperparameters are tuned against the dev set. Final results are obtained by averaging results from five trials with random weight initializations in which we trained on the combined training and dev sets and evaluated on the test set. As previous work using the CoNLL04 dataset has reported both micro-and macro-averages, we report both sets of metrics.</p><p>In evaluating NER performance on these datasets, a predicted entity is only considered a true positive if both the entity's span and span type are correctly predicted. In evaluating RE performance, we follow previous work in adopting a strict evaluation method wherein a predicted relation is only considered correct if the spans corresponding to the two argu-ments of this relation and the entity types of these spans are also predicted correctly. We experimented with LSTMs and GRUs for all BiRNN layers in the model and experimented with using 1 − 3 shared BiRNN layers and 0 − 3 task-specific BiRNN layers for each task. Hyperparameters used for final training are listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>Full results for the performance of our model, as well as other recent work, are shown in <ref type="table" target="#tab_3">Table 2</ref>. In addition to precision, recall, and F1 scores for both tasks, we show the average of the F1 scores across both tasks. On the ADE dataset, we achieve SOTA results for both the NER and RE tasks. On the CoNLL04 dataset, we achieve SOTA results on the NER task, while our performance on the RE task is competitive with other recent models. On both datasets, we achieve SOTA results when considering the average F1 score across both tasks. The largest gain relative to the previous SOTA performance is on the RE task of the ADE dataset, where we see an absolute improvement of 4.5 on the macro-average F1 score.</p><p>While the model of Eberts and Ulges [2019] outperforms our proposed architecture on the CoNLL04 RE task, their results come at the cost of greater model complexity. As mentioned above, Eberts and Ulges fine-tune the BERT BASE model, which has 110 million trainable parameters. In contrast, given the hyperparameters used for final training on the CoNLL04 dataset, our proposed architecture has approximately 6 million trainable parameters.</p><p>The fact that the optimal number of task-specific layers differed between the two datasets demonstrates the value of taking the number of shared and task-specific layers to be a hyperparameter of our model architecture. As shown in <ref type="table">Table  1</ref>, the final hyperparameters used for the CoNLL04 dataset included an additional RE-specific BiRNN layer than did the final hyperparameters used for the ADE dataset. We suspect that this is due to the limited number of relations and entities in the ADE dataset. For most examples in this dataset, it is sufficient to correctly identify a single Drug entity, a single Adverse-Effect entity, and an Adverse-Effect relation between the two entities. Thus, the NER and RE tasks for this dataset are more closely related than they are in the case of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions:</p><p>1. We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero taskspecific BiRNN layers of any kind.</p><p>2. We increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model.</p><p>3. We average the results for each set of hyperparameter across three trials with random weight initializations. <ref type="table" target="#tab_5">Table 3</ref> contains the results from the ablation study. These results show that the proposed architecture benefits from the inclusion of both NER-and RE-specific layers. However, the RE task benefits much more from the inclusion of these taskspecific layers than does the NER task. We take this to reflect the fact that the RE task is more difficult than the NER task for the CoNLL04 dataset, and therefore benefits the most from its own task-specific layers. This is consistent with the fact that the hyperparameter setting that performs best on the RE task is that with no NER-specific BiRNN layers, i.e. the setting that retained RE-specific BiRNN layers. In contrast, the inclusion of task-specific BiRNN layers of any kind had relatively little impact on the performance on the NER task.</p><p>Note that the setting with no NER-specific layers is somewhat similar to the setup of Nguyen and Verspoor's [2019] model, but includes an additional shared and an additional RE-specific layer. That this setting outperforms Nguyen et al.'s model reflects the contribution of having deeper shared and RE-specific layers, separate from the contribution of NER-specific layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our results demonstrate the utility of using deeper taskspecificity in models for joint NER and RE and of tuning the level of task-specificity separately for different datasets. We conclude that prior work on joint NER and RE undervalues the importance of task-specificity. More generally, these results underscore the importance of correctly balancing the number of shared and task-specific parameters in MTL.</p><p>We note that other approaches that employ a single model architecture across different datasets are laudable insofar as we should prefer models that can generalize well across domains with little domain-specific hyperparameter tuning. On the other hand, the similarity between the NER and RE tasks varies across domains, and improved performance can be achieved on these tasks by tuning the number of shared and task-specific parameters. In our work, we treated the number of shared and task-specific layers as a hyperparameter to be tuned for each dataset, but future work may explore ways to select this aspect of the model architecture in a more principled way. For example, <ref type="bibr" target="#b9">Vandenhende et al. [2019]</ref> propose using a measure of affinity between tasks to determine how many layers to share in MTL networks. Task affinity scores of NER and RE could be computed for different textual domains or datasets, which could then guide the decision regarding the number of shared and task-specific layers to employ for joint NER and RE models deployed on these domains.</p><p>Other extensions to the present work could include finetuning the model used to obtain contextual word embeddings, e.g. ELMo or BERT, during training. In order to minimize the number of trainable parameters, we did not employ such finetuning in our model, but we suspect a fine-tuning approach could lead to improved performance relative to our results. An additional opportunity for future work would be an extension of this work to other related NLP tasks, such as coreference resolution and cross-sentential relation extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ELMo 5.5B model [Peters et al., 2018]. 1 For each token in the input text t i , this model returns three vectors, which we combine via a weighted averaging layer. Each token t i 's weighted ELMo embedding t elmo i is concatenated to a pretrained GloVe embedding [Pennington et al., 2014] t glove i , a character-level word embedding t char i learned via a single BiRNN layer [Lample et al., 2016] and a one-hot encoded casing vector t casing i . The full representation of t i is given by v i (where • denotes concatenation):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Illustration of our proposed architecture. Token representations are derived from a pre-trained ELMo model, pre-trained GloVe embeddings, learned character-based embeddings, and one-hot encoded casing vectors. The number of shared and task-specific BiRNN layers is treated as a hyperparameter of the model architecture. Only the final token in each entity span is used for predictions for the RE task; grey boxes indicate tokens that are not used for relation predictions. The output for the RE task is a vector of size |R| for all pairs of entities, where R is the set of all possible relations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RE Output</cell></row><row><cell>NER Output</cell><cell>B-LOC</cell><cell>O</cell><cell>B-PEOP</cell><cell>I-PEOP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RE Scoring Layer</cell></row><row><cell>CRF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Label Embedding</cell></row><row><cell>NER Scoring Layer</cell><cell></cell><cell></cell><cell></cell><cell>Concatenation &amp; Filtering</cell></row><row><cell>NER-Specific</cell><cell></cell><cell></cell><cell></cell><cell>RE-Specific</cell></row><row><cell>BiRNN Layer(s)</cell><cell></cell><cell></cell><cell></cell><cell>BiRNN Layer(s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Shared BiRNN</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Layer(s)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Token Representations</cell><cell></cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Precision, Recall, and F1 scores for our model and other recent models on the ADE and CoNLL04 datasets. Because our scores are averaged across multiple trials, F1 scores shown here cannot be directly calculated from the precision and recall scores shown here. Note that Nguyen and Verspoor do not report precision and recall scores.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results from an ablation study using the CoNLL04 dataset. All models have the same number of total parameters. the CoNLL04 dataset. Intuitively, cases in which the NER and RE problems can be solved by relying on more shared information should require fewer task-specific layers.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also experimented with using a pre-trained BERT model rather than ELMo, but performance when using ELMo was slightly higher than when using BERT. In order to help minimize the total number of trainable parameters in our model, we did not experiment with fine-tuning ELMo or BERT.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rich Caruana. Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D Manning ;</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<idno>arXiv:1909.07755</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page">885</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Markus Eberts and Adrian Ulges</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Jiang ; Arzoo Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">198</biblScope>
		</imprint>
	</monogr>
	<note>A neural joint model for entity and relation extraction from biomedical text</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<idno type="arXiv">arXiv:1908.07820</idno>
		<idno>arXiv:1905.05529</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 2019 Conference of the North</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal ; Makoto Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki ; Makoto Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Doha, Qatar; Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
		<respStmt>
			<orgName>Nguyen and Verspoor</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Long Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A modulation module for multi-task learning with applications in image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih ; Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Vandenhende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02920</idno>
		<idno>arXiv:1412.6575</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="415" to="432" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the Eighth Conference on Computational Natural Language Learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
