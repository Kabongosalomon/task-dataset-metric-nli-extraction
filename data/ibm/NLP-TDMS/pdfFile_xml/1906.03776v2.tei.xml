<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Spatio-Temporal Neural Networks for Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-04">2019. August 4-8, 2019. August 4-8, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuwu</forename><surname>Zhang</surname></persName>
							<email>xiuwu.zxw@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zou</surname></persName>
							<email>zouheng.zh@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xing</surname></persName>
							<email>xingxin.xx@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojie</forename><surname>Liu</surname></persName>
							<email>zhaojie.lzj@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlong</forename><surname>Du</surname></persName>
							<email>yanlong.dyl@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Marketing Platform</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuwu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xing</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojie</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Spatio-Temporal Neural Networks for Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<title level="m">The 25th ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining (KDD &apos;19)</title>
						<meeting> <address><addrLine>Anchorage, AK, USA; Anchorage, AK, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">19</biblScope>
							<date type="published" when="2019-08-04">2019. August 4-8, 2019. August 4-8, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330655</idno>
					<note>ACM, New York, NY, USA, 9 pages. https:// ACM ISBN 978-1-4503-6201-6/19/08. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Online advertising</term>
					<term>Computational advertising</term>
					<term>KEYWORDS Click-through rate prediction</term>
					<term>Online advertising</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate (CTR) prediction is a critical task in online advertising systems. A large body of research considers each ad independently, but ignores its relationship to other ads that may impact the CTR. In this paper, we investigate various types of auxiliary ads for improving the CTR prediction of the target ad. In particular, we explore auxiliary ads from two viewpoints: one is from the spatial domain, where we consider the contextual ads shown above the target ad on the same page; the other is from the temporal domain, where we consider historically clicked and unclicked ads of the user. The intuitions are that ads shown together may influence each other, clicked ads reflect a user's preferences, and unclicked ads may indicate what a user dislikes to certain extent. In order to effectively utilize these auxiliary data, we propose the Deep Spatio-Temporal neural Networks (DSTNs) for CTR prediction. Our model is able to learn the interactions between each type of auxiliary data and the target ad, to emphasize more important hidden information, and to fuse heterogeneous data in a unified framework. Offline experiments on one public dataset and two industrial datasets show that DSTNs outperform several state-of-the-art methods for CTR prediction. We have deployed the best-performing DSTN in Shenma Search, which is the second largest search engine in China. The A/B test results show that the online CTR is also significantly improved compared to our last serving model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Click-through rate (CTR) prediction is to predict the probability that a user will click on an item. It plays an important role in online advertising systems. For example, the ad ranking strategy generally depends on CTR × bid, where bid is the benefit the system receives if an ad is clicked. Moreover, according to the common cost-perclick charging model, advertisers are only charged once their ads are clicked by users. Therefore, in order to maximize the revenue and to maintain a desirable user experience, it is crucial to estimate the CTR of ads accurately.</p><p>CTR prediction has attracted lots of attention from both academia and industry <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">33]</ref>. One line of research is to take advantage of machine learning approaches to predict the CTR for each ad independently. For example, Factorization Machines (FMs) <ref type="bibr" target="#b22">[22]</ref> are proposed to model pairwise feature interactions in terms of the latent vectors corresponding to the involved features. In recent years, Deep Neural Networks (DNNs) are exploited for CTR prediction and item recommendation in order to automatically learn feature representations and high-order feature interactions <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b31">31]</ref>. To take advantage of both shallow and deep models, hybrid models are also proposed. For example, Wide&amp;Deep <ref type="bibr" target="#b4">[5]</ref> combines Logistic Regression (LR) with DNN, in order to improve both the memorization and generalization abilities of the model. DeepFM <ref type="bibr" target="#b10">[10]</ref> combines FM with DNN, which further improves the model ability of learning feature interactions.</p><p>This line of research considers each ad independently, but ignores other ads that may impact the CTR of the target ad. In this paper, we explore auxiliary ads beyond the target ad for improving the CTR prediction (cf. <ref type="figure">Figure 1</ref>). In particular, we explore auxiliary ads from two viewpoints. One is from the spatial domain: we consider the contextual ads shown above the target ad on the same page <ref type="bibr" target="#b0">1</ref> . The intuition is that ads shown together may compete for a user's attention. The second viewpoint is from the temporal domain: we consider the historically clicked and unclicked ads 2 of the user. The intuition is that clicked ads reflect a user's preferences and unclicked ads may indicate what a user dislikes to certain extent.</p><p>In order to effectively utilize these auxiliary data, we must address the following issues: 1) As the numbers of each type of auxiliary ads may vary, the model must be able to accommodate all possible cases. For example, there are 1 contextual ad, 2 clicked ads and 4 unclicked ads with target ad a 1 and there are 0 contextual ad, 3 clicked ads and 2 unclicked ads with target ad a 2 . 2) As auxiliary ads may not be necessarily related to the target ad, the model should <ref type="bibr" target="#b0">1</ref> Contextual ads are available to advertising systems when ads are ranked sequentially. Please refer to §4.1 for more detail. <ref type="bibr" target="#b1">2</ref> Unclicked ads are ads that are shown to a user but not clicked by the user. They are not created by sampling from an ad pool excluding clicked ads. arXiv:1906.03776v2 <ref type="bibr">[cs.</ref>LG] 19 Jul 2019 <ref type="figure">Figure 1</ref>: Illustration of different types of auxiliary ads for improving the CTR prediction. We consider 1) contextual ads shown above the target ad, 2) historically clicked ads of the user, and 3) historically unclicked ads of the user. <ref type="table">Table 1</ref>: Each row is an instance for CTR prediction. The first column is the label, where "1" denotes the user clicked the ad and "0" otherwise. Each of the other columns is a field. Instantiation of a field is a feature. (1) We explore three types of auxiliary data for improving the CTR prediction of the target ad. These auxiliary data include contextual ads shown above the target ad on the same page and historically clicked and unclicked ads of the user who views the target ad. (2) We propose DSTNs that effectively fuse these auxiliary data to predict the target CTR. The model is able to learn the interactions between auxiliary data and the target ad, and to emphasize more important hidden information. We make the implementation code publicly available 3 . (3) We conduct extensive offline experiments on three largescale datasets from real advertisement systems to test the performance of DSTNs and several state-of-the-art methods.</p><p>We also conduct case studies to provide further insights behind the model. (4) We have deployed the best-performing DSTN in Shenma Search, which is the second largest search engine in China. We have also conducted online A/B test to evaluate its performance in real-world CTR prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DSTN MODELS</head><p>In this section, we first introduce the CTR prediction problem and then present three variants of DSTN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The task of CTR prediction in online advertising is to build a prediction model to estimate the probability of a user clicking on a specific ad. Each ad instance can be described by multiple fields such as user information ("User ID", "City", "Age", etc.) and ad information ("Creative ID", "Campaign ID", "Title", etc.). The instantiation of a field is a feature. For example, the "User ID" field may contain features such as "2135147" and "3467291". <ref type="table">Table 1</ref> shows some examples. Classical CTR prediction models such as FM <ref type="bibr" target="#b22">[22]</ref>, DNN <ref type="bibr" target="#b31">[31]</ref> and Wide&amp;Deep <ref type="bibr" target="#b4">[5]</ref> mainly consider the target ad (illustrated in <ref type="figure" target="#fig_1">Figure  3</ref>(a)). The focus is on how to represent the ad instance in terms of informative features and on how to learn feature interactions. Differently, in this paper, we explore auxiliary data for improving the CTR prediction. We must address the following issues: 1) how to accommodate all different cases with varying numbers of auxiliary ads; 2) how to distill useful information and suppress noise in auxiliary ads; 3) how to differentiate the contributions of each type of auxiliary ads; 4) how to fuse all the available information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embedding</head><p>Before we introduce any model structure, we first present the embedding process (layer) that is common in all the models below. The embedding process is to first map each feature into an embedding vector and then represent each ad instance as the concatenation of corresponding feature embedding vectors.</p><p>Denote the number of unique features as N . We create an embedding matrix E ∈ R N ×K where each row represents a K-dim embedding vector for a feature. Given a feature index such as i, its embedding is then the i-th row of the matrix E. The embedding matrix E is a variable to be learned during model training.</p><p>We segregate features into the following three groups, which are processed differently according to <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>1) Univalent categorical features. This type of feature contributes only a single value and "User ID" is an example (cf. <ref type="table">Table  1</ref>). If we use the one-hot feature representation, the resulting feature vector is very sparse because the number of unique user IDs can be on the order of 10 8 . We thus map sparse, high-dimensional categorical features to dense, low-dimensional embedding vectors suitable for neural networks. These embeddings also carry richer information than one-hot representations <ref type="bibr" target="#b18">[18]</ref>.</p><p>2) Multivalent categorical features. This type of feature contributes a set of values and the bag of bi-grams in the "Ad Title" is an example (cf. <ref type="table">Table 1</ref>). To illustrate, the bi-grams of title "ABCD" are "AB", "BC" and "CD". As the set cardinality may vary, we first map each value in the set to an embedding vector and then perform sum pooling to generate an aggregated vector of fixed length.</p><p>3) Numerical features. "User Age" is an example of numerical features (cf. <ref type="table">Table 1</ref>). Each numerical feature is first quantized into discrete buckets, and is then represented by the bucket ID. Each bucket ID is mapped to an embedding vector.</p><p>The representation x of an ad instance after the embedding process is the concatenation of all the embedding vectors, each for a field (cf. <ref type="figure" target="#fig_0">Figure 2</ref>). Note that the fields used for each type of ads may be different. For example, the fields for the target ad contain user information such as "User ID" and "Age", while the fields for the clicked ads will not contain such information because these ads are for the same user and duplicated information in unnecessary.</p><p>After embedding, we obtain one embedding vector x t ∈ R D t for the target ad, n c embedding vectors {x ci ∈ R D c } n c i=1 for the corresponding contextual ads, n l embedding vectors {x l j ∈ R D l } n l j=1 for clicked ads, and n u embedding vectors {x uq ∈ R D u } n u q=1 for unclicked ads. D * ( * ∈ {t, c, l, u}) is the vector dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">DSTN -Pooling Model</head><p>Since the numbers n c , n l and n u of auxiliary ads may vary for different target ads, it creates a problem for the deep neural network. Therefore, the first issue we need to solve is to process each type of variable-length auxiliary instances into a fixed-length vector.</p><p>In the DSTN -Pooling model, we use sum pooling to achieve this goal. The model architecture is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b). The aggregated representation vectors x c of n c contextual ads, x l of n l clicked ads and x u of n u unclicked ads are given by</p><formula xml:id="formula_0">x c = n c i=1 x ci , x l = n l j=1 x l j , x u = n u q=1 x uq .</formula><p>If one type of auxiliary ad is completely missing (e.g., no contextual ads at all), we use an all 0 vector as its aggregated representation. Now we have the target representation x t and the aggregated representations x c , x l and x u of auxiliary ads, the next issue is to fuse the information contained in these representations. In particular, we generate the fused</p><formula xml:id="formula_1">representation v ∈ R D v as v = W t x t + W c x c + W l x l + W u x u + b,<label>(1)</label></formula><formula xml:id="formula_2">where W t ∈ R D v ×D t , W c ∈ R D v ×D c , W l ∈ R D v ×D l and W u ∈ R D v ×D u are weight matrices that transform different representa- tions into the same semantic space; b ∈ R D v is a bias parameter.</formula><p>As can be seen, we actually use different weights to fuse the input from different types of data. This property is desired. It is because the degree of influence by different types of auxiliary data to the target ad may be different and we do distinguish such differences. Moreover, the fused representation v has a property that it is not impacted if one or more auxiliary ads are completely missing. For example, if there is no contextual ads at all, we then set x c = 0. As a result, we have W c x c = 0 and thus v is not impacted.</p><p>If we concatenate the representations as</p><formula xml:id="formula_3">m = [x t , x c , x l , x u ], we can rewrite Eq. (1) as v = Wm+b, where W ∈ R D v ×(D t +D c +D l +D u )</formula><p>is the concatenation of all the weight matrices. This much simplifies the model. Therefore, the final design is to first concatenate respective representations to obtain an intermediate representation m and then let m go through several fully connected layers with the ReLU activation function (defined as ReLU(x) = max(0, x)), in order to exploit high-order feature interaction as well as nonlinear transformation. Nair and Hinton <ref type="bibr" target="#b19">[19]</ref> show that ReLU has significant benefits over sigmoid and tanh activation functions in terms of the convergence rate and the quality of obtained results.</p><p>Formally, the fully connected layers are defined as follows:</p><formula xml:id="formula_4">z 1 = ReLU(Wm + b), z 2 = ReLU(W 2 z 1 + b 2 ), · · · z L = ReLU(W L z L−1 + b L ),</formula><p>where L denotes the number of hidden layers; W l and b l denote the weight matrix and bias vector (to be learned) for the lth layer. Finally, the output vector z L goes through a sigmoid function to generate the predicted CTR of the target ad aŝ</p><formula xml:id="formula_5">y = 1 1 + exp[−(w T z L + b)] ,</formula><p>where w and b are the weight and bias parameters to be learned.</p><p>To avoid model overfitting, we apply dropout <ref type="bibr" target="#b25">[25]</ref> after each fully connected layer. Dropout prevents feature co-adaptation by setting to zero a portion of hidden units during parameter learning <ref type="bibr" target="#b9">[9]</ref>. All the model parameters are learned by minimizing the average logistic loss on a training set as</p><formula xml:id="formula_6">loss = − 1 |Y| y ∈Y [y logŷ + (1 − y) log(1 −ŷ)],<label>(2)</label></formula><p>where y ∈ {0, 1} is the true label of the target ad corresponding to the estimated CTRŷ and Y is the collection of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Analysis.</head><p>It is observed that when different target ads are shown at a given position for a given user, only x t varies while all the auxiliary representations x c , x l and x u keep unchanged. It means that the auxiliary representations only serve as static base information. Moreover, as x c , x l and x u are generated by sum pooling, useful information could be easily buried in noise. For example, if the target ad is about coffee but most of the clicked ads are about clothing, then these clicked ads contribute little information to the target ad but the result of sum pooling is clearly dominated by these ads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">DSTN -Self-Attention Model</head><p>Given the above limitations, we consider the attention mechanism <ref type="bibr" target="#b0">[1]</ref> that is firstly introduced in the encoder-decoder framework for the machine translation task. It allows a model to automatically search for parts of a source sentence that are relevant to predicting a target word. In our DSTN -Self-attention model, the attention is applied over the instances of a particular type of auxiliary data to emphasize more important information. Take contextual ads as an example. The aggregated representationx c is modeled aš</p><formula xml:id="formula_7">x c = n c i=1 α ci ({x ci } i )x ci ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_8">α ci = exp(β ci ) n c i ′ =1 exp(β ci ′ )</formula><p>,</p><formula xml:id="formula_9">β ci = f (x ci ). f (·)</formula><p>is a function that transforms the vector representation x ci to a scalar weight β ci . A possible instantiation of the f (·) function could be a Multilayer Perceptron (MLP) <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4.1</head><p>Analysis. This self-attention mechanism has the advantage that useful information can be emphasized and noise can be suppressed because it weights different auxiliary ads x ci differently according to <ref type="bibr" target="#b2">(3)</ref>. Nevertheless, it still has the following limitations: 1) The weight β ci is calculated solely based on the auxiliary ad x ci . It does not capture the relationship between this auxiliary ad and the target ad x t . For example, no matter the target ad x t is about coffee or clothing, the importance of auxiliary ads keeps the same. 2) The normalized weight α ci is calculated based on the relative importance with respect to all the {x ci } n c i=1 and n c i=1 α ci = 1. As a result, even when all the {x ci } n c i=1 are irrelevant to the target ad x t , the final importance α ci is still large due to normalization. 3) The absolute number of each type of auxiliary ads matters, but normalization does not capture such an effect either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">DSTN -Interactive Attention Model</head><p>We thus propose the DSTN -Interactive attention model in this section that avoids the above limitations. It introduces explicit interaction between each type of auxiliary ads and the target ad. The model architecture is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c).</p><p>Take contextual ads as an example. We now model the aggregated representation vector as</p><formula xml:id="formula_10">x c = n c i=1 α ci (x t , x ci )x ci .<label>(4)</label></formula><p>Comparing <ref type="formula" target="#formula_10">(4)</ref> with <ref type="formula" target="#formula_7">(3)</ref>, it is observed that α ci is now a function of both the target ad x t and the auxiliary ad x ci . In this way, α ci dynamically adjusts the importance of x ci with respect to x t . Moreover, α ci does not depend on other {x ci ′ } i ′ i . If none of auxiliary ads is informative, then all the α ci should be small. We obtainx l andx u for clicked ads and unclicked ads similarly. We learn the weight α ci through an MLP with one hidden layer and the ReLU activation function as <ref type="bibr" target="#b4">(5)</ref> where h, W tc , b tc1 and b tc2 are model parameters.</p><formula xml:id="formula_11">α ci (x t , x ci ) = exp(h T ReLU(W tc [x t , x ci ] + b tc1 ) + b tc2 ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Analysis. In this model, the fused representation</head><formula xml:id="formula_12">v is v = W t x t + W cxc + W lxl + W uxu + b.<label>(6)</label></formula><p>Comparing <ref type="formula" target="#formula_12">(6)</ref> with <ref type="formula" target="#formula_1">(1)</ref>, it is observed that the auxiliary representations do not serve as static base information now, but dynamically change when the target ad changes. It is because the weights in (4) depends on the target ad as well. It means that the model adaptively distills more useful information in auxiliary data with respect to the target ad. For example, the clicked ads of a user are about coffee, clothing and car. When the target ad x t is about coffee, the clicked ad of coffee should contribute more tox l ; but when the target ad is about car, the clicked ad of car should contribute more tox l . The model also preserves the property that it uses different weights to fuse the input from different types of auxiliary data. Furthermore, the weight α ci in this model is not compared with other auxiliary ads, avoiding the problems that normalization would cause. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OFFLINE EXPERIMENTS</head><p>In this section, we conduct experiments on three large-scale datasets to evaluate the performance of the proposed DSTNs as well as several state-of-the-art methods for CTR prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The statistics of the datasets are listed in <ref type="table" target="#tab_1">Table 2</ref>. It is observed that the number of distinct features can be up to 46 million. 1) Avito advertising dataset. This is the dataset used for the Avito context ad clicks competition 4 . Note that the "context ads" here are different from the contextual ads that we bring forward in this paper. In Avito, "context ads" refer to ads tailored to the user and the context (such as device and time), in contrast to "regular ads" that are ordered by the time when they are added and "highlighted ads" that are stuck to the top of a page for some period of time.</p><p>This dataset contains a random sample of ad logs from avito.ru, the largest general classified website in Russia. We use the ad logs from 2015-04-28 to 2015-05-18 for training, those on 2015-05-19 for validation, and those on 2015-05-20 for testing. The features used include 1) ad features such as ad ID, ad title, ad category and ad parent category, 2) user features such as user ID, IP ID, user agent, user agent OS and user device, and 3) query features such as search query, search location, search category and search parameters.</p><p>2) Search advertising dataset. This dataset contains a random sample of ad impression and click logs from a commercial search advertising system in Alibaba. We use ad logs of 7 consecutive days in June 2018 for training, logs of the next day for validation, and logs of the day after the next day for testing. The features used include 1) ad features such as ad title, ad ID and industry, 2) user features such as user ID, IP ID and user agent, and 3) query features such as query and search location.</p><p>3) News feed advertising dataset. This dataset contains a random sample of ad impression and click logs from a commercial news feed advertising system in Alibaba. We use ad logs of 7 consecutive days in July 2018 for training, logs of the next day for validation, and logs of the day after the next day for testing. The features used include 1) ad features such as ad title, ad ID and industry, 2) user features such as user ID and the number of matched ad topics, and 3) cross-features such as AdType-AdResource. The auxiliary data in this dataset do not contain contextual ads. This is because only one ad is shown on a page in our news feed advertising system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methods Compared</head><p>We compare the following methods for CTR prediction.</p><p>(1) LR. Logistic Regression <ref type="bibr" target="#b1">[2]</ref>. It is a generalized linear model.</p><p>(2) FM. Factorization Machine <ref type="bibr" target="#b22">[22]</ref>. It models both first-order feature importance and second-order feature interactions.  <ref type="bibr" target="#b31">[31]</ref> and Productbased Neural Network (PNN) <ref type="bibr" target="#b21">[21]</ref>. (6) CRF. The Conditional Random Field-based method in <ref type="bibr" target="#b29">[29]</ref>.</p><p>It considers both the features of an ad and its similarity to the surrounding ads. The predicted log odds of CTR is given by w T x − 0.5βs, where w and β are model parameters, x is the feature vector of the target ad, and s is the sum of similarity to the surrounding ads. The similarity is manually defined on strings in ad titles and ad descriptions. CRF is somewhat impractical because in commercial advertising systems, one cannot know ads below a target ad in advance. Therefore, we only use contextual ads (i.e., ads above the target ad) as the surrounding ads. (7) GRU. The Gated Recurrent Unit <ref type="bibr" target="#b6">[6]</ref>, one of the most advanced Recurrent Neural Networks (RNNs). It has been shown to be able to avoid gradient vanishing and explosion problems and to result in better performance than the vanilla RNN. GRU is based on the RNN model proposed in <ref type="bibr" target="#b32">[32]</ref>. It utilizes the sequence of clicked ads of a user. Among these methods, CRF, GRU and DSTNs consider auxiliary ads, while all other methods focus on the target ad. In particular, CRF considers surrounding ads, GRU considers clicked ads, and DSTNs consider contextual ads, clicked ads and unclicked ads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter Settings</head><p>We set the dimension of the embedding vectors for each feature as 10, because the number of distinct features is huge. We set the number of fully connected layers in DNN, Wide&amp;Deep, DeepFM, GRU and DSTNs as 2, each with dimensions 512 and 256. The dropout ratio is set to 0.5. The hidden dimension of GRU is set to 128. The f (·) function in DSTN-S is an MLP with one hidden layer, with dimension 128. The dimension of h in DSTN-I is also set to  128. All the methods are implemented in Tensorflow and optimized by the Adagrad algorithm <ref type="bibr" target="#b8">[8]</ref>. We set the batch size as 128.</p><p>We use a user's historical behavior in recent 3 days. To reduce the memory requirement, we further restrict that n c ≤ 5, n l ≤ 5 and n u ≤ 5. That is, if there are no more than 5 clicked ads, we use them all; otherwise, we use the most recent 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Metrics</head><p>We use the following metrics to evaluate the compared methods.</p><p>(1) AUC: the Area Under the ROC Curve over the test set. It is a widely used metric for CTR prediction tasks. It reflects the probability that a model ranks a randomly chosen positive instance higher than a randomly chosen negative instance. The larger the better. A small improvement in the AUC is likely to lead to a significant increase in the online CTR <ref type="bibr" target="#b4">[5]</ref>. (2) Logloss: the value of Eq. (2) over the test set. The smaller the better. CRF performs much better than LR, because it can be considered as rectifying the LR prediction by a term which summarizes the similarity to surrounding ads. However, the similarity is manually defined based on raw strings, thus suffering from the semantic gap problem. GRU performs better than LR, FM, DNN, Wide&amp;Deep and DeepFM on two datasets, because GRU additionally utilizes clicked ads. The improvement is the most obvious on the News Feed ad dataset. This is because users do not submit a query in news feed ads and historical behaviors are quite informative. It is also observed that DSTN-P outperforms GRU. The reasons are two-fold. First, consecutive actions in a user's behavior sequence may not be well correlated. For example, a user clicked an ad of toothpaste one month ago and clicked ads of snacks and coffee recently. The next clicked ad may be about toothpaste again rather than food, simply because of the need of the user, rather than the    correlation with the preceding clicked ads. Therefore, considering the order of a user's behavior may not necessarily help improve the prediction performance. Second, DSTN-P can additionally utilize the information in contextual ads and unclicked ads. When we compare different variants of DSTNs, it is observed that DSTN-S performs better than DSTN-P, and DSTN-I further outperforms DSTN-S. These results show that self-attention can better emphasize useful information than simple sum pooling. The interactive attention mechanism explicitly introduces the interaction between the target ad and auxiliary ads, and can thus adaptively distill more relevant information than self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effectiveness</head><p>It is also observed that Logloss is not necessarily correlated with AUC. Nevertheless, DSTN-I also results in the smallest Logloss on different datasets, showing its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Effect of the Type of Auxiliary Data</head><p>In order to examine the effect of different types of auxiliary data, we feed only contextual ads, clicked ads or unclicked ads to DSTN-I. To quantify the effect, we define and compute the following two metrics: absolute AUC improvement (AbsImp) and normalized AUC improvement (NlzImp): where ctxt is short for contextual. AbsImp considers the overall AUC improvement and NlzImp normalizes the effect to each auxiliary ad. We care about the absolute rather than the relative AUC improvement because in industrial practice the former is more meaningful and indicative. The results are plotted in <ref type="figure" target="#fig_6">Figure 4</ref>. It is observed in <ref type="figure" target="#fig_6">Figure 4</ref>(a) that the overall effect of different types of auxiliary data varies on different datasets. Contextual ads achieve the highest AbsImp on the Avito dataset, while unclicked ads achieve the highest AbsImp on the Search ad dataset. From <ref type="figure" target="#fig_6">Figure 4(b)</ref>, it is interesting to observe that once normalized, the power of a contextual or clicked ad is much higher than that of an unclicked ad. This complies with intuitions because a contextual ad may distract a user's attention and a clicked ad usually reflects a user's interest. In contrast, an unclicked ad is much noisy. It may indicate that the user is not interested in the ad or the user does not view the ad. <ref type="figure" target="#fig_8">Figure 5</ref> plots the test AUC of DSTN-I vs. the number of fully connected layers. The settings are: 1 layer -256 dimensions; 2 layers -512 and 256 dimensions; 3 layers -1024, 512 and 256 dimensions. It is observed that increasing the number of fully connected layers can improve the AUC in the beginning, but the benefit diminishes when more layers are added. Adding more layers may even result in slight performance degradation, possibly due to more model parameters and increased difficulty of training deeper neural networks. <ref type="figure" target="#fig_10">Figure 6</ref> illustrates the visualization of ad embeddings with t-SNE <ref type="bibr" target="#b16">[16]</ref> learned by DNN and by DSTN-I. It is based on 20 sub-categories from 5 major categories (electronics, clothing, furniture, computers and personal care). We randomly pick 100 ads in each sub-category. Different colors represent different sub-categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Effect of the Network Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Visualization of Ad Embeddings</head><p>It is observed that the embeddings learned by both methods show clear clusters, each representing a group of similar ads. Nevertheless, DNN mixes up a portion of "iPhone" vs. "Samsung phone", "beds" vs. "cabinets", and "dresses" vs. "footwear". In contrast, DSTN-I learns sharper clusters and clearly distinguishes different sub-categories. These results demonstrate that DSTN-I can learn more representative embeddings by the aid of auxiliary ads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Visualization of Attention Weights</head><p>In this section, we examine the attention weights of auxiliary ads in DSTN-I through several case studies on the Avito dataset. We examine each type of auxiliary ads separately because it is hard   to find a case containing sufficient ads of all types. For ease of illustration, we sort the auxiliary ads by their semantic similarity to the target ad. The leftmost is the most dissimilar and the rightmost is the most similar.</p><p>Contextual ads. In <ref type="figure" target="#fig_11">Figure 7</ref>, the target ad is about a YotaPhone. Three contextual ads are shown, which are about a phone lens, a Samsung phone and a HTC phone. It is observed that the weights of the two phone ads do not differ much (around 0.6), but the weight of the lens ad (most dissimilar) is much higher (over 0.8). Such an observation complies with the analysis in <ref type="bibr" target="#b29">[29]</ref>, where the authors find that the more similar the surrounding ads are to an ad, the lower the CTR of the ad is. This is because similar ads can distract a user's attention since all these ads offer similar products or services. In contrast, a dissimilar ad can help make the target ad more notable and is thus assigned a larger weight by DSTN-I.</p><p>Clicked ads. In <ref type="figure" target="#fig_3">Figure 8</ref>, the target ad is about monopod and remote for self-portrait photograph. The first clicked ad is about a baby car seat, which is clearly not relevant to the target ad. Its weight is 0.5223. The second clicked ad is about a flash light, which is an accessory of digital cameras for photography; its weight is much higher (0.7057). The third click ad is about a tripod which is more similar to a monopod, and thus the weight is even higher (0.8449). Finally, the fourth clicked ad is also about a monopod and its weight is the highest (0.9776). These observations show that the more similar a clicked ad is to the target ad, the higher its weight during aggregation. This is because if a user has clicked a similar ad, it is likely that the user will click the target ad as well. Unclicked ads. In <ref type="figure" target="#fig_13">Figure 9</ref>, the target ad is about a Sony camera kit. The four unclicked ads are about a bike, a monopod, a camera lens and a camera kit respectively. These ads have increased similarity to the target ad and the corresponding weights also increase. These observations show that the more similar an unclicked ad is to the target ad, the higher its weight during aggregation. It is because if a user does not click a similar ad in the past, it is likely that the user will also not click the target ad as well.</p><p>Comparing <ref type="figure" target="#fig_13">Figure 9</ref> with <ref type="figure" target="#fig_3">Figure 8</ref>, it is interesting to observe that the average weight of unclicked ads are much smaller than that of clicked ads, even when an unclicked ad is quite similar to the target ad. This is because clicked ads reflect possible user preferences while unclicked ads are much more ambiguous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM DEPLOYMENT AND EVALUATION</head><p>In this section, we introduce the deployment of our advertising system and present the online evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deployment</head><p>We deployed DSTN-I in Shenma Search, the second largest search engine in China. We call the model DSTN hereafter for simplicity. <ref type="figure" target="#fig_14">Figure 10</ref> depicts the architecture of the system, which contains an offline phase, a streaming phase and an online phase.</p><p>• Offline phase: Online user behaviors (ad impression / click) are continuously logged into the user log database. The system exacts training data from the log database and trains the DSTN model. The offline training is first performed in a batch manner, and the resulting model is then updated incrementally and periodically using recent log data. • Streaming phase: Online user behaviors are also sent to the User Session Server (with a delay of no more than 10 seconds), where a hashmap for each user is maintained and updated. To reduce the memory requirement and the online computation load, the hashmap records at most 5 clicked ads and 5 unclicked ads in the recent 3 days for each user. • Online phase: Once a user request is sent, the Ad Server first retrieves the user history data from the User Session Server. The Ad Server then requests the Model Server for pCTR (predicted CTR) of a set of candidate ads. This is done in several steps as shown in <ref type="figure" target="#fig_15">Figure 11</ref>. The steps are: 1 ○ The Ad Server sends the set of candidate target ads, along with the clicked and unclicked ads of the given user to the Model Server. There is no contextual ad now. 2 ○ The Model Server returns the pCTRs. 3 ○ The Ad Server picks out the target ad with the highest score based on certain strategy (which depends on the pCTR). Assume this ad is the Target ad 2. Then it becomes the contextual ad for other target ads. The Ad Server then sends the remaining target ads, along with the contextual ad, the clicked and unclicked ads of the user to the Model Server. <ref type="bibr" target="#b3">4</ref> ○ The Model Server returns the pCTRs for the remaining candidate ads.</p><p>Theoretically, Steps 3 ○ and 4 ○ could be performed several times to pick out ads one by one and to update contextual ads sequentially. However, there exists a tradeoff between prediction accuracy and service delay. Therefore, in our current implementation, we only perform Steps 3 ○ and 4 ○ once. After Step 4 ○, the Ad Server picks out 2-3 remaining candidate ads with highest scores and sends the final ad list to the user. Shenma Search now holds 3-4 ad slots per mobile page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Online A/B Test</head><p>We conducted online experiments in an A/B test framework over two weeks in Jan. 2019. The benchmark model is Wide&amp;Deep, which is our last online serving model. Our online evaluation metric is the real CTR, which is defined as the number of clicks over the number of ad impressions. A larger online CTR indicates the enhanced effectiveness of a CTR prediction model. We observe that DSTN outperforms Wide&amp;Deep consistently, resulting in an increase of daily online CTR from 5.13% to 9.72%. The average CTR increase is 6.92%. These results demonstrate the effectiveness of DSTN in practical CTR prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>CTR prediction. Learning the effect of feature interactions seems to be crucial for accurate CTR prediction <ref type="bibr" target="#b22">[22]</ref>. Generalized linear models, such as Logistic Regression (LR) <ref type="bibr" target="#b23">[23]</ref> and Follow-The-Regularized-Leader (FTRL) <ref type="bibr" target="#b17">[17]</ref>, have shown decent performance in practice. However, a linear model lacks the ability to learn sophisticated feature interactions <ref type="bibr" target="#b3">[4]</ref>. Factorization Machines (FMs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">22]</ref> are proposed to model pairwise feature interactions in terms of the latent vectors corresponding to the involved features. Fieldaware FM <ref type="bibr" target="#b14">[14]</ref> and Field-weighted FM <ref type="bibr" target="#b20">[20]</ref> further improve FM by considering the impact of the field that a feature belongs to.</p><p>In recent years, Deep Neural Networks (DNNs) have shown powerful ability of automatically learning informative feature representations <ref type="bibr" target="#b15">[15]</ref>. DNNs are thus also exploited for CTR prediction and item recommendation in order to automatically learn feature representations and high-order feature interactions <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>. Factorization-machine supported Neural Network (FNN) <ref type="bibr" target="#b31">[31]</ref> pretrains an FM before applying a DNN. Product-based Neural Network (PNN) <ref type="bibr" target="#b21">[21]</ref> introduces a product layer between the embedding layer and the fully connected layer. The Wide&amp;Deep model <ref type="bibr" target="#b4">[5]</ref> combines LR and DNN to capture both low-and high-order feature interactions. Such a structure also improves both the memorization and generalization abilities of the model. DeepFM <ref type="bibr" target="#b10">[10]</ref> models loworder feature interactions like FM and models high-order feature interactions like DNN. Neural Factorization Machine <ref type="bibr" target="#b11">[11]</ref> combines the linearity of FM and the non-linearity of neural networks.</p><p>Exploiting auxiliary data for CTR prediction. Another line of research exploits auxiliary data for improving the CTR prediction performance. Zhang et al. <ref type="bibr" target="#b32">[32]</ref> consider users' historical behaviors (e.g., what ads she clicked). They use Recurrent Neural Networks (RNNs) to model the dependency on users' sequential behaviors. Tan et al. <ref type="bibr" target="#b26">[26]</ref> propose improved RNNs for session-based recommendations. One major problem with RNN-based models is that it generates an overall embedding vector of a behavior sequence, which can only preserve very limited information of a user. Longterm dependencies are still hard to be preserved even using the advanced memory cell structures like Long Short-Term Memory (LSTM) <ref type="bibr" target="#b13">[13]</ref> and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[6]</ref>. Moreover, both the offline training and the online prediction process of RNNs are time-consuming, due to its recursive structure.</p><p>Xiong et al. <ref type="bibr" target="#b29">[29]</ref> consider the pairwise relationship between ads shown on the same page and propose a Conditional Random Field (CRF)-based model for CTR prediction. Yin et al. <ref type="bibr" target="#b30">[30]</ref> consider various contextual factors such as ad depth, query diversity and ad interaction for click modeling. One major problem of these models is that the vertex and edge feature functions need to be manually defined based on data analysis and it is difficult to generalize the model to other types of data.</p><p>Differences. DSTNs proposed in this paper differ from prior work in that: 1) DSTNs integrate heterogeneous auxiliary data (i.e., contextual, clicked and unclicked ads) in a unified framework, while the RNN-based model <ref type="bibr" target="#b32">[32]</ref> cannot utilize contextual and unclicked ads, and the CRF-based model <ref type="bibr" target="#b29">[29]</ref> cannot incorporate clicked and unclicked ads; 2) DSTNs are not based on RNNs and they are much easier to implement and are much faster to train and evaluate online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we address the problem of CTR prediction in online advertising systems. In contrast to classical CTR prediction models that focus on the target ad, we explore three types of auxiliary data (i.e., contextual, clicked and unclicked ads) and propose DSTNs for improving the CTR prediction. DSTNs are able to distill useful information in auxiliary ads and to fuse heterogeneous data in a unified framework. Offline experimental results on three largescale datasets demonstrate the effectiveness of DSTNs over several state-of-the-art methods. Case studies show that DSTN-I is able to learn representative ad embeddings and meaningful attention weights. We have deployed DSTN-I in Shenma Search. Online A/B test results show that the online CTR is also improved compared to our last serving model, demonstrating the effectiveness of DSTN-I in real-world CTR prediction tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the embedding process. The representation x of an ad is the concatenation of all the embedding vectors, each for a field. (cate. -categorical, ft. -feature)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of model architectures (fts -features). (a) DNN model, which considers only the target ad. (b) DSTN -Pooling model, which further considers auxiliary ads. The aggregation of each type of auxiliary ads is by sum pooling. (c) DSTN -Interactive attention model, which introduces explicit interaction between the auxiliary ads and the target ad.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4 https://www.kaggle.com/c/avito-context-ad-clicks/data (3) DNN. Deep Neural Network. Each target ad first goes through an embedding layer and then goes through several fully connected layers. Finally, an output layer predicts the CTR through a sigmoid function. (4) Wide&amp;Deep. The Wide&amp;Deep model in [5]. It combines LR (wide part) with DNN (deep part). (5) DeepFM. The DeepFM model in [10]. It combines FM (wide part) with DNN (deep part), and shares the same input and the embedding vector in its wide part and deep part. DeepFM has been shown to outperform Wide&amp;Deep, Factorizationmachine supported Neural Network (FNN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 8 )</head><label>8</label><figDesc>DSTN-P. The DSTN -Pooling model proposed in Section 2.3. It uses sum pooling to aggregate auxiliary data. (9) DSTN-S. The DSTN -Self-attention model proposed in Section 2.4. It uses self-attention to aggregate auxiliary data. (10) DSTN-I. The DSTN -Interaction attention model proposed in Section 2.5. It introduces explicit interaction between auxiliary data and the target ad.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FM</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Absolute and normalized AUC improvement: DSTN-I with only one type of auxiliary data, compared with DNN (cf. §3.6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Test AUC vs. the number of fully connected layers of DSTN-I on Avito and News Feed ad datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>AbsImp(ctxt) = AUC(DSTN-I with ctxt ads only) − AUC(DNN), NlzImp(ctxt) = AbsImp(ctxt) Avgerage number of ctxt ads per target ad ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Embeddings learned by DNN and by DSTN-I. Each color represents a sub-category of ads (cf. §3.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Attention weights α c of contextual ads. The more similar to the target ad, the smaller the weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Attention weights α l of clicked ads. The more similar to the target ad, the larger the weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Attention weights α u of unclicked ads. The more similar to the target ad, the larger the weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Architecture of the online advertising system. The deployed DSTN model is DSTN-I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Illustration of how the Ad server requests pCTRs from the Model server. Please refer to §4.1 for more detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of experimental datasets. (avg -average, ctxt -contextual, pta -per target ad) Dataset # Target ads # Fields # Features Avg # ctxt ads pta Avg # clicked ads pta Avg # unclicked ads pta</figDesc><table><row><cell>Avito</cell><cell>11,211,794</cell><cell>27</cell><cell>42,301,586</cell><cell>0.9633</cell><cell>0.4595</cell><cell>4.6739</cell></row><row><cell>Search</cell><cell>15,007,303</cell><cell>20</cell><cell>46,529,832</cell><cell>0.4456</cell><cell>0.7729</cell><cell>3.2840</cell></row><row><cell>News Feed</cell><cell>1,661,588</cell><cell>41</cell><cell>6,259,571</cell><cell>N/A</cell><cell>0.8966</cell><cell>2.8853</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test AUC and Logloss on three datasets.</figDesc><table><row><cell></cell><cell>Avito</cell><cell>Search</cell><cell>News Feed</cell></row><row><cell cols="4">Algorithm AUC Logloss AUC Logloss AUC Logloss</cell></row><row><cell>LR</cell><cell cols="3">0.7556 0.05918 0.7914 0.5372 0.6098 0.4122</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>lists the AUC and Logloss values of different methods. It is observed that Wide&amp;Deep achieves higher AUC than LR and DNN. Similarly, DeepFM achieves higher AUC than FM and DNN. These results show that combining a wide component and a deep component can improve the prediction power of individual models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/oywtece/dstn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Higher-order factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and scalable response prediction for display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romer</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">DLRS. ACM</title>
		<imprint>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<title level="m">Maxout networks. In ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepfm: a factorization-machine based neural network for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADKDD. ACM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golovin</surname></persName>
		</author>
		<editor>KDD. ACM</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfonso</forename><forename type="middle">Lobos</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjun</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep crossing: Web-scale modeling without manually crafted combinatorial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
		<editor>KDD. ACM</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved recurrent neural networks for session-based recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxing</forename><surname>Yong Kiam Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DLRS. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relational click prediction for sponsored search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM. ACM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="493" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting contextual factors for click modeling in sponsored search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shike</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM. ACM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1369" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<editor>KDD. ACM</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
