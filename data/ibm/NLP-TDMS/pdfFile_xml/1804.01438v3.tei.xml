<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Discriminative Features with Multiple Granularities for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-10-22">2018. October 22-26. 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
							<email>guanshuo.wang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CloudWalk Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CloudWalk Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">CloudWalk Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
							<email>zhouxi@cloudwalk.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">CloudWalk Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><forename type="middle">Zhou</forename></persName>
						</author>
						<title level="a" type="main">Learning Discriminative Features with Multiple Granularities for Person Re-Identification</title>
					</analytic>
					<monogr>
						<title level="m">2018 ACM Multimedia Conference (MM &apos;18), October *Equal contribution. MM &apos;18</title>
						<meeting> <address><addrLine>Seoul, Republic of Korea; Seoul</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="22" to="26"/>
							<date type="published" when="2018-10-22">2018. October 22-26. 2018. 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3240508.3240552</idno>
					<note>KEYWORDS Republic of Korea. ACM, New York, NY, USA, 9 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Object identification</term>
					<term>Learn- ing to rank</term>
					<term>Supervised learning by classification</term>
					<term>Image rep- resentations</term>
					<term>Neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The combination of global and partial features has been an essential solution to improve discriminative performances in person re-identification (Re-ID) tasks. Previous part-based methods mainly focus on locating regions with specific pre-defined semantics to learn local representations, which increases learning difficulty but not efficient or robust to scenarios with large variances. In this paper, we propose an end-to-end feature learning strategy integrating discriminative information with various granularities. We carefully design the Multiple Granularity Network (MGN), a multi-branch deep network architecture consisting of one branch for global feature representations and two branches for local feature representations. Instead of learning on semantic regions, we uniformly partition the images into several stripes, and vary the number of parts in different local branches to obtain local feature representations with multiple granularities. Comprehensive experiments implemented on the mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03 indicate that our method robustly achieves state-of-the-art performances and outperforms any existing approaches by a large margin. For example, on Market-1501 dataset in single query mode, we obtain a top result of Rank-1/mAP=96.6%/94.2% with this method after re-ranking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Body part partitions from coarse to fine granularities. We regard original pedestrian images with the whole body as the coarsest level of granularity in the left column. The middle and right column are respectively pedestrian partitions divided into 2 and 3 stripes from the original images. The more stripes images are divided into, the finer the granularity of partitions is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Person re-identification (Re-ID) is a challenging task to retrieve a given person among all the gallery pedestrian images captured across different security cameras. Due to the scene complexity of images from surveillance videos, the main challenges for person Re-ID come from large variations on persons such as pose, occlusion, clothes, background clutter, detection failure, etc. The prosperity of deep convolutional network has introduced more powerful representations with better discrimination and robustness for pedestrian images, which pushed the performance of Re-ID to a new level. Some recent deep Re-ID methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref> have achieved breakthrough with high-level identification rates and mean average precision.</p><p>The intuitive approach of pedestrian representations is to extract discriminative features from the whole body on images. The aim of global feature learning is to capture the most salient clues of appearance to represent identities of different pedestrians. However, high complexities for images captured in surveillance scenes usually restrict the accuracy for feature learning in large scale Re-ID scenarios. Due to the limited scale and weak diversity of person Re-ID training datasets, some non-salient or infrequent detailed information can be easily ignored and make no contribution for better discrimination during global feature learning procedure, which makes global features hard to adapt similar inter-class common properties or large intra-class differences.</p><p>To relieve this dilemma, locating significant body parts from images to represent local information of identities has been confirmed to be an effective approach for better Re-ID accuracy in many previous works. Each located body part region only contains a small percentage of local information from the whole body, and at the same time distraction by other related or unrelated information outside the regions is actually filtered by locating operations, with which local features can be learned to concentrate more on identities and used as an important complement for global features. Part-based methods for person Re-ID can be divided into three main pathways according to their part locating methods: 1) Locating part regions with strong structural information such as empirical knowledge about human bodies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref> or strong learning-based pose information <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44]</ref>; 2) Locating part regions by region proposal methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref>; 3) Enhancing features by middle-level attention on salient partitions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref>. However, obvious limitations impede the effectiveness of these methods. First, pose or occlusion variations can affect the reliability of local representation. Second, these methods almost only focus on specific parts with fixed semantics, but cannot cover all the discriminative information. Last but not least, most of these methods are not end-to-end learning process, which increases the complexity and difficulty of feature learning.</p><p>In this paper, we propose a feature learning strategy combining global and local information in different granularities. As shown in <ref type="figure">Figure 1</ref>, various numbers of partition stripes introduce a diversity of content granularity. We define the original image containing only one whole partition with global information as the coarsest case, and as the number of partitions increase, features of local parts can concentrate more on finer discriminative information in each part stripe, filtering information on the other stripes. Since deep learning mechanism can capture approximate response preferences on the main body from the whole image, it is also possible to capture more fine-grained saliency for local features extracted from smaller part regions. Notice that these part regions are not necessary to be located partitions with specific semantics, but only a piece of equally-split stripe on the original images. From the observation, we find that the granularity of discriminative responses indeed becomes finer as the number of horizontal stripes increases. Based on this motivation, we design the Multiple Granularity Network (MGN), a multi-branch network architecture divided into one global and two local branches with delicated parameters from the 4th residual stage of the ResNet-50 <ref type="bibr" target="#b12">[13]</ref> backbone. In each local branch of MGN, we divide globally-pooled feature maps into different numbers of stripes as part regions to learn local feature representations independently, referring the methods in <ref type="bibr" target="#b35">[36]</ref>.</p><p>Comparing to the previous part-based methods, our method only utilize equally-divided parts for local representation, but can achieve outstanding performance exceeding all previous methods. Besides, our method is completely a end-to-end learning process, which is easy for learning and implementation. Extensive experiment results show that our method can achieve state-of-the-art performances on several mainstream Re-ID datasets, even with Figure 2: Feature response maps in different granularities extracted from the last output of different models. The response intensity is calculated by the L2-norm of feature vectors from all the spatial locations. Middle Column: a pedestrian image. Left Column: global response map by IDE embedding. Right Column: three local response maps corresponding to three split stripes of the origin image, extracted by part-based model. Best viewed in color.</p><p>settings without any additional external data or re-ranking <ref type="bibr" target="#b49">[50]</ref> operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>With the prosperity of deep learning, feature learning by deep networks has become a common practice in person Re-ID tasks. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref> first introduce deep siamese network architecture into Re-ID and combine the body part feature learning, achieving higher performances comparing to the contemporary hand-crafted methods. <ref type="bibr" target="#b46">[47]</ref> proposes ID-discriminative Embedding (IDE) with simple ResNet-50 backbones as a baseline of the performance level for modern deep Re-ID systems. A number of methods are proposed to improve the performance for deep person Re-ID. In <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37]</ref>, midlevel features of image pairs are computed to depict interrelation of local parts with carefully designed mechanism. <ref type="bibr" target="#b38">[39]</ref> introduces Domain Guided Dropout to enhance the generalization ability across different domains of pedestrian scenarios. <ref type="bibr" target="#b49">[50]</ref> brings re-ranking strategy into Re-ID tasks to modify the ranking results for accuracy improvement.</p><p>Recently some deep Re-ID methods pushed the performances to a new level comparing to the former systems. <ref type="bibr" target="#b42">[43]</ref> introduces a part-based alignment matching in training phase with shortest path programming and mutual learning to improve metric learning performance. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref> both equally slice the feature maps of input images into several stripes in vertical orientation. <ref type="bibr" target="#b2">[3]</ref> merges slices of local features with LSTM network and combine with global features learned from classification metric learning. Instead <ref type="bibr" target="#b35">[36]</ref> directly concatenates the features from local parts as the final representation, and applies refined part pooling to modify the mapping validation of part features. However, according to the report in <ref type="bibr" target="#b42">[43]</ref>, these systems just achieve similar performances as human, which we still need a highway to surpass.  Among all the strategies for performance improvement, we argue that combining the local representations from parts of images is the most effective. As mentioned in Section 1, we summarize three main pathways for part-based learning: determining regions according to structural information about human body, locating body parts by region proposal methods and enhancing features by spatial attention. In <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref>, images are all split into several stripes in horizontal orientation according to intrinsic human body structure knowledge, on which local feature representations are learned. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44]</ref> utilize structural information of body landmarks predicted by pose estimation methods to crop more accurate region areas with semantics. To locate semantic partitions without strongly learning-based predictors, region proposal methods such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> are employed in some part-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref>. Attention information can be a powerful complement for discrimination, which are enhanced in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. In our proposed method, we only use simple horizontal stripes as part regions for local feature learning but achieve outstanding performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-50 backbone (Before res_conv4_2)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part of res_conv4x&amp;conv5x</head><p>Loss functions are used as supervisory signals in feature learning. In the training phase for deep Re-ID systems, the most common loss functions are classification losses and metric losses. Softmax loss is almost the only choice of classification loss function for its strong robustness to various kinds of multi-class classification tasks, which can be used individually <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref> or combined with other losses <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43]</ref> in embedding learning procedures for Re-ID. For metric losses used in embedding learning for Re-ID, there are more variants with different ranking metrics. Contrastive loss <ref type="bibr" target="#b11">[12]</ref> is commonly used in siamese-liked networks <ref type="bibr" target="#b36">[37]</ref>, which focuses on maximizing the distances between inter-class pairs and minimizing that between intra-class pairs. Triplet loss <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref> enforces a margin between the intra and inter distances to the same anchor sample with in a triplet. Based on triplet loss, many variants <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref> are proposed to solve learning or performance issues in metric learning. We employ a setting of joint learning with both softmax and triplet losses in our proposed method. <ref type="figure">Figure 2</ref> shows feature response maps of a certain image extracted from the IDE baseline model <ref type="bibr" target="#b46">[47]</ref> and a part-based model based on IDE. We can observe that even if no explicit attention mechanisms are imposed to enhance the preferences to some salient components, the deep network can still learn the preliminary distinction of response preferences on different body parts according to their inherent semantic meanings. However, to eliminate the distraction of unrelated patterns in pedestrian images with high complexity, higher responses are just concentrated on the main body of pedestrians instead of any concrete body parts with semantic patterns. As we narrow the area of represented regions and train as a classification task to learn local features, we can observe that responses on the local feature maps starts to cluster on some salient semantic patterns, which also varies with the sizes of represented regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTIPLE GRANULARITY NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Branch Part No. Map Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dims</head><p>Feature <ref type="table">Table 1</ref>: Comparison of the settings for three branches in MGN. Notice that the size of input images is set to 384 × 128. "Branch" refers to the name of branches. "Part No." refers to the number of partitions on feature maps. "Map Size" refers to the size of output feature maps from each branch. "Dims" refers to the dimensionality and number of features for the output representations. "Feature" means the symbols for the output feature representations.</p><formula xml:id="formula_0">Global 1 12 × 4 256 f G д Part-2 2 24 × 8 256*2+256 {f P 2 p i | 1 i =0 }, f P 2 д Part-3 3 24 × 8 256*3+256 {f P 3 p i | 2 i =0 }, f P 3 д</formula><p>This observation reflects a relationship between the volume of image contents, i.e. the granularity of regions, and capability of deep networks to focus on specific patterns for representations. We believe this phenomenon comes from the limitation of information in restricted regions. In general, comparing from a global image, it is intuitively hard to discriminate the identities for pedestrians from a local part region. Supervising signals of the classification task enforce the features to be correctly classified as the target identity, which also push the learning procedure trying to explore useful fine-grained details among limited information. Actually, local feature learning in previous part-based methods only introduces a basic granularity diversity of partitions to the total feature learning procedure with or without empirical prior knowledge. Assume that there are appropriate levels of granularities, details with most discriminative information might be almost concentrated by deep networks. Motivated by the observation and analysis above, we propose the Multiple Granularity Network (MGN) architecture to combine global and multi-granularity local feature learning for more powerful pedestrian representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The architecture of Multiple Granularity Network is shown in <ref type="figure">Figure 3</ref>. The backbone of our network is ResNet-50 which helps to achieve competitive performances in some Re-ID systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref>. The most obvious modification different from the original version is that we divide the subsequent part after res_conv4_1 block into three independent branches, sharing the similar architecture with the original ResNet-50. <ref type="table">Table 1</ref> lists the settings of these branches. In the upper branch, we employ down-sampling with a stride-2 convolution layer in res_conv5_1 block, following a global max-pooling (GMP) <ref type="bibr" target="#b1">[2]</ref> operation on the corresponding output feature map and a 1 × 1 convolution layer with batch normalization <ref type="bibr" target="#b16">[17]</ref> and ReLU to reduce 2048-dim features z G д to 256-dim f G д . This branch learns the global feature representations without any partition information, so we name this branch as the Global Branch.</p><p>The middle and lower branches both share the similar network architecture with Global Branch. The difference is that we employ no down-sampling operations in res_conv5_1 block to preserve proper areas of reception fields for local features, and output feature maps in each branch are uniformly split into several stripes in horizontal orientation, on which we independently perform the same following operations as Global Branch to learn local feature representations. We call these branches Part-N Branch, where N refers to the number of partitions on the unreduced feature maps, e.g. the middle and lower branches in <ref type="figure">Figure 3</ref> can be named as Part-2 and Part-3 Branch.</p><p>During testing phases, to obtain the most powerful discrimination, all the features reduced to 256-dim are concatenated as the final feature, combining both the global and local information to perfect the comprehensiveness for learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Functions</head><p>To unleash the discrimination ability of the learned representations of this network architecture, we employ softmax loss for classfication, and triplet loss for metric learning as the loss functions in training phases, which are both widely used in various deep Re-ID methods.</p><p>For basic discrimination learning, we regard the identification task as a multi-class classfication problem. For i-th learned features f i , softmax loss is formulated as:</p><formula xml:id="formula_1">L sof tmax = − N i=1 log e W T y i f i C k=1 e W T k f i<label>(1)</label></formula><p>where W k corresponds to a weight vector for class k, with the size of mini-batch in training process N and the number of classes in the training dataset C. Different from the traditional softmax loss, the form we employ here abandons bias terms in linear multi-class classifiers according to <ref type="bibr" target="#b37">[38]</ref>, which contributes to better discrimination performances. Among all the learned embeddings, we employ the softmax loss to the global features before 1×1 convolution reduction {z G д , z P 2 д , z P 3 д } and part features after reduction {f P 2</p><formula xml:id="formula_2">p i | 2 i=1 , f P 3 p i | 3 i=1 }.</formula><p>All the global features after reduction {f G д , f P 2 д , f P 3 д } are trained with triplet loss to enhance ranking performances. We use the batchhard triplet loss <ref type="bibr" target="#b13">[14]</ref>, an improved version based on the original semi-hard triplet loss. This loss function is formulated as follows:</p><formula xml:id="formula_3">L t r ipl et = − P i=1 K a=1 [α + max p=1...K ∥f (i) a − f (i) p ∥ 2 − min n=1...K j=1...P j i ∥f (i) a − f (j) n ∥ 2 ] +<label>(2)</label></formula><p>where f</p><formula xml:id="formula_4">(i) a , f (i) p , f (i)</formula><p>n are the features extracted from anchor, positive and negative samples receptively, and α is the margin hyperparameter to control the differences of intra and inter distances. Here positive and negative samples refer to the pedestrians with same or different identity with the anchor. The candidate triplets are built by the furthest positive and closest negative sampled pairs, i.e. the hardest positive and negative pairs in a mini-batch with P selected identities and K images from each identity. This improved version of triplet loss enhances the robustness in metric learning, and further improve the performances at the same time.</p><p>In MGN achitecture, to avoid loss weight tuning troubles and difficulties in convergence, we novelly propose classfication-beforemetric achitecture, which applies the softmax losses to reduced 256-dim local features in Part-2 and Part-3 Branches, and all the non-reduced global-pooled 2048-dim global features, but applies triplet losses to all the reduced features, different from existing methods using triplet losses. This setting is inspired from coarse-tofine mechanism, regarding non-reduced features as coarse information to learn classification and reduced features as fine information with learned metric. The proposed setting achieves robust convergence comparing to that of imposing joint effects at the same level of reduced features. Besides, we employ no triplet loss on local features. Due to misalignment or other issues, the contents of local regions might vary dramatically, which makes the triplet loss tend to corrupt the model during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions</head><p>In our proposed Multiple Granularity Network architecture, there are some issues worth our separate discussion. In this paragraph, we specifically discuss the issues as follows:</p><p>Multi-branch architecture According to our initial motivation for MGN architecture, it seems to be reasonable that the global and local representations are both learned in one single branch. We can directly split the same final feature maps extracted by res_conv5_3 in different numbers of stripes, and apply corresponding supervisory signals as our proposed methods. However, we find this setting is not efficient for further performance improving. Borrowing the ideas in <ref type="bibr" target="#b33">[34]</ref>, the reason might be that the branches sharing the similar network architecture(mainly the fourth residual stage of ResNet-50) just response to different levels of detailed information on images. Learning features in multiple granularities with one mixed single branch might dilute the importance of detailed information. Besides, we try to split the backbone network after shallower or deeper layers, which also achieve no better performances.</p><p>Diversity of granularity Three branches in our network architecture actually learn representing information with different perferences. Global Branch with larger reception field and global max-pooling captures integral but coarse features from the pedestrian images, and features learned by Part-2 and Part-3 Branches without strided convolution and split parts of stripes tend to be local but fine. The branch with more partitions will learn finer representation for pedestrian images. Branches learning different preferences can cooperatively supplement low-level discriminating information to the common backbone parts, which is the reason for performance boosting in any single branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Implementation</head><p>To capture more detailed information from pedestrian images, we refer to <ref type="bibr" target="#b35">[36]</ref> and resize input images to 384×128. We use the weights of ResNet-50 pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref> to initialize the backbone and branches of MGN. Notice that different branches in the network are all initialized with the same pretrained weights of the corresponding layers after the res_conv4_1 block. During training phases, we only deploy random horizontal flipping to images in the training dataset for data augmentation. Each mini-batch is sampled with randomly selected P identities and randomly sampled K images for each identity from the training set to cooperate the requirement of triplet loss. Here we recommend to set P = 16 and K = 4 to train our proposed model. For the margin parameter for triplet loss, we set to 1.2 in all our experiments. We choose SGD as the optimizer with momentum 0.9. The weight decay factor for L2 regularization is set to 0.0005. As for the learning rate strategy, we set the initial learning rate to 0.01, and decay the learning rate to 1e-3 and 1e-4 after training for 40 and 60 epochs. The total training process lasts for 80 epochs. During evaluation, we both extract the features corresponding to original images and the horizontally flipped versions, then use the average of these as the final features. Our model is implemented on PyTorch framework. To conduct a complete training procedure on Market-1501 dataset, it takes about 2 hours with data-parallel acceleration by two NVIDIA TITAN Xp GPUs. All our experiments on different datasets follow the settings above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and Protocols</head><p>The experiments to evaluate our proposed method are conducted on three mainstream Re-ID datasets: Market-1501 <ref type="bibr" target="#b45">[46]</ref>, DukeMTMC-reID <ref type="bibr" target="#b48">[49]</ref> and CUHK03 <ref type="bibr" target="#b19">[20]</ref>. It is necessary to introduce these datasets and their evaluation protocols before we show our results.</p><p>Market-1501 This dataset includes images of 1,501 persons captured from 6 different cameras. The pedestrians are cropped with bounding-boxes predicted by DPM detector <ref type="bibr" target="#b9">[10]</ref>. The whole dataset is divided into training set with 12,936 images of 751 persons and testing set with 3,368 query images and 19,732 gallery images of 750 persons. There are single-query and multiple-query modes in evaluation, the difference of which is the number of images from the same identity. In multiple-query mode, all features extracted from the images of a person captured by the same camera are merged by avg-or max-pooling, which contains more complete information than single query mode with only 1 query image.</p><p>DukeMTMC-reID This dataset is a subset of the DukeMTMC <ref type="bibr" target="#b25">[26]</ref> used for person re-identification by images. It consists of 36,411 images of 1,812 persons from 8 high-resolution cameras. 16,522 images of 702 persons are randomly selected from the dataset as the training set, and the remaining 702 persons are divided into the testing set where contains 2,228 query images and 17,661 gallery images. It might be the most challenging datasets for person Re-ID at present, with common situations in high similarity across persons and large variations within the same identity.</p><p>CUHK03 This dataset consists of 14,097 images of 1,467 persons from 6 cameras. Two types of annotations are provided in this dataset: manually labeled pedestrian bounding boxes and DPMdetected bounding boxes. Originally the whole dataset is divided into 20 random splits for cross-validation, which is designed for hand-crafted methods and very time-consuming to conduct experiments for deep-learning-based methods.</p><p>Protocols In our experiments, to evaluate the performances of Re-ID methods, we report the cumulative matching characteristics (CMC) at rank-1, rank-5 and rank-10, and mean average precision (mAP) on all the candidate datasets. On Market-1501 dataset, we conduct experiments both in single-query and multiple-query mode. On CUHK03 dataset, to simplify the evaluation procedure and meanwhile enhance the accuracy of the performance reflected by the results, we adopt the protocol used in <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Query</head><p>Multiple Query Rank-1 mAP Rank-1 mAP TriNet <ref type="bibr" target="#b13">[14]</ref> 84.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Art Methods</head><p>We compare our proposed method with current state-of-the-art methods on all the candidate datasets to show our considerable performance advantage over all the existing competitors. Results in detail are given as follow:</p><p>Market-1501 The results on Market-1501 dataset is shown in Table 2. For the special effects of re-ranking method for improvement on mAP and rank-1 accuracy, we divide the results into two groups according to whether re-ranking is implemented or not. In single query mode, PCB+RPP <ref type="bibr" target="#b35">[36]</ref> achieved the best published result without re-ranking , but our MGN achieves Rank-1/mAP=95.7%/86.9%, exceeding the former method by 1.9% in Rank-1 accuracy and 5.3% in mAP. After implementing re-ranking, the result can be improved to Rank-1/mAP=96.6%/94.2%, which surpasses all existing methods by a large margin. <ref type="figure" target="#fig_2">Figure 4</ref> shows top-10 ranking results for some given query pedestrian images. The first two results shows the great robustness: regardless of the pose or gait of these captured pedestrian, MGN features can robustly represent discriminative information of their identities. The third query image is captured in a low-resolution condition, losing an amount of important information. However, from some detailed clues such as the strap of the bag and his black suits, most of the ranking results are accurate and with high quality. The last pedestrian shows his back carrying a black backpack, but we can obtain his captured images in front view in rank-3, 6 and 9. We attribute this surprising result to the effects of local features, which establish relationships when some salient parts are lost.</p><p>DukeMTMC-reID According to <ref type="table" target="#tab_2">Table 3</ref>, our MGN architecture also performs excellently on the challenging DukeMTMC-reID dataset. GP-reid <ref type="bibr" target="#b1">[2]</ref> is a good practice of many useful strategies combined in person Re-ID tasks and achieved the best published result. MGN achieves state-of-the-art result of Rank-1/mAP=88.7%/78.4%, outperforming GP-reid by +3.5% in Rank-1 and +5.6% in mAP. Standing on this level of performance on the most challenging datasets currently, we believe there are still some issues to be conquered for further perfect deep Re-ID systems.</p><p>CUHK03 As shown in <ref type="table">Table 4</ref>, our MGN achieves Rank-1/mAP= 68.0%/67.4% on CUHK03 labeled setting and 66.8%/66.0% on CUHK03 detected setting, which outperform all the published results by a large margin. Here we can observe an obvious gap between results of labeled and detected conditions. We argue that it reflects an important affect of detection failure on person Re-ID performance, which emphasizes the importance of high-performance pedestrian detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of Components</head><p>To verify effectiveness of each component in MGN, we conduct several ablation experiments with different component settings on Market-1501 dataset in single query mode. Notice that other unrelated settings in each comparative experiment are the same as MGN implementation in Section 4.1, and we have carefully tuned all the candidate models and report the best performance with our settings. <ref type="table">Table 5</ref> shows the comparison results in different settings related to components of MGN. We separately analyze each component as follows:   <ref type="table">Table 5</ref>: Results with different settings on Market-1501 datasets. "TP" refers to triplet loss. "Branch" refers to a subbranch of MGN. "Single" refers to a single network with the same setting as the branch with the corresponding name in MGN. The model "ResNet-50+TP" can be regarded as "Global (Single)". "G+P2+P3" refers to an ensemble setting by Global (Single), Part-2 (Single) and Part-3 (Branch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MGN vs ResNet-50</head><p>Comparing the results by the baseline ResNet-50 model with our MGN model without triplet loss, we can observe MGN makes a significant performance improvement from Rank-1/mAP=87.5%/71.4% to 95.3%/86.2% (+7.8%/14.8%). We also implement the same experiment with ResNet-101 model, which has a similar scale of weights with MGN. The deeper ResNet-101 network indeed brings a considerable performance boost (+2.9%/7.4%), but there is still a large gap with our MGN model, which shows that extra weights from additional branches are not the main contributors of the improvement, but the carefully-designed network architecture. Results above prove that our proposed MGN has incredible capability of feature representations for person Re-ID.</p><p>Multiple branch vs Multiple networks The multi-branch setting in one single network is very similar to ensemble of multiple independent networks, but we believe the cooperation of multiple branches can achieve a better performance than ensemble learning. We train three independent ResNet-50 networks separately, each of which respectively replicates the corresponding configuration of three branches, i.e. Global, Part-2 and Part-3 Branch in MGN . In our experiments, we explore the effects of multi-branch architecture in two aspects. On the one hand, from a global view, we compare the performance of MGN with the ensemble of three single networks. The ensemble strategy indeed achieves better performance than any single participating network, but MGN still outperforms about 1% ∼ 2% on both Rank-1 and mAP. It shows that the cooperation of branches learns more discriminative feature representations than independent networks. On the other hand, from a local view, we respectively compare the performances of features learned by subbranches of MGN with single networks in corresponding setting of branch. As our expect, the features from sub-branches also perform better than that from single networks. We argue that the mutual effects between sub-branches complement the blind spots in their individual learning procedure. Multi-branch architecture settings The multi-branch deep network architectures are very common for person Re-ID tasks <ref type="bibr" target="#b7">[8]</ref>. Our proposed MGN exceeds all the previous architectures by the power of learning representations with multiple granularities. Based on our proposed architecture, we can intuitively infer many variants architectures. On the one hand, based on the MGN model, we can add or reduce the number of local branches. Comparing the model removed the Part-3 branch with that added a Part-4 branch, we find that the removal of Part-3 Branch (MGN w/o Part-3) brings obvious performance degradation by Rank-1/mAP=−0.9%/0.7%, and the addition of Part-4 Branch (MGN w/ Part-4) introduce no obvious performance boost. This confirms the necessity and efficiency of our proposed 3-branch setting. On the other hand, for the Part-N local branches, the number of partitions is a hyperparameter effecting the granularity of learning representations. We divide the feature maps into 2, 3, 4 in each local branches alternatively, and still find that the proposed Part-2/Part-3 setting is optimal. The Part-2/Part-4 setting skips the granularity level of Part-3, and brings performance degradation by Rank-1/mAP=−0.9%/1.3%, which shows the importance of Part-3 granularity. Comparing the results of Part-2/Part-4 setting with Part-3/Part-4, the former setting causes greater performance loss than the later. The reason is that the uniform division by 2 and 4 introduces no overlap areas between stripes, but the Part-3/Part-4 setting does. We argue this overlap can introduce the correlation between different partitions, from which more discriminative information can be learned.</p><p>Triplet Loss A number of previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43]</ref> have shown the effectiveness of joint training with softmax loss for classification and triplet loss for metric learning in person Re-ID tasks.</p><p>In our experiments, we reproduce the boosting effect with both ResNet-50 and MGN models. With the help of triplet loss on all the candidate datasets, we can observe +1.2%/3.6% Rank-1/mAP improvement to the baseline model, and +0.4%/0.7% Rank-1/mAP improvement to MGN model. We can observe two interesting effects from the improvement figures: 1) The improvement on mAP is more obvious than that on rank-1 accuracy, which proves the ranking effects of metric learning losses. 2) Triplet loss brings larger improvement to the baseline model than that to MGN. Comparing to softmax loss, triplet loss helps to capture more detailed information to meet the margin condition <ref type="bibr" target="#b27">[28]</ref>. Our proposed network architecture is initially designed to enhance the local representation, which dilutes the original effects of triplet loss. Notice that in the MGN without triplet loss setting (MGN w/o TP), we employ no softmax loss on 2048-dim features, and alternatively on the reduced 256-dim features.</p><p>Feature response with multiple granularity We insist on our proposed MGN model learns the global and local feature representations with multiple levels of granularities. <ref type="figure" target="#fig_3">Figure 5</ref> shows some feature response maps for some input pedestrian images, extracted from all the top of branches in MGN. All the response maps filter most of the complex background, which contain no useful information about identities for pedestrians. The responses from Global Branch is mainly focused on main body parts, and the information on limbs, waists or feet is commonly ignored. On local branches, the global responses on main body are lost, but concentrate more on some particular parts. For example, we can observe preferences on body parts such as shoulders and joints in Part-2 cases. As for the Part-3 cases, responses are more scattered on body parts, but some pivotal semantic information is preferred. Notice that on the Part-3 response map of the last pedestrian images, we can observe a bright circle region in front of the chest of this pedestrian. It is a mark of his T-shirt, which can be regarded as a very discriminative characteristic for his identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose the Multiple Granularity Network (MGN), a novel multi-branch deep network for learning discriminative representations in person re-identification tasks. Each branch in MGN learns global or local representation with certain granularity of body partition. Our method directly learns local features on horizontally-split feature stripes, which is completely end-to-end and introduces no part locating operations such as region proposal or pose estimation. Extensive experiments have indicated that our method not only achieves state-of-the-art results on several mainstream person Re-ID datasets, but also pushes the performance to an exceptional level comparing to existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Figure 3 :</head><label>23</label><figDesc>Multiple Granularity Network architecture. The ResNet-50 backbone is split into three branches after res_conv4_1 residual block: Global Branch, Part-2 Branch and Part-3 Branch. During testing, all the reduced features are concatenated together as the final feature representation of a pedestrian image. Notice that the 1 × 1 convolutions for dimension reduction and fully connected layers for identity prediction in each branch DO NOT share weights with each other. Each path from the feature to the specific loss function represents an independent supervisory signal. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Top-10 ranking list for some query images on Market-1501 datasets by MGN. The retrieved images are all from in the gallery set, but not from the same camera shot. The images with green borders belong to the same identity as the given query, and that with red borders do not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Feature response maps extracted from output layers of every branches. First column: the original pedestrian images. Second column: response maps from Global Branch. Third column: response maps from Part-2 Branch. Fourth column: response maps from Part-3 Branch. The brighter the area is, the more concentrated it is. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of results on Market-1501 with Single Query setting (SQ) and Multiple Query setting (MQ). "RK" refers to implementing re-ranking operation.</figDesc><table><row><cell>Methods</cell><cell cols="2">Rank-1 mAP</cell></row><row><cell>SVDNet[35]</cell><cell>76.7</cell><cell>56.8</cell></row><row><cell>AOS[16]</cell><cell>79.2</cell><cell>62.1</cell></row><row><cell>HA-CNN[22]</cell><cell>80.5</cell><cell>63.8</cell></row><row><cell>GSRW[29]</cell><cell>80.7</cell><cell>66.4</cell></row><row><cell>DuATM[31]</cell><cell>81.8</cell><cell>64.6</cell></row><row><cell>PCB+RPP[36]</cell><cell>83.3</cell><cell>69.2</cell></row><row><cell>PSE+ECN[27]</cell><cell>84.5</cell><cell>75.7</cell></row><row><cell>DNN_CRF[5]</cell><cell>84.9</cell><cell>69.5</cell></row><row><cell>GP-reid[2]</cell><cell>85.2</cell><cell>72.8</cell></row><row><cell>MGN(Ours)</cell><cell>88.7</cell><cell>78.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of results on DukeMTMC-reID.</figDesc><table><row><cell>Methods</cell><cell cols="4">Labeled Rank-1 mAP Rank-1 mAP Detected</cell></row><row><cell>BOW+XQDA[46]</cell><cell>7.9</cell><cell>7.3</cell><cell>6.4</cell><cell>6.4</cell></row><row><cell>LOMO+XQDA[23]</cell><cell>14.8</cell><cell>13.6</cell><cell>12.8</cell><cell>11.5</cell></row><row><cell>IDE[47]</cell><cell>22.2</cell><cell>21.0</cell><cell>21.3</cell><cell>19.7</cell></row><row><cell>PAN[48]</cell><cell>36.9</cell><cell>35.0</cell><cell>36.3</cell><cell>34.0</cell></row><row><cell>SVDNet[35]</cell><cell>40.9</cell><cell>37.8</cell><cell>41.5</cell><cell>37.3</cell></row><row><cell>HA-CNN[22]</cell><cell>44.4</cell><cell>41.0</cell><cell>41.7</cell><cell>38.6</cell></row><row><cell>MLFN[4]</cell><cell>54.7</cell><cell>49.2</cell><cell>52.8</cell><cell>47.8</cell></row><row><cell>PCB+RPP[36]</cell><cell>-</cell><cell>-</cell><cell>63.7</cell><cell>57.5</cell></row><row><cell>MGN(Ours)</cell><cell>68.0</cell><cell>67.4</cell><cell>66.8</cell><cell>66.0</cell></row><row><cell cols="5">Table 4: Comparison of results on CUHK03 with evaluation</cell></row><row><cell>protocols in [50].</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ejaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Re-ID done right: towards good practices for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojana</forename><surname>Gajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naila</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05339</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10658</idno>
		<title level="m">Deep-Person: Learning Discriminative Deep Features for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-Level Factorisation Net for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2109" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group Consistent Similarity Learning via Deep CRF for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8649" to="8658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person Re-Identification by Deep Learning Multi-Scale Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2590" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarially Occluded Samples for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5098" to="5107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2194" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Harmonious Attention Network for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Person reidentification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-toend comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meibin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Pose-Sensitive Embedding for Person Re-Identification With Expanded Cross Neighborhood Re-Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Saquib</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Group-Shuffling Random Walk for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2265" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-End Deep Kronecker-Product Matching for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6886" to="6895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dual Attention Matching Network for Context-Aware Feature Sequence Based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlou</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5363" to="5372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pose-driven Deep Convolutional Model for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3980" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2892" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">SVDNet for Pedestrian Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2590" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Beyond Part Models: Person Retrieval with Refined Part Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In ECCV. In press</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Normface: l2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention-Aware Compositional Network for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scalable Person Re-identification: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00408</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3774" to="3782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
