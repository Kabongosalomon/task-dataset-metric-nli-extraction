<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skand</forename><surname>Vishwanath</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Learning Affect and Semantic Image AnalysIs (LASII) Group</orgName>
								<orgName type="institution">Indian Institute of Technology Ropar</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peri</forename><forename type="middle">Abhinav</forename><surname>Dhall</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Learning Affect and Semantic Image AnalysIs (LASII) Group</orgName>
								<orgName type="institution">Indian Institute of Technology Ropar</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our approach for the Disguised Faces in the Wild (DFW) 2018 challenge. The task here is to verify the identity of a person among disguised and impostors images. Given the importance of the task of face verification it is essential to compare methods across a common platform. Our approach is based on VGG-face architecture paired with Contrastive loss based on cosine distance metric. For augmenting the data set, we source more data from the internet. The experiments show the effectiveness of the approach on the DFW data. We show that adding extra data to the DFW dataset with noisy labels also helps in increasing the gen 11 eralization performance of the network. The proposed network achieves 27.13% absolute increase in accuracy over the DFW baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the years, the research in the area of face recognition has received tremendous amount of attention. Many innovative and novel methods have been put forward for the tasks visual face recognition and verification <ref type="bibr" target="#b22">[24]</ref>. Due to its importance, in the past, researchers have concentrated over different problems in face recognition 'in the wild'. Here 'in the wild' refers to scenarios with varying illumination and pose <ref type="bibr" target="#b16">[18]</ref>, prediction over varying age of the same person <ref type="bibr" target="#b3">[5]</ref>, prediction across different facial expressions <ref type="bibr" target="#b4">[6]</ref> and prediction across different modality such as sketches and visual medias etc. <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b12">14]</ref>.</p><p>There has been some work on face verification of disguised faces in different imaging modalities like thermal and visual medias <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b5">7]</ref> in which they exploit both the modalities to get the best of both worlds, but the problem of "disguise" in a single modality has not been explored in detail. The problem of disguise deals with determining whether the given pair of images are of the same person in disguise or of different persons (one of them being the imposter). The   <ref type="bibr" target="#b11">[13]</ref>. The first column is a genuine image of a celebrity, the second column is the same person in a disguise and the third columns shows the images of imposters who look like the celebrities in column one. Green bounding box signifies a same identity and red signifies an impostor.</p><p>It was found that successful face recognition systems such as the VGG-Face <ref type="bibr" target="#b13">[15]</ref> are not efficient, when it comes to the problem of disguise vs impostor recognition <ref type="bibr" target="#b11">[13]</ref>. VGG-Face achieves 33.76% Genuine Acceptance Rate (GAR) at 1% False Acceptance Rate (FAR) and 17.73% GAR at 0.1% FAR, which is a clear indication that the usual facial recognition models may not be that helpful to capture the rich representations required for distinguishing the disguised from the imposter images.</p><p>Dhamecha et. al. <ref type="bibr" target="#b5">[7]</ref>, proposed a two-stage verification process. During the first stage, a patch classifier is computed to decide if a patch is important wrt the task. The hypothesis is that not all facial parts of a disguised person are equally important (particularly occluded parts of a face).</p><p>The second stage consisted of a patch based face recognition method, in which texture features are extracted from the biometrics-wise important patches. Further, this information is used to verify the personalities with the help of a support vector machine and χ 2 distance metric.</p><p>Singh et. al. <ref type="bibr" target="#b18">[20]</ref>, used spacial convolutional networks <ref type="bibr" target="#b14">[16]</ref> to infer the values of fourteen facial key points. Given a disguised image and a gallery of non-disguised images, geometric features are computed based on the angles between the facial key points. The gallery image, which has the least L1 distance based on the geometric feature is assigned as the corresponding non-disguised image of the given disguised image.</p><p>Some of the recent face recognition works have also used different loss functions such as the Contrastive loss <ref type="bibr" target="#b7">[9]</ref> and the triplet-loss <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b13">15]</ref> to bring the features representation of the two facial images closer and learn a modality invariant representation.</p><p>Disguised faces recognition is an important issue from the perspective of biometrics, as doing this would help surveillance systems recognize imposters trying to steal the identities of other people. The fact that disguised faces increases the within-class variation of the faces and imposter faces decreases the between-class variation of the faces, makes this task non-trivial. In fact, the imposter images in <ref type="figure" target="#fig_1">Figure 1</ref> seem to be similar looking to the original identity. Although, when closely observed, we can distinguish between the identities and claim if the person is an imposter or not. From an automated computer vision based method perspective, it is important to extract rich representations of the facial images in-order to distinguish among the identities and verify them correctly.</p><p>In recent times, Convolutional Neural Networks (CNN) have been extensively used in computer vision to achieve the state-of-the-art performances <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b17">19]</ref> in many classification, object detection tasks and many other vision tasks. CNNs have also provided robust face descriptors <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b19">21]</ref>, which have in turn enabled to achieve state-of-art accuracy in face verification and recognition tasks <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b15">17]</ref>.</p><p>In order to learn rich representations of faces, we use CNNs to extract the features from the images via convolution. To differentiate between the disguised and the imposter images, we explicitly impose Contrastive loss constraint on the feature representation extracted via CNN. This is done so as to bring the representation of the identity and the disguised face closer and to make the representation of the identity and imposter far apart by a margin. Apart from just making the representations far apart, we also regress similarity score of the two images (score = 1, if the images are similar and score = 0, if they are not). The major contributions of the paper are as follows:</p><p>• We propose a CNN model based on VGG-Face <ref type="bibr" target="#b13">[15]</ref> for the verification of disguised images in the same modal-ity unlike other previous works <ref type="bibr" target="#b5">[7]</ref>, which use CNN for cross-modal verification task of disguised faces.</p><p>• In order to decrease the intra-class variation and increase the inter-class variation of the feature representation, we use Contrastive loss with cosine similarity measure.</p><p>• To improve the validation accuracy, we also include a regression mean-squared loss apart form the usual classification cross-entropy loss which tells us the similarity of the two images.</p><p>• To increase the performance, we extended the DFW dataset with images from the internet. A total of 1380 images of 325 subjects were downloaded using keyword based search. The new images and their corresponding noisy labels (based on the keyword based search) are used to augment the genuine images in the Train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Disguise Faces in Wild (DFW) Dataset</head><p>We used the Disguise Faces in Wild (DFW) dataset <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b6">8]</ref>, which is so far the largest dataset of disguised and imposter images. There have previously been few datasets on makeup disguise in the wild <ref type="bibr" target="#b1">[3]</ref> and datasets on crossmodality (visual and thermal) disguise faces dataset <ref type="bibr" target="#b0">[2]</ref>. However, to the best of knowledge, there is no dataset, which addresses the issue of disguised faces and imposter faces in the wild. The DFW dataset consists of 1000 identities (400 in training set and 600 in the testing set), with a total of 11155 images. There consists of 3 types of images: genuine images, disguised images and the imposter images. The genuine images are the usual visual image (photograph) of the person, the disguised images is the image of the same identity in disguise and the imposter images are the images consisting of images of other persons who look like the identity. An example of each is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. We used the coordinates given by the organizers to crop the face out from the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Weakly Labelled noisy Data from the Internet</head><p>The limitation of the DFW dataset is that for every identity, there is only 1 genuine image for training and 1 genuine image for validation. Our experiments suggested that the data with respect to the genuine images (when compared to imposter and disguised images) were less. To overcome the limitation, we downloaded 2-4 more genuine image (totaling the training images to 3-5 for each of the identities in the Train set) using the BING image search API [1] and got a total of 1380 images for 325 celebrities in the DFW Train set. We cropped out the faces from the newly added genuine images using the OpenFace library <ref type="bibr" target="#b2">[4]</ref>. The results of our model with and without the extra data from the internet are mentioned in detail in <ref type="table">Table 2</ref>.</p><p>It is interesting to note that the network's performance increased considerably, after the downloaded data was used along with the DFW data. No cleaning of the data was performed and the search engine's results were used to assign the identity to the downloaded images. We noted that in a few cases, images retrieved against the search query generated impostor images too. However, they are used as is in the training without any manual pruning. Some examples of the above mentioned weakly labelled images (which do not belong to the correct identity) are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We refer to this data as weakly labelled as it is possible that an incorrect identity retrieved during the web search can be due to an attribute of the face in the retrieved image, which makes the image as a good candidate for being the disguised representation of the original identity. Also it becomes a difficult task for the annotator as well to label the images of the imposter and disguised people. These noisy labels can be generated when the annotator may not be aware about the identity of the subject and other factors such as different ethnicity of the annotator and the sub-ject in the database also add up to the problem of labelling the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Protocol</head><p>There are three protocols given with the DFW dataset : Impersonation, Obfuscation and Overall Performance. The impersonation protocol consists of image pairs with genuine image and imposter faces, while the obfuscation protocol consists of images of genuine faces with disguised faces. The last protocol consists of all possible pairs (even pair of 2 disguised faces) as a part of the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DisguiseNet 1</head><p>Let x 1 i denote the genuine image of an identity i and x 2 i be either the disguised image of identity i or the imposter of identity i. We have a Siamese VGG-Face architecture i.e. two sets of 16-layered VGG-Face pre-trained network with tied weights to extract the same features from the input pairs. Note that we need same features as the modality of the images is the same (visual modality). However, the feature representations will be different for genuine image and the disguised/imposter image. We pass the genuine image from the first stream of CNN network and either disguised image or imposter image through the second CNN stream. Let F m i be the feature descriptor of i th identity and m={genuine / imposter / disguise}, then F m i = Conv (x m i , Θ), where Θ is the convolutional net parameters. Further, for a Siamese network, the convolutional net parameters: Θ is the same for all the m. Since we use 16 layer deep CNN, the first four convolutional layers are frozen during the fine tuning. The rationale behind this being the observation of no significant gradient flow through the initial layers during the fine tuning process. We have used the Contrastive loss at the second last fully connected layer, which is defined as follows:</p><formula xml:id="formula_0">L C = 1 2B B i=1 y i d 2 + (1 − y i ) max (margin − d, 0) 2</formula><p>(1) where B is the batch size, d is a similarity score. In our experiments, cosine similarity distance metric is used for computing d. Further, y i is the label of the pair, with y i = 1 signifying a (genuine, disguise) pair and y i = 0 signifying a (genuine, imposter) pair. Here, the parameter margin is a scalar value representing the minimum desired distance between a negative (imposter) and positive (disguised) sample.</p><p>The second loss, which we include in our network is the verification binary cross-entropy loss. We added this classification task as a regularizer to the Contrastive loss similar to the classification network used as regularizer in <ref type="bibr" target="#b20">[22]</ref>.</p><p>Learning directly the hard label (0/1) can be a problem in data with such wide intra-class and less inter-class variance. So we use the regression based mean-squared error loss between the predicted score and the actual pair label (0/1). We found that doing this was indirectly acting as a regularizer as well as forcing the representations of same labels to be similar.</p><formula xml:id="formula_1">L R = y i − p i 2 (2)</formula><p>where y i is the ground truth label and p i is the predicted value after sigmoid activation. We use sigmoid to make sure that the regressed value is between 0 and 1.</p><p>Our final loss function is as follows:</p><formula xml:id="formula_2">L = L C + L R + L BCE<label>(3)</label></formula><p>where L BCE , is the binary cross-entropy loss for verification. The contrastive loss is applied on FC2 and hence affecting the weights of all the previous layers of the network and the BCE loss and MSE loss is applied at the end of the network and affects all the layers of the network (except the first four layer as they are freezed). Also since the positive (genuine, disguise) and negative (genuine, imposter) examples generated from the training dataset is not equal (negative samples are little less than the positive samples as some of the identities don't have imposter images but have only disguised images), so we also use class balancing in the loss i.e weight the negative samples' loss more and weight the positive samples' less. The weights are inversely proportional to number of corresponding samples present in the training data. The architecture is shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results &amp; Ablations</head><p>In order to increase the dataset size for training and also to make our model robust to slight noise and augmentation, we applied data augmentation techniques such as adding Gaussian noise, flipping, random rotation and random translation to the images. This lead to significant increase in the validation accuracy on the test set. We used stochastic gradient descent optimization in all our experiments with a learning rate of 10 −3 . Apart from the DFW Test dataset, we also selected random pairs of genuine and disguise/imposter images as our Validation set. We show the results of all the ablations on this Validation set. We later the compare the results of experiments with the organizer's test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablations on Contrastive loss margin</head><p>On changing the value of margin, we could get the best validation accuracy at 0.5. We did not go beyond margin = 1 as the similarity metric, we are using is co-  <ref type="table">Table 3</ref>. GAR values at different FAR values on the DFW Test set. The baseline method extracted features from VGG-Face model and compared the features with cosine similarity metric. The methods with (W) are the methods with the given loss function without the weakly labelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Validation Accuracy (%) L C 76.45 </p><formula xml:id="formula_3">L C + L R 78.95 L C + L R + L BCE 79.86</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Is Weakly Labelled Data necessary?</head><p>Yes, because it is extremely evident from the below experiment that doing adding the extra dataset with weak labels helps the model to extract robust features in-turn boosts up the performance by a large margin. The results of this ablation are shown in <ref type="table">Table 2</ref> on the Validation set created from the DFW Train set and the results on DFW Test set are shown in <ref type="table">Table 3</ref>. We can see a significant increase in the accuracy after adding the weakly labelled data in both the tables. On the DFW Test Set there is an increase of 10% for GAR@1%. All the mentioned experiments are done with margin = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation with different loss functions</head><p>We did experiments with Contrastive Loss (margin = 0.5), Regression loss and binary cross-entropy loss with different combinations of each. It is clear that having regression loss and binary cross-entropy loss act as a regularizer. The results of this are present in <ref type="table" target="#tab_2">Table 4</ref>.3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on the Test set</head><p>The results of DisguiseNet on the Test set (provided by the organizers) is shown in <ref type="table">Table 3</ref>. The GAR @ 10% FAR is close to 99% and GAR@1% FAR is around 61%. The results of the model with different combinations of loss function is shown and it has turned out that have the Regression and Binary Cross-Entropy Loss along with the Contrastive Loss has helped the model out-perform the other models. The <ref type="figure" target="#fig_4">Figure 4</ref>, shows the ROC curve for the proposed model. Some failure cases of our model for various pairs are shown in <ref type="figure" target="#fig_5">Figure 5</ref> and <ref type="figure" target="#fig_6">Figure 6</ref>. The <ref type="figure" target="#fig_5">Figure 5</ref> shows the cases of True Negative i.e. the pairs shown are actually of the same identity but our model has classified them wrongly. In <ref type="figure" target="#fig_6">Figure 6</ref>, examples of False Positives are shown where in the images are actually of different people but our networks has classified them as the same identity.</p><p>Our success cases are shown in <ref type="figure" target="#fig_7">Figure 7</ref>, in which we show both False Negatives and True Positives. The pairs highlighted in green are of the same identity, one being the genuine image and the other being the disguised image whereas the pairs in which one of the images is highlighted  in green and the other in red belong to different identities. In all the image pairs shown, our model has correctly classified if they are disguised or imposter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a Contrastive loss based approach for verification of disguised faces in the wild (Dis-guiseNet). The method exploits, the usage of three different loss functions. It is evident from the experiments that the ensemble of three loss functions is beneficial towards the task. Apart from the DFW data, we also add our own data with weak noisy labels in order to enhance the performance of the model for disguise detection. The increase in the data and the use ensemble of loss functions reflects positively on the final results on the DFW Test set, giving an absolute increase of 27.13% over the DFW baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1, clearly show examples of disguised people and imposters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The Figure shows examples of disguised and imposters in the DFW database</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the proposed method: Siamese architecture with 16 layered VGG-Face pre-trained weights. A combination of Contrastive Loss (LC ), Regression Loss (LR) and Binary Cross-Entropy Loss (LBCE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Some of the examples from the extended DFW dataset in which the query returned imposter images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>ROC Curve for the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The Figure shows eg. of True Negative i.e. both the images in each of the six pairs are of the same identity (the one on the left is a genuine image and the one on the right is a disguised image), but our model says that the two images are that of an imposter images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>The Figure shows eg. of False Positives i.e. both the images in each of the six pairs are of the different identity (the one with green border is the genuine image and the one with the red border is the imposter image), but our model says that the two images are that of the same identity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>The Figure shows examples of False Negatives and True Positives i.e the both the images either correspond to imposter pair (the pairs with green and red borders) or disguised pair (the pairs with only green border) and our model has predicted the relationship correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Parameter analysis of margin. Note that the validation accuracy is on the Validation set created by us from the DFW Train set. Effect of data augmentation on validation accuracy. Note that the validation accuracy is on the Validation set created by us from the DFW Train set.</figDesc><table><row><cell cols="2">margin Validation Accuracy (%)</cell></row><row><cell>0.1</cell><cell>74.59</cell></row><row><cell>0.5</cell><cell>79.86</cell></row><row><cell>0.6</cell><cell>76.07</cell></row><row><cell>Data Setting</cell><cell>Validation Accuracy (%)</cell></row><row><cell>w/o weakly labelled data</cell><cell>60.43</cell></row><row><cell>Weakly labelled data + DFW</cell><cell>79.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Effect of using different combinations of loss functions on the Validation set.</figDesc><table><row><cell>sine similarity, which always returns a value between [0, 1].</cell></row><row><cell>The quantitative results are shown in Table 1. Note that in</cell></row><row><cell>the table, we use accuracy metric. This was used for tuning</cell></row><row><cell>the model parameters on a Validation set created by us from</cell></row><row><cell>the DFW Training data.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is present at https://github.com/pvskand/DisguiseNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We would like to thank NVIDIA Corporation for donating the TitanXP GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">IIIT-D disguise version 1 face database</title>
		<ptr target="http://iab-rubric.org/resources/facedisguise.html.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://www.antitza.com/makeup-datasets.html.2" />
		<title level="m">Makeup in wild database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Openface: A general-purpose face recognition library with mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ludwiczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<idno>CMU-CS-16-118</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>CMU School of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition performance under aging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From individual to group-level emotion recognition: Emotiw 5.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="524" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Disguise detection and face recognition in visible and thermal spectrums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics (ICB)</title>
		<meeting>the International Conference on Biometrics (ICB)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing disguised faces: Human and machine evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Heterogeneous face recognition: Matching nir to visible light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1513" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matching forensic sketches to mug shot photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="639" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Disguised Faces in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kushwaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ratha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>IIIT Delhi</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Forgetmenot: Memory-aware forensic facial sketch matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disguised face identification with facial keypoints using spatial fusion convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Omkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A lightened cnn for deep face representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1511.02683</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
