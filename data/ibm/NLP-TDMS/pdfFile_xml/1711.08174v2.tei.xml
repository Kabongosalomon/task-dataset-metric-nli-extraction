<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Object Discovery by Generative Adversarial &amp; Ranking Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esat-Psi</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cv</forename><forename type="middle">:</forename><surname>Hci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karlsruhe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Cvl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zürich</surname></persName>
						</author>
						<title level="a" type="main">Weakly Supervised Object Discovery by Generative Adversarial &amp; Ranking Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The deep generative adversarial networks (GAN) recently have been shown to be promising for different computer vision applications, like image editing, synthesizing high resolution images, generating videos, etc. These networks and the corresponding learning scheme can handle various visual space mappings. We approach GANs with a novel training method and learning objective, to discover multiple object instances for three cases: 1) synthesizing a picture of a specific object within a cluttered scene; 2) localizing different categories in images for weakly supervised object detection; and 3) improving object discovery in object detection pipelines. A crucial advantage of our method is that it learns a new deep similarity metric, to distinguish multiple objects in one image. We demonstrate that the network can act as an encoder-decoder generating parts of an image which contain an object, or as a modified deep CNN to represent images for object detection in supervised and weakly supervised scheme. Our ranking GAN offers a novel way to search through images for object specific patterns. We have conducted experiments for different scenarios and demonstrate the method performance for object synthesizing and weakly supervised object detection and classification using the MS-COCO and PASCAL VOC datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>authors running</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Generator</head><p>Real/ Fake?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminator Net</head><p>Bird? Cat? Dog?</p><p>Ranking Net</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discovering objects in scenes is one of the fundamental problems in the computer vision field. Deep neural networks have been promising for the purpose, but still need a large-scale, annotated dataset for training. There have been numerous efforts to work with an unsupervised setup, e.g. based on generative or, most notably, mixed generativediscriminative networks (GANs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Some recent works have focused on extending the training dataset, using a supervised dataset to synthesize additional, realistic data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Powered by adversarial learning, supervised GANs can generate more accurate images based on distinguishing different classes and fake from real data.</p><p>To tackle the weakly supervised object detection, creating accurate object templates for each image independently and finding location of object instances by those templates is one solution. To the best of our knowledge, our work is the first to tackle object detection and discovery using GANs, and that also in a weakly supervised manner. As a wider context for the work, using a combination of encoder based conditional GANs with ranking objective can be utilized to extract the correct templates of objects in an image. The image is encoded by a CNN and our generative network draws a realistic sample of a specific object in the image using indication of its location. The arXiv:1711.08174v2 [cs.CV] 17 Apr 2018 <ref type="figure">Fig. 1</ref>. The proposed generative ranking and adversarial networks take an image containing different object categories and synthesize a specific object instance among the other objects in a scene. The proposed method allows generative models to tackle recognition and detection problems, and is validated for weakly supervised object detection. discriminator and our ranking network help the generator to synthesize the most realistic and correct sample. Our scheme combines conditional GANs, adversarial learning, and a feature space similarity metric. The objective, i.e. the new ranking loss, helps the generator at training time to discriminate between the objects, and to learn the relevant features from the encoder network that allows it to draw the object samples (see <ref type="figure">Fig. 1</ref>). Thus our motivation to propose such a pipeline is to solve the problem of finding nice object instance templates for each image and to do so powerful and novel generative models are promising solutions.</p><p>Nguyen et al. <ref type="bibr" target="#b4">[5]</ref> showed that adding supervision information like class labels helps to learn generating more realistic images. In addition to using object labels, we incorporate a similarity ranking between different category of samples. This strengthens the cooperation between the generator and discriminator when synthesizing the object instances.</p><p>Another advantage of our work is that it exploits deep generative models to allow for weak supervision. For training the object detector in weakly labeled setting, we do not use ground-truth bounding boxes. Instead, the weakly supervised object detector searches itself for the possible locations of object samples in the training images. Our novel GAN generates object templates similar to the actual object samples present in the image. After this step, it uses the synthesized templates which look like the real objects to find the accurate location of object instance. Eventually these locations are handed over to train the detector as a pseudo ground-truth. Our GAN and ranking networks use ImageNet-like categories without bounding boxes to learn feature space similarity and supervise the ranking process.</p><p>Our approach has been evaluated in several scenarios: realistic image generation, object instance extraction, multi-class object classification, and object localization with single/multi instances per image. Our contributions are as follows:</p><p>-Presenting a new approach for supervised learning of generative ranking &amp; adversarial networks to synthesize realistic objects. -Proposing a GAN with multiple objective losses to create accurate samples of objects in an image and to draw realistic-looking images.</p><p>-Training a weakly supervised generative network as a solution to weakly supervised object detection and training with hallucinated samples.</p><p>The rest of the paper is organized as follows. Section 2 discusses related work. We introduce the GANs in section 3 and explain the core of the work in section 4 and describe the weakly supervised GAN for object detection in section 5. The experiments are presented in section 6. Finally, in section 7 conclusions are drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generative Models: Recently several attempts have been made to improve image generation using generative models. The most popular generative model approaches are Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref>, Variational Autoencoders (VAEs) <ref type="bibr" target="#b5">[6]</ref>, and Autoregressive models <ref type="bibr" target="#b6">[7]</ref>. And their variants, e.g. conditional GANs, Invertible Conditional GANs, Deep Convolutional GANs (DC-GANs) <ref type="bibr">[</ref>  <ref type="bibr" target="#b13">[14]</ref> and Larsen et al. <ref type="bibr" target="#b14">[15]</ref> use a similar idea to <ref type="bibr" target="#b1">[2]</ref>, but combining a VAE and GAN to improve the realism of the generated images. Also with GANs an appropriate reconstruction loss is necessary to avoid blurry results, because the distances are computed in the image space. Solutions have been proposed to mitigate this problem. One is to measure the similarity in the feature space, instead of the image space, as proposed in <ref type="bibr" target="#b14">[15]</ref>. Similarly in <ref type="bibr" target="#b3">[4]</ref>, Dosovitskiy et al. use perceptual similarity metrics between image features. This results in sharp and realistic generated images.</p><p>Several applications have been based on different types of GANs. Mathieu et al. <ref type="bibr" target="#b15">[16]</ref> predict future frames in videos, conditioned on previous frames. Larsen et al. <ref type="bibr" target="#b14">[15]</ref> generate realistic images of faces. Dosovitskiy et al. <ref type="bibr" target="#b16">[17]</ref> and Rifai et al. <ref type="bibr" target="#b17">[18]</ref> generate images of object categories given high-level information about the desired object. Reed et al. <ref type="bibr" target="#b18">[19]</ref> generate realistic images from text and landmarks. Pathak et al. <ref type="bibr" target="#b19">[20]</ref> use context encoders to generate the contents of an arbitrary missing image region conditioned on its surroundings. Isola et al. <ref type="bibr" target="#b20">[21]</ref> learn the mapping from input images to target images. Nguyen et al. <ref type="bibr" target="#b4">[5]</ref> generate high-resolution, photo-realistic images using text-to-image generative models. Wang et al. <ref type="bibr" target="#b21">[22]</ref> generate images from the surface normal map. Zhu et al. <ref type="bibr" target="#b22">[23]</ref> modify the appearance of an image while preserving realism, guided by user constraints. Zhou et al. <ref type="bibr" target="#b23">[24]</ref> create depictions of objects at future times in time-lapse videos. Li et al. <ref type="bibr" target="#b24">[25]</ref> efficiently synthesize textures for style transfer. Yoo et al. <ref type="bibr" target="#b25">[26]</ref> show pixel-level domain transfer to generate realistic target images.</p><p>Weakly supervised object detection −Weakly supervised learning: Over the last decade, several weakly supervised object detection methods have been studied that were using multiple instance learning (MIL) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. Because of its non-convexity, MIL tends to get stuck in local optima, thus making it dependent on the initialization of object proposal instances in the positive/negative bags. To alleviate this shortcoming, many proposed strategies have been seeking a better initialization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> or have focused on regularizing the optimization strategies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. The majority <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> used large and noisy collections of object proposals to train the object detectors. We took a new way and approach it via generative networks to deal with weakly supervised object detection. In our work, we show that providing a level of supervision to the generative networks is beneficial for an accurate object localization.</p><p>−CNN based weakly supervised object detection: Recently, several efforts have been made to let CNNs classify objects with weak supervision <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Oquab et al. <ref type="bibr" target="#b39">[40]</ref> compute a mid-level image representation for feature discrimination, employing a pre-trained CNN. In <ref type="bibr" target="#b40">[41]</ref>, the same authors modify the CNN architecture to coarsely localize object instances in images, thus improving the classification performance. Bilen et al. <ref type="bibr" target="#b37">[38]</ref> use CNNs to operate at the level of image regions, simultaneously selecting regions and classifying. Li et al. <ref type="bibr" target="#b38">[39]</ref> address the problem via progressive domain adaptation for joint classification and detection, using a pre-trained CNN network. Diba et al. <ref type="bibr" target="#b41">[42]</ref> proposed cascaded stages for both object proposal and detection using multiple loss functions.</p><p>The main distinction between the aforementioned work and ours is that we use GANs for weakly supervised object detection. To the best of our knowledge it is the first end-to-end network initializing the location of objects using GANs for such task. Furthermore, we introduce generative ranking and adversarial networks for the realistic synthesis of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generative Adversarial Networks</head><p>Generative adversarial networks normally include two parts: a generator network G and a discriminator network D. The generator tries to beat the discriminator in a minmax game to distinguish between real and fake images. If the network was perfectly modeled, the generator will have learned the distribution of p data (real distribution of real images) so that it can deceive the discriminator network in believing synthesized patterns to be real ones. The formulation of the min-max game is:</p><formula xml:id="formula_0">min G max D v(G, D) = E x∼p data (x) [logD(x)]+ E z∼pz(z) [log(1 − D(G(z)))]<label>(1)</label></formula><p>where z is an input vector drawn from the distribution p z . It can be a noisy sample <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref> or a feature vector representation <ref type="bibr" target="#b4">[5]</ref>. When the game reaches convergence p z equals p data .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional GAN</head><p>Conditional GANs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref> were introduced as a more sophisticated extension of GANs, to widen their applicability. These networks are useful for cases like image editing.</p><formula xml:id="formula_1">C O N V U P C O N V C O N V C O N V x + x - S Ranking loss (ℓ #$%&amp; )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real/Fake loss (ℓ $() )</head><p>Loc encoder</p><formula xml:id="formula_2">x + S Image space loss (ℓ *+, )</formula><p>Encoder Generator Discriminator <ref type="figure">Fig. 2</ref>. The end-to-end training pipeline of our generative network, using different types of losses.</p><p>1) The encoder network produces the input of the generator.</p><p>2) The generator network synthesizes an object instance.</p><p>3) The synthesized object image is fed to the discriminator block, which is composed of ranking, adversarial and image space objectives. This joint objectives help to improve the generation and discrimination capabilities of the whole network.</p><p>Formally, we have:</p><formula xml:id="formula_3">min G max D v(G, D) = E x,y∼p data (x) [logD(x, y)]+ E z∼pz(z),y ∼py [log(1 − D(G(z, y ), y ))]<label>(2)</label></formula><p>One can apply different conditions y to control the generator when training a cGAN. Our method uses a feature representation extracted from images by an encoder network (CNN) and combined with a location feature, similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>, as the condition for training. This feature vector needs to be embedded into the input of our GAN in order to obtain the distribution of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Object Discovery by GAN</head><p>In this section, we present our proposed pipeline, i.e. a new deep generative network which is able to localize different object samples and to draw them at high resolution on demand. It creates templates identical to the real objects in an image, where multiple categories may be present simultaneously. The main motivation of this work is to effectively localize objects in a scene with or without using any ground-truth object locations in the training phase of the deep generative networks. We show that our new training pipeline of generative networks is capable of object discovery in complex scenes. Since the main application of this work is its deployment for weakly supervised object detection, we believe our proposed method can help the task to find accurate object locations for final detector training. We show through our extensive experiments that the new proposed generative model can localize the object samples effectively, and thus the synthesized instances improve the detection pipeline by a significant margin. Here, we firstly discuss the feasibility of our method to synthesize object instances in a supervised setting and later we employ this solution on weakly supervised task. <ref type="figure">Figure 2</ref> illustrates the pipeline of our proposed architecture. Our architecture has three components: 1) a visual and location encoder, 2) a generator network (can be considered as a decoder), and 3) a discriminator and ranking networks. The encoder extracts a high level visual representation of an input image. Afterwards a regional condition (discussed later) is added to the encoded features as input for the generator. The generator takes the entire encoded vector and produces the object it believes to be present in the specified region. This part provides information about the visual distribution of the image and about the location that we are interested in. After a synthetic image (object instance) is produced, the discriminator (with our novel configuration and set of losses) keeps correcting the generator in two ways: (i) by evaluating the similarity between the created sample and the actual object instance vs another object category, through the ranking network; and (ii) by maximizing the benefits from adversarial objective, through the discriminator network. We also show that image space loss which minimizes the difference between the synthesized object image and the real object instance is beneficial. We use the pre-trained GAN model from <ref type="bibr" target="#b3">[4]</ref>, which provides a generative model to create realistic, synthetic images by perceptual similarity metric learning. So in this way, we handle object instance discovery by a generative network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder network</head><p>The encoder network in our pipeline is convolutional network, like AlexNet <ref type="bibr" target="#b42">[43]</ref> or VGG-16 <ref type="bibr" target="#b43">[44]</ref> trained on the ImageNet dataset <ref type="bibr" target="#b44">[45]</ref> and in fully supervised cases finetuned on a target dataset of objects (e.g. PASCAL or COCO) which is not the same case in the weakly supervised method. We use AlexNet or VGG, both trained by global average pooling <ref type="bibr" target="#b45">[46]</ref>, but replace their fully connected layers by a new one, inspired by <ref type="bibr" target="#b19">[20]</ref>. The last fully connected layer carries the object activation neurons, after the conv layers. In the case of AlexNet, the size of this layer is 6400 (5 × 5 × 256). The weights of this network are updated during training, to improve the encoder's ability to distinguish separate object instances.</p><p>Location encoding (Loc encoder): As shown in <ref type="figure">Fig. 2</ref>, the class activation map (or heat map) of objects is fed to a small network with a convolution and a pooling layer, followed by a fully connected layer to encode the location. This encoded feature is used as the location condition for generative network. The location encoded features size is 320.</p><p>As the final input to the generative network, we combine the features that were obtained from the visual encoder with the location feature (6400 + 320).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generative network</head><p>The generative part of network synthesizes object instances based on the encoded features, obtained from the previous stage instead of noise input as in regular GANs. Inspired by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46]</ref>, our generator network contains multiple up-convolutional (deconvolutional) layers, which were pre-trained based on <ref type="bibr" target="#b3">[4]</ref>. This decoder focuses on a specified image region and draws the existing object instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discriminator-Ranking networks</head><p>Most of the novelty in our work comes with this block. It evaluates the object instance synthesized by the generator to improve both the system's generating and discriminating ability. We propose to use multiple new losses to do the task in the discriminator section. The combined objective functions (discussed further) together learn to capture the most important visual relations within and between object classes in the image. It empowers the generator to draw more accurate object instances. Exploiting the supervision that comes with bounding boxes, the ranking loss can visually compare the synthesized sample with its instance of origin and another object instance in the image. This loss function helps to discriminate between the samples in the encoding feature space. This term increases the probability of creating samples that activate different neurons for different categories. The discriminator part of the pipeline has two separate branch: one to calculate the ranking loss and one for adversarial training.</p><p>Ranking Network Given the synthesized object instance S, the original object instance x + and another object instance x − from a different category, we have (S, x + ) as a 'positive pair' and (S, x − ) as a 'negative pair'. As an example: suppose an image shows a cat and a dog, and we want to draw the cat instance only, so the cat is the positive and the dog is the negative sample. Using these two pairs, the ranking loss trains with a new similarity metric to improve the generative model. This network is identical to the encoder and has the same weights. We extract the features f from the last layer of ranking network (the network with ranking loss in <ref type="figure">Fig. 2</ref>) to calculate the loss using the cosine distance. We have:</p><formula xml:id="formula_4">Dist(S, x + ) = 1 − f (S).f (x + ) f (S) f (x + )<label>(3)</label></formula><p>The concept learned by ranking amounts to reducing the distance between the synthesized image and the positive sample, and making it smaller than the distance between the synthesized image and the negative sample. The objective function makes the feature space of similar objects correspondent. The hinge loss for the ranking task is given as:</p><formula xml:id="formula_5">rank = max{0, Dist(S, x + ) − Dist(S, x − )}<label>(4)</label></formula><p>Some related works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> use feature space loss that helps to minimize the feature distance between synthesized and real images. Using the proposed ranking loss overcomes the need for such a feature space loss in the condition when we have bounding box of object during training.</p><p>Adversarial Network In our pipeline, the adversarial training helps to prevent the network from producing blurred images and converge to much more realistic object instances. The discriminator network D tries to distinguish between fake versus real images, and is trained by minimizing the objective function given by:</p><formula xml:id="formula_6">disc = − i log(D(x + i )) + log(1 − D(S i ))<label>(5)</label></formula><p>and the generator minimizes the objective function, given as:</p><formula xml:id="formula_7">adv = − i log(D(S i ))<label>(6)</label></formula><p>The discriminator network is composed of 5 convolutional and two fully connected layers with softmax layer for fake/real prediction. The network is pre-trained based on regular GAN training from <ref type="bibr" target="#b3">[4]</ref>.</p><p>Image loss. This loss optimizes the generator's input vector (i.e. the output of Encoder and Loc encoder) to produce object samples that can stimulate the class activation to distinguish between objects effectively. Additionally it also provides solid gradients to fix unstable behavior of adversarial training. The following function should be maximized, given as:</p><formula xml:id="formula_8">img = i S i − x + i 2 2<label>(7)</label></formula><p>Using such a image objective function along with adversarial and ranking helps to improve the image reconstruction ability of the pipeline.</p><p>End-to-End training. The whole encoding-generating pipeline, with its three objective functions, is learned jointly by end-to-end stochastic gradient descent optimization. The total loss function of the network is given as:</p><p>T otal = α rank rank + α img img + α adv adv <ref type="bibr" target="#b7">(8)</ref> We provide more details about the training parameters such as loss coefficients α's in the experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Weakly Supervised GAN</head><p>Here, we propose weakly supervised object detection using our proposed encoderdecoder set of networks, without bounding boxes for training. Hence, instead we need a method to discover the most probable locations of objects for training. Since, we do not use object ground-truth bounding boxes (weakly supervised condition) in our ranking network, that means the object samples are not available to calculate the ranking objective function, for this reason we use the similar categories of objects from ImageNet dataset.</p><p>In preparation x + for a category like cat, we pick up a random image from the same category of ImageNet, and x − is taken from different category of ImageNet like dog. So we have the positive and negative samples to evaluate the synthesized cat using the ranking loss.</p><p>Since we do not have a good reference to calculate the image objective function, the feature space loss is added to the network to boost object instance localization for generating samples. This loss is a complement to the ranking loss in the case of weak supervision, as real samples can not be used to rank the synthesized images. Consequently, we have to modify the way in which we use the feature space loss in the current situation. The object proposals <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> are extracted and based on the resulting object heat map (similar to the supervised setup), we choose the top K=5 boxes with the highest heat score. To address the feature space loss, we average the features extracted from these five boxes as global representation of the possible object instance. And the averaged feature vector is applied to the feature space loss. The loss for features is computed as:</p><formula xml:id="formula_9">f + avg = j f (x + ij )/K f eat = i f (S i ) − f + avg 2 2 (9)</formula><p>The whole network is trained with the total loss of Eq. 8, but we replace the image loss by the feature loss.</p><p>After the training, we can synthesize realistic object instances as they exist at different image locations. These synthesized object instances can be used as the likely object templates to accurately localize real objects in the input image. As an example in <ref type="figure" target="#fig_0">Figure 3</ref>, we compare the difference between synthesized images in supervised and weakly supervised methods. To extract the bounding box location, we run a simple template matching convolving at 3 different scales to create an accurate heat map as illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. Once all of the object samples are extracted, they can be used as pseudo ground-truth. So we train a state-of-the-art object detector like fast RCNN or SSD <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> with our pseudo ground-truth object bounding boxes. The experiments section shows our results, which outperform other weakly supervised methods on PAS-CAL VOC and MS-COCO. In some of the images, there exists multiple samples of the same object category. Similar to <ref type="bibr" target="#b45">[46]</ref> where they apply connected-component method on the object heat-map to separate the samples, we also employ the same technique as a pre-processing step. Thus, for the model training we treat each of these samples as an independent instance.</p><p>Synthetic objects for detection. Another idea which improves the weakly supervised object detection, is to use synthetic data <ref type="bibr" target="#b50">[51]</ref> or hallucinated samples. Based on our model we obtain a set of synthetic images which look quite similar to object instances and thus is used as an extra image resource. This is shown to be effective in our weakly supervised object detection experiments. In future work, we plan to investigate it further. More details are provided in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we discuss the experimental details and evaluations of our proposed method. First, we introduce the datasets on which the evaluations were performed. In our experiments, we have two main evaluations: (i) evaluating the capability of our model to generate single object instance from multiple objects in an image; and (ii) evaluating the effectiveness of our weakly supervised object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation details</head><p>Our implementation is done in Torch and uses some pre-trained models from Caffe implementation of <ref type="bibr" target="#b3">[4]</ref>. All the networks are trained on two TitanX GPUs. For the network optimization, we use ADAM solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architectures:</head><p>Encoder:</p><p>In our experiments, we tested AlexNet and VGG-16 networks as image encoder to prepare the input to the generator. These networks are pre-trained on the ILSVRC'12 ImageNet dataset. In the fully supervised task, they were fine-tuned on the target datasets PASCAL and COCO for classification. These networks were trained with global average pooling to improve their localization ability, as shown in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>Generator: The configuration for this network is based on <ref type="bibr" target="#b3">[4]</ref> which has 3 fully connected layers initially, 6 up-convolution layers and also 3 convolution layers between the upconvs.</p><p>Discriminator block: This part has two network components. The network for the ranking loss is identical to the encoder and has shared weights. The discriminator network with adversarial loss has 5 layers of convolution and thereafter a global average pooling, followed by two fully connected layers. A drop-out of 0.5 is applied before the f c layers.</p><p>Training details: To train the total loss, we set the coefficients of the losses as: α rank = 0.05, α img = 10 −6 , α f eat = 10 −5 and α adv = 100. For the Adam optimization parameters, we set the β1 = 0.9, β2 = 0.99 and start with a learning rate of 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Datasets and evaluation metrics</head><p>The two datasets that we targeted for our evaluations are the PASCAL VOC 2007, 2010 and MS-COCO object datasets. The VOC datasets contain 20 categories, with almost 5K images for training and validation. The MS-COCO dataset is larger, including 80 categories with 80K training images, 40K validation images, and 20K testing images.</p><p>Experimental metrics: For the object detection evaluation, average precision (AP) and correct localization (CorLoc) have been used. Average precision is the standard metric as used in the PASCAL VOC competition. It takes a bounding box as a true detection when it has an intersection-over-union (IoU) of more than 50% with the ground-truth box. The Corloc is the fraction of images for which the method obtained the correct location for at least one object instance, cf. MS-COCO. Average precision is also used for the classification evaluation.</p><p>For quantitative evaluation of synthesized images, we measure pixel-level similarity error i.e. RMSE and SSIM. The similarity error is calculated between the generated object instance and the real object. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Synthesizing quantitative evaluation</head><p>Since there has been no other work very similar to ours, we have designed a new experiment on the PASCAL VOC dataset. We have evaluated the synthesizing power to produce object instances from test sets with 1000 samples. The object instances were cropped from the original images and we then generate the synthesized version of them using our pipeline and <ref type="bibr" target="#b3">[4]</ref>. We also measure the synthesis performance when other objects are present in the scene. In such a case, we have results with and without bounding box positions. <ref type="table" target="#tab_1">Table 1</ref> summarizes the quantitative evaluations. It is shown that under the same conditions as used by <ref type="bibr" target="#b3">[4]</ref>, our network can reproduce images with a higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation study of losses</head><p>We have evaluated different combinations of losses, with results shown in <ref type="table" target="#tab_2">Table 2</ref> and combining three losses performs the best. We have tested several mixes too, combining:  <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, removing the adversarial objective results to blurry images, and also drops the quantitative and qualitative performances too.</p><p>In <ref type="table">Table 3</ref>, we show the ablation study of the impact of different losses on weakly supervised object detection performance. We can clearly observe that, <ref type="table" target="#tab_2">Table 2</ref> relates very well to that of its individual performance of image reconstruction quantitatively shown in <ref type="table">Table 3</ref>. We observe that the best case in the image reconstruction performs the best in the detection too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Weakly supervised detection and classification</head><p>Comparison with the state-of-the-art: In this part the weakly supervised detection performance of our generative model is evaluated. Different methods with deep learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, clustering <ref type="bibr" target="#b27">[28]</ref> and multiple instance learning <ref type="bibr" target="#b33">[34]</ref> approaches are compared with our work. Once our model is trained, it produces the templates for each object instance and the corresponding object heat-maps are extracted via template matching. Using these maps, each object bounding box can be retrieved by looking for the maximum score based on a connected-component method like that of <ref type="bibr" target="#b45">[46]</ref>. After finding the boxes, Fast-RCNN or SSD object detectors is trained based on our pseudo groundtruth boxes. We report the results for both. <ref type="table" target="#tab_3">Table 4</ref> and <ref type="table" target="#tab_4">Table 5</ref> present average precision results on PASCAL VOC 2007, 2010 and MS-COCO for object detection. As can be seen, our method outperforms others for this task that use different methods. The CorLoc localization performance is also shown in <ref type="table">Table 6</ref>, for PASCAL VOC 2007. Our best performances are 45.3% and 46.4 (with extra synthesized objects) which is achieved with VGG-16 as the encoder. When using AlexNet, our approach works better than other methods using this network. As to the object proposals EdgeBox <ref type="bibr" target="#b47">[48]</ref> and SelectiveSearch <ref type="bibr" target="#b46">[47]</ref> are compared based on the Fast-RCNN detector trained by our boxes, we found that Edgebox performs better. <ref type="table" target="#tab_4">Table 5</ref> shows the effect of adding synthesized samples to a weakly supervised object detection. For each object sample found, we add one synthesized instance to increase the data size for training. With this extra augmentation method the results are improved by 1.1% and achieved 46.4%.</p><p>Object classification:</p><p>The proposed method is also tested for classification. The classification results for PASCAL VOC'07 are given in <ref type="table">Table 6</ref>. Once the training for the generative model is completed, the encoder network is fine-tuned for classification with pseudo ground-truth boxes. The main competitor to our work is <ref type="bibr" target="#b41">[42]</ref>, which used cascaded CNNs to train an object detector and classifier. We can clearly see that our proposed method shows improvement over <ref type="bibr" target="#b41">[42]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Our novel encoder-generator network contributes to solving important problems in visual recognition, like object discovery and weakly supervised detection. Synthesizing a particular object among the other objects present in an image has been rarely touched upon before. We propose to use different configurations of objective functions to train the visual encoder and generative network, and to utilize the resulting pipeline to separate the objects in an image from each other and the rest of the image. Our approach is a new method to synthesize object categories and to exploit that capability for object detection.</p><p>Combining supervision at the level of object locations and category labels with a ranking hypothesis was shown to be beneficial for training. We have demonstrated the power of our method through different experiments on the PASCAL VOC and MS-COCO datasets. Another important opportunity offered by our method is to train weakly supervised object detectors with the help of object discovery. In future work, we plan to use our proposed pipeline to annotate object bounding boxes for large-scale datasets, such as ImageNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Left: The figure shows an example of synthesizing the object in image (a) without/with using boxes in (b) and (c). Right: Finding an object location via template matching across the entire image, using the synthesized image as an effective template.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative performance: object synthesis on 1000 samples of PASCAL VOC objects.</figDesc><table><row><cell>Method</cell><cell>SSIM</cell><cell>RMSE</cell></row><row><cell>Dosovitskiy et al. [4]</cell><cell>0.16</cell><cell>0.36</cell></row><row><cell>Ours AlexNet</cell><cell>0.25</cell><cell>0.30</cell></row><row><cell>Ours VGG</cell><cell>0.28</cell><cell>0.27</cell></row><row><cell>Ours (whole img, box)</cell><cell>0.22</cell><cell>0.32</cell></row><row><cell>Ours (whole img)</cell><cell>0.20</cell><cell>0.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluation for the ablation study on losses by quantitative results.</figDesc><table><row><cell>Method</cell><cell>SSIM</cell><cell>RMSE</cell></row><row><cell>Ours (adv &amp; img &amp; ranking loss)</cell><cell>0.25</cell><cell>0.30</cell></row><row><cell>Ours (adv &amp; img loss)</cell><cell>0.21</cell><cell>0.34</cell></row><row><cell>Ours (adv &amp; ranking loss)</cell><cell>0.23</cell><cell>0.32</cell></row><row><cell>Ours (img &amp; ranking loss)</cell><cell>0.19</cell><cell>0.39</cell></row><row><cell cols="3">Table 3. The ablation study on losses for object detection performance in mean average preci-</cell></row><row><cell>sion (%).</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">VOC 2007</cell></row><row><cell cols="2">Ours (adv &amp; feature &amp; ranking loss)</cell><cell>45.3</cell></row><row><cell>Ours (adv &amp; feature loss)</cell><cell></cell><cell>40.1</cell></row><row><cell>Ours (adv &amp; ranking loss)</cell><cell></cell><cell>43.7</cell></row><row><cell>Ours (feature &amp; ranking loss)</cell><cell></cell><cell>34.2</cell></row><row><cell cols="3">(i) adversarial &amp; image space; (ii) adversarial &amp; ranking; and (iii) image space &amp; rank-</cell></row><row><cell>ing losses. As shown also in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Weakly supervised object detection performance in average precision (%) comparison on the VOC 2007, 2010, and COCO test set.</figDesc><table><row><cell>Method</cell><cell>VOC2007</cell><cell>VOC2010</cell><cell>MS-COCO</cell></row><row><cell>Cinbis et al. [34]</cell><cell>30.2</cell><cell>27.4</cell><cell>−</cell></row><row><cell>Wang et al. [52]</cell><cell>30.9</cell><cell>−</cell><cell>−</cell></row><row><cell>Li et al., AlexNet [39]</cell><cell>31</cell><cell>21.4</cell><cell>−</cell></row><row><cell>Li et al., VGG16 [39]</cell><cell>39.5</cell><cell>30.7</cell><cell>−</cell></row><row><cell>WSDDN [38]</cell><cell>39.3</cell><cell>36.2</cell><cell>11.5</cell></row><row><cell>WCCN AlexNet [42]</cell><cell>37.3</cell><cell>−</cell><cell>10.1</cell></row><row><cell>Jie et al [53]</cell><cell>41.7</cell><cell>38.3</cell><cell>−</cell></row><row><cell>WCCN VGG16 [42]</cell><cell>42.8</cell><cell>39.5</cell><cell>12.3</cell></row><row><cell>Ours AlexNet (FRCNN)</cell><cell>39.3</cell><cell>38.1</cell><cell>10.9</cell></row><row><cell>Ours VGG (FRCNN)</cell><cell>44.3</cell><cell>41.5</cell><cell>12.8</cell></row><row><cell>Ours VGG (SSD)</cell><cell>45.4</cell><cell>43.2</cell><cell>13.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Weakly supervised object detection average precision (%) on the PASCAL VOC 2007 dataset test set. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP Bilen et al. [28] 46.2 46.9 24.1 16.4 12.2 42.2 47.1 35.2 7.8 28.3 12.7 21.5 30.1 42.4 7.8 20.0 26.8 20.8 35.8 29.6 27.7 Cinbis et al. [34] 39.3 43.0 28.8 20.4 8.0 45.5 47.9 22.1 8.4 33.5 23.6 29.2 38.5 47.9 20.3 20.0 35.8 30.8 41.0 20.1 30.2 Wang et al. [52] 48.8 41.0 23.6 12.1 11.1 42.7 40.9 35.5 11.1 36.6 18.4 35.3 34.8 51.3 17.2 17.4 26.8 32.8 35.1 45.6 30.9 Li et al., VGG16 [39] 54.5 47.4 41.3 20.8 17.7 51.9 63.5 46.1 21.8 57.1 22.1 34.4 50.5 61.8 16.2 29.9 40.7 15.9 55.3 40.2 39.5 WCCN AlexNet [42] 43.9 57.6 34.9 21.3 14.7 64.7 52.8 34.2 6.5 41.2 20.5 33.8 47.6 56.8 12.7 18.8 39.6 46.9 52.9 45.1 37.3 HCP [53] 52.2 47.1 35.0 26.7 15.4 61.3 66.0 54.3 3.0 53.6 24.7 43.6 48.4 65.8 6.6 18.8 51.9 43.6 53.6 62.4 41.7 WCCN VGG16 [42] 49.5 60.6 38.6 29.2 16.2 70.8 56.9 42.5 10.9 44.1 29.9 42.2 47.9 64.1 13.8 23.5 45.9 54.1 60.8 54.5 42.8 SGWSOD [54] 48.5 63.2 33.2 31.0 14.5 69.4 61.7 56.6 8.5 41.3 37.6 50.0 54.1 62.7 22.9 20.6 42.1 50.7 54.3 55.2 43.9 Ours AlexNet 45.7 58.1 37.2 24.8 19 64.8 53.7 35.2 9.7 44.8 22.6 33.7 50.4 57.8 15.9 21.7 40.8 48.2 55.4 45.8 39.3 Ours VGG 50.9 61.2 40.5 31.4 21.1 71.6 58.1 42.9 11.7 46.4 30.7 44.5 48.3 64.9 16.8 24.8 47.1 55.7 61.7 55.8 44.3 Ours VGG (SSD) 51.7 62.7 40.6 33.8 22.3 71.4 59.8 43.3 12.5 48.1 32.5 44.8 49.1 64.7 17.4 25.8 48.9 56.7 63.5 57.1 45.3 Ours(+Synthesized data) 52.4 63.8 41.8 35.1 22.9 72.3 61.1 44.7 13.9 48.6 32.9 46.1 50.7 66.3 18.5 27 49.7 56.9 64.8 58.6 46.4</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Classification average precision (%) on the PASCAL VOC 2007 test set. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP WSDDN [38] 95.0 92.6 91.2 90.4 79.0 89.2 92.8 92.4 78.5 90.5 80.4 95.1 91.6 92.5 94.7 82.2 89.9 80.3 93.1 89.1 89.0 Oquab et al. [40] 88.5 81.5 87.9 82.0 47.5 75.5 90.1 87.2 61.6 75.7 67.3 85.5 83.5 80.0 95.6 60.8 76.8 58.0 90.4 77.9 77.7 94.8 92.8 91.7 84.1 93 93.5 93.9 80.7 91.9 85.3 97.5 93.4 92.6 96.1 84.2 91.1 83.3 95.5 89.6 90.9 SGWSOD [54] 98.3 97.4 96.5 95.7 79.6 93.9 97.5 96.9 79.7 92.3 82.7 97.6 97.2 95.9 99.1 84.2 92.5 83.7 97.3 92.7 92.5 Ours AlexNet 93.9 91.8 90.1 89.2 82.7 89.9 91.4 91.6 77.8 90.5 81.7 92.6 91 90.5 92.8 83.4 89.9 79.6 93 90.3 88.7 Ours VGG16 94.8 95 93.2 91.9 85 93.4 94.1 94.8 83 92.6 86.7 97.6 94 93.5 96.4 85.9 92.7 85 95.7 90.7 91.8 Ours(+Synthesized data) 94.9 95.2 93.7 92.7 85.7 93.8 94.7 94.7 84.3 92.8 87.5 98 94.5 93.6 96.7 86.4 93 86.8 96 91.1 92.3 Weakly supervised correct localization (%) on PASCAL VOC 2007 on positive (Cor-Loc) trainval set. Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP Bilen et al. [28] 66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14.0 38.5 49.5 60.0 19.8 39.2 41.7 30.1 50.2 44.1 43.7 Cinbis et al. [34] 65.3 55.0 52.4 48.3 18.2 66.4 77.8 35.6 26.5 67.0 46.9 48.4 70.5 69.1 35.2 35.2 69.6 43.4 64.6 43.7 52.0 Wang et al. [52] 80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3 69.5 14.1 38.3 58.8 47.2 49.1 60.9 48.5 Li et al., AlexNet [39] 77.3 62.6 53.3 41.4 28.7 58.6 76.2 61.1 24.5 59.6 18.0 49.9 56.8 71.4 20.9 44.5 59.4 22.3 60.9 48.8 49.8 Li et al., VGG16 [39] 78.2 67.1 61.8 38.1 36.1 61.8 78.8 55.2 28.5 68.8 18.5 49.2 64.1 73.5 21.4 47.4 64.6 22.3 60.9 52.3 52.4 WSDDN [38] 65.1 63.4 59.7 45.9 38.5 69.4 77.0 50.7 30.1 68.8 34.0 37.3 61.0 82.9 25.1 42.9 79.2 59.4 68.2 64.1 56.1 WCCN AlexNet [42] 79.7 68.1 60.4 38.9 36.8 61.1 78.6 56.7 27.8 67.7 20.3 48.1 63.9 75.1 21.5 46.9 64.8 23.4 60.2 52.4 52.6 WCCN VGG16 [42] 83.9 72.8 64.5 44.1 40.1 65.7 82.5 58.9 33.7 72.5 25.6 53.7 67.4 77.4 26.8 49.1 68.1 27.9 64.5 55.7 56.7 Ours AlexNet 83.5 70.9 65.4 42.4 39 63.9 80.8 58.6 30.2 69.5 24.8 51 66.2 78.4 25.2 48.7 66.6 26.7 63.3 55.9 55.6 Ours VGG16 85.5 75 66.9 47.5 43.6 67.4 83.6 61.7 36.8 75.1 29.8 55.9 70.4 80.6 29 52.9 71 31.2 66.9 58.1 59.4</figDesc><table><row><cell>SPPnet [55]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>− 82.4</cell></row><row><cell>AlexNet [38]</cell><cell cols="20">95.3 90.4 92.5 89.6 54.4 81.9 91.5 91.9 64.1 76.3 74.9 89.7 92.2 86.9 95.2 60.7 82.9 68.0 95.5 74.4 82.4</cell></row><row><cell>VGG16-net [44]</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>− 89.3</cell></row><row><cell>WCCN AlexNet [42]</cell><cell cols="13">93.1 91.1 89.6 88.9 81 89.6 90.7 91.2 76.4 89.2 80.8 92.2 90.1</cell><cell>89</cell><cell>92.7</cell><cell cols="5">82 89.3 78.1 92.8 89.1 87.8</cell></row><row><cell>WCCN VGG16 [42]</cell><cell>94.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">authors running</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets. In: NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Álvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06355</idno>
		<title level="m">Invertible conditional gans for image editing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances In Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="613" to="621" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00005</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Disentangling factors of variation for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning temporal transformations from time-lapse videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pixel-level domain transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping. In: ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Imagenet classification with deep convolutional neural networks. In: NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<title level="m">Selective search for object recognition. IJCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning deep object detectors from 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Saliency guided end-to-end learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06768</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
