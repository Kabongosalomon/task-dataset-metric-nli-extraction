<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Fidelity Image Generation With Fewer Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
						</author>
						<title level="a" type="main">High-Fidelity Image Generation With Fewer Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, highdimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self-and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels and outperform it using 20% of the labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep generative models have received a great deal of attention due to their power to learn complex highdimensional distributions, such as distributions over natural images <ref type="bibr" target="#b37">(van den Oord et al., 2016b;</ref><ref type="bibr" target="#b9">Dinh et al., 2017;</ref><ref type="bibr" target="#b3">Brock et al., 2019)</ref>, videos , and audio (van den <ref type="bibr" target="#b37">Oord et al., 2016a)</ref>. Recent progress was driven by scalable training of large-scale models <ref type="bibr" target="#b3">(Brock et al., 2019;</ref><ref type="bibr" target="#b25">Menick &amp; Kalchbrenner, 2019)</ref>, architectural modifications <ref type="bibr" target="#b40">(Zhang et al., 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2019a;</ref><ref type="bibr" target="#b19">Karras et al., 2019)</ref>, and normalization techniques .</p><p>Currently, high-fidelity natural image generation hinges upon having access to vast quantities of labeled data. The labels induce rich side information into the training process which effectively decomposes the extremely challenging image generation task into semantically meaningful sub-tasks. <ref type="bibr">Proceedings</ref>   However, this dependence on vast quantities of labeled data is at odds with the fact that most data is unlabeled, and labeling itself is often costly and error-prone. Despite the recent progress on unsupervised image generation, the gap between conditional and unsupervised models in terms of sample quality is significant.</p><p>In this work, we take a significant step towards closing the gap between conditional and unsupervised generation of high-fidelity images using generative adversarial networks (GANs). We leverage two simple yet powerful concepts: (i) Self-supervised learning: A semantic feature extractor for the training data can be learned via self-supervision, and the resulting feature representation can then be employed to guide the GAN training process. (ii) Semi-supervised learning: Labels for the entire training set can be inferred from a small subset of labeled training images and the inferred labels can be used as conditional information for GAN training. Our contributions In this work, we 1. propose and study various approaches to reduce or fully omit ground-truth label information for natural image generation tasks, 2. achieve a new state of the art (SOTA) in unsupervised generation on <ref type="bibr">IMAGENET,</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and related work</head><p>High-fidelity GANs on IMAGENET Besides BIGGAN <ref type="bibr" target="#b3">(Brock et al., 2019)</ref> only a few prior methods have managed to scale GANs to ImageNet, most of them relying on class-conditional generation using labels. One of the earliest attempts are GANs with auxiliary classifier (AC-GANs) <ref type="bibr" target="#b31">(Odena et al., 2017)</ref> which feed one-hot encoded label information with the latent code to the generator and equip the discriminator with an auxiliary head predicting the image class in addition to whether the input is real or fake. More recent approaches rely on a label projection layer in the discriminator, essentially resulting in per-class real/fake classification , and self-attention in the generator <ref type="bibr" target="#b40">(Zhang et al., 2019)</ref>. Both methods use modulated batch normalization <ref type="bibr" target="#b7">(De Vries et al., 2017)</ref> to provide label information to the generator. On the unsupervised side, <ref type="bibr" target="#b6">Chen et al. (2019b)</ref> showed that auxiliary rotation loss added to the discriminator has a stabilizing effect on the training. Finally, appropriate gradient regularization enables scaling MMD-GANs to ImageNet without using labels <ref type="bibr" target="#b1">(Arbel et al., 2018)</ref>.</p><p>Semi-supervised GANs Several recent works leveraged GANs for semi-supervised learning of classifiers. Both <ref type="bibr" target="#b34">Salimans et al. (2016)</ref> and <ref type="bibr" target="#b30">Odena (2016)</ref> train a discriminator that classifies its input into K + 1 classes: K image classes for real images, and one class for generated images. Similarly, <ref type="bibr" target="#b36">Springenberg (2016)</ref> extends the standard GAN objective to K classes. This approach was also considered by <ref type="bibr" target="#b23">Li et al. (2017)</ref> where separate discriminator and classifier models are applied. Other approaches incorporate inference models to predict missing labels <ref type="bibr" target="#b8">(Deng et al., 2017)</ref> or harness joint distribution (of labels and data) matching for semi-supervised learning <ref type="bibr" target="#b12">(Gan et al., 2017)</ref>. Up to our knowledge, improvements in sample quality through partial label information are only reported in <ref type="bibr" target="#b23">Li et al. (2017)</ref>; <ref type="bibr" target="#b8">Deng et al. (2017)</ref>; <ref type="bibr" target="#b37">Sricharan et al. (2017)</ref>, all of which consider only low-resolution data sets from a restricted domain.</p><p>Self-supervised learning Self-supervised learning methods employ a label-free auxiliary task to learn a semantic feature representation of the data. This approach was successfully applied to different data modalities, such as images <ref type="bibr" target="#b10">(Doersch et al., 2015;</ref><ref type="bibr" target="#b4">Caron et al., 2018)</ref>, video <ref type="bibr" target="#b0">(Agrawal et al., 2015;</ref><ref type="bibr" target="#b22">Lee et al., 2017)</ref>, and robotics <ref type="bibr" target="#b17">(Jang et al., 2018;</ref><ref type="bibr" target="#b32">Pinto &amp; Gupta, 2016)</ref>. The current state-of-the-art method on IMAGENET is due to <ref type="bibr" target="#b13">Gidaris et al. (2018)</ref> who proposed predicting the rotation angle of rotated training images as an auxiliary task. This simple self-supervision approach yields representations which are useful for downstream image classification tasks. Other forms of self-supervision include predicting relative locations of disjoint image patches of a given image <ref type="bibr" target="#b10">(Doersch et al., 2015;</ref><ref type="bibr" target="#b28">Mundhenk et al., 2018)</ref> or estimating the permutation of randomly swapped image  patches on a regular grid <ref type="bibr" target="#b29">(Noroozi &amp; Favaro, 2016)</ref>. A study on self-supervised learning with modern neural architectures is provided in <ref type="bibr" target="#b20">Kolesnikov et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reducing the appetite for labeled data</head><p>In a nutshell, instead of providing hand-annotated ground truth labels for real images to the discriminator, we will provide inferred ones. To obtain these labels we will make use of recent advancements in self-and semi-supervised learning. We propose and study several different methods with different degrees of computational and conceptual complexity. We emphasize that our work focuses on using few labels to improve the quality of the generative model, rather than training a powerful classifier from a few labels as extensively studied in prior work on semi-supervised GANs.</p><p>Before introducing these methods in detail, we discuss how label information is used in state-of-the-art GANs. The following exposition assumes familiarity with the basics of the GAN framework <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref>.</p><p>Incorporating the labels To provide the label information to the discriminator we employ a linear projection layer as proposed by . To make the exposition self-contained, we will briefly recall the main ideas. In a "vanilla" (unconditional) GAN, the discriminator D learns to predict whether the image x at its input is real or generated by the generator G. We decompose the discriminator into a learned discriminator representation,D, which is fed into a linear classifier, c r/f , i.e., the discriminator is given by c r/f (D(x)). In the projection discriminator, one learns an embedding for each class of the same dimension as the representationD(x). Then, for a given image, label input x, y the decision on whether the sample is real or generated is based on two components: (a) on whether the representationD(x) itself is consistent with the real data, and (b) on whether the representationD(x) is consistent with the real data from class y. More formally, the discrim- inator takes the form D(x, y) = c r/f (D(x)) + P (D(x), y), where P (x, y) =x W y is a linear projection layer with learned weight matrix W applied to a feature vectorx and the one-hot encoded label y as an input. As for the generator, the label information y is incorporated through classconditional BatchNorm <ref type="bibr" target="#b11">(Dumoulin et al., 2017;</ref><ref type="bibr" target="#b7">De Vries et al., 2017)</ref>. The conditional GAN with projection discriminator is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>We proceed with describing the proposed pre-trained and co-training approaches to infer labels for GAN training in Sections 3.1 and 3.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-trained approaches</head><p>Unsupervised clustering-based method We first learn a representation of the real training data using a stateof-the-art self-supervised approach <ref type="bibr" target="#b13">(Gidaris et al., 2018;</ref><ref type="bibr" target="#b20">Kolesnikov et al., 2019)</ref>, perform clustering on this representation, and use the cluster assignments as a replacement for labels. Following <ref type="bibr" target="#b13">Gidaris et al. (2018)</ref> we learn the feature extractor F (typically a convolutional neural network) by minimizing the following self-supervision loss</p><formula xml:id="formula_0">L R = − 1 |R| r∈R E x∼p data (x) [log p(c R (F (x r )) = r)], (1) where R is the set of the 4 rotation degrees {0 • , 90 • , 180 • , 270 • },</formula><p>x r is the image x rotated by r, and c R is a linear classifier predicting the rotation degree r. After learning the feature extractor F , we apply mini batch k-Means clustering <ref type="bibr" target="#b35">(Sculley, 2010)</ref> on the representations of the training images. Finally, given the cluster assignment functionŷ CL = c CL (F (x)) we train the GAN using the hinge loss, alternatively minimizing the <ref type="figure">Figure 4</ref>. CLUSTERING: Unsupervised approach based on clustering the representations obtained by solving a self-supervised task. F corresponds to the feature extractor learned via self-supervision and cCL is the cluster assignment function. After learning F and cCL on the real training images in the pre-training step, we proceed with conditional GAN training by inferring the labels aŝ yCL = cCL(F (x)).</p><formula xml:id="formula_1">D D G y f zD x f x r y f cr/f P F cCL</formula><p>discriminator loss L D and generator loss L G , namely</p><formula xml:id="formula_2">L D = −E x∼p data (x) [min(0, −1 + D(x, c CL (F (x))))] − E (z,y)∼p(z,y) [min(0, −1 − D(G(z, y), y))] L G = −E (z,y)∼p(z,y) [D(G(z, y), y)],</formula><p>wherep(z, y) = p(z)p(y) is the prior distribution with p(z) = N (0, I) andp(y) the empirical distribution of the cluster labels c CL (F (x)) over the training set. We call this approach CLUSTERING and illustrate it in <ref type="figure">Figure 4</ref>.</p><p>Semi-supervised method While semi-supervised learning is an active area of research and a large variety of algorithms has been proposed, we follow <ref type="bibr" target="#b39">Zhai et al. (2019)</ref> and simply extend the self-supervised approach described in the previous paragraph with a semi-supervised loss. This ensures that the two approaches are comparable in terms of model capacity and computational cost. Assuming we are provided with labels for a subset of the training data, we attempt to learn a good feature representation via selfsupervision and simultaneously train a good linear classifier on the so-obtained representation (using the provided labels). 1 More formally, we minimize the loss</p><formula xml:id="formula_3">L S 2 L = − 1 |R| r∈R E x∼p data (x) [log p(c R (F (x r )) = r)] + γE (x,y)∼p data (x,y) [log p(c S 2 L (F (x r )) = y)] ,<label>(2)</label></formula><p>where c R and c S 2 L are linear classifiers predicting the rotation angle r and the label y, respectively, and γ &gt; 0 balances the loss terms. The first term in (2) corresponds to the self-supervision loss from (1) and the second term to a (semi-supervised) cross-entropy loss. During training, the latter expectation is replaced by the empirical average over the subset of labeled training examples, whereas the former is set to the empirical average over the entire training set (this convention is followed throughout the paper). After we obtain F and c S 2 L we proceed with GAN training where we label the real images asŷ S 2 L = c S 2 L (F (x)). In particular, we alternatively minimize the same generator and discriminator losses as for CLUSTERING except that we use c S 2 L and F obtained by minimizing <ref type="formula" target="#formula_3">(2)</ref>:</p><formula xml:id="formula_4">L D = −E x∼p data (x) [min(0, −1 + D(x, c S 2 L (F (x))))] − E (z,y)∼p(z,y) [min(0, −1 − D(G(z, y), y))] L G = −E (z,y)∼p(z,y) [D(G(z, y), y)],</formula><p>where p(z, y) = p(z)p(y) with p(z) = N (0, I) and p(y) uniform categorical. We use the abbreviation S 2 GAN for this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Co-training approach</head><p>The main drawback of the transfer-based methods is that one needs to train a feature extractor F via self-supervision and learn an inference mechanism for the labels (linear classifier or clustering). In what follows we detail co-training approaches that avoid this two-step procedure and learn to infer label information during GAN training.</p><p>Unsupervised method We consider two approaches. In the first one, we completely remove the labels by simply labeling all real and generated examples with the same label 2 and removing the projection layer from the discriminator, i.e., we set D(x) = c r/f (D(x)). We use the abbreviation  <ref type="figure">Figure 6</ref>. Self-supervision by rotation-prediction during GAN training. Additionally to predicting whether the images at its input are real or generated, the discriminator is trained to predict rotations of both rotated real and fake images via an auxiliary linear classifier cR. This approach was successfully applied by <ref type="bibr" target="#b6">Chen et al. (2019b)</ref> to stabilize GAN training. Here we combine it with our pre-trained and co-training approaches, replacing the ground truth labels yr with predicted ones.</p><p>SINGLE LABEL for this method. For the second approach we assign random labels to (unlabeled) real images. While the labels for the real images do not provide any useful signal to the discriminator, the sampled labels could potentially help the generator by providing additional randomness with different statistics than z, as well as additional trainable parameters due to the embedding matrices in class-conditional BatchNorm. Furthermore, the labels for the fake data could facilitate the discrimination as they provide side information about the fake images to the discriminator. We term this method RANDOM LABEL.</p><p>Semi-supervised method When labels are available for a subset of the real data, we train an auxiliary linear classifier c CT directly on the feature representationD of the discriminator, during GAN training, and use it to predict labels for the unlabeled real images. In this case the discriminator loss takes the form</p><formula xml:id="formula_5">L D = − E (x,y)∼p data (x,y) [min(0, −1 + D(x, y))] − λE (x,y)∼p data (x,y) [log p(c CT (D(x)) = y)] − E x∼p data (x) [min(0, −1 + D(x, c CT (D(x))))] − E (z,y)∼p(z,y) [min(0, −1 − D(G(z, y), y))],<label>(3)</label></formula><p>where the first term corresponds to standard conditional training on (k%) labeled real images, the second term is the cross-entropy loss (with weight λ &gt; 0) for the auxiliary classifier c CT on the labeled real images, the third term is an unsupervised discriminator loss where the labels for the unlabeled real images are predicted by c CT , and the last term is the standard conditional discriminator loss on the generated data. We use the abbreviation S 2 GAN-CO for this method. See <ref type="figure" target="#fig_3">Figure 5</ref> for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-supervision during GAN training</head><p>So far we leveraged self-supervision to either craft good feature representations, or to learn a semi-supervised model (cf. Section 3.1). However, given that the discriminator itself is just a classifier, one may benefit from augmenting this classifier with an auxiliary task-namely self-supervision through rotation prediction. This approach was already explored in <ref type="bibr" target="#b6">Chen et al. (2019b)</ref>, where it was observed to stabilize GAN training. Here we want to assess its impact when combined with the methods introduced in Sections 3.1 and 3.2. To this end, similarly to the training of F in <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_3">(2)</ref>, we train an additional linear classifier c R on the discriminator feature representationD to predict rotations r ∈ R of the rotated real images x r and rotated fake images G(z, y) r . The corresponding loss terms added to the discriminator and generator losses are</p><formula xml:id="formula_6">− β |R| r∈R E x∼p data (x) [log p(c R (D(x r ) = r)]<label>(4)</label></formula><p>and</p><formula xml:id="formula_7">− α |R| E (z,y)∼p(z,y) [log p(c R (D(G(z, y) r ) = r)],<label>(5)</label></formula><p>respectively, where α, β &gt; 0 are weights to balance the loss terms. This approach is illustrated in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>Architecture and hyperparameters GANs are notoriously unstable to train and their performance strongly depends on the capacity of the neural architecture, optimization hyperparameters, and appropriate regularization <ref type="bibr" target="#b21">Kurach et al., 2019)</ref>. We implemented the conditional BIGGAN architecture <ref type="bibr" target="#b3">(Brock et al., 2019)</ref> which achieves state-of-the-art results on ImageNet. <ref type="bibr">3</ref> We use exactly the same optimization hyper-parameters as <ref type="bibr" target="#b3">Brock et al. (2019)</ref>. Specifically, we employ the Adam optimizer with the learning rates 5 · 10 −5 for the generator and 2 · 10 −4 for the discriminator (β 1 = 0, β 2 = 0.999). We train for 250k generator steps with 2 discriminator steps before each generator step. The batch size was fixed to 2048, and we use a latent code z with 120 dimensions. We employ spectral normalization in both generator and discriminator. In contrast to BIGGAN, we do not apply orthogonal regularization as this was observed to only marginally improve  <ref type="formula" target="#formula_3">(2019)</ref>) and we do not use the truncation trick.</p><p>Datasets We focus primarily on IMAGENET, the largest and most diverse image data set commonly used to evaluate GANs. IMAGENET contains 1.3M training images and 50k test images, each corresponding to one of 1k object classes. We resize the images to 128 × 128 × 3 as done in  and <ref type="bibr" target="#b40">Zhang et al. (2019)</ref>. Partially labeled data sets for the semi-supervised approaches are obtained by randomly selecting k% of the samples from each class.</p><p>Evaluation metrics We use the Fréchet Inception Distance (FID) <ref type="bibr" target="#b16">(Heusel et al., 2017)</ref> and Inception Score <ref type="bibr" target="#b34">(Salimans et al., 2016)</ref> to evaluate the quality of the generated samples. To compute the FID, the real data and generated samples are first embedded in a specific layer of a pre-trained Inception network. Then, a multivariate Gaussian is fit to the data and the distance computed as FID(x, g) = ||µ x − µ g || 2 2 + Tr(Σ x + Σ g − 2(Σ x Σ g ) 1 2 ), where µ and Σ denote the empirical mean and covariance, and subscripts x and g denote the real and generated data respectively. FID was shown to be sensitive to both the addition of spurious modes and to mode dropping <ref type="bibr" target="#b33">(Sajjadi et al., 2018;</ref><ref type="bibr" target="#b24">Lucic et al., 2018)</ref>. Inception Score posits that conditional label distribution of samples containing meaningful objects should have low entropy, and the variability of the samples should be high leading to the following formulation: IS = exp(E x∼Q [d KL (p(y | x), p(y))]). Although it has some flaws <ref type="bibr" target="#b2">(Barratt &amp; Sharma, 2018)</ref>, we report it to enable comparison with existing methods. Following <ref type="bibr" target="#b3">Brock et al. (2019)</ref>, the FID is computed using the 50k IMAGENET testing images and 50k randomly sampled fake images, and the IS is computed from 50k randomly sampled fake images. All metrics are computed for 5 different randomly sampled sets of fake images and are then averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>We conduct an extensive comparison of methods detailed in <ref type="table" target="#tab_3">Table 1</ref>, namely: Unmodified BIGGAN, the unsupervised methods SINGLE LABEL, RANDOM LABEL, CLUSTERING, and the semi-supervised methods S 2 GAN and S 2 GAN-CO. In all S 2 GAN-CO experiments we use soft labels, i.e., the soft-max output of c CT instead of onehot encoded hard estimates, as we observed in preliminary experiments that this stabilizes training. For S 2 GAN we use hard labels by default, but investigate the effect of soft labels in separate experiments. For all semi-supervised methods we have access only to k% of the ground truth labels where k ∈ {5, 10, 20}. As an additional baseline, we retain k% labeled real images and discard all unlabeled real images, then using the remaining labeled images to train BIGGAN (the resulting model is designated by BIGGAN-k%). Finally, we explore the effect of self-supervision during GAN training on the unsupervised and semi-supervised methods. We train every model three times with a different random seed and report the median FID and the median IS. With the exception of the SINGLE LABEL and BIGGAN-k%, the standard deviation of the mean across three runs is very low. We therefore defer tables with the mean FID and IS values and standard deviations to Appendix D. All models are trained on 128 cores of a Google TPU v3 Pod with BatchNorm statistics synchronized across cores.</p><p>Unsupervised approaches For CLUSTERING we simply used the best available self-supervised rotation model from <ref type="bibr" target="#b20">Kolesnikov et al. (2019)</ref>. The number of clusters for CLUSTERING is selected from the set {50, 100, 200, 500, 1000}. The other unsupervised approaches do not have hyper-parameters.</p><p>Pre-trained and co-training approaches We employ the wide ResNet-50 v2 architecture with widening factor 16 <ref type="bibr" target="#b38">(Zagoruyko &amp; Komodakis, 2016)</ref> for the feature extractor F in the pre-trained approaches described in Section 3.1.</p><p>We optimize the loss in <ref type="formula" target="#formula_3">(2)</ref>   <ref type="figure">(F (x)</ref>) on the IMAGENET validation set is reported in <ref type="table">Table 3</ref>. The parameter λ in the loss used for S 2 GAN-CO in (3) is selected form the set {0.1, 0.2, 0.4}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervision during GAN training</head><p>For all approaches we use the recommend parameter α = 0.2 from <ref type="bibr" target="#b6">(Chen et al., 2019b)</ref> in <ref type="formula" target="#formula_7">(5)</ref> and do a small sweep for β in (4). For the values tried ({0.25, 0.5, 1.0, 2}) we do not see a large effect and use β = 0.5 for S 3 GAN. For S 3 GAN-CO we did not repeat the sweep, and instead used β = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and discussion</head><p>Recall that the main goal of this work is to match (or outperform) the fully supervised BIGGAN in an unsupervised fashion, or with a small subset of labeled data. In the following, we discuss the advantages and drawbacks of the analyzed approaches with respect to this goal.</p><p>As a baseline, our reimplementation of BIGGAN obtains an FID of 8.4 and IS of 75.0, and hence reproduces the result reported by <ref type="bibr" target="#b3">Brock et al. (2019)</ref> in terms of FID. We observed some differences in training dynamics, which we discuss in detail in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Unsupervised approaches</head><p>The results for unsupervised approaches are summarized in <ref type="figure">Figure 7</ref> and <ref type="table">Table 2</ref>. The fully unsupervised RANDOM LABEL and SINGLE LABEL models both achieve a similar FID of ∼ 25 and IS of ∼ 20. This is a quite considerable gap compared to BIGGAN and indicates that additional supervision is necessary. We note that one of the three SIN-GLE LABEL models collapsed whereas all three RANDOM LABEL models trained stably for 250k generator iterations.</p><p>Pre-training a semantic representation using self-supervision and clustering the training data on this representation as done by CLUSTERING reduces the FID by about 10% and increases IS by about 10%. These results were obtained for 50 clusters, all other options led to worse results. While this performance is still considerably worse than that of BIGGAN this result is the current SOTA in unsupervised image generation <ref type="bibr" target="#b6">(Chen et al. (2019b)</ref> report an FID of 33 for unsupervised generation).</p><p>Example images from the clustering are shown in Figures 14, <ref type="table">Table 2</ref>. Median FID and IS for the unsupervised approaches (see <ref type="table" target="#tab_3">Table 14</ref> in the appendix for mean and standard deviation). 15, and 16 in the supplementary material. The clustering is clearly meaningful and groups similar objects within the same cluster. Furthermore, the objects generated by CLUS-TERING conditionally on a given cluster index reflect the distribution of the training data belonging to the corresponding cluster. On the other hand, we can clearly observe multiple classes being present in the same cluster. This is to be expected when under-clustering to 50 clusters. Interestingly, clustering to many more clusters (say 500) yields results similar to SINGLE LABEL.  <ref type="figure">Figure 7</ref>. Median FID obtained by our unsupervised approaches. The vertical line indicates the median FID of our BIGGAN implementation which uses labels for all training images. While the gap between unsupervised and fully supervised approaches remains significant, using a pre-trained self-supervised representation (CLUSTERING) improves the sample quality compared to SINGLE LABEL and RANDOM LABEL, leading to a new SOTA in unsupervised generation on IMAGENET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Semi-supervised approaches</head><p>Pre-trained The S 2 GAN model where we use the classifier pre-trained with both a self-supervised and semisupervised loss (cf. Section 3.1) matches the BIGGAN baseline when 20% of the labels are used and incurs a minor increase in FID when 10% and 5% are used (cf. <ref type="table">Table 3</ref>). We stress that this is despite the fact that the classifier used to infer the labels has a top-1 accuracy of only 50%, 63%, and 71% for 5%, 10%, and 20% labeled data, respectively (cf <ref type="table">. Table 3</ref>), compared to 100% of the original labels. The results are shown in <ref type="table">Table 4</ref> and <ref type="figure" target="#fig_8">Figure 8</ref>, and random samples as well as interpolations can be found in <ref type="figure" target="#fig_0">Figures 9-17</ref> in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Co-trained</head><p>The results for our co-trained model S 2 GAN-CO which trains a linear classifier in semi-supervised fashion on top of the discriminator representation during GAN training (cf. Section 3.2) are shown in <ref type="table">Table 4</ref>. It can be seen that S 2 GAN-CO outperforms all fully unsupervised approaches for all considered label percentages. While the gap between S 2 GAN-CO with 5% labels and CLUSTER-ING in terms of FID is small, S 2 GAN-CO has a considerably larger IS. When using 20% labeled training examples S 2 GAN-CO obtains an FID of 13.9 and an IS of 49.2, which is remarkably close to BIGGAN and S 2 GAN given the simplicity of the S 2 GAN-CO approach. As the the percentage of labels decreases, the gap between S 2 GAN and S 2 GAN-CO increases.</p><p>Interestingly, S 2 GAN-CO does not seem to train less stably than S 2 GAN approaches even though it is forced to learn the classifier during GAN training. This is particularly remarkable as the BIGGAN-k% approaches, where we only retain the labeled data for training and discard all unlabeled data, are very unstable and collapse after 60k to 120k iterations, for all three random seeds and for both 10% and 20% labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Self-supervision during GAN training</head><p>So far we have seen that the pre-trained semi-supervised approach, namely S 2 GAN, is able to achieve state-of-theart performance for 20% labeled data. Here we investigate whether self-supervision during GAN training as described in Section 3.3 can lead to further improvements. <ref type="table">Table 4</ref> and <ref type="figure" target="#fig_8">Figure 8</ref> show the experimental results for S 3 GAN, namely S 2 GAN coupled with self-supervision in the discriminator.</p><p>Self-supervision leads to a reduction in FID and increase in IS across all considered settings. In particular we can match the state-of-the-art BIGGAN with only 10% of the labels and outperform it using 20% labels, both in terms of FID and IS.</p><p>For S 3 GAN the improvements due to self-supervision during GAN training in FID are considerable, around 10% in most of the cases. Tuning the parameter β of the discriminator self-supervision loss in (4) did not dramatically increase the benefits of self-supervision during GAN training, at least for the range of values considered. As shown in Tables 2 and 4, self-supervision during GAN training (with default parameters α, β) also leads to improvements by 5 to 10% for both S 2 GAN-CO and SINGLE LABEL. In summary, self- <ref type="table">Table 3</ref>. Top-1 and top-5 error rate (%) on the IMAGENET validation set of c S 2 L (F (x)) using both self-and semi-supervised losses as described in Section 3.1. While the models are clearly not state-of-the-art compared to the fully supervised IMAGENET classification task, the quality of labels is sufficient to match and in some cases improve the state-of-the-art GAN natural image synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LABELS METRIC</head><p>5% 10% 20%</p><p>TOP-1 ERROR 50.08 36.74 29.21 TOP-5 ERROR 26.94 16.04 10.33 <ref type="table">Table 4</ref>. Pre-trained vs co-training approaches, and the effect of self-supervision during GAN training (see <ref type="table" target="#tab_3">Table 12</ref> in the appendix for mean and standard deviation). While co-training approaches outperform fully unsupervised approaches, they are clearly outperformed by the pre-trained approaches. Self-supervision during GAN training helps in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FID IS</head><p>5% 10% 20% 5% 10% 20% supervision during GAN training with default parameters leads to a stable improvement across all approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Other insights</head><p>Effect of soft labels A design choice available to practitioners is whether to use hard labels (i.e., the argmax over the logits), or soft labels (softmax over the logits) for S 2 GAN and S 3 GAN (recall that we use soft labels by default for S 2 GAN-CO and S 3 GAN-CO). Our initial expectation was that soft labels should help when very little labeled data is available, as soft labels carry more information which can potentially be exploited by the projection discriminator. Surprisingly, the results presented in <ref type="table">Table 5</ref> show clearly that the opposite is true. Our current hypothesis is that this is due to the way labels are incorporated in the projection discriminator, but we do not have empirical evidence yet.</p><p>Optimization dynamics <ref type="bibr" target="#b3">Brock et al. (2019)</ref> report the FID and IS of the model just before the collapse, which can <ref type="table">Table 5</ref>. Training with hard (predicted) labels leads to better models than training with soft (predicted) labels (see <ref type="table" target="#tab_3">Table 13</ref> in the appendix for mean and standard deviation be seen as a form of early stopping. In contrast, we manage to stably train the proposed models for 250k generator iterations. In particular, we also observe stable training for our "vanilla" BIGGAN implementation. The evolution of the FID and IS as a function of the training steps is shown in <ref type="figure" target="#fig_0">Figure 21</ref> in the appendix. At this point we can only speculate about the origin of this difference. We finally note that by tuning the learning rate we obtained slightly different (but still stable) training dynamics in terms of IS, achieving FID 6.9 and IS 98 for S 3 GAN with 20% labels.</p><p>Higher resolution and going below 5% labels Training these models at higher resolution becomes computationally harder and it necessitates tuning the learning rate. We trained several S 3 GAN models at 256 × 256 resolution and show the resulting samples in <ref type="figure" target="#fig_0">Figures 12-13</ref> and interpolations in <ref type="figure" target="#fig_0">Figures 19-20</ref>. We also conducted S 3 GAN experiments in which only 2.5% of the labels are used and observed FID of 13.6 and IS of 46.3. This indicates that given a small number of samples one can significantly outperform the unsupervised approaches (c.f. <ref type="figure">Figure 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future Work</head><p>In this work we investigated several avenues to reduce the appetite for labeled data in state-of-the-art GANs. We showed that recent advances in self and semi-supervised learning can be used to achieve a new state of the art, both for unsupervised and supervised natural image synthesis.</p><p>We believe that this is a great first step towards the ultimate goal of few-shot high-fidelity image synthesis. There are several important directions for future work: (i) investigating the applicability of these techniques for even larger and more diverse data sets, and (ii) investigating the impact of other self-and semi-supervised approaches on the model quality. (iii) investigating the impact of self-supervision in other deep generative models. Finally, we would like to emphasize that further progress might be hindered by the engineering challenges related to training large-scale generative adversarial networks. To help alleviate this issue and to foster reproducibility, we have open-sourced all the code used for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architectural details</head><p>The ResNet architecture implemented following <ref type="bibr" target="#b3">Brock et al. (2019)</ref> is described in <ref type="table" target="#tab_5">Tables 6 and 8</ref>. We use the abbreviations RS for resample, BN for batch normalization, and cBN for conditional BN <ref type="bibr" target="#b11">(Dumoulin et al., 2017;</ref><ref type="bibr" target="#b7">De Vries et al., 2017)</ref>. In the resample column, we indicate downscale(D)/upscale(U)/none(-) setting and in the spectral norm column shows whether spectral normalization is applied to all weights in the layer. In <ref type="table" target="#tab_6">Table 8</ref>, y stands for the labels and h is the output from the layer before (i.e., the pre-logit layer). <ref type="table">Tables 7 and 9</ref> show ResBlock details. The addition layer merges the shortcut path and the convolution path by adding them. h and w are the input height and width of the ResBlock, c i and c o are the input channels and output channels for a ResBlock. For the last ResBlock in the discriminator without downsampling, we simply drop the shortcut layer from ResBlock. We list all the trainable variables and their shape in <ref type="table" target="#tab_3">Tables 10 and 11</ref>.    </p><formula xml:id="formula_8">U 2h × 2w × c o cBN, ReLU - - h × w × c i Conv [3, 3, 1] U 2h × 2w × c o cBN, ReLU - - 2h × 2w × c o Conv [3, 3, 1] - 2h × 2w × c o Addition - - 2h × 2w × c o</formula><formula xml:id="formula_9">S 2 GAN-CO (5%) S 3 GAN-CO (5%) S 2 GAN (5%) S 3 GAN (5%) S 2 GAN-CO (10%) S 3 GAN-CO (10%) S 2 GAN (10%) S 3 GAN (10%) S 2 GAN-CO (20%) S 3 GAN-CO (20%) S 2 GAN (20%)<label>S</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. FID and IS: Mean and standard deviations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Median FID of the baselines and the proposed method. The vertical line indicates the baseline (BIGGAN) which uses all the labeled data. The proposed method (S 3 GAN) is able to match the state-of-the-art while using only 10% of the labeled data and outperform it with 20%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Top row: 128 × 128 samples from our implementation of the fully supervised current SOTA model BIGGAN. Bottom row: Samples form the proposed S 3 GAN which matches BIGGAN in terms of FID and IS using only 10% of the ground-truth labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Conditional GAN with projection discriminator. The discriminator tries to predict from the representationD whether a real image xr (with label yr) or a generated image xf (with label yf) is at its input, by combining an unconditional classifier cr/f and a class-conditional classifier implemented through the projection layer P . This form of conditioning is used in BIGGAN. Outwardpointing arrows feed into losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>S 2 GAN-CO: During GAN training we learn an auxiliary classifier cCT on the discriminator representationD, based on the labeled real examples, to predict labels for the unlabeled ones. This avoids training a feature extractor F and classifier c S 2 L prior to GAN training as in S 2 GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>using SGD for 65 epochs. The batch size is set to 2048, composed of B unlabeled examples and 2048 − B labeled examples. Following the recommendations from Goyal et al. (2017) for training with large batch size, we (i) set the learning rate to 0.1 B 256 , and (ii) use linear learning rate warm-up during the initial 5 epochs. The learning rate is decayed twice with a factor of 10 at epoch 45 and epoch 55. The parameter γ in (2) is set to 0.5 and the number of unlabeled examples per batch B is 1536. The parameters γ and B are tuned on 0.1% labeled examples held out from the training set, the search space is {0.1, 0.5, 1.0} × {1024, 1536, 1792}. The accuracy of the so-obtained classifier c S 2 L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>The vertical line indicates the median FID of our BIG-GAN implementation which uses all labeled data. The proposed S 3 GAN approach is able to match the performance of the state-ofthe-art BIGGAN model using 10% of the ground-truth labels and outperforms it using 20%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 21 .</head><label>21</label><figDesc>Mean FID and IS (3 runs) on ImageNet (128 × 128) for the models considered in this paper, as a function of the number of generator steps. All models train stably, except SINGLE LABEL (where one run collapsed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table><row><cell>Random label</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single label</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single label (SS)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clustering</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clustering (SS)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S 2 GAN S 3 GAN</cell><cell></cell><cell>5% labels</cell><cell></cell><cell></cell></row><row><cell>S 2 GAN S 3 GAN</cell><cell></cell><cell>10% labels</cell><cell></cell><cell></cell></row><row><cell>S 2 GAN S 3 GAN</cell><cell></cell><cell>20% labels</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>10</cell><cell>15 FID Score</cell><cell>20</cell><cell>25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>match the SOTA on 128×128 IMAGENET using only 10% of the labels, and set a new SOTA (measured by FID) using 20% of the labels, and 3. open-source all the code used for the experiments at github.com/google/compare gan.</figDesc><table /><note>arXiv:1903.02271v2 [cs.LG] 14 May 2019High-Fidelity Image Generation With Fewer Labels</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Top row: 128 × 128 samples from our implementation of the fully supervised current SOTA model BIGGAN. Bottom row: Samples form the proposed S 3 GAN which matches BIGGAN in terms of FID and IS using only 10% of the ground-truth labels.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>A short summary of the analyzed methods. The detailed descriptions of pre-training and co-trained approaches can be found in Sections 3.1 and 3.2, respectively. Self-supervision during GAN training is described in Section 3.3.</figDesc><table><row><cell>METHOD</cell><cell>DESCRIPTION</cell></row><row><cell>BIGGAN</cell><cell>Conditional (Brock et al., 2019)</cell></row><row><cell>SINGLE LABEL</cell><cell>Co-training: Single label</cell></row><row><cell cols="2">RANDOM LABEL Co-training: Random labels</cell></row><row><cell>CLUSTERING</cell><cell>Pre-trained: Clustering</cell></row><row><cell>BIGGAN-k%</cell><cell>BIGGAN using only k% labeled data</cell></row><row><cell>S 2 GAN-CO</cell><cell>Co-training: Semi-supervised</cell></row><row><cell>S 2 GAN</cell><cell>Pre-trained: Semi-supervised</cell></row><row><cell>S 3 GAN</cell><cell>S 2 GAN with self-supervision</cell></row><row><cell>S 3 GAN-CO</cell><cell>S 2 GAN-CO with self-supervision</cell></row></table><note>sample quality (cf. Table 1 in Brock et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>ResNet generator architecture. "ch" represents the channel width multiplier and is set to 96.Table 7. ResBlock generator with upsample.</figDesc><table><row><cell>LAYER</cell><cell cols="2">RS SN</cell><cell>OUTPUT</cell></row><row><cell>z ∼ N (0, 1)</cell><cell>-</cell><cell>-</cell><cell>120</cell></row><row><cell>Dense</cell><cell>-</cell><cell>-</cell><cell>4 × 4 × 16 · ch</cell></row><row><cell>ResBlock</cell><cell>U</cell><cell>SN</cell><cell>8 × 8 × 16 · ch</cell></row><row><cell>ResBlock</cell><cell>U</cell><cell>SN</cell><cell>16 × 16 × 8 · ch</cell></row><row><cell>ResBlock</cell><cell>U</cell><cell>SN</cell><cell>32 × 32 × 4 · ch</cell></row><row><cell>ResBlock</cell><cell>U</cell><cell>SN</cell><cell>64 × 64 × 2 · ch</cell></row><row><cell>Non-local block</cell><cell>-</cell><cell>-</cell><cell>64 × 64 × 2 · ch</cell></row><row><cell>ResBlock</cell><cell>U</cell><cell cols="2">SN 128 × 128 × 1 · ch</cell></row><row><cell>BN, ReLU</cell><cell>-</cell><cell>-</cell><cell>128 × 128 × 3</cell></row><row><cell>Conv [3, 3, 1]</cell><cell>-</cell><cell>-</cell><cell>128 × 128 × 3</cell></row><row><cell>Tanh</cell><cell>-</cell><cell>-</cell><cell>128 × 128 × 3</cell></row><row><cell>LAYER</cell><cell cols="3">KERNEL RS OUTPUT</cell></row><row><cell>Shortcut</cell><cell>[1, 1, 1]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>ResNet discriminator architecture. "ch" represents the channel width multiplier and is set to 96. Spectral normalization is applied to all layers.C. FID and IS training curves</figDesc><table><row><cell></cell><cell>120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Random label Single label Single label (SS) Clustering Clustering (SS)</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FID</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>50000</cell><cell>100000</cell><cell>step</cell><cell>150000</cell><cell>200000</cell><cell>250000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 12 .Table 13 .</head><label>1213</label><figDesc>Pre-trained vs co-training approaches, and the effect of self-supervision during GAN training. While co-training approaches outperform fully unsupervised approaches, they are clearly outperformed by the pre-trained approaches.Self-supervision during GAN training helps in all cases. 0±0.31 9.0±0.30 8.4±0.02 57.6±0.86 72.9±1.41 77.7±1.24 S 2 GAN-CO 21.6±0.64 17.6±0.27 13.8±0.48 29.8±0.21 37.1±0.54 50.1±1.45 S 3 GAN 10.3±0.16 8.1±0.14 7.8±0.20 59.9±0.74 78.3±1.08 82.1±1.89 S 3 GAN-CO 20.2±0.14 16.5±0.12 12.8±0.51 31.1±0.18 38.7±0.36 52.7±1.08 Training with hard (predicted) labels leads to better models than training with soft (predicted) labels. 0±0.31 9.0±0.30 8.4±0.02 57.6±0.86 72.9±1.41 77.7±1.24 S 2 GAN SOFT 15.6±0.58 13.3±1.71 11.3±1.42 40.1±0.97 49.3±4.67 58.5±5.84 Table 14. Mean FID and IS for the unsupervised approaches. 7±66.32 15.4±7.57 SINGLE LABEL(SS) 23.6±0.14 22.2±0.10</figDesc><table><row><cell></cell><cell></cell><cell>FID</cell><cell></cell><cell></cell><cell>IS</cell></row><row><cell></cell><cell>5%</cell><cell>10%</cell><cell>20%</cell><cell>5%</cell><cell>10%</cell><cell>20%</cell></row><row><cell>S 2 GAN</cell><cell cols="2">11.FID</cell><cell></cell><cell></cell><cell>IS</cell></row><row><cell></cell><cell>5%</cell><cell>10%</cell><cell>20%</cell><cell>5%</cell><cell>10%</cell><cell>20%</cell></row><row><cell>S 2 GAN</cell><cell cols="3">11.FID</cell><cell>IS</cell><cell></cell></row><row><cell></cell><cell cols="2">CLUSTERING</cell><cell cols="2">22.7±0.80 22.8±0.42</cell><cell></cell></row><row><cell></cell><cell cols="2">CLUSTERING(SS)</cell><cell cols="2">21.9±0.08 23.6±0.19</cell><cell></cell></row><row><cell></cell><cell cols="2">RANDOM LABEL</cell><cell cols="2">27.2±1.46 20.2±0.33</cell><cell></cell></row><row><cell></cell><cell cols="2">SINGLE LABEL</cell><cell>71.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that an even simpler approach would be to first learn the representation via self-supervision and subsequently the linear classifier, but we observed that learning the representation and classifier simultaneously leads to better results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that this is not necessarily equivalent to replacing classconditional BatchNorm with standard (unconditional) BatchNorm as the variant of conditional BatchNorm used in this paper also uses chunks of the latent code as input; besides the label information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We dissected the model checkpoints released by<ref type="bibr" target="#b3">Brock et al. (2019)</ref> to obtain exact counts of trainable parameters and their dimensions, and match them to byte level (cf. Tables 11 and 10 in Appendix B). We want to emphasize that at this point this methodology is bleeding-edge and successful state-of-theart methods require careful architecture-level tuning. To foster reproducibility we meticulously detail this architecture at tensor-level detail in Appendix B and open-source our code at https://github.com/google/compare_gan.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Ting Chen and Neil Houlsby for fruitful discussions on self-supervision and its application to GANs. We would like to thank Lucas Beyer, Alexander Kolesnikov, and Avital Oliver for helpful discussions on self-supervised semi-supervised learning. We would like to thank Karol Kurach and Marcin Michalski their major contributions the Compare GAN library. We would also like to thank the BigGAN team (Andy Brock, Jeff Donahue, and Karen Simonyan) for their insights into training GANs on TPUs. Finally, we are grateful for the support of members of the Google Brain team in Zurich. This work was partially done while Michael Tschannen was at Google Research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(1, 1, 768, 1536) 1,179,648 discriminator/B5/down conv shortcut/bias:0 (1536,) 1,536 discriminator/B6/same conv1/kernel:0 (3, 3, 1536, 1536) 21,233,664 discriminator/B6/same conv1/bias:0 (1536,) 1,536 discriminator/B6/same conv2/kernel:0 (3, 3, 1536, 1536) 21,233,664 discriminator/B6/same conv2/bias:0 (1536,) 1,536 discriminator/final fc/kernel:0 (1536, 1) 1,536 discriminator/final fc/bias:0 (1,) 1 discriminator projection/kernel:0 (1000, 1536) 1,536,000 <ref type="table">Table 11</ref>. Tensor-level description of the discriminator containing a total of 87,982,370 parameters.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On gradient regularizers for MMD GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised GANs via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Triangle generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">large minibatch SGD: training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning object representations from selfsupervised grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grasp2vec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The GAN Landscape: Losses, architectures, regularization, and normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Triple Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Are GANs created equal? A large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">cgans with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Web-scale k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sricharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shreve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saketh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wavenet ; Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05789</idno>
		<idno>arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Semi-supervised conditional GANs</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03670</idno>
		<title level="m">Self-Supervised Semi-Supervised Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
