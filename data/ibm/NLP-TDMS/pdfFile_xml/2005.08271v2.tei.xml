<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IASHIN, RAHTU: A BETTER USE OF AUDIO-VISUAL CUES A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Iashin</surname></persName>
							<email>vladimir.iashin@tuni.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Computing Sciences</orgName>
								<orgName type="institution">Tampere University Tampere</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
							<email>esa.rahtu@tuni.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Computing Sciences</orgName>
								<orgName type="institution">Tampere University Tampere</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IASHIN, RAHTU: A BETTER USE OF AUDIO-VISUAL CUES A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense video captioning aims to localize and describe important events in untrimmed videos. Existing methods mainly tackle this task by exploiting only visual features, while completely neglecting the audio track. Only a few prior works have utilized both modalities, yet they show poor results or demonstrate the importance on a dataset with a specific domain. In this paper, we introduce Bi-modal Transformer which generalizes the Transformer architecture for a bi-modal input. We show the effectiveness of the proposed model with audio and visual modalities on the dense video captioning task, yet the module is capable of digesting any two modalities in a sequence-to-sequence task. We also show that the pre-trained bi-modal encoder as a part of the bi-modal transformer can be used as a feature extractor for a simple proposal generation module. The performance is demonstrated on a challenging ActivityNet Captions dataset where our model achieves outstanding performance. The code is available: v-iashin.github.io/bmt 1 Introduction Current video sharing platforms contain a large amount of video material. The ability to generate descriptions of this content would be highly valuable for many tasks, such as contentbased retrieval or recommendation <ref type="bibr" target="#b12">[25,</ref><ref type="bibr" target="#b31">44]</ref>. Moreover, they would enable visually-impaired people to consume video material and improve their quality of life <ref type="bibr" target="#b25">[38]</ref>.</p><p>This kind of video descriptions are usually provided as natural language sentences or captions, a compact and intuitive format and, most importantly, can be digested by humans. Early works <ref type="bibr" target="#b33">[46,</ref><ref type="bibr" target="#b34">47,</ref><ref type="bibr" target="#b43">56,</ref><ref type="bibr" target="#b45">58]</ref> described the video content with only one sentence, which might be too "sparse" for long videos -one might try to think up a relatively short sentence which describes the whole film. To mitigate this issue, [20]  proposed dense video captioning which requires a model to, first, localize "events", and, then, to produce one-sentence description for each of them instead of generating one caption for the entire film (see <ref type="figure">Fig. 1</ref>).</p><p>The task is usually formulated as a sequence-to-sequence (video to caption) task. Therefore, the progress in the field is significantly influenced by advances in machine translation. Hence, many models rely on an encoder-decoder architecture which consists of two recurrent neural networks (RNNs) or, recently-proposed Transformer-like model <ref type="bibr" target="#b32">[45]</ref>. An event GT: A man is seen blind folded on a stage and a woman hands him darts while speaking to him Ours: The man and the woman are talking to the camera GT: The man then throws the darts and the woman laughs at his results while he takes the blindfold off Ours: The man is then shown throwing darts at the board 0:00 0:33 Figure 1: Example video with the predictions of our model alongside the ground truth.</p><p>localization module usually utilizes an RNN structure which first encodes the input to produce a hidden representation and, then, makes predictions using this representation.</p><p>Considering the natural co-occurrence of visual and audio tracks in a video and the fact that human perception is multi-modal, recent advances in deep learning practice audio-visual training [24,<ref type="bibr" target="#b14">27,</ref><ref type="bibr" target="#b46">59,</ref> 63, 64]. Yet, most of the existing works on dense video captioning employ only visual inputs. In this work, we address this issue by introducing a novel bi-modal transformer with the multi-headed proposal generator. Our captioning module is inspired by the transformer architecture and, more precisely, how the attention module fuses the information from both sequences. While an efficient object detector YOLO [35]  inspires the design of each proposal head in the bi-modal multi-headed proposal generator.</p><p>The proposed method effectively utilizes audio and visual cues. We demonstrate the performance of our model on the challenging open-domain ActivityNet Captions dataset [20]. The results show the state-of-the-art performance of our bi-modal dense video captioning module as well as our bi-modal proposal generator on BLEU@3-4 and F1 metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The dense video captioning task requires a model to, first, localize events within a video and, then, to produce a textual one-sentence description of what is happening during the event. The dense video captioning task branches out from the video captioning which task is to caption a video without localizing the event. The video captioning field evolved from handcrafted rule models <ref type="bibr" target="#b5">[6,</ref> 19, 21]  to encoder-decoder architectures <ref type="bibr" target="#b33">[46,</ref><ref type="bibr" target="#b34">47,</ref><ref type="bibr" target="#b43">56,</ref><ref type="bibr" target="#b45">58]</ref> inspired by advances in machine translation <ref type="bibr" target="#b26">[39]</ref>. Later, the captioning models were further enhanced by semantic tagging <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">28]</ref>, reinforcement learning [51], attention [55], extended memory [31, 50], and other modalities [13, 16, 52, 54].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dense Video Captioning</head><p>The task of dense video captioning, as well as a test-bed, ActivityNet Captions dataset, were introduced by Krishna et al. [20]  who utilized the idea of the Deep Action Proposals network [10] to generate event proposals and an LSTM network to encode the context and generate captions. The idea of context-awareness was further developed in <ref type="bibr" target="#b36">[49]</ref> who employed a bi-directional variant of Single-Stream Temporal Action proposal network (SST) [3] which makes better use of the video context, an LSTM network with attentive fusion and context</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>gating was used to generate context-aware captions. <ref type="bibr">Zhou et al. [62]</ref> adapted Transformer architecture <ref type="bibr" target="#b32">[45]</ref> to tackle the task and used transformer encoder's output as input to a modification of ProcNets <ref type="bibr" target="#b48">[61]</ref> to generate proposals.</p><p>Recently, the idea of reinforcement learning was found to be beneficial for image captioning (Self-critical Sequence Training (SCST)) <ref type="bibr" target="#b24">[37]</ref> and, hence, applied in dense video captioning as well. More precisely, the SCST was used in a captioning module to optimize the non-differentiable target metric, e.g. METEOR <ref type="bibr" target="#b6">[7]</ref>. Specifically, Li et al.</p><p>[22] integrated the reward system and enriched Single-Shot-Detector-like structure [23] with descriptiveness regression for proposal generation. Similarly, Xiong et al. <ref type="bibr" target="#b40">[53]</ref> used an LSTM network trained with the sentence-and paragraph-level rewards for maintaining coherent and concise story-telling, while the event proposal module was adopted from Structured Segment Networks <ref type="bibr" target="#b47">[60]</ref>. Mun et al. <ref type="bibr" target="#b13">[26]</ref> further developed the idea of coherent captioning by observing the overall context and optimizing two-level rewards, an SST module is used for proposal generation, and a Pointer Network <ref type="bibr" target="#b35">[48]</ref> to distill proposal candidates.</p><p>Another direction of research relies on weak supervision which is designed to mitigate the problem of laborious annotation of the datasets. To this end, Duan et al. <ref type="bibr" target="#b8">[9]</ref> proposed an autoencoder architecture which generates proposals and, then, captions them while being supervised only with a set of non-localized captions in a cycle-consistency manner. However, the results appeared to be far from the supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-modal Dense Video Captioning</head><p>It is natural to assume that, besides visual information, a video understanding system might benefit from the cues contained in other modalities like audio <ref type="bibr" target="#b20">[33]</ref>, speech (subtitles) <ref type="bibr" target="#b27">[40]</ref>, or both <ref type="bibr">[17]</ref>. Specifically, Rahman et al. <ref type="bibr" target="#b20">[33]</ref> were the first to include audio modality into the dense video captioning set up. They borrowed the idea of cycle-consistency from <ref type="bibr" target="#b8">[9]</ref> and employed multi-modal Tucker decomposition <ref type="bibr" target="#b1">[2]</ref> to combine information from both modalities and pass it to a GRU-based <ref type="bibr" target="#b4">[5]</ref> caption decoder. However, since the model is trained in a weakly supervised setting, the results do not reach the performance of the supervised models.</p><p>Shi et al. <ref type="bibr" target="#b27">[40]</ref> proposed to utilize the corresponding speech along with frame features to further improve captioning performance on cooking videos. They suggested employing a transformer's encoder to encode video frames and subtitle embeddings produced by a pretrained BERT model <ref type="bibr" target="#b7">[8]</ref>. Next, an LSTM generates proposals, and the other two LSTMs were used for the encoder-decoder captioning module. Despite the significant gains in captioning performance, we believe these findings are not conclusive as instructional videos is an ill-suited domain to show the benefits of the speech modality for a captioning task since subtitles alone can be a very accurate proxy for captions in such videos (see <ref type="bibr" target="#b12">[25]</ref>).</p><p>In contrast, Iashin et al. <ref type="bibr">[17]</ref> showed the importance of the speech modality on a freedomain dataset. They proposed to train three transformers for each modality individually and fuse features by concatenation before predicting the next caption word while borrowing the proposal generator from <ref type="bibr" target="#b36">[49]</ref>. However, the suggested approach for feature fusion is rather straightforward and inefficient. Moreover, the adopted proposal generator is based solely on video features which contrasts with the idea of the dense video captioning task.</p><p>Our method is mostly similar to <ref type="bibr">[17]</ref>, yet we show significantly better results on the task while utilizing only visual and audio cues. Besides, our proposal generator does employ both modalities and significantly outperforms the state-of-the-art. Furthermore, we present a single model which utilizes bi-modal encoder for both: the proposal generator and captioning module, making it an elegant approach for the dense video captioning task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-Modal Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-Modal Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully-Connected</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully-Connected</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Framework</head><p>Our approach consists of two parts: the bi-modal transformer and multi-headed proposal generator (see <ref type="figure">Fig. 2</ref>). The model expects the input to be a set of continuous features stacked together in a sequence. To represent a visual stream, we use a pre-trained Inflated 3D (I3D) network <ref type="bibr" target="#b3">[4]</ref> while for the audio stream we employ pre-trained VGGish [15], the tokens (roughly, words) are embedded with pre-trained GloVe <ref type="bibr" target="#b19">[32]</ref> (see Sec. 6.2 for implementation details). Also, since the transformer is permutation invariant it has no sense of recurrence. Thus, the order of features within a sequence is preserved by adding the positional encoding to the output of the embedding layers. Following <ref type="bibr" target="#b32">[45]</ref>, we use cosine and sine functions. Next, the audio and visual sequences, are passed through the transformer's bi-modal N-layered encoder to produce bi-modal sequence representations utilizing novel bi-modal multi-headed attention blocks to fuse the features from both sequences. Then, the novel proposal generator utilizes these features to generate proposals and their confidence scores. After, a pre-defined number of most confident proposals are selected to clip the input feature sequences. Next, the clipped features are processed with the encoder to re-represent the features considering only the features which are left after clipping.</p><p>The bi-modal encoder's representation is used at every layer in the bi-modal decoder. Concretely, the encoder's outputs are passed to the corresponding bi-modal attention blocks in the decoder layer along with the representation of the previously generated caption words. The last-layer representation of the decoder is used in the generator where the next caption word is produced. To avoid an empty input to the decoder in the beginning, a special starttoken is used. The caption is generated word-by-word until a special end-token is sampled.</p><p>This section, first, presents the design of the captioning module (Sec. 3.1) and, second, the proposal generator (Sec. 3.2) while the training procedure is explained in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Captioning Module</head><p>The task of dense video captioning requires to produce a caption for each proposal. Therefore, bi-modal encoder inputs audio A and visual V feature sequences which temporally correspond to the proposal and outputs two sequences: audio-attended visual features V a and visual-attended audio features A v . These features are used by the bi-modal decoder which attends to these features and the previous caption words (c 1 , c 2 , . . . , c t ). Finally, the bi-modal decoder outputs the representation which is employed to model a distribution of the next caption word (c t+1 ) over the vocabulary. The proposal index is omitted for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-modal Encoder</head><p>In contrast to the encoder in <ref type="bibr" target="#b32">[45]</ref>, our bi-modal encoder inputs two streams: audio (A ∈ R T a ×d a ) and visual (V ∈ R T v ×d v ) features corresponding to the proposal. Then, the features are passed in a stack of N encoder layers. Instead of two, each layer has three sub-layers: self-attention, bi-modal attention (new), and position-wise fully-connected layers. Specifically, given A fc 0 = A and V fc 0 = V , an n th encoder layer is defined as</p><formula xml:id="formula_0">A self n = MultiHeadAttention(A fc n−1 , A fc n−1 , A fc n−1 ), // audio self-attention (1) V self n = MultiHeadAttention(V fc n−1 ,V fc n−1 ,V fc n−1 ), // visual self-attention (2) A mm n = MultiHeadAttention(A self n ,V self n ,V self n ), // visual-attended audio feats. (3) V mm n = MultiHeadAttention(V self n , A self n , A self n ), // audio-attended visual feats.<label>(4)</label></formula><formula xml:id="formula_1">A fc n = TwoFullyConnected(A mm n ), // R T a ×d a ← R T a ×4d a ← R T a ×d a (5) V fc n = TwoFullyConnected(V mm n ), // R T v ×d v ← R T v ×4d v ← R T v ×d v<label>(6)</label></formula><p>where all sub-layers have distinct sets of trainable weights and mostly resemble the blocks of Transformer <ref type="bibr" target="#b32">[45]</ref>, yet we allow the dimension of the weights in multi-headed attention in (3) &amp; (4) to be different for both modalities because we expect them to have a different size. We define the multi-headed attention in Sec. 6.1. The encoder outputs visual-attended audio features (A v = A fc N ) and audio-attended visual features (V a = V fc N ), which are used the decoder. Bi-modal Decoder The bi-modal decoder inputs the previous sequence of caption words C t = (c 1 , c 2 , . . . , c t ) ∈ R t×d c and, opposed to the original Transformer's decoder <ref type="bibr" target="#b32">[45]</ref>, ours gets the output from the bi-modal encoder (A v ∈ R T a ×d a , V a ∈ R T v ×d v ). Thus, instead of three, it has four sub-layers: self-attention, bi-modal encoder-decoder attention (new), bridge (new), &amp; position-wise fully-connected layers. For C fc 0 = C t , an n th decoder layer is defined as C self n = MultiHeadAttention(C fc n−1 ,C fc n−1 ,C fc n−1 ), // caption self-attention (7)</p><formula xml:id="formula_2">C A v n = MultiHeadAttention(C self n , A v , A v ), // audio-visual attended prev. caps. (8)</formula><p>C V a n = MultiHeadAttention(C self n ,V a ,V a ), // visual-audio attended prev. caps. (9)</p><formula xml:id="formula_3">C mm n = OneFullyConnected [C A v n , C V a n ] , // R t×d c ← R t×2d c ; [·, ·] -concat. (10) C fc n = TwoFullyConnected(C mm n ), // R t×d c ← R t×4d c ← R t×d c<label>(11)</label></formula><p>where, as in the encoder, trainable weights have distinct dimensions depending on a modality and are not shared across sub-layers. The decoder outputs caption features (C av t = C fc N ). Generator The purpose of the generator is to model the distribution for the next caption word c t+1 given the output of the decoder C av t ∈ R t×d c . Therefore, the generator is, usually, a fully-connected layer with the softmax activation which maps the caption features of size d c into a dimension corresponding to the size of the vocabulary in the training set. T v ×d v T a ×d a <ref type="figure">Figure 3</ref>: The Bi-modal Multiheaded Proposal Generator inputs the two-stream output from the bimodal encoder, processes it with two stacks of proposal generation heads. The predictions from all heads form a common pool of predictions. Thus, the pool consists of T v · K v · |Ψ v | + T a · K a · |Ψ a | proposals, which are sorted on the confidence score and passed back to clip input features to the captioning module.</p><p>Residual Connection Following the original Transformer architecture, we employ the residual connection [14] surrounding each sub-layer of the encoder and decoder except for the bridge layer since in-and out-dimensions are different. Additionally, we adopt Layer Normalization <ref type="bibr" target="#b0">[1]</ref> before applying a sub-layer: x + sub-layer LayerNorm(x) . Dropout We also regularize our model with dropout <ref type="bibr" target="#b28">[41]</ref> which is applied: a) before adding the residual in the residual connection, b) before the activation in the bridge layer, c) on outputs of the positional encoding, d) between layers in the position-wise fully-connected network, and e) after the softmax operation in the scaled dot-product attention (see Sec. 6.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Event Proposal Generation Module</head><p>The proposal generator generates a set of proposals for a given video. It consists of two blocks: a bi-modal encoder and bi-modal multi-headed proposal generator (not related to multi-headed attention). The bi-modal encoder in this module inputs the whole sequence opposed to the bi-modal encoder in the captioning module, which inputs a sequence of features corresponding to a proposal. Specifically, it inputs both: visual-attended audio features A v ∈ R T a ×d a and audio-attended visual features V a ∈ R T v ×d v . Since the sequence lengths (T a , T v ) might be distinct, the fusion of predictions cannot be done at each time-step. To this end, we propose the module which makes predictions for each modality at every timestamp individually forming a common pool of cross-modal predictions (see <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposal Generation Head</head><p>The proposal generation head inputs a sequence of T features, and makes predictions at each timestamp on the interval [1, T ], and for every prior segment length anchor in the set Ψ. The design of the proposal generation head is partly inspired by YOLO object detector <ref type="bibr" target="#b21">[34,</ref><ref type="bibr" target="#b22">35,</ref><ref type="bibr" target="#b23">36]</ref>. Specifically, it is a fully-convolutional network which, in our case, consists of only three layers. Opposed to YOLO, we preserve the sequence length across all layers using padding and identity stride. Moreover, YOLO utilizes predictions from three different scales to predict different-scale objects. Hence, only three sizes of receptive fields are used. Instead, our model makes predictions at a single scale while controlling the receptive field with a kernel size k which is distinct in each proposal generation head. More precisely, the 1 st convolutional layer has a kernel size k while in the 2 nd and the 3 rd the kernel size is 1. The layers are separated with ReLU activations and dropout.</p><p>Predictions Temporal boundaries and confidence for a proposal are obtained using three values which were predicted by the proposal generation head: a location of a segment center σ (c) relative to a position p in the sequence while σ (·) is a sigmoid function which bounds the values into [0, 1] interval, a coefficient exp(l) for an anchor, and objectness score σ (o)</p><formula xml:id="formula_4">center = p + σ (c); length = anchor · exp (l); confidence = σ (o).<label>(12)</label></formula><p>The prediction of the center and length are in grid-cells (not in seconds). To obtain seconds, both are multiplied by a cell size which corresponds to a temporal span of the feature.</p><p>Bi-modal Multi-headed Proposal Generator The common pool of predictions is formed with predictions made by each of the proposal generation heads. Specifically, our model has K a and K v heads for audio and visual modalities with distinct sets of kernel sizes. Overall,</p><formula xml:id="formula_5">our model generates 3 · T a · K a · |Ψ a | + T v · K v · |Ψ v | proposals.</formula><p>For the final predictions, we select top-100 proposals out of the common pool based on the confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segment Length Priors &amp; Kernel Sizes</head><p>To select a set of anchors, we use K-Means clustering algorithm with the Euclidean distance metric, as opposed to intersection over the union in YOLO. Due to granularity of feature extractors, feature lengths (T a , T v ) might not necessarily equal. Thus, we obtain distinct numbers of anchors for audio and visual modalities |Ψ a |, |Ψ v | to keep T a · |Ψ a | close to T v · |Ψ v | to balance the impact of each modality to the common pool of predictions. Similarly, the kernel sizes are determined by K-Means. We motivate it with an expectation that the receptive field will correspond to an event with a higher probability. We scale the resulting cluster centroids (in secs) by the feature time span to obtain values in grid-cell coordinates. Next, we round the values to the next odd integer for more elegant padding. Again, to preserve the balance in the share of predictions from each modality, we obtain an equal number of kernel sizes K a = K v both modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Procedure</head><p>Our model is trained in two stages: first, the captioning module is trained with ground truth proposals and, then, the proposal generator is trained using the pre-trained bi-modal encoder from the captioning model. Similar to <ref type="bibr" target="#b32">[45]</ref> and [17], we optimize KL-divergence loss and apply Label Smoothing <ref type="bibr" target="#b30">[43]</ref> to force a model to be less confident about predictions anticipating noisy annotations. Also, masking is used to ignore padding and prevent the model from attending to the next positions in the ground truth caption. During training of the event proposal generation module, all proposal generation heads for each modality are trained simultaneously summing up losses from all heads and both modalities. Each head uses YOLO-like loss: MSE for the localization losses (no square root) and cross-entropy for (no)objectness losses. The NMS is avoided for efficiency and to preserve the possibility of dense events. For the implementation details, a reader is referred to supplementary material (Sec. 6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We employ ActivityNet Captions dataset <ref type="bibr">[20]</ref>, which consists of 100k temporally localized sentences for 20k YouTube videos. The dataset is split into 50/25/25 % parts for training, validation, and testing. The validation set of videos is annotated by two different annotators. We report the results on the validation subsets as ground truth is not available for the testing set. Since the dataset is distributed as a set of links to YouTube videos, it is not possible to collect the whole dataset as some videos became unavailable. The authors also provide C3D features which are not suitable for our experimentation as they are missing audio information. In total, we had 91 % of the videos. We omit the unavailable videos from the validation  sets. We compared the results of other methods on the 91 % and 100 % of videos in Sec. 6.4.1 and observed similar performance suggesting the videos to be missing completely at random.</p><p>To evaluate the event proposal generation module we employ precision, recall, and mainly rely on F1-score (harmonic mean of precision and recall). While METEOR <ref type="bibr" target="#b6">[7]</ref> and BLEU@3-4 <ref type="bibr" target="#b16">[29]</ref> were used for captioning as they are highly correlated with human judgement. All metrics are averaged for every video and temporal Intersection over Union thresholds: [0.3, 0.5, 0.7, 0.9]. As it has been noted in <ref type="bibr" target="#b13">[26]</ref>, the original evaluation script had a critical issue which resulted in an incorrect evaluation of previous models. Therefore, we re-implement <ref type="bibr" target="#b36">[49,</ref><ref type="bibr">62]</ref> and compare with the results obtained with the corrected script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison to the State-of-the-art</head><p>We present the comparison between the bi-modal transformer with multi-headed proposal generator <ref type="bibr">(Ours)</ref> and other methods in the existing literature <ref type="bibr">[17,</ref><ref type="bibr">20,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b13">26,</ref><ref type="bibr" target="#b20">33,</ref><ref type="bibr" target="#b36">49,</ref><ref type="bibr" target="#b40">53</ref>, 62] on the dense video captioning task. The results of the comparison for captioning both ground truth (GT) and learned proposals are shown in Tab. 1. Since evaluating captioning is still challenging and METEOR is probably the best among other options, yet it only provides a proxy for how good a caption is. Therefore we believe that the direct optimization of METEOR using a reinforcement learning technique (RL) might not necessarily result in a better caption. To this end, we also include the results of <ref type="bibr">[22,</ref><ref type="bibr" target="#b13">26]</ref> without the RL module. Moreover, we obtained the results of [17] on the same subset of videos as we have since they additionally removed the videos with no speech modality from the evaluation.</p><p>According to the results, in the learned proposals setup, our dense video captioning model outperforms all of the models, which have no reward maximization on METEOR  Comparing to the RL methods, our model still outperforms them on BLEU metrics in both setups but loses in METEOR due to the absence of reward-maximization module. We draw the attention of a reader to the performance of <ref type="bibr">[22]</ref> with and without the RL module -METEOR has dropped significantly yet other metrics remained on the same level.</p><p>Interestingly, we also outperform [17] who also use the transformer in multi-modal setup yet has more parameters (149M vs 51M). We note again that the results are not fair to neither of <ref type="bibr">[17,</ref><ref type="bibr" target="#b20">33]</ref> and ours since models have been trained on fewer videos.</p><p>Next, we compare our bi-modal multi-headed proposal generation module with other proposal generation modules from other dense video captioning models. The results for [62] and <ref type="bibr" target="#b36">[49]</ref> are reported for 100 proposals per video. The results of the comparison are presented in Tab. 2. Despite our model being trained on fewer videos, our proposal generation model achieves state-of-the-art performance on the F1 metric. Specifically, our model provides impressive ground truth segment coverage while being accurate in its predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In this section, we show how the training procedure and modality impact the final results. The results are presented in Tab. 3 for both settings: captioning ground truth (performance of the captioning module) and leaned proposal (full dense video captioning model).</p><p>Training Procedures Our final model is trained in the following way. First, we train the captioning model on the ground truth proposal. Second, we freeze the weights on the encoder and train the proposal generator using the frozen encoder. The final results are obtained by captioning the proposals obtained from the trained proposal generator. Hence, the acronym "Cap → Prop" which reads as: "the proposal generator is trained using the pretrained encoder from the captioning module". We compare this training procedure to other two methods: a) when both captioning and proposal generator modules are trained separately and b) when, first, the proposal module is trained and, then, the captioning module uses the pre-trained encoder with frozen weights during training. This is the opposite of the training procedure used for the final model, thus, abbreviated to "Prop → Cap".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different Sets of Modalities</head><p>The final model uses both audio and visual modalities to make predictions. We compare the performance of a bi-modal model with uni-modal ones. Specifically, for uni-modal settings, we employ the uni-modal transformer architecture similar to one in <ref type="bibr">[17]</ref>. The difference between the hyper-parameters used for the final model and the uni-modal transformer is in the input dimension. For the uni-modal transformer, we follow the original paper where the input is first embedded into D q dimension (see (14)) and  <ref type="table">Table 3</ref>: The impact of training procedures and input modalities. We compare the training procedure of the final model when the proposal generator uses the pre-trained encoder on the captioning task ("Cap → Prop") to an opposite scenario ("Prop → Cap"), and the situation when both of them are trained separately. The results are shown on validation sets of ActivityNet Captions when captioning ground truth (GT) and learned proposals.</p><p>remains the same everywhere later. We select 1024 for visual-only and 128 for audio-only transformers; the size of the pre-trained GloVe is projected with a FC layer to match the size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We report every combination of the settings in Tab. 3. Specifically, we observed that the captioning module does not benefit from the pre-training for the proposal generation ("Prop → Cap" vs "Cap → Prop" &amp; "Separate"). The results of the learned proposal setting show the importance of the pre-training but only in the "Cap → Prop" setting. Overall, we claim that the captioning training does not benefit from utilizing the pre-trained proposal generator's encoder and, even, performs worse with it. While, the proposal generator ends up with better performance if pre-trained captioning module's encoder is used. The comparison of the cross-modal performance shows that using both modalities (audio and visual) gives the best result in nearly all cases in both settings. However, it is shown that the audio modality is the weakest among the three implying that visual modality might contain a stronger signal for video understanding. Nevertheless, the gap between the visualonly and bi-modal case is consistent in all settings. This suggests that the audio still provides essential cues for dense video captioning. More ablations studies can be found in Sec. 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We believe that the handling of multiple modalities is under-explored in the computer vision community. In this paper, we present a novel bi-modal transformer with a bi-modal multiheaded proposal generation module showing how audio might facilitate the performance of dense video captioning. We perform our experimentation on the ActivityNet Captions dataset and achieve state-of-the-art results on F1 and BLEU metrics. The the ablation study results show that the proposed model provides an effective and elegant way of fusing audio and visual features while outperforming the uni-modal configurations in all settings.</p><p>[13] Wangli Hao, Zhaoxiang Zhang, and He Guan. Integrating both visual and audio cues for enhanced video caption. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p><p>[ 6 Supplementary Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multi-headed Attention</head><p>Scaled dot-product attention The notion of multi-headed attention is based on the idea of scaled dot-product attention which is defined as follows</p><formula xml:id="formula_6">Attention(Q, K, V ) = Softmax QK T √ d V,<label>(13)</label></formula><p>where √ d is a scaling factor designed to keep the Softmax gradients in a sufficient range, Q, K, V are sequences of queries, keys, and values, Softmax is applied row-wise.</p><p>Attention with Many Heads The concept of multiple heads was introduced in [45] to allow a model to learn H distinct representation sub-spaces at each position while preserving the same computation efficiency. An attention head is usually presented as (13) with parametrized inputs</p><formula xml:id="formula_7">head h (q, k, v) = Attention(qW q h , kW k h , vW v h ), h ∈ [1, H] (14) where q ∈ R T q ×D q , k ∈ R T k ×D k , v ∈ R T k ×D k and W * h ∈ R D * ×D in .</formula><p>Note that the inputs k and v are expected to have the same dimension (T k × D k ) while q might have a different one. The weights W * h are mapping the corresponding inputs into an internal space D in = D q H such that D q is a multiple of H. The mapping into D in space allows the attention to be calculated between the features which originally were of distinct dimensions (D q = D k ). The multiheaded attention is, then, defined as the concatenation of H attention heads mapped back to sub-space of queries (D q ) with W out ∈ R H·D in ×D q MultiHeadAttention(q, k, v) = head 1 (q, k, v), head 2 (q, k, v), . . . , head H (q, k, v) W out . (15)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Feature Extraction</head><p>Both audio and visual features are pre-calculated before training. The audio features are extracted with the VGGish network <ref type="bibr">[15]</ref>, which was pre-trained on AudioSet <ref type="bibr" target="#b11">[12]</ref>. More specifically, the VGGish model processes 0.96 seconds long segments. The audio segments, in turn, are represented as log mel-scaled spectrograms of size 96 × 64 which are obtained via Short-time Fourier Transform. The STFT utilizes a 25 ms Hann window with 15 ms step applied to the 16 kHz mono audio track. The pre-classification layer of VGGish outputs a 128-d embedding for each spectrogram. Therefore, the audio track of an i th video in the dataset is represented with a sequence of 128-d features of length T i a , each feature in the stack represents 0.96 seconds of the original audio track.</p><p>To extract features from the visual stream, we employ the I3D network <ref type="bibr" target="#b3">[4]</ref> pre-trained on Kinetics dataset. Specifically, I3D inputs 64 RGB and 64 optical flow frames of size 224 2 extracted at 25 fps. Similar to [17], we extract the flow frames using PWCNet <ref type="bibr" target="#b29">[42]</ref>. Both sets of frames are, first, resized such that min(Height, Width) = 256, and, then, the central region of size 224 2 is cropped. After, both stacks of frames are passed through the corresponding streams of I3D. It outputs a 1024-d representation for RGB and flow 64frame stacks from the second-to-the-last layer. Following the authors of I3D, we sum the representations from both streams. It results in a single 1024-d representation for every stack of 64 frames. Therefore, the visual track of ith video is represented with a sequence of 1024-d features of length T i v where every features spans 2.56 seconds (64 frames) of the original video.</p><p>The tokens (or, roughly, words) from captions are embedded with Global Vector (GloVe) representations pre-trained on the Common Crawl dataset (2.2M vocabulary) <ref type="bibr" target="#b19">[32]</ref>. The pretrained model is represented as a lookup table which maps a token to a 300-d embedding. If a token is missing in the vocabulary, an average vector among all vocabulary words is returned. Therefore, each previous token of a caption is represented with a 300-d vector.</p><p>Therefore, the bi-modal encoder's in and out dimensions are d a = 128 and d v = 1024 for audio and visual streams while the decoder inputs and outputs d c = 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Implementation Details</head><p>The batch of size 32 and 16 were used during training of captioning and proposal generation modules, respectively. To form a batch, in the captioning module, the features are padded up to the longest sequence in the batch. For the proposal generator, the features are extracted from entire videos and padded up to 300 for visual and 800 for audio to form a batch. These number were selected to cover all possible lengths of the features in the training set. The padding is masked out as it is done for the next caption tokens in the decoder (see Sec. 3.3). Each head in the bi-modal multi-headed generator predicts Ψ a = 48 and Ψ v = 128 anchors for audio and visual modalities. We used the following lists each of size K a = K v = 10 for kernel sizes (given in cell-coordinates): <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">13,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b22">35,</ref><ref type="bibr" target="#b38">51,</ref><ref type="bibr">69,</ref><ref type="bibr">91,</ref><ref type="bibr">121,</ref><ref type="bibr">161,</ref><ref type="bibr">211]</ref> for audio and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">13,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b12">25,</ref><ref type="bibr" target="#b22">35,</ref><ref type="bibr" target="#b32">45,</ref><ref type="bibr" target="#b48">61,</ref><ref type="bibr">79]</ref> for visual modalities which are determined by K-Means algorithm. The size of both intermediate layers in proposal generation head is 512. Note that 128 48 = 800 300 = 2.56 0.96 which preserves the balance between predictions from both modalities (see Sec. 3.2).</p><p>Since the modality features might have a different size, we also need to map them into an internal space inside of the bi-modal attention modules (D in ), see Eq. (14) for more details. We select the internal space to be of size D in = 1024. Both the encoder and decoder of the bi-modal transformer have N = 2 layers and H = 4 heads in each of the multi-headed attention modules. The caption vocabulary size and, hence, the generator's output dimension is 10 172. We use γ = 0.7 in the label smoothing and the probability of dropout p = 0.1. The localization and objectness loss coefficients are 1, and the noobjectness coefficient is 100. Adam optimizer with default hyper-parameters [18] and learning rate 5 · 10 −5 is used to train both caption and proposal generator. The hyper-parameters are selected on the validation set.</p><p>We highlight that the whole process of training both parts of the model was designed to keep a unified training procedure avoiding using different techniques such as reduce-onplateau, weight decay, different learning rate, optimizer when training the proposal generator, sometimes, favoring elegance at the cost of performance. We encourage others to try different combinations when training both stages to achieve better results.</p><p>The captioning module was trained until METEOR on the validation set has not improved for 30 epochs while the proposal generator is trained for 70 epochs at most. In our experiments, the training of the final captioning module reaches the peak performance at 26 th epoch while the proposal generator achieves the highest F1-score on the validation set at 17 th epoch. We select the proposals on the epoch with the highest metric and caption them with the best captioning model. The training of the captioning module until the best performance takes 10 hours and 3.5 hours to train the proposal generator on one Nvidia GeForce RTX 2080Ti. We use PyTorch <ref type="bibr" target="#b17">[30]</ref> as our primary library for the implementation.   In our experimentation, we exclude videos which are no longer available on YouTube (9 %) from the ground truth validation datasets as it would be unfair to compare our model to the methods which could make a prediction based on the video content while our model gets zero scores on a missing video. Therefore, we evaluate the predictions made by other models <ref type="bibr" target="#b36">[49,</ref><ref type="bibr">62]</ref> on the same validation set as we have. We selected only these two methods as they made either a code or evaluation results publicly available.</p><p>In other words, we hypothesise that the performance of other methods will not change after excluding videos from both predictions and ground truth while the performance of our method will be higher by around 9 % (a portion of the missing videos). The results of the comparison are shown in Tab. 4 and, indeed, imply that the performance of other methods remains on the same level (less than 2 % change). We remind a reader that the compared methods were trained on the full training dataset while ours was trained on only 91 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Might Your Model Improve Results of Other Methods?</head><p>Since <ref type="bibr" target="#b36">[49]</ref> have not made the results publicly available for captioning ground truth (see Tab. 1), we cannot compare it with our model directly. To this end, we apply our final captioning model on the generated proposals from <ref type="bibr" target="#b36">[49]</ref> to eliminate the effect caused by different proposal generator modules. The results of the comparison are reported on the filtered ActivityNet Caption validation datasets (see Sec. 6.4.1) and shown in Tab. 5. The results suggest that our model has a better captioning performance on this set of metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">What is the Impact of Audio and Visual Cues across Different Video Types?</head><p>Following [17], we inspect if the final model's performance consistently improves across different types of videos. To form a list of video types, we retrieve a YouTube video category for each video in the validation dataset. The YouTube category is annotated by the author when they upload a video to the service. YouTubeAPI <ref type="bibr" target="#b44">[57]</ref> was used to retrieve the categories automatically. Since there was a time gap between downloading videos and their categories, 67 were no longer available. Such videos were removed from the comparison. Also, we removed one video category with less than 20 videos. <ref type="figure" target="#fig_1">Fig. 4</ref> shows the performance comparison between bi-modal (final), audio-only, and  <ref type="table">Table 6</ref>: The effect of replacing the bi-modal attention with a self-attention module in encoder layers. The comparison in shown on validation subsets of ActivityNet Captions in the learned proposal setting. The metrics are BLEU3-4 and METEOR. visual-only models across different video categories in two settings: captioning ground truth and learned proposals. The results suggest the consistent gain in performance when both modalities are used compared to the uni-modal models. This pattern holds across both settings and all categories. In addition, it appears that the visual modality provides more cues to the model than the audio modality in nearly all cases. Moreover, the dataset seems to be biased to "Sports" and "People &amp; Blogs" videos, which hold almost half of the dataset. Yet, the results show no evidence of over-fitting to these categories. Among all categories, "Music" appears to be the "easiest" one, which might be explained by a small variety of ways to describe the content of this kind. Meanwhile, the models perform the worst on "Gaming" and "Nonprofits &amp; Activism" categories, which might occur because of the lack of such videos in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.4">What Happens if the Bi-Modal Attention Block is Replaced by Uni-modal</head><p>Self-Attention?</p><p>In this ablation study, we would like to estimate the influence of the bi-modal attention blocks on the model performance (see middle blocks of Encoder and Decoder layers in <ref type="figure">Fig. 2</ref>). Yet, we can ablate only the encoder as the bi-modal attention is essential for the decoder since it inputs two streams. One solution would be to fuse the outputs of the encoder. This would, in turn, allow us to replace two bi-modal attention blocks in the decoder with one. However, it is not possible since the temporal spans of the encoder's output streams, in general, are distinct (A v ∈ R T a ×d a and V a ∈ R T v ×d v ). Therefore, in this setting, each encoder layer has two pairs of self-attention blocks which preserves the final model's number of parameters. The results are presented in Tab. 6. We observed a substantial decline in performance among all metrics when the bi-modal attention block is replaced by self-attention. This further suggests the importance of the proposed approach. Besides, the results of the model with self-attention in encoder layers are still much stronger than any V-and A-only models (see Tab 3), which proves the importance of an audio-visual approach to the task. <ref type="figure" target="#fig_2">Fig. 5</ref> provides the qualitative analysis of the final captioning model compared to ground truth captions. Additionally, we provide captions produced by uni-modal captioning models (audio-and visual-only). The results show that the caption, produced by a bi-modal captioning module as well as the audio-only model managed to grasp the concept of talking when captioning the largest segment (the top one) while video-only model neglects it. The video, by itself, consists of an explanation of how to do a martial art movement and highly verbose. Therefore, even though the ground truth does not mention that the person talks during the video, the predictions of our final model are not entirely erroneous. However, the colour of the man's shirt is incorrectly guessed, which might be explained by the presence of the black punching bag on the screen. Finally, the caption produced by the visual-only model also makes sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Qualitative Analysis</head><p>Moreover, if we consider the results of the audio-only model, we may notice that it mostly gets the signal of "talking" and exploits it in a prediction. Indeed, it might be challenging even for a non-English human to understand what the video is about given only the audio track. We also notice that captions provided by an annotator are significantly more detailed compared to the predictions of the captioning model, which are somewhat more general. This is the issue which needs more attention in future research as it seems to be a problem for any dense video captioning system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The performance comparison between different modalities (Audio-only, Visualonly, and Bi-modal) in two settings (ground truth and learned proposals) across different YouTube video categories. The video categories are sorted according to the performance of the Audio-Visual model in the learned proposal setup. The number of videos in a category is shown in brackets. ActivityNet Captions validation subset is used for the comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The results of the qualitative analysis for a video from ActivityNet Caption validation dataset. The predictions of our bi-modal model are compared to the uni-modal model predictions and ground truth (GT) annotations. The video shows a man who explains how to do a martial art movement-the YouTube video id EIibo7aTpys.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art results on the dense video captioning task. The results are reported on the validation subset of ActivityNet Captions in both settings: captioning ground truth (GT) and learned proposals on BLEU@3-4 (B@3-4) and METEOR (M) metrics. For a fair comparison on METEOR, we additionally report the results of models without the reward (METEOR) maximization (RL) and indicate whether full dataset was available for training. The best and the 2 nd best results are highlighted.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Full Dataset was Available Prec. Rec. F1 Xiong et al. [53] yes 51.41 24.31 33.01 Wang et al. [49] yes 44.80 57.60 50.40 Zhou et al. [62] yes 38.57 86.33 53.31 Mun et al. [26] yes 57.57 55.58 56.56 Ours no 48.23 80.31 60.27</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-ofthe-art proposal generation methods on dense video captioning task. Results are reported on the validation set of ActivityNet Captions. Metrics: Precision, Recall, &amp; F1measure. The top-2 is highlighted. (no RL) while being on par when captioning ground truth proposals. Notably, our model has the highest BLEU metrics in the learned proposal setup yet lies far away from [62] when captioning ground truth proposals on BLEU and performs on par with this model on METEOR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. Save: A framework for semantic annotation of visual events. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 1-8, 2008. [22] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. Jointly localizing and describing events for dense video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7492-7500, 2018. 62] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. Endto-end dense video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739-8748, 2018.[63] Lingyu Zhu and Esa Rahtu. Separating sounds from a single image. arXiv preprint arXiv:2007.07984, 2020.[64] Lingyu Zhu and Esa Rahtu. Visually guided sound source separation using cascaded opponent filter network. arXiv preprint arXiv:2006.03028, 2020.</figDesc><table><row><cell>[15] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen,</cell></row><row><cell>R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al.</cell></row><row><cell>Cnn architectures for large-scale audio classification. In 2017 ieee international con-</cell></row><row><cell>ference on acoustics, speech and signal processing (icassp), pages 131-135, 2017.</cell></row><row><cell>[16] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang, Bret Harsham, John R Her-</cell></row><row><cell>shey, Tim K Marks, and Kazuhiko Sumi. Attention-based multimodal fusion for video</cell></row><row><cell>description. In Proceedings of the IEEE international conference on computer vision,</cell></row><row><cell>pages 4193-4202, 2017.</cell></row><row><cell>[23] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-</cell></row><row><cell>Yang Fu, and Alexander C Berg. SSD: Single shot multibox detector. In European</cell></row><row><cell>conference on computer vision, pages 21-37, 2016.</cell></row></table><note>[17] Vladimir Iashin and Esa Rahtu. Multi-modal dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 958-959, 2020.[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.[19] Atsuhiro Kojima, Takeshi Tamura, and Kunio Fukunaga. Natural language description of human activities from video images based on concept hierarchy of actions. Interna- tional Journal of Computer Vision, 50(2):171-184, 2002.[20] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense- captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706-715, 2017.[21] Mun Wai Lee, Asaad Hakeem, Niels Haering, and Song-Chun Zhu.[24] Xiang Long, Chuang Gan, Gerard De Melo, Jiajun Wu, Xiao Liu, and Shilei Wen. At- tention clusters: Purely attention based local feature integration for video classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7834-7843, 2018.[</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The performance of other methods on the filtered ActivityNet Captions validation set for videos which are no longer available (around 91 % (as ours)). The results are reported in the learned proposal setting. As expected, the performance of other models remains at the same level while ours gains the missing 9 %. Metrics are BLEU3-4, METEOR, recall, precision, and F1-measure.</figDesc><table><row><cell>Caption</cell><cell>Proposal</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Module</cell><cell cols="3">Generator B@3 B@4</cell><cell cols="2">M Recall Prec.</cell><cell>F1</cell></row><row><cell cols="2">Wang et al. [49] [49]</cell><cell>2.29</cell><cell cols="2">1.15 6.14</cell><cell>57.86 44.88 50.55</cell></row><row><cell>Ours</cell><cell>[49]</cell><cell>2.87</cell><cell cols="2">1.41 7.03</cell><cell>57.86 44.88 50.55</cell></row><row><cell>Ours</cell><cell>Ours</cell><cell>3.84</cell><cell cols="2">1.88 8.44</cell><cell>80.31 48.23 60.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>The comparison of the captioning performance between our model and<ref type="bibr" target="#b36">[49]</ref> on the learned proposals provided in<ref type="bibr" target="#b36">[49]</ref>. The results are reported on the filtered ActivityNet Caption validation datasets.</figDesc><table><row><cell>6.4 More Ablation Studies</cell></row></table><note>6.4.1 Why Do You Exclude Videos from the Validation Set?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>2 nd Encoder's Sub-layer B3 B4 M Self-Attention 3.60 1.74 8.14 Bi-modal Attention 3.84 1.88 8.44</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments Funding for this research was provided by the Academy of Finland projects 327910 &amp; 324346. We also acknowledge CSC -IT Center for Science, Finland, for computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MUTAN: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2612" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SST: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2911" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradipto</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2634" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised dense event captioning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3059" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DAPs: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5630" to="5639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Streamlined dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6588" to="6597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video captioning with transferred semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6504" to="6512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memory-attended recurrent network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8347" to="8356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Watch, listen and tell: Multi-modal weakly supervised dense event captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanzila</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8908" to="8917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">YOLOv3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="120" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense procedure captioning in narrated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6382" to="6391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-1173</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7190" to="7198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">M3: Multimodal memory modelling for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7512" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video captioning via hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4213" to="4222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Watch, listen, and describe: Globally and locally aligned cross-modal attentions for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="795" to="801" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Move forward and tell: A progressive generator of video descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="468" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning multimodal attention lstm networks for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">STAT: spatial-temporal attention mechanism for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="241" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtube</forename><surname>Data</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Api</forename></persName>
		</author>
		<ptr target="https://developers.google.com/youtube/v3/docs/videoCategories" />
		<imprint>
			<date type="published" when="2019-11-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4584" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The sound of pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="570" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">punching bag Ours (Audio): A man is standing in a room talking to the camera Ours (Visual): A man is standing in a room, swinging his arms and legs around Ours (Audio Visual): A man in a black shirt is standing in a room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno>talking 0:00 2:47</idno>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Towards automatic learning of procedures from web instructional videos</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">GT: A barefoot man in a red t-shirt and wearing boxing gloves stands in a mirror walled gym room, on a padded floor and demonstrates how to jab</title>
		<imprint/>
	</monogr>
	<note>all while talking to the camera</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Audio Visual): He then demonstrates how to do a karate moves GT: The man then demonstrates foot work and guard and block boxing techniques Ours (Audio): A man is seen speaking to the camera and leads into him speaking to the camera Ours (Visual): He continues to demonstrate several moves as he moves around the room and ends by hitting his head Ours (Audio Visual): He is standing in a gym talking to the camera GT: The man then incorporates the use of the punching bag to demonstrate block, jab and foot work moves on the bag Ours (Audio): The man then begins to talk to the camera and leads into him speaking to the camera Ours (Visual): He continues to hit the bag while several others watch him and ends by hitting the ball Ours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ours</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A man is seen speaking to the camera and leads into him holding a bow and speaking to the camera Ours (Visual): The man then begins to move on the floor and begins to move on the machine Ours</title>
		<imprint/>
	</monogr>
	<note>Audio Visual): The man continues to play with the bag and leads into him hitting the bag</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
