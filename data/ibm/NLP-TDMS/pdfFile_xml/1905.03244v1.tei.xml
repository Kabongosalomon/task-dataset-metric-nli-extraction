<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Mesh Regression for Single-Image Human Shape Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Mesh Regression for Single-Image Human Shape Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of 3D human pose and shape estimation from a single image. Previous approaches consider a parametric model of the human body, SMPL, and attempt to regress the model parameters that give rise to a mesh consistent with image evidence. This parameter regression has been a very challenging task, with modelbased approaches underperforming compared to nonparametric solutions in terms of pose estimation. In our work, we propose to relax this heavy reliance on the model's parameter space. We still retain the topology of the SMPL template mesh, but instead of predicting model parameters, we directly regress the 3D location of the mesh vertices. This is a heavy task for a typical network, but our key insight is that the regression becomes significantly easier using a Graph-CNN. This architecture allows us to explicitly encode the template mesh structure within the network and leverage the spatial locality the mesh has to offer. Image-based features are attached to the mesh vertices and the Graph-CNN is responsible to process them on the mesh structure, while the regression target for each vertex is its 3D location. Having recovered the complete 3D geometry of the mesh, if we still require a specific model parametrization, this can be reliably regressed from the vertices locations. We demonstrate the flexibility and the effectiveness of our proposed graphbased mesh regression by attaching different types of features on the mesh vertices. In all cases, we outperform the comparable baselines relying on model parameter regression, while we also achieve state-of-the-art results among model-based pose estimation approaches. 1 arXiv:1905.03244v1 [cs.CV] 8 May 2019</p><p>• We achieve state-of-the-art results among model-based pose estimation approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Analyzing humans from images goes beyond estimating the 2D pose for one person <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref> or multiple people <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>, or even estimating a simplistic 3D skeleton <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Our understanding relies heavily on being able to properly reconstruct the complete 3D pose and shape of people from monocular images. And while this problem is well addressed in settings with multiple cameras <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, <ref type="bibr" target="#b0">1</ref> Project Page: seas.upenn.edu/˜nkolot/projects/cmr <ref type="figure">Figure 1</ref>: Summary of our approach. Given an input image we directly regress a 3D shape with graph convolutions. Optionally, from the 3D shape output we can regress the parametric representation of a body model. the excessive ambiguity, the limited training data, and the wide range of imaging conditions make this task particularly challenging in the monocular case.</p><p>Traditionally, optimization-based approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b48">49]</ref> have offered the most reliable solution for monocular pose and shape recovery. However, the slow running time, the reliance on a good initialization and the typical failures due to bad local minima have recently shifted the focus to learning-based approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>, that regress pose and shape directly from images. The majority of these works investigate what is the most reliable modality to regress pose and shape from. Surface landmarks <ref type="bibr" target="#b17">[18]</ref>, pose keypoints and silhouettes <ref type="bibr" target="#b30">[31]</ref>, semantic part segmentation <ref type="bibr" target="#b27">[28]</ref>, or raw pixels <ref type="bibr" target="#b14">[15]</ref> have all been considered as the network input. And while the input representation topic has received much debate, all the above approaches nicely conform to the SMPL model <ref type="bibr" target="#b20">[21]</ref> and use its parametric representation as the regression target of choice. However, taking the decision to commit to a particular parametric space can be quite constraining itself. For example, SMPL is not modeling hand pose or facial expressions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref>. What is even more alarming is that the model parameter space might not be appropriate as a regression target. In the case of SMPL, the pose space is expressed in the form of 3D  <ref type="figure">Figure 2</ref>: Overview of proposed framework. Given an input image, an image-based CNN encodes it in a low dimensional feature vector. This feature vector is embedded in the graph defined by the template human mesh by attaching it to the 3D coordinates (x t i , y t i , z t i ) of every vertex i. We then process it through a series of Graph Convolutional layers and regress the 3D vertex coordinates (x i ,ŷ i ,ẑ i ) of the deformed mesh.</p><p>rotations, a pretty challenging prediction target <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>. Depending on the selected 3D rotation representation (e.g., axis angle, rotation matrices, quaternions), we might face problems of periodicity, non-minimal representation, or discontinuities, which complicate the prediction task. And in fact, all the above model-based approaches underperfom in pose estimation metrics compared to approaches regressing a less informative, yet more accurate, 3D skeleton through 3D joint regression <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>In this work, we propose to take a more hybrid route towards pose and shape regression. Even though we preserve the template mesh introduced by SMPL, we do not directly regress the SMPL model parameters. Instead, our regression target is the 3D mesh vertices. Considering the excessive number of vertices of the mesh, if addressed naively, this would be a particular heavy burden for the network.</p><p>Our key insight though, is that this task can be effectively and efficiently addressed by the introduction of a Graph-CNN. This architecture enables the explicit encoding of the mesh structure in the network, and leverages the spatial locality of the graph. Given a single image ( <ref type="figure">Figure 2</ref>), any typical CNN can be used for feature extraction. The extracted features are attached on the vertex coordinates of the template mesh, and the processing continues on the graph structure defined for the Graph-CNN. In the end, each vertex has as target its 3D location in the deformed mesh. This allows us to recover the complete 3D geometry of the human body without explicitly committing to a pre-specified parametric space, leaving the mesh topology as the only hand-designed choice. Conveniently, after es-timating the 3D position for each vertex, if we need our prediction to conform to a specific model, we can regress its parameters quite reliably from the mesh geometry <ref type="figure">(Figure 1</ref>). This enables a more hybrid usage for our approach, making it directly comparable to model-based approaches. Furthermore, our graph-based processing is largely agnostic to the input type, allowing us to attach features extracted from RGB pixels <ref type="bibr" target="#b14">[15]</ref>, semantic part segmentation <ref type="bibr" target="#b27">[28]</ref>, or even from dense correspondences <ref type="bibr" target="#b5">[6]</ref>. In all these cases we demonstrate that our approach outperforms the baselines that regress model parameters directly from the same type of features, while overall we achieve state-of-the-art pose estimation results among model-based baselines.</p><p>Our contributions can be summarized as follows:</p><p>• We reformulate the problem of human pose and shape estimation in the form of regressing the 3D locations of the mesh vertices, to avoid the difficulties of direct model parameter regression.</p><p>• We propose a Graph CNN for this task which encodes the mesh structure and enables the convolutional mesh regression of the 3D vertex locations.</p><p>• We demonstrate the flexibility of our framework by considering different input representations, always outperforming the baselines regressing the model parameters directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There is rich recent literature on 3D pose estimation in the form of a simplistic body skeleton, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. However, in this Section, we focus on the more relevant works recovering the full shape and pose of the human body. Optimization-based shape recovery: Going beyond a simplistic skeleton, and recovering the full pose and shape, initially, the most successful approaches followed optimization-based solutions. The work of Guan et al. <ref type="bibr" target="#b4">[5]</ref> relied on annotated 2D landmarks and optimized for the parameters of the SCAPE parametric model that generated a mesh optimally matching this evidence. This procedure was made automatic with the SMPLify approach of Bogo et al. <ref type="bibr" target="#b0">[1]</ref>, where the 2D keypoints where localized through the help of a CNN <ref type="bibr" target="#b31">[32]</ref>. Lassner et al. <ref type="bibr" target="#b17">[18]</ref> included auxiliary landmarks on the surface of the human body, and additionally considered the estimated silhouette during the fitting process. Zanfir et al. <ref type="bibr" target="#b48">[49]</ref> similarly optimized for consistency of the reprojected mesh with semantic parts of the human body, while extending the approach to work for multiple people as well. Despite the reliable results obtained, the main concern for approaches of this type is that they pose a complicated non-convex optimization problem. This means that the final solution is very sensitive to the initialization, the optimization can get stuck in local minima, and simultaneously the whole procedure can take several minutes to complete. These drawbacks have motivated the increased interest in learning-based approaches, like ours, where the pose and shape are regressed directly from images. Direct parametric regression: When it comes to pose and shape regression, the vast majority of works adopt the SMPL parametric model and consider regression of pose and shape parameters. Lassner et al. <ref type="bibr" target="#b17">[18]</ref> detect 91 landmarks on the body surface and use a random forest to regress the SMPL model parameters for pose and shape. Pavlakos et al. <ref type="bibr" target="#b30">[31]</ref> rely on a smaller number of keypoints and body silhouettes to regress the SMPL parameters. Omran et al. <ref type="bibr" target="#b27">[28]</ref> follow a similar strategy but use a part segmentation map as the intermediate representation. On the other hand, Kanazawa et al. <ref type="bibr" target="#b14">[15]</ref> attempt to regress the SMPL parameters directly from images, using a weakly supervised approach relying on 2D keypoint reprojection and a pose prior learnt in an adversarial manner. Tung et al. <ref type="bibr" target="#b42">[43]</ref> present a self-supervised approach for the same problem, while Tan et al. <ref type="bibr" target="#b38">[39]</ref> rely on weaker supervision in the form of body silhouettes. The common theme of all these works is that they have focused on using the SMPL parameter space as a regression target. However, the 3D rotations involved as the pose parameters have created issues in the regression (e.g., discontinuities or periodicity) and typically underperform in terms of pose estimation compared to skeleton-only baselines. In this work, we propose to take an orthogonal approach to them, by regressing the 3D location of the mesh vertices by means of a Graph-CNN. Our approach is transparent to the type of the input representation we use, since the flexibility of the Graph network allows us to consider different types of input representations employed in prior work, like semantic part-based features <ref type="bibr" target="#b27">[28]</ref>, features extracted directly from raw pixels <ref type="bibr" target="#b14">[15]</ref>, or even dense correspondences <ref type="bibr" target="#b5">[6]</ref>.</p><p>Nonparametric shape estimation: Recently, nonparametric approaches have also been proposed for pose and shape estimation. Varol et al. <ref type="bibr" target="#b43">[44]</ref> use a volumetric reconstruction approach with a voxel output. Different tasks are simultaneously considered for intermediate supervision. Jackson et al. <ref type="bibr" target="#b11">[12]</ref> also propose a form of volumetric reconstruction by extending their recent face reconstruction network <ref type="bibr" target="#b10">[11]</ref> to work for full body images. The main drawback of these approaches adopting a completely nonparametric route, is that even if they recover an accurate voxelized sculpture of the human body, there is none or very little semantic information captured. In fact, to recover the body pose, we need to explicitly perform an expensive body model fitting step using the recovered voxel map, as done in <ref type="bibr" target="#b43">[44]</ref>. In contrast to them, we retain the SMPL mesh topology, which allows us to get dense semantic correspondences of our 3D prediction with the image, and in the end we can also easily regress the model's parameters given the vertices 3D location.</p><p>Graph CNNs: Wang et al. <ref type="bibr" target="#b45">[46]</ref> use a Graph CNN to reconstruct meshes of objects from images by deforming an initial ellipsoid. However, mesh reconstruction of arbitrary objects is still an open problem, because shapes of objects even in the same class, e.g., chairs, do not have the same genus. Contrary to generic objects, arbitrary human shapes can be reconstructed as continuous deformations of a template model. In fact, recently there has been a lot of research in applying Graph Convolutions for human shape applications. Verma et al. <ref type="bibr" target="#b44">[45]</ref> propose a new data-driven Graph Convolution operator with applications on shape analysis. Litany et al. <ref type="bibr" target="#b19">[20]</ref> use a Graph VAE to learn a latent space of human shapes, that is useful for shape completion. Ranjan et al. <ref type="bibr" target="#b32">[33]</ref> use a mesh autoencoder network to recover a latent representation of 3D human faces from a series of meshes. The main difference of our approach is that we do not aim to learn a generative shape model from 3D shapes, but instead perform single-image shape reconstruction; the input to our network is an image, not a 3D shape. The use of a Graph CNN alone is not new, but we consider as a contribution the insight that Graph CNNs provide a very natural structure to enable our hybrid approach. They assist us in avoiding the SMPL parameter space, which has been reported to have issues with regression <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref>, while simultaneously allowing the explicit encoding of the graph structure in the network, so that we can leverage spatial locality and preserve the semantic correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical approach</head><p>In this Section we present our proposed approach for predicting 3D human shape from a single image. First, in Subsection 3.1 we briefly describe the image-based architecture that we use as a generic feature extractor. In Subsection 3.2 we focus on the core of our approach, the Graph CNN architecture that is responsible to regress the 3D vertex coordinates of the mesh that deforms to reconstruct the human body. Then, Subsection 3.3 describes a way to combine our non-parametric regression with the prediction of SMPL model parameters. Finally, Subsection 3.4 focuses on important implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image-based CNN</head><p>The first part of our pipeline consists of a typical imagebased CNN following the ResNet-50 architecture <ref type="bibr" target="#b6">[7]</ref>. From the original design we ignore the final fully connected layer, keeping only the 2048-D feature vector after the average pooling layer. This CNN is used as a generic feature extractor from the input representation. To demonstrate the flexibility of our approach, we experiment with a variety of inputs, i.e., RGB images, part segmentation and Dense-Pose input <ref type="bibr" target="#b5">[6]</ref>. For RGB images we simply use raw pixels as input, while for the other representations, we assume that another network <ref type="bibr" target="#b5">[6]</ref>, provides us with the predicted part segmentation or DensePose. Although we present experiments with a variety of inputs, our goal is not to investigate the effect of the input representation, but rather we focus our attention on the graph-based processing that follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph CNN</head><p>At the heart of our approach, we propose to employ a Graph CNN to regress the 3D coordinates of the mesh vertices. For our network architecture we draw inspiration from the work of Litany et al. <ref type="bibr" target="#b19">[20]</ref>. We start from a template human mesh with N vertices as depicted in <ref type="figure">Figure 2</ref>. Given the 2048-D feature vector extracted by the generic image-based network, we attach these features to the 3D coordinates of each vertex in the template mesh. From a high-level perspective, the Graph CNN uses as input the 3D coordinates of each vertex along with the input features and has the goal of estimating the 3D coordinates for each vertex in the output, deformed mesh. This processing is performed by a series of Graph Convolution layers.</p><p>For the graph convolutions we use the formulation from Kipf et al. <ref type="bibr" target="#b16">[17]</ref> which is defined as:</p><formula xml:id="formula_0">Y =ÃXW<label>(1)</label></formula><p>where X ∈ R N ×k is the input feature vector, W ∈ R k× the weight matrix andÃ ∈ R N ×N is the row-normalized adjacency matrix of the graph. Essentially, this is equivalent to performing per-vertex fully connected operations followed by a neighborhood averaging operation. The neighborhood averaging is essential for producing a high quality shape because it enforces neighboring vertices to have similar features, and thus the output shape is smooth. With this design choice we observed that there is no need of a smoothness loss on the shape, as for example in <ref type="bibr" target="#b15">[16]</ref>. We also experimented with the more powerful graph convolutions proposed in <ref type="bibr" target="#b44">[45]</ref> but we did not observe quantitative improvement in the results, so we decided to keep our original and simpler design choice. For the graph convolution layers, we make use of residual connections as they help in speeding up significantly the training and also lead in higher quality output shapes. Our basic building block is similar to the Bottleneck residual block <ref type="bibr" target="#b6">[7]</ref> where 1 × 1 convolutions are replaced by pervertex fully connected layers and Batch Normalization <ref type="bibr" target="#b8">[9]</ref> is replaced by Group Normalization <ref type="bibr" target="#b47">[48]</ref>. We noticed that Batch Normalization leads to unstable training and poor test performance, whereas with no normalization the training is very slow and the network can get stuck at local minima and collapse early during training.</p><p>Besides the 3D coordinates for each vertex, our Graph CNN also regresses the camera parameters for a weakperspective camera model. Following Kanazawa et al. <ref type="bibr" target="#b14">[15]</ref>, we predict a scaling factor s and a 2D translation vector t.</p><p>Since the prediction of the network is already on the camera frame, we do not need to regress an additional global camera rotation. The camera parameters are regressed from the graph embedding and not from the image features directly. This way we get a much more reliable estimate that is consistent with the output shape.</p><p>Regarding training, letŶ ∈ R N ×3 be the predicted 3D shape, Y the ground truth shape and X the ground truth 2D keypoint locations of the joints. From our 3D shape we can also regress the location for the predicted 3D jointŝ J 3D employing the same regressor that the SMPL model is using to recover joints from vertices. Given these 3D joints, we can simply project them on the image plane,X = sΠ(Ĵ 3D ) + t. Now, we train the network using two forms of supervision. First, we apply a per-vertex L 1 loss between the predicted and ground truth shape, i.e.,</p><formula xml:id="formula_1">L shape = N i=1 ||Ŷ i − Y i || 1 .<label>(2)</label></formula><p>Empirically we found that using L 1 loss leads to more stable training and better performance than L 2 loss. Additionally, to enforce image-model alignment, we also apply an L 1 loss between the projected joint locations and the ground truth keypoints, i.e.,</p><formula xml:id="formula_2">L J = M i=1 ||X i − X i || 1 .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regressed shape</head><p>Parametric shape MLP θ β SMPL <ref type="figure">Figure 3</ref>: Predicting SMPL parameters from regressed shape. Given a regressed 3D shape from the network of <ref type="figure">Figure 2</ref>, we can use a Multi-Layer Perceptron (MLP) to regress the SMPL parameters and produce a shape that is consistent with the original non-parametric shape Finally, our complete training objective is:</p><formula xml:id="formula_3">L = L shape + L J .<label>(4)</label></formula><p>This form of supervised training requires us to have access to images with full 3D ground truth shape. However, based on our empirical observation, it is not necessary for all the training examples to come with ground truth shape. In fact, following the observation of Omran et al. <ref type="bibr" target="#b27">[28]</ref>, we can leverage additional images that provide only 2D keypoint ground truth. In these cases, we simply ignore the first term of the previous equation and train only with the keypoint loss. We have included evaluation under this setting of weaker supervision in the Sup. Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SMPL from regressed shape</head><p>Although we demonstrate that non-parametric regression is an easier task for the network, there are still many applications where a parametric representation of the human body can be very useful (e.g., motion prediction). In this Subsection, we present a straightforward way to combine our non-parametric prediction with a particular parametric model, i.e., SMPL. To achieve this goal, we train another network that regresses pose (θ) and shape (β) parameters of the SMPL parametric model given the regressed 3D shape as input. The architecture of this network can be very simple, i.e., a Multi-Layer Perceptron (MLP) <ref type="bibr" target="#b36">[37]</ref> for our implementation. This network is presented in <ref type="figure">Figure 3</ref> and the loss function for training is:</p><formula xml:id="formula_4">L = L shape + L J + L θ + λL β .<label>(5)</label></formula><p>Here, L shape and L J are the losses on the 3D shape and 2D joint reprojection as before, while L θ and L β are L 2 losses on the SMPL pose and shape parameters respectively. As observed by previous works, e.g., <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24]</ref>, it is challenging to regress the pose parameters θ, which represent 3D rotations in the axis-angle representation. To avoid this, we followed the strategy employed by Omran et al. <ref type="bibr" target="#b27">[28]</ref>. More specifically, we convert the parameters from axisangle representation to a rotation matrix representation using the Rodrigues formula, and we set the output of our network to regress the elements of the rotation matrices. To ensure that the output is a valid rotation matrix we project it to the manifold of rotation matrices using the differentiable SVD operation. Although this representation does not explicitly improve our quantitative results, we observed faster convergence during training, so we selected it as a more practical option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation details</head><p>An important detail regarding our Graph CNN is that we do not operate directly on the original SMPL mesh, but we first subsample it by a factor of 4 and then upsample it again to the original scale using the technique described in <ref type="bibr" target="#b32">[33]</ref>. This is essentially performed by precomputing downsampling and upsampling matrices D and U and left-multiply them with the graph every time we need to do resampling. This downsampling step helps to avoid the high redundancy in the original mesh due to the spatial locality of the vertices, and decrease memory requirements during training.</p><p>Regarding the training of the MLP, we employ a 2-step training procedure. First we train the network that regresses the non-parametric shape and then with this network fixed we train the MLP that predicts the SMPL parameters. We also experimented with training them end-to-end but we observed a decrease in the performance of the network for both the parametric and non-parametric shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical evaluation</head><p>In this Section, we present the empirical evaluation of our approach. First, we discuss the datasets we use in our evaluation (Subsection 4.1), then we provide training details for our pipeline (Subsection 4.2), and finally, the quantitative and qualitative evaluation (Subsection 4.3) follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We employ two datasets that provide 3D ground truth for training, Human3.6M <ref type="bibr" target="#b9">[10]</ref> and UP-3D <ref type="bibr" target="#b17">[18]</ref>, while we evaluate our approach on Human3.6M and the LSP dataset <ref type="bibr" target="#b12">[13]</ref>. Human3.6M: It is an indoor 3D pose dataset including subjects performing activities like Walking, Eating and Smoking. We use the subjects S1, S5, S6, S7 and S8 for training, and keep the subjects S9 and S11 for testing. We present results for two popular protocols (P1 and P2, as defined in <ref type="bibr" target="#b14">[15]</ref>) and two error metrics (MPJPE and Reconstruction error, as defined in <ref type="bibr" target="#b50">[51]</ref>). UP-3D: It is a dataset created by applying SMPLify <ref type="bibr" target="#b0">[1]</ref> on natural images of humans and selecting the successful fits. We use the training set of this dataset for training.  <ref type="table">Table 1</ref>: Evaluation of 3D pose estimation in Human3.6M (Protocol 2). The numbers are MPJPE and Reconstruction errors in mm. Our graph-based mesh regression (with or without SMPL parameter regression) is compared with a method that regresses SMPL parameters directly, as well as with a naive mesh regression using fully connected (FC) layers instead of a Graph-CNN.</p><p>LSP: It is a 2D pose dataset, including also segmentation annotations provided by Lassner et al. <ref type="bibr" target="#b17">[18]</ref>. We use the test set of this dataset for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details</head><p>For the image-based encoder, we use a ResNet50 model <ref type="bibr" target="#b6">[7]</ref> pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref>. All other network components (Graph CNN and MLP for SMPL parameters) are trained from scratch. For our training, we use the Adam optimizer, and a batch size of 16, with the learning rate set to 3e -4. We did not use learning rate decay. Training with data only from Human3.6M lasts for 10 epochs, while mixed training with data from Human3.6M and UP-3D requires training for 25 epochs, because of the greater image diversity. To train the MLP that regresses SMPL parameters from our predicted shape, we use 3D shapes from Human3.6M and UP-3D. Finally, for the models using Part Segmentation or DensePose <ref type="bibr" target="#b5">[6]</ref> predictions as input, we use the pretrained network of <ref type="bibr" target="#b5">[6]</ref> to provide the corresponding predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental analysis</head><p>Regression target: For the initial ablative study, we aim to investigate the importance of our mesh regression for 3D human shape estimation. To this end, we focus on the Human3.6M dataset and we evaluate the regressed shape through 3D pose accuracy. First, we evaluate the direct regression of the 3D vertex coordinates, in comparison to generating the 3D shape implicitly through regression of the SMPL model parameters directly from images. The most relevant baseline in this category is the HMR method of <ref type="bibr" target="#b14">[15]</ref>. In <ref type="table">Table 1</ref>, we present the comparison of this approach (SMPL parameter regression) with our nonparametric shape regression (Mesh Regression -(Graph)). For a more fair comparison, we also include our results for the MLP that regresses SMPL parameters using our non-parametric mesh as input (Mesh Regression -(Graph + SMPL)). In both cases, we outperform the strong baseline of <ref type="bibr" target="#b14">[15]</ref>, which demonstrates the benefit of estimating Image FC Graph CNN <ref type="figure">Figure 4</ref>: Using a series of fully connected (FC) layers to regress the vertex 3D coordinates severely complicates the regression task and gives non-smooth meshes, since the network cannot leverage directly the topology of the graph.  a more flexible non-parametric regression target, instead of regressing the model parameters in one shot. Beyond the regression target, one of our contributions is also the insight that the task of regressing 3D vertex coordinates can be greatly simplified when a Graph CNN is used for the prediction. To investigate this design choice, we compare it with a naive alternative that regresses vertex coordinates with a series of fully connected layers on top of our image-based encoder (Mesh Regression -(FC)). This design clearly underperforms compared to our Graphbased architecture, demonstrating the importance of leveraging the mesh structure through the Graph CNN during the regression. The benefit of graph-based processing is demonstrated also qualitatively in <ref type="figure">Figure 4</ref>.</p><p>Input representation: For the next ablative, we demonstrate the effectiveness of our mesh regression for different   <ref type="bibr" target="#b30">[31]</ref> 75.9 NBF <ref type="bibr" target="#b27">[28]</ref> 59.9 HMR <ref type="bibr" target="#b14">[15]</ref> 56.8 Ours 50.1   SMPL from regressed shape: Additionally we examine the effect of estimating the SMPL model parameters from our predicted 3D shape. As it can be seen in <ref type="table" target="#tab_5">Table 3</ref>, adding the SMPL prediction, using a simple MLP on top of our non-parametric shape estimate, only has a small effect in the performance (positive in some cases, negative in others). This means that our regressed 3D shape encapsulates all the important information needed for the model reconstruction, making it very simple to recover a parametric representation (if needed), from our non-parametric shape prediction.</p><p>Comparison with the state-of-the-art: Next, we present comparison of our approach with other state-ofthe-art methods for 3D human pose and shape estimation. For Human3.6M, detailed results are presented in <ref type="table" target="#tab_6">Table 4</ref>, where we outperform the other baselines. We clarify here that different methods use different training data (e.g., Pavlakos et al. <ref type="bibr" target="#b30">[31]</ref> do not use any Human3.6M data for training, NBF et al. <ref type="bibr" target="#b27">[28]</ref> uses only data from Human3.6M, while HMR <ref type="bibr" target="#b14">[15]</ref> makes use of additional images with 2D ground truth only). However, here we collected the best results reported by each approach on this dataset.</p><p>Besides 3D pose, we also evaluate 3D shape through silhouette reprojection on the LSP test set. Our approach outperforms the regression-based approach of Kanazawa et al. <ref type="bibr" target="#b14">[15]</ref>, and is competitive to optimization-based baselines, e.g., <ref type="bibr" target="#b0">[1]</ref>, which tend to perform better than regression approaches (like ours) in this task, because they explicitly optimize for the image-model alignment.</p><p>Qualitative evaluation: <ref type="figure" target="#fig_0">Figures 5 and 6</ref> present qual-Image Non-parametric Parametric Image Non-parametric Parametric <ref type="figure">Figure 6</ref>: Successful reconstructions of our approach. Rows 1-3: LSP <ref type="bibr" target="#b12">[13]</ref>. Rows 4-5: Human3.6M <ref type="bibr" target="#b9">[10]</ref>. With light pink color we indicate the regressed non parametric shape and with light blue the SMPL model regressed from the previous shape.</p><p>itative examples of our approach, including both the nonparametric mesh and the corresponding SMPL mesh regressed using our shape as input. Typical failures can be attributed to challenging poses, severe self-occlusions, as well as interactions among multiple people. Runtime: On a 2080 Ti GPU, network inference for a single image lasts 33ms, which is effectively real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>The goal of this paper was to address the problem of pose and shape estimation by attempting to relax the heavy reliance of previous works on a parametric model, typically SMPL <ref type="bibr" target="#b20">[21]</ref>. While we retain the SMPL mesh topology, instead of directly predicting the model parameters for a given image, our target is to first estimate the locations of the 3D mesh vertices. For this to be achieved effectively, we pro-pose a Graph-CNN architecture, which explicitly encodes the mesh structure and processes image features attached to its vertices. Our convolutional mesh regression outperforms the relevant baselines that regress model parameters directly for a variety of input representations, while ultimately, it achieves state-of-the-art results among model-based pose estimation approaches. Future work can focus on current limitations (e.g., low resolution of output mesh, missing details in the recovered shape), as well as opportunities that this non-parametric representation provides (e.g., capture aspects missing in many human body models, like hand articulation, facial expressions, clothing and hair).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Examples of erroneous reconstructions. Typical failures can be attributed to challenging poses, severe selfocclusions, or interactions among multiple people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Comparison of direct SMPL parameter regression versus our proposed mesh regression on Human3.6M (Pro-tocol 1 and 2) for different input representations. The num-bers are mean 3D joint errors in mm, with and without Pro-crustes alignment (Rec. Error and MPJPE respectively). Our results are computed after regressing SMPL parameters from our non-parametric shape. Number are taken from the respective works, except for the baseline of [15] on Dense-Pose images, which is evaluated by us.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison on Human3.6M (Protocol 1 and 2) of our non-parametric mesh with the SMPL parametric mesh regressed from our shape. Numbers are 3D joint errors in mm. The performance of the two baselines is similar.</figDesc><table><row><cell>Method</cell><cell>Reconst. Error</cell></row><row><cell>Lassner et al. [18] SMPLify [1] Pavlakos et al.</cell><cell>93.9 82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-art on Hu-man3.6M (Protocol 2). Numbers are Reconstruction errors in mm. Our approach outperforms the previous baselines.</figDesc><table><row><cell></cell><cell cols="2">FB Seg.</cell><cell cols="2">Part Seg.</cell></row><row><cell></cell><cell>acc.</cell><cell>f1</cell><cell>acc.</cell><cell>f1</cell></row><row><cell>SMPLify oracle [1]</cell><cell>92.17</cell><cell>0.88</cell><cell>88.82</cell><cell>0.67</cell></row><row><cell>SMPLify [1]</cell><cell>91.89</cell><cell>0.88</cell><cell>87.71</cell><cell>0.64</cell></row><row><cell>SMPLify on [31]</cell><cell>92.17</cell><cell>0.88</cell><cell>88.24</cell><cell>0.64</cell></row><row><cell>Bodynet [44]</cell><cell>92.75</cell><cell>0.84</cell><cell>-</cell><cell>-</cell></row><row><cell>HMR [15]</cell><cell>91.67</cell><cell>0.87</cell><cell>87.12</cell><cell>0.60</cell></row><row><cell>Ours</cell><cell>91.46</cell><cell>0.87</cell><cell>88.69</cell><cell>0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Segmentation evaluation on the LSP test set. The numbers are accuracies and f1 scores. We include approaches that are purely regression-based (bottom) and approaches that perform some optimization (post)-processing (top). Our approach is competitive with the state-of-the-art. types of input representations, i.e., RGB images, Part Segmentation as well as DensePose images<ref type="bibr" target="#b5">[6]</ref>. The complete results are presented inTable 2. The RGB model is trained on Human3.6M + UP-3D whereas the two other models only on Human3.6M. For every input type, we compare with state-of-the-art methods<ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref> and show that our method outperforms them in all setting and metrics. Interestingly, when training only with Human3.6M data, RGB input performs worse than the other representations (Table 1), because of over-fitting. However, we observed that RGB features capture richer information for in-the-wild images, thus we select it for the majority of our experiments.</figDesc><table><row><cell>Image</cell><cell>Non-parametric</cell><cell>Parametric</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We gratefully appreciate support through the following grants: NSF-IIP-1439681 (I/UCRC), NSF-IIS-1703319, NSF MRI 1626008, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, ARL DCIST CRA W911NF-17-2-0181, the DARPA-SRC C-BRIC, and by Honda Research Institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 3D human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Rıza Alp Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Classner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large pose 3D face reconstruction from a single image via direct volumetric CNN regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Aaron S Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chris Manafas, and Georgios Tzimiropoulos. 3D human body reconstruction from a single image via volumetric regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCVW</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Total capture: A 3D deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural 3D mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deformable shape completion with graph convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A mixed classification-regression framework for 3D pose estimation from 2D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haider</forename><surname>Siddharth Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3D human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeepCut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating 3D faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3D pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structured prediction of 3D human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to fuse 2D and 3D image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Marquez</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">FeaStNet: Feature-steered graph convolutions for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: Generating 3D mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a CNN coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
